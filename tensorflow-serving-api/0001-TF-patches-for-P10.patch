From e800aadd492cac275ce8ab1e6d2ba20956dd5fda Mon Sep 17 00:00:00 2001
From: Nishidha Panpaliya <npanpa23@in.ibm.com>
Date: Mon, 27 Jun 2022 07:09:39 +0000
Subject: [PATCH] TF P10 patches

---
 WORKSPACE                                     |     1 +
 third_party/tensorflow/BUILD                  |     0
 .../tensorflow/tensorflow_patches.patch       | 53472 ++++++++++++++++
 3 files changed, 53473 insertions(+)
 create mode 100644 third_party/tensorflow/BUILD
 create mode 100644 third_party/tensorflow/tensorflow_patches.patch

diff --git a/WORKSPACE b/WORKSPACE
index 95abe3c3..98bc1640 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,6 +17,7 @@ tensorflow_http_archive(
     name = "org_tensorflow",
     sha256 = "8abbdb42bdf93fb9d957005ecdf4f5b7f6a7efa2fe251096824956cda4568000",
     git_commit = "2ea19cbb575d076b4f521d3603211c8316ad5f8f",
+    patch = "//third_party/tensorflow:tensorflow_patches.patch",
 )
 
 # Import all of TensorFlow Serving's external dependencies.
diff --git a/third_party/tensorflow/BUILD b/third_party/tensorflow/BUILD
new file mode 100644
index 00000000..e69de29b
diff --git a/third_party/tensorflow/tensorflow_patches.patch b/third_party/tensorflow/tensorflow_patches.patch
new file mode 100644
index 00000000..da45415e
--- /dev/null
+++ b/third_party/tensorflow/tensorflow_patches.patch
@@ -0,0 +1,53472 @@
+From 52a43f74af6cd350537ebd88d1dcdb6dbb856cae Mon Sep 17 00:00:00 2001
+From: Nishidha Panpaliya <npanpa23@in.ibm.com>
+Date: Mon, 27 Jun 2022 07:08:27 +0000
+Subject: [PATCH] TF patched for P10
+
+---
+ .../core/profiler/utils/xplane_utils.cc       |     3 +-
+ tensorflow/python/lib/core/ndarray_tensor.cc  |     2 +-
+ tensorflow/tools/pip_package/setup.py         |     3 +-
+ tensorflow/workspace2.bzl                     |    18 +-
+ .../0001-Added-bazel-build-files.patch        | 53139 ++++++++++++++++
+ third_party/cpuinfo/cpuinfo.BUILD             |     1 +
+ third_party/grpc/generate_cc_env_fix.patch    |    60 +-
+ third_party/grpc/upb_gcc10.patch              |    54 +
+ third_party/llvm/fix_ppc64le.patch            |    24 +
+ third_party/systemlibs/sqlite.BUILD           |    27 +-
+ 10 files changed, 53321 insertions(+), 10 deletions(-)
+ create mode 100644 third_party/boringssl/0001-Added-bazel-build-files.patch
+ create mode 100644 third_party/grpc/upb_gcc10.patch
+ create mode 100644 third_party/llvm/fix_ppc64le.patch
+
+diff --git a/tensorflow/core/profiler/utils/xplane_utils.cc b/tensorflow/core/profiler/utils/xplane_utils.cc
+index 5efcdcda4b0..f012d7f99e2 100644
+--- a/tensorflow/core/profiler/utils/xplane_utils.cc
++++ b/tensorflow/core/profiler/utils/xplane_utils.cc
+@@ -101,7 +101,8 @@ const XPlane* FindPlaneWithName(const XSpace& space, absl::string_view name) {
+ 
+ std::vector<const XPlane*> FindPlanesWithNames(
+     const XSpace& space, const std::vector<absl::string_view>& names) {
+-  absl::flat_hash_set<absl::string_view> names_set(names.begin(), names.end());
++  absl::flat_hash_set<absl::string_view> names_set;
++  names_set.insert(names.begin(), names.end());
+   std::vector<int> indices =
+       FindAll(space.planes(), [&names_set](const XPlane* plane) {
+         return names_set.contains(plane->name());
+diff --git a/tensorflow/python/lib/core/ndarray_tensor.cc b/tensorflow/python/lib/core/ndarray_tensor.cc
+index 6db74a943f7..c434ccf2970 100644
+--- a/tensorflow/python/lib/core/ndarray_tensor.cc
++++ b/tensorflow/python/lib/core/ndarray_tensor.cc
+@@ -16,7 +16,7 @@ limitations under the License.
+ #include "tensorflow/python/lib/core/ndarray_tensor.h"
+ 
+ #include <cstring>
+-#include <optional>
++//#include <optional>
+ 
+ #include "tensorflow/c/eager/tfe_context_internal.h"
+ #include "tensorflow/c/tf_tensor_internal.h"
+diff --git a/tensorflow/tools/pip_package/setup.py b/tensorflow/tools/pip_package/setup.py
+index 9f25a343b83..87f974f2a85 100644
+--- a/tensorflow/tools/pip_package/setup.py
++++ b/tensorflow/tools/pip_package/setup.py
+@@ -78,7 +78,6 @@ REQUIRED_PACKAGES = [
+     'google_pasta >= 0.1.1',
+     'h5py >= 2.9.0',
+     'keras_preprocessing >= 1.1.1', # 1.1.0 needs tensorflow==1.7
+-    'libclang >= 9.0.1',
+     'numpy >= 1.20',
+     'opt_einsum >= 2.3.2',
+     # TODO(b/182876485): Protobuf 3.20 results in linker errors on Windows
+@@ -100,7 +99,7 @@ REQUIRED_PACKAGES = [
+     # When updating these, please also update the nightly versions below
+     'tensorboard >= 2.8, < 2.9',
+     'tensorflow-estimator >= 2.8, < 2.9',
+-    'keras >= 2.8.0rc0, < 2.9',
++    'keras >= 2.8.0, < 2.9',
+     'tensorflow-io-gcs-filesystem >= 0.23.1',
+ ]
+ 
+diff --git a/tensorflow/workspace2.bzl b/tensorflow/workspace2.bzl
+index a8566a3f702..cdcf0b52650 100644
+--- a/tensorflow/workspace2.bzl
++++ b/tensorflow/workspace2.bzl
+@@ -496,6 +496,17 @@ def _tf_repositories():
+         urls = tf_mirror_urls("https://curl.haxx.se/download/curl-7.83.1.tar.gz"),
+     )
+ 
++    tf_http_archive(
++        name = "upb",
++        sha256 = "61d0417abd60e65ed589c9deee7c124fe76a4106831f6ad39464e1525cef1454",
++        strip_prefix = "upb-9effcbcb27f0a665f9f345030188c0b291e32482",
++        urls = [
++            "https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz",
++            "https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz",
++        ],
++        patch_file = ["//third_party/grpc:upb_gcc10.patch"],
++    )
++
+     # WARNING: make sure ncteisen@ and vpai@ are cc-ed on any CL to change the below rule
+     tf_http_archive(
+         name = "com_github_grpc_grpc",
+@@ -553,10 +564,11 @@ def _tf_repositories():
+ 
+     tf_http_archive(
+         name = "boringssl",
+-        sha256 = "a9c3b03657d507975a32732f04563132b4553c20747cec6dc04de475c8bdf29f",
+-        strip_prefix = "boringssl-80ca9f9f6ece29ab132cce4cf807a9465a18cfac",
++        sha256 = "f691191468f6a7585e5d9464d1e9760af536806166565c6822e1c458efc02b8f",
++        strip_prefix = "boringssl-597ffef971dd980b7de5e97a0c9b7ca26eec94bc",
+         system_build_file = "//third_party/systemlibs:boringssl.BUILD",
+-        urls = tf_mirror_urls("https://github.com/google/boringssl/archive/80ca9f9f6ece29ab132cce4cf807a9465a18cfac.tar.gz"),
++        urls = tf_mirror_urls("https://github.com/google/boringssl/archive/597ffef971dd980b7de5e97a0c9b7ca26eec94bc.tar.gz"),
++        patch_file = ["//third_party/boringssl:0001-Added-bazel-build-files.patch"],
+     )
+ 
+     # Note: if you update this, you have to update libpng too. See cl/437813808
+diff --git a/third_party/boringssl/0001-Added-bazel-build-files.patch b/third_party/boringssl/0001-Added-bazel-build-files.patch
+new file mode 100644
+index 00000000000..75642eda027
+--- /dev/null
++++ b/third_party/boringssl/0001-Added-bazel-build-files.patch
+@@ -0,0 +1,53139 @@
++From 4844880d1f4d3a0337698e143ebf1157cfae9c64 Mon Sep 17 00:00:00 2001
++From: Nishidha Panpaliya <npanpa23@in.ibm.com>
++Date: Thu, 23 Jun 2022 00:27:59 -0400
++Subject: [PATCH] Added Bazel Build files
++
++---
++ BUILD                                         |  139 +
++ BUILD.generated.bzl                           |  567 ++
++ err_data.c                                    | 1475 +++
++ linux-ppc64le/ypto/fipsmodule/aesp8-ppc.S     | 3670 +++++++
++ linux-ppc64le/ypto/fipsmodule/ghashp8-ppc.S   |  587 ++
++ linux-ppc64le/ypto/test/trampoline-ppc.S      | 1410 +++
++ linux-x86_64/ypto/chacha/chacha-x86_64.S      | 1633 +++
++ .../ypto/cipher_extra/aes128gcmsiv-x86_64.S   | 3079 ++++++
++ .../cipher_extra/chacha20_poly1305_x86_64.S   | 8922 +++++++++++++++++
++ .../ypto/fipsmodule/aesni-gcm-x86_64.S        |  852 ++
++ linux-x86_64/ypto/fipsmodule/aesni-x86_64.S   | 2506 +++++
++ .../ypto/fipsmodule/ghash-ssse3-x86_64.S      |  427 +
++ linux-x86_64/ypto/fipsmodule/ghash-x86_64.S   | 1127 +++
++ linux-x86_64/ypto/fipsmodule/md5-x86_64.S     |  702 ++
++ .../ypto/fipsmodule/p256-x86_64-asm.S         | 4543 +++++++++
++ .../ypto/fipsmodule/p256_beeu-x86_64-asm.S    |  343 +
++ linux-x86_64/ypto/fipsmodule/rdrand-x86_64.S  |   63 +
++ linux-x86_64/ypto/fipsmodule/rsaz-avx2.S      | 1749 ++++
++ linux-x86_64/ypto/fipsmodule/sha1-x86_64.S    | 5468 ++++++++++
++ linux-x86_64/ypto/fipsmodule/sha256-x86_64.S  | 3973 ++++++++
++ linux-x86_64/ypto/fipsmodule/sha512-x86_64.S  | 2992 ++++++
++ linux-x86_64/ypto/fipsmodule/vpaes-x86_64.S   | 1133 +++
++ linux-x86_64/ypto/fipsmodule/x86_64-mont.S    | 1260 +++
++ linux-x86_64/ypto/fipsmodule/x86_64-mont5.S   | 3790 +++++++
++ linux-x86_64/ypto/test/trampoline-x86_64.S    |  518 +
++ 25 files changed, 52928 insertions(+)
++ create mode 100644 BUILD
++ create mode 100644 BUILD.generated.bzl
++ create mode 100644 err_data.c
++ create mode 100644 linux-ppc64le/ypto/fipsmodule/aesp8-ppc.S
++ create mode 100644 linux-ppc64le/ypto/fipsmodule/ghashp8-ppc.S
++ create mode 100644 linux-ppc64le/ypto/test/trampoline-ppc.S
++ create mode 100644 linux-x86_64/ypto/chacha/chacha-x86_64.S
++ create mode 100644 linux-x86_64/ypto/cipher_extra/aes128gcmsiv-x86_64.S
++ create mode 100644 linux-x86_64/ypto/cipher_extra/chacha20_poly1305_x86_64.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/aesni-gcm-x86_64.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/aesni-x86_64.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/ghash-ssse3-x86_64.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/ghash-x86_64.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/md5-x86_64.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/p256-x86_64-asm.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/p256_beeu-x86_64-asm.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/rdrand-x86_64.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/rsaz-avx2.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/sha1-x86_64.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/sha256-x86_64.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/sha512-x86_64.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/vpaes-x86_64.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/x86_64-mont.S
++ create mode 100644 linux-x86_64/ypto/fipsmodule/x86_64-mont5.S
++ create mode 100644 linux-x86_64/ypto/test/trampoline-x86_64.S
++
++diff --git a/BUILD b/BUILD
++new file mode 100644
++index 000000000..536569e55
++--- /dev/null
+++++ b/BUILD
++@@ -0,0 +1,139 @@
+++# Copyright (c) 2016, Google Inc.
+++#
+++# Permission to use, copy, modify, and/or distribute this software for any
+++# purpose with or without fee is hereby granted, provided that the above
+++# copyright notice and this permission notice appear in all copies.
+++#
+++# THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
+++# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
+++# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
+++# SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
+++# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION
+++# OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
+++# CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. */
+++
+++licenses(["notice"])
+++
+++exports_files(["LICENSE"])
+++
+++load(
+++    ":BUILD.generated.bzl",
+++    "crypto_headers",
+++    "crypto_internal_headers",
+++    "crypto_sources",
+++    "crypto_sources_linux_x86_64",
+++    "crypto_sources_linux_ppc64le",
+++    "fips_fragments",
+++    "ssl_headers",
+++    "ssl_internal_headers",
+++    "ssl_sources",
+++    "tool_sources",
+++    "tool_headers",
+++)
+++
+++config_setting(
+++    name = "linux_x86_64",
+++    values = {"cpu": "k8"},
+++)
+++
+++config_setting(
+++    name = "linux_ppc64le",
+++    values = {"cpu": "ppc"},
+++)
+++
+++
+++posix_copts = [
+++    # Assembler option --noexecstack adds .note.GNU-stack to each object to
+++    # ensure that binaries can be built with non-executable stack.
+++    "-Wa,--noexecstack",
+++
+++    # This is needed on Linux systems (at least) to get rwlock in pthread.
+++    "-D_XOPEN_SOURCE=700",
+++
+++    # This list of warnings should match those in the top-level CMakeLists.txt.
+++    "-Wall",
+++    "-Werror",
+++    "-Wformat=2",
+++    "-Wsign-compare",
+++    "-Wmissing-field-initializers",
+++    "-Wwrite-strings",
+++    "-Wshadow",
+++    "-fno-common",
+++
+++    # Modern build environments should be able to set this to use atomic
+++    # operations for reference counting rather than locks. However, it's
+++    # known not to work on some Android builds.
+++    # "-DOPENSSL_C11_ATOMIC",
+++]
+++
+++boringssl_copts = select({
+++    ":linux_x86_64": posix_copts,
+++    ":linux_ppc64le": posix_copts,
+++    "//conditions:default": ["-DOPENSSL_NO_ASM"],
+++})
+++
+++crypto_sources_asm = select({
+++    ":linux_x86_64": crypto_sources_linux_x86_64,
+++    ":linux_ppc64le": crypto_sources_linux_ppc64le,
+++    "//conditions:default": [],
+++})
+++
+++# For C targets only (not C++), compile with C11 support.
+++posix_copts_c11 = [
+++    "-std=c11",
+++    "-Wmissing-prototypes",
+++    "-Wold-style-definition",
+++    "-Wstrict-prototypes",
+++]
+++
+++boringssl_copts_c11 = boringssl_copts + select({
+++    ":linux_x86_64": posix_copts_c11,
+++    ":linux_ppc64le": posix_copts_c11,
+++    "//conditions:default": [],
+++})
+++
+++# For C++ targets only (not C), compile with C++11 support.
+++posix_copts_cxx = [
+++    "-std=c++11",
+++    "-Wmissing-declarations",
+++]
+++
+++boringssl_copts_cxx = boringssl_copts + select({
+++    ":linux_x86_64": posix_copts_cxx,
+++    ":linux_ppc64le": posix_copts_cxx,
+++    "//conditions:default": [],
+++})
+++
+++cc_library(
+++    name = "crypto",
+++    srcs = crypto_sources + crypto_internal_headers + crypto_sources_asm,
+++    hdrs = crypto_headers + fips_fragments,
+++    copts = boringssl_copts_c11,
+++    includes = ["include"],
+++    linkopts = select({
+++        # Android supports pthreads, but does not provide a libpthread
+++        # to link against.
+++        "//conditions:default": ["-lpthread"],
+++    }),
+++    visibility = ["//visibility:public"],
+++)
+++
+++cc_library(
+++    name = "ssl",
+++    srcs = ssl_sources + ssl_internal_headers,
+++    hdrs = ssl_headers,
+++    copts = boringssl_copts_cxx,
+++    includes = ["include"],
+++    visibility = ["//visibility:public"],
+++    deps = [
+++        ":crypto",
+++    ],
+++)
+++
+++cc_binary(
+++    name = "bssl",
+++    srcs = tool_sources + tool_headers,
+++    copts = boringssl_copts_cxx,
+++    visibility = ["//visibility:public"],
+++    deps = [":ssl"],
+++)
++diff --git a/BUILD.generated.bzl b/BUILD.generated.bzl
++new file mode 100644
++index 000000000..f146c2755
++--- /dev/null
+++++ b/BUILD.generated.bzl
++@@ -0,0 +1,567 @@
+++# This file is created by generate_build_files.py. Do not edit manually.
+++
+++ssl_headers = [
+++    "include/openssl/dtls1.h",
+++    "include/openssl/srtp.h",
+++    "include/openssl/ssl.h",
+++    "include/openssl/ssl3.h",
+++    "include/openssl/tls1.h",
+++]
+++
+++fips_fragments = [
+++    "crypto/fipsmodule/aes/aes.c",
+++    "crypto/fipsmodule/aes/aes_nohw.c",
+++    "crypto/fipsmodule/aes/key_wrap.c",
+++    "crypto/fipsmodule/aes/mode_wrappers.c",
+++    "crypto/fipsmodule/bn/add.c",
+++    "crypto/fipsmodule/bn/asm/x86_64-gcc.c",
+++    "crypto/fipsmodule/bn/bn.c",
+++    "crypto/fipsmodule/bn/bytes.c",
+++    "crypto/fipsmodule/bn/cmp.c",
+++    "crypto/fipsmodule/bn/ctx.c",
+++    "crypto/fipsmodule/bn/div.c",
+++    "crypto/fipsmodule/bn/div_extra.c",
+++    "crypto/fipsmodule/bn/exponentiation.c",
+++    "crypto/fipsmodule/bn/gcd.c",
+++    "crypto/fipsmodule/bn/gcd_extra.c",
+++    "crypto/fipsmodule/bn/generic.c",
+++    "crypto/fipsmodule/bn/jacobi.c",
+++    "crypto/fipsmodule/bn/montgomery.c",
+++    "crypto/fipsmodule/bn/montgomery_inv.c",
+++    "crypto/fipsmodule/bn/mul.c",
+++    "crypto/fipsmodule/bn/prime.c",
+++    "crypto/fipsmodule/bn/random.c",
+++    "crypto/fipsmodule/bn/rsaz_exp.c",
+++    "crypto/fipsmodule/bn/shift.c",
+++    "crypto/fipsmodule/bn/sqrt.c",
+++    "crypto/fipsmodule/cipher/aead.c",
+++    "crypto/fipsmodule/cipher/cipher.c",
+++    "crypto/fipsmodule/cipher/e_aes.c",
+++    "crypto/fipsmodule/cipher/e_des.c",
+++    "crypto/fipsmodule/des/des.c",
+++    "crypto/fipsmodule/dh/check.c",
+++    "crypto/fipsmodule/dh/dh.c",
+++    "crypto/fipsmodule/digest/digest.c",
+++    "crypto/fipsmodule/digest/digests.c",
+++    "crypto/fipsmodule/ec/ec.c",
+++    "crypto/fipsmodule/ec/ec_key.c",
+++    "crypto/fipsmodule/ec/ec_montgomery.c",
+++    "crypto/fipsmodule/ec/felem.c",
+++    "crypto/fipsmodule/ec/oct.c",
+++    "crypto/fipsmodule/ec/p224-64.c",
+++    "crypto/fipsmodule/ec/p256-x86_64.c",
+++    "crypto/fipsmodule/ec/p256.c",
+++    "crypto/fipsmodule/ec/scalar.c",
+++    "crypto/fipsmodule/ec/simple.c",
+++    "crypto/fipsmodule/ec/simple_mul.c",
+++    "crypto/fipsmodule/ec/util.c",
+++    "crypto/fipsmodule/ec/wnaf.c",
+++    "crypto/fipsmodule/ecdh/ecdh.c",
+++    "crypto/fipsmodule/ecdsa/ecdsa.c",
+++    "crypto/fipsmodule/hmac/hmac.c",
+++    "crypto/fipsmodule/md4/md4.c",
+++    "crypto/fipsmodule/md5/md5.c",
+++    "crypto/fipsmodule/modes/cbc.c",
+++    "crypto/fipsmodule/modes/cfb.c",
+++    "crypto/fipsmodule/modes/ctr.c",
+++    "crypto/fipsmodule/modes/gcm.c",
+++    "crypto/fipsmodule/modes/gcm_nohw.c",
+++    "crypto/fipsmodule/modes/ofb.c",
+++    "crypto/fipsmodule/modes/polyval.c",
+++    "crypto/fipsmodule/rand/ctrdrbg.c",
+++    "crypto/fipsmodule/rand/fork_detect.c",
+++    "crypto/fipsmodule/rand/rand.c",
+++    "crypto/fipsmodule/rand/urandom.c",
+++    "crypto/fipsmodule/rsa/blinding.c",
+++    "crypto/fipsmodule/rsa/padding.c",
+++    "crypto/fipsmodule/rsa/rsa.c",
+++    "crypto/fipsmodule/rsa/rsa_impl.c",
+++    "crypto/fipsmodule/self_check/fips.c",
+++    "crypto/fipsmodule/self_check/self_check.c",
+++    "crypto/fipsmodule/sha/sha1-altivec.c",
+++    "crypto/fipsmodule/sha/sha1.c",
+++    "crypto/fipsmodule/sha/sha256.c",
+++    "crypto/fipsmodule/sha/sha512.c",
+++    "crypto/fipsmodule/tls/kdf.c",
+++]
+++
+++ssl_internal_headers = [
+++    "ssl/internal.h",
+++]
+++
+++ssl_sources = [
+++    "ssl/bio_ssl.cc",
+++    "ssl/d1_both.cc",
+++    "ssl/d1_lib.cc",
+++    "ssl/d1_pkt.cc",
+++    "ssl/d1_srtp.cc",
+++    "ssl/dtls_method.cc",
+++    "ssl/dtls_record.cc",
+++    "ssl/encrypted_client_hello.cc",
+++    "ssl/handoff.cc",
+++    "ssl/handshake.cc",
+++    "ssl/handshake_client.cc",
+++    "ssl/handshake_server.cc",
+++    "ssl/s3_both.cc",
+++    "ssl/s3_lib.cc",
+++    "ssl/s3_pkt.cc",
+++    "ssl/ssl_aead_ctx.cc",
+++    "ssl/ssl_asn1.cc",
+++    "ssl/ssl_buffer.cc",
+++    "ssl/ssl_cert.cc",
+++    "ssl/ssl_cipher.cc",
+++    "ssl/ssl_file.cc",
+++    "ssl/ssl_key_share.cc",
+++    "ssl/ssl_lib.cc",
+++    "ssl/ssl_privkey.cc",
+++    "ssl/ssl_session.cc",
+++    "ssl/ssl_stat.cc",
+++    "ssl/ssl_transcript.cc",
+++    "ssl/ssl_versions.cc",
+++    "ssl/ssl_x509.cc",
+++    "ssl/t1_enc.cc",
+++    "ssl/t1_lib.cc",
+++    "ssl/tls13_both.cc",
+++    "ssl/tls13_client.cc",
+++    "ssl/tls13_enc.cc",
+++    "ssl/tls13_server.cc",
+++    "ssl/tls_method.cc",
+++    "ssl/tls_record.cc",
+++]
+++
+++crypto_headers = [
+++    "include/openssl/aead.h",
+++    "include/openssl/aes.h",
+++    "include/openssl/arm_arch.h",
+++    "include/openssl/asn1.h",
+++    "include/openssl/asn1_mac.h",
+++    "include/openssl/asn1t.h",
+++    "include/openssl/base.h",
+++    "include/openssl/base64.h",
+++    "include/openssl/bio.h",
+++    "include/openssl/blake2.h",
+++    "include/openssl/blowfish.h",
+++    "include/openssl/bn.h",
+++    "include/openssl/buf.h",
+++    "include/openssl/buffer.h",
+++    "include/openssl/bytestring.h",
+++    "include/openssl/cast.h",
+++    "include/openssl/chacha.h",
+++    "include/openssl/cipher.h",
+++    "include/openssl/cmac.h",
+++    "include/openssl/conf.h",
+++    "include/openssl/cpu.h",
+++    "include/openssl/crypto.h",
+++    "include/openssl/curve25519.h",
+++    "include/openssl/des.h",
+++    "include/openssl/dh.h",
+++    "include/openssl/digest.h",
+++    "include/openssl/dsa.h",
+++    "include/openssl/e_os2.h",
+++    "include/openssl/ec.h",
+++    "include/openssl/ec_key.h",
+++    "include/openssl/ecdh.h",
+++    "include/openssl/ecdsa.h",
+++    "include/openssl/engine.h",
+++    "include/openssl/err.h",
+++    "include/openssl/evp.h",
+++    "include/openssl/evp_errors.h",
+++    "include/openssl/ex_data.h",
+++    "include/openssl/hkdf.h",
+++    "include/openssl/hmac.h",
+++    "include/openssl/hpke.h",
+++    "include/openssl/hrss.h",
+++    "include/openssl/is_boringssl.h",
+++    "include/openssl/lhash.h",
+++    "include/openssl/md4.h",
+++    "include/openssl/md5.h",
+++    "include/openssl/mem.h",
+++    "include/openssl/nid.h",
+++    "include/openssl/obj.h",
+++    "include/openssl/obj_mac.h",
+++    "include/openssl/objects.h",
+++    "include/openssl/opensslconf.h",
+++    "include/openssl/opensslv.h",
+++    "include/openssl/ossl_typ.h",
+++    "include/openssl/pem.h",
+++    "include/openssl/pkcs12.h",
+++    "include/openssl/pkcs7.h",
+++    "include/openssl/pkcs8.h",
+++    "include/openssl/poly1305.h",
+++    "include/openssl/pool.h",
+++    "include/openssl/rand.h",
+++    "include/openssl/rc4.h",
+++    "include/openssl/ripemd.h",
+++    "include/openssl/rsa.h",
+++    "include/openssl/safestack.h",
+++    "include/openssl/sha.h",
+++    "include/openssl/siphash.h",
+++    "include/openssl/span.h",
+++    "include/openssl/stack.h",
+++    "include/openssl/thread.h",
+++    "include/openssl/trust_token.h",
+++    "include/openssl/type_check.h",
+++    "include/openssl/x509.h",
+++    "include/openssl/x509_vfy.h",
+++    "include/openssl/x509v3.h",
+++]
+++
+++crypto_internal_headers = [
+++    "crypto/asn1/asn1_locl.h",
+++    "crypto/bio/internal.h",
+++    "crypto/bytestring/internal.h",
+++    "crypto/chacha/internal.h",
+++    "crypto/cipher_extra/internal.h",
+++    "crypto/conf/conf_def.h",
+++    "crypto/conf/internal.h",
+++    "crypto/cpu-arm-linux.h",
+++    "crypto/curve25519/curve25519_tables.h",
+++    "crypto/curve25519/internal.h",
+++    "crypto/dsa/internal.h",
+++    "crypto/ec_extra/internal.h",
+++    "crypto/err/internal.h",
+++    "crypto/evp/internal.h",
+++    "crypto/fipsmodule/aes/internal.h",
+++    "crypto/fipsmodule/bn/internal.h",
+++    "crypto/fipsmodule/bn/rsaz_exp.h",
+++    "crypto/fipsmodule/cipher/internal.h",
+++    "crypto/fipsmodule/delocate.h",
+++    "crypto/fipsmodule/des/internal.h",
+++    "crypto/fipsmodule/digest/internal.h",
+++    "crypto/fipsmodule/digest/md32_common.h",
+++    "crypto/fipsmodule/ec/internal.h",
+++    "crypto/fipsmodule/ec/p256-x86_64-table.h",
+++    "crypto/fipsmodule/ec/p256-x86_64.h",
+++    "crypto/fipsmodule/ec/p256_table.h",
+++    "crypto/fipsmodule/ecdsa/internal.h",
+++    "crypto/fipsmodule/md5/internal.h",
+++    "crypto/fipsmodule/modes/internal.h",
+++    "crypto/fipsmodule/rand/fork_detect.h",
+++    "crypto/fipsmodule/rand/getrandom_fillin.h",
+++    "crypto/fipsmodule/rand/internal.h",
+++    "crypto/fipsmodule/rsa/internal.h",
+++    "crypto/fipsmodule/sha/internal.h",
+++    "crypto/fipsmodule/tls/internal.h",
+++    "crypto/hrss/internal.h",
+++    "crypto/internal.h",
+++    "crypto/obj/obj_dat.h",
+++    "crypto/pkcs7/internal.h",
+++    "crypto/pkcs8/internal.h",
+++    "crypto/poly1305/internal.h",
+++    "crypto/pool/internal.h",
+++    "crypto/trust_token/internal.h",
+++    "crypto/x509/charmap.h",
+++    "crypto/x509/internal.h",
+++    "crypto/x509/vpm_int.h",
+++    "crypto/x509v3/ext_dat.h",
+++    "crypto/x509v3/internal.h",
+++    "crypto/x509v3/pcy_int.h",
+++    "third_party/fiat/curve25519_32.h",
+++    "third_party/fiat/curve25519_64.h",
+++    "third_party/fiat/p256_32.h",
+++    "third_party/fiat/p256_64.h",
+++]
+++
+++crypto_sources = [
+++    "crypto/asn1/a_bitstr.c",
+++    "crypto/asn1/a_bool.c",
+++    "crypto/asn1/a_d2i_fp.c",
+++    "crypto/asn1/a_dup.c",
+++    "crypto/asn1/a_enum.c",
+++    "crypto/asn1/a_gentm.c",
+++    "crypto/asn1/a_i2d_fp.c",
+++    "crypto/asn1/a_int.c",
+++    "crypto/asn1/a_mbstr.c",
+++    "crypto/asn1/a_object.c",
+++    "crypto/asn1/a_octet.c",
+++    "crypto/asn1/a_print.c",
+++    "crypto/asn1/a_strnid.c",
+++    "crypto/asn1/a_time.c",
+++    "crypto/asn1/a_type.c",
+++    "crypto/asn1/a_utctm.c",
+++    "crypto/asn1/a_utf8.c",
+++    "crypto/asn1/asn1_lib.c",
+++    "crypto/asn1/asn1_par.c",
+++    "crypto/asn1/asn_pack.c",
+++    "crypto/asn1/f_enum.c",
+++    "crypto/asn1/f_int.c",
+++    "crypto/asn1/f_string.c",
+++    "crypto/asn1/tasn_dec.c",
+++    "crypto/asn1/tasn_enc.c",
+++    "crypto/asn1/tasn_fre.c",
+++    "crypto/asn1/tasn_new.c",
+++    "crypto/asn1/tasn_typ.c",
+++    "crypto/asn1/tasn_utl.c",
+++    "crypto/asn1/time_support.c",
+++    "crypto/base64/base64.c",
+++    "crypto/bio/bio.c",
+++    "crypto/bio/bio_mem.c",
+++    "crypto/bio/connect.c",
+++    "crypto/bio/fd.c",
+++    "crypto/bio/file.c",
+++    "crypto/bio/hexdump.c",
+++    "crypto/bio/pair.c",
+++    "crypto/bio/printf.c",
+++    "crypto/bio/socket.c",
+++    "crypto/bio/socket_helper.c",
+++    "crypto/blake2/blake2.c",
+++    "crypto/bn_extra/bn_asn1.c",
+++    "crypto/bn_extra/convert.c",
+++    "crypto/buf/buf.c",
+++    "crypto/bytestring/asn1_compat.c",
+++    "crypto/bytestring/ber.c",
+++    "crypto/bytestring/cbb.c",
+++    "crypto/bytestring/cbs.c",
+++    "crypto/bytestring/unicode.c",
+++    "crypto/chacha/chacha.c",
+++    "crypto/cipher_extra/cipher_extra.c",
+++    "crypto/cipher_extra/derive_key.c",
+++    "crypto/cipher_extra/e_aesccm.c",
+++    "crypto/cipher_extra/e_aesctrhmac.c",
+++    "crypto/cipher_extra/e_aesgcmsiv.c",
+++    "crypto/cipher_extra/e_chacha20poly1305.c",
+++    "crypto/cipher_extra/e_null.c",
+++    "crypto/cipher_extra/e_rc2.c",
+++    "crypto/cipher_extra/e_rc4.c",
+++    "crypto/cipher_extra/e_tls.c",
+++    "crypto/cipher_extra/tls_cbc.c",
+++    "crypto/cmac/cmac.c",
+++    "crypto/conf/conf.c",
+++    "crypto/cpu-aarch64-fuchsia.c",
+++    "crypto/cpu-aarch64-linux.c",
+++    "crypto/cpu-aarch64-win.c",
+++    "crypto/cpu-arm-linux.c",
+++    "crypto/cpu-arm.c",
+++    "crypto/cpu-intel.c",
+++    "crypto/cpu-ppc64le.c",
+++    "crypto/crypto.c",
+++    "crypto/curve25519/curve25519.c",
+++    "crypto/curve25519/spake25519.c",
+++    "crypto/dh_extra/dh_asn1.c",
+++    "crypto/dh_extra/params.c",
+++    "crypto/digest_extra/digest_extra.c",
+++    "crypto/dsa/dsa.c",
+++    "crypto/dsa/dsa_asn1.c",
+++    "crypto/ec_extra/ec_asn1.c",
+++    "crypto/ec_extra/ec_derive.c",
+++    "crypto/ec_extra/hash_to_curve.c",
+++    "crypto/ecdh_extra/ecdh_extra.c",
+++    "crypto/ecdsa_extra/ecdsa_asn1.c",
+++    "crypto/engine/engine.c",
+++    "crypto/err/err.c",
+++    "crypto/evp/digestsign.c",
+++    "crypto/evp/evp.c",
+++    "crypto/evp/evp_asn1.c",
+++    "crypto/evp/evp_ctx.c",
+++    "crypto/evp/p_dsa_asn1.c",
+++    "crypto/evp/p_ec.c",
+++    "crypto/evp/p_ec_asn1.c",
+++    "crypto/evp/p_ed25519.c",
+++    "crypto/evp/p_ed25519_asn1.c",
+++    "crypto/evp/p_rsa.c",
+++    "crypto/evp/p_rsa_asn1.c",
+++    "crypto/evp/p_x25519.c",
+++    "crypto/evp/p_x25519_asn1.c",
+++    "crypto/evp/pbkdf.c",
+++    "crypto/evp/print.c",
+++    "crypto/evp/scrypt.c",
+++    "crypto/evp/sign.c",
+++    "crypto/ex_data.c",
+++    "crypto/fipsmodule/bcm.c",
+++    "crypto/fipsmodule/fips_shared_support.c",
+++    "crypto/hkdf/hkdf.c",
+++    "crypto/hpke/hpke.c",
+++    "crypto/hrss/hrss.c",
+++    "crypto/lhash/lhash.c",
+++    "crypto/mem.c",
+++    "crypto/obj/obj.c",
+++    "crypto/obj/obj_xref.c",
+++    "crypto/pem/pem_all.c",
+++    "crypto/pem/pem_info.c",
+++    "crypto/pem/pem_lib.c",
+++    "crypto/pem/pem_oth.c",
+++    "crypto/pem/pem_pk8.c",
+++    "crypto/pem/pem_pkey.c",
+++    "crypto/pem/pem_x509.c",
+++    "crypto/pem/pem_xaux.c",
+++    "crypto/pkcs7/pkcs7.c",
+++    "crypto/pkcs7/pkcs7_x509.c",
+++    "crypto/pkcs8/p5_pbev2.c",
+++    "crypto/pkcs8/pkcs8.c",
+++    "crypto/pkcs8/pkcs8_x509.c",
+++    "crypto/poly1305/poly1305.c",
+++    "crypto/poly1305/poly1305_arm.c",
+++    "crypto/poly1305/poly1305_vec.c",
+++    "crypto/pool/pool.c",
+++    "crypto/rand_extra/deterministic.c",
+++    "crypto/rand_extra/forkunsafe.c",
+++    "crypto/rand_extra/fuchsia.c",
+++    "crypto/rand_extra/passive.c",
+++    "crypto/rand_extra/rand_extra.c",
+++    "crypto/rand_extra/windows.c",
+++    "crypto/rc4/rc4.c",
+++    "crypto/refcount_c11.c",
+++    "crypto/refcount_lock.c",
+++    "crypto/rsa_extra/rsa_asn1.c",
+++    "crypto/rsa_extra/rsa_print.c",
+++    "crypto/siphash/siphash.c",
+++    "crypto/stack/stack.c",
+++    "crypto/thread.c",
+++    "crypto/thread_none.c",
+++    "crypto/thread_pthread.c",
+++    "crypto/thread_win.c",
+++    "crypto/trust_token/pmbtoken.c",
+++    "crypto/trust_token/trust_token.c",
+++    "crypto/trust_token/voprf.c",
+++    "crypto/x509/a_digest.c",
+++    "crypto/x509/a_sign.c",
+++    "crypto/x509/a_strex.c",
+++    "crypto/x509/a_verify.c",
+++    "crypto/x509/algorithm.c",
+++    "crypto/x509/asn1_gen.c",
+++    "crypto/x509/by_dir.c",
+++    "crypto/x509/by_file.c",
+++    "crypto/x509/i2d_pr.c",
+++    "crypto/x509/rsa_pss.c",
+++    "crypto/x509/t_crl.c",
+++    "crypto/x509/t_req.c",
+++    "crypto/x509/t_x509.c",
+++    "crypto/x509/t_x509a.c",
+++    "crypto/x509/x509.c",
+++    "crypto/x509/x509_att.c",
+++    "crypto/x509/x509_cmp.c",
+++    "crypto/x509/x509_d2.c",
+++    "crypto/x509/x509_def.c",
+++    "crypto/x509/x509_ext.c",
+++    "crypto/x509/x509_lu.c",
+++    "crypto/x509/x509_obj.c",
+++    "crypto/x509/x509_req.c",
+++    "crypto/x509/x509_set.c",
+++    "crypto/x509/x509_trs.c",
+++    "crypto/x509/x509_txt.c",
+++    "crypto/x509/x509_v3.c",
+++    "crypto/x509/x509_vfy.c",
+++    "crypto/x509/x509_vpm.c",
+++    "crypto/x509/x509cset.c",
+++    "crypto/x509/x509name.c",
+++    "crypto/x509/x509rset.c",
+++    "crypto/x509/x509spki.c",
+++    "crypto/x509/x_algor.c",
+++    "crypto/x509/x_all.c",
+++    "crypto/x509/x_attrib.c",
+++    "crypto/x509/x_crl.c",
+++    "crypto/x509/x_exten.c",
+++    "crypto/x509/x_info.c",
+++    "crypto/x509/x_name.c",
+++    "crypto/x509/x_pkey.c",
+++    "crypto/x509/x_pubkey.c",
+++    "crypto/x509/x_req.c",
+++    "crypto/x509/x_sig.c",
+++    "crypto/x509/x_spki.c",
+++    "crypto/x509/x_val.c",
+++    "crypto/x509/x_x509.c",
+++    "crypto/x509/x_x509a.c",
+++    "crypto/x509v3/pcy_cache.c",
+++    "crypto/x509v3/pcy_data.c",
+++    "crypto/x509v3/pcy_lib.c",
+++    "crypto/x509v3/pcy_map.c",
+++    "crypto/x509v3/pcy_node.c",
+++    "crypto/x509v3/pcy_tree.c",
+++    "crypto/x509v3/v3_akey.c",
+++    "crypto/x509v3/v3_akeya.c",
+++    "crypto/x509v3/v3_alt.c",
+++    "crypto/x509v3/v3_bcons.c",
+++    "crypto/x509v3/v3_bitst.c",
+++    "crypto/x509v3/v3_conf.c",
+++    "crypto/x509v3/v3_cpols.c",
+++    "crypto/x509v3/v3_crld.c",
+++    "crypto/x509v3/v3_enum.c",
+++    "crypto/x509v3/v3_extku.c",
+++    "crypto/x509v3/v3_genn.c",
+++    "crypto/x509v3/v3_ia5.c",
+++    "crypto/x509v3/v3_info.c",
+++    "crypto/x509v3/v3_int.c",
+++    "crypto/x509v3/v3_lib.c",
+++    "crypto/x509v3/v3_ncons.c",
+++    "crypto/x509v3/v3_ocsp.c",
+++    "crypto/x509v3/v3_pci.c",
+++    "crypto/x509v3/v3_pcia.c",
+++    "crypto/x509v3/v3_pcons.c",
+++    "crypto/x509v3/v3_pmaps.c",
+++    "crypto/x509v3/v3_prn.c",
+++    "crypto/x509v3/v3_purp.c",
+++    "crypto/x509v3/v3_skey.c",
+++    "crypto/x509v3/v3_utl.c",
+++    "err_data.c",
+++]
+++
+++tool_sources = [
+++    "tool/args.cc",
+++    "tool/ciphers.cc",
+++    "tool/client.cc",
+++    "tool/const.cc",
+++    "tool/digest.cc",
+++    "tool/fd.cc",
+++    "tool/file.cc",
+++    "tool/generate_ed25519.cc",
+++    "tool/genrsa.cc",
+++    "tool/pkcs12.cc",
+++    "tool/rand.cc",
+++    "tool/server.cc",
+++    "tool/sign.cc",
+++    "tool/speed.cc",
+++    "tool/tool.cc",
+++    "tool/transport_common.cc",
+++]
+++
+++tool_headers = [
+++    "tool/internal.h",
+++    "tool/transport_common.h",
+++]
+++
+++
+++crypto_sources_linux_ppc64le = [
+++    "linux-ppc64le/ypto/fipsmodule/aesp8-ppc.S",
+++    "linux-ppc64le/ypto/fipsmodule/ghashp8-ppc.S",
+++    "linux-ppc64le/ypto/test/trampoline-ppc.S",
+++]
+++
+++crypto_sources_linux_x86 = [
+++    "linux-x86/ypto/chacha/chacha-x86.S",
+++    "linux-x86/ypto/fipsmodule/aesni-x86.S",
+++    "linux-x86/ypto/fipsmodule/bn-586.S",
+++    "linux-x86/ypto/fipsmodule/co-586.S",
+++    "linux-x86/ypto/fipsmodule/ghash-ssse3-x86.S",
+++    "linux-x86/ypto/fipsmodule/ghash-x86.S",
+++    "linux-x86/ypto/fipsmodule/md5-586.S",
+++    "linux-x86/ypto/fipsmodule/sha1-586.S",
+++    "linux-x86/ypto/fipsmodule/sha256-586.S",
+++    "linux-x86/ypto/fipsmodule/sha512-586.S",
+++    "linux-x86/ypto/fipsmodule/vpaes-x86.S",
+++    "linux-x86/ypto/fipsmodule/x86-mont.S",
+++    "linux-x86/ypto/test/trampoline-x86.S",
+++]
+++
+++crypto_sources_linux_x86_64 = [
+++    "crypto/hrss/asm/poly_rq_mul.S",
+++    "linux-x86_64/ypto/chacha/chacha-x86_64.S",
+++    "linux-x86_64/ypto/cipher_extra/aes128gcmsiv-x86_64.S",
+++    "linux-x86_64/ypto/cipher_extra/chacha20_poly1305_x86_64.S",
+++    "linux-x86_64/ypto/fipsmodule/aesni-gcm-x86_64.S",
+++    "linux-x86_64/ypto/fipsmodule/aesni-x86_64.S",
+++    "linux-x86_64/ypto/fipsmodule/ghash-ssse3-x86_64.S",
+++    "linux-x86_64/ypto/fipsmodule/ghash-x86_64.S",
+++    "linux-x86_64/ypto/fipsmodule/md5-x86_64.S",
+++    "linux-x86_64/ypto/fipsmodule/p256-x86_64-asm.S",
+++    "linux-x86_64/ypto/fipsmodule/p256_beeu-x86_64-asm.S",
+++    "linux-x86_64/ypto/fipsmodule/rdrand-x86_64.S",
+++    "linux-x86_64/ypto/fipsmodule/rsaz-avx2.S",
+++    "linux-x86_64/ypto/fipsmodule/sha1-x86_64.S",
+++    "linux-x86_64/ypto/fipsmodule/sha256-x86_64.S",
+++    "linux-x86_64/ypto/fipsmodule/sha512-x86_64.S",
+++    "linux-x86_64/ypto/fipsmodule/vpaes-x86_64.S",
+++    "linux-x86_64/ypto/fipsmodule/x86_64-mont.S",
+++    "linux-x86_64/ypto/fipsmodule/x86_64-mont5.S",
+++    "linux-x86_64/ypto/test/trampoline-x86_64.S",
+++]
+++
++diff --git a/err_data.c b/err_data.c
++new file mode 100644
++index 000000000..6a7ee066a
++--- /dev/null
+++++ b/err_data.c
++@@ -0,0 +1,1475 @@
+++/* Copyright (c) 2015, Google Inc.
+++ *
+++ * Permission to use, copy, modify, and/or distribute this software for any
+++ * purpose with or without fee is hereby granted, provided that the above
+++ * copyright notice and this permission notice appear in all copies.
+++ *
+++ * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
+++ * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
+++ * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
+++ * SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
+++ * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION
+++ * OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
+++ * CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
+++
+++ * This file was generated by err_data_generate.go. */
+++
+++#include <openssl/base.h>
+++#include <openssl/err.h>
+++#include <openssl/type_check.h>
+++
+++
+++OPENSSL_STATIC_ASSERT(ERR_LIB_NONE == 1, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_SYS == 2, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_BN == 3, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_RSA == 4, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_DH == 5, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_EVP == 6, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_BUF == 7, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_OBJ == 8, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_PEM == 9, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_DSA == 10, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_X509 == 11, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_ASN1 == 12, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_CONF == 13, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_CRYPTO == 14, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_EC == 15, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_SSL == 16, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_BIO == 17, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_PKCS7 == 18, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_PKCS8 == 19, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_X509V3 == 20, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_RAND == 21, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_ENGINE == 22, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_OCSP == 23, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_UI == 24, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_COMP == 25, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_ECDSA == 26, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_ECDH == 27, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_HMAC == 28, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_DIGEST == 29, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_CIPHER == 30, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_HKDF == 31, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_TRUST_TOKEN == 32, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_LIB_USER == 33, "library value changed");
+++OPENSSL_STATIC_ASSERT(ERR_NUM_LIBS == 34, "number of libraries changed");
+++
+++const uint32_t kOpenSSLReasonValues[] = {
+++    0xc320847,
+++    0xc328861,
+++    0xc330870,
+++    0xc338880,
+++    0xc34088f,
+++    0xc3488a8,
+++    0xc3508b4,
+++    0xc3588d1,
+++    0xc3608f1,
+++    0xc3688ff,
+++    0xc37090f,
+++    0xc37891c,
+++    0xc38092c,
+++    0xc388937,
+++    0xc39094d,
+++    0xc39895c,
+++    0xc3a0970,
+++    0xc3a8854,
+++    0xc3b00f7,
+++    0xc3b88e3,
+++    0x10320854,
+++    0x103295ca,
+++    0x103315d6,
+++    0x103395ef,
+++    0x10341602,
+++    0x10348f34,
+++    0x10350c6d,
+++    0x10359615,
+++    0x1036163f,
+++    0x10369652,
+++    0x10371671,
+++    0x1037968a,
+++    0x1038169f,
+++    0x103896bd,
+++    0x103916cc,
+++    0x103996e8,
+++    0x103a1703,
+++    0x103a9712,
+++    0x103b172e,
+++    0x103b9749,
+++    0x103c176f,
+++    0x103c80f7,
+++    0x103d1780,
+++    0x103d9794,
+++    0x103e17b3,
+++    0x103e97c2,
+++    0x103f17d9,
+++    0x103f97ec,
+++    0x10400c31,
+++    0x104097ff,
+++    0x1041181d,
+++    0x10419830,
+++    0x1042184a,
+++    0x1042985a,
+++    0x1043186e,
+++    0x10439884,
+++    0x1044189c,
+++    0x104498b1,
+++    0x104518c5,
+++    0x104598d7,
+++    0x1046060a,
+++    0x1046895c,
+++    0x104718ec,
+++    0x10479903,
+++    0x10481918,
+++    0x10489926,
+++    0x10490e80,
+++    0x10499760,
+++    0x104a162a,
+++    0x14320c14,
+++    0x14328c22,
+++    0x14330c31,
+++    0x14338c43,
+++    0x143400b9,
+++    0x143480f7,
+++    0x18320090,
+++    0x18328f8a,
+++    0x183300b9,
+++    0x18338fa0,
+++    0x18340fb4,
+++    0x183480f7,
+++    0x18350fd3,
+++    0x18358feb,
+++    0x18361000,
+++    0x18369014,
+++    0x1837104c,
+++    0x18379062,
+++    0x18381076,
+++    0x18389086,
+++    0x18390a82,
+++    0x18399096,
+++    0x183a10bc,
+++    0x183a90e2,
+++    0x183b0c8c,
+++    0x183b9131,
+++    0x183c1143,
+++    0x183c914e,
+++    0x183d115e,
+++    0x183d916f,
+++    0x183e1180,
+++    0x183e9192,
+++    0x183f11bb,
+++    0x183f91d4,
+++    0x184011ec,
+++    0x184086e2,
+++    0x18411105,
+++    0x184190d0,
+++    0x184210ef,
+++    0x18428c79,
+++    0x184310ab,
+++    0x18439117,
+++    0x18440fc9,
+++    0x18449038,
+++    0x20321226,
+++    0x20329213,
+++    0x24321232,
+++    0x243289a2,
+++    0x24331244,
+++    0x24339251,
+++    0x2434125e,
+++    0x24349270,
+++    0x2435127f,
+++    0x2435929c,
+++    0x243612a9,
+++    0x243692b7,
+++    0x243712c5,
+++    0x243792d3,
+++    0x243812dc,
+++    0x243892e9,
+++    0x243912fc,
+++    0x28320c61,
+++    0x28328c8c,
+++    0x28330c31,
+++    0x28338c9f,
+++    0x28340c6d,
+++    0x283480b9,
+++    0x283500f7,
+++    0x28358c79,
+++    0x2c3231de,
+++    0x2c329313,
+++    0x2c3331ec,
+++    0x2c33b1fe,
+++    0x2c343212,
+++    0x2c34b224,
+++    0x2c35323f,
+++    0x2c35b251,
+++    0x2c363281,
+++    0x2c36833a,
+++    0x2c37328e,
+++    0x2c37b2ba,
+++    0x2c3832df,
+++    0x2c38b2f6,
+++    0x2c393314,
+++    0x2c39b324,
+++    0x2c3a3336,
+++    0x2c3ab34a,
+++    0x2c3b335b,
+++    0x2c3bb37a,
+++    0x2c3c1325,
+++    0x2c3c933b,
+++    0x2c3d338e,
+++    0x2c3d9354,
+++    0x2c3e33ab,
+++    0x2c3eb3b9,
+++    0x2c3f33d1,
+++    0x2c3fb3e9,
+++    0x2c403413,
+++    0x2c409226,
+++    0x2c413424,
+++    0x2c41b437,
+++    0x2c4211ec,
+++    0x2c42b448,
+++    0x2c43072f,
+++    0x2c43b36c,
+++    0x2c4432cd,
+++    0x2c44b3f6,
+++    0x2c453264,
+++    0x2c45b2a0,
+++    0x2c463304,
+++    0x30320000,
+++    0x30328015,
+++    0x3033001f,
+++    0x30338038,
+++    0x30340057,
+++    0x30348071,
+++    0x30350078,
+++    0x30358090,
+++    0x303600a1,
+++    0x303680b9,
+++    0x303700c6,
+++    0x303780d5,
+++    0x303800f7,
+++    0x30388104,
+++    0x30390117,
+++    0x30398132,
+++    0x303a0147,
+++    0x303a815b,
+++    0x303b016f,
+++    0x303b8180,
+++    0x303c0199,
+++    0x303c81b6,
+++    0x303d01c4,
+++    0x303d81d8,
+++    0x303e01e8,
+++    0x303e8201,
+++    0x303f0211,
+++    0x303f8224,
+++    0x30400233,
+++    0x3040823f,
+++    0x30410254,
+++    0x30418264,
+++    0x3042027b,
+++    0x30428288,
+++    0x3043029b,
+++    0x304382aa,
+++    0x304402bf,
+++    0x304482e0,
+++    0x304502f3,
+++    0x30458306,
+++    0x3046031f,
+++    0x3046833a,
+++    0x30470357,
+++    0x30478369,
+++    0x30480377,
+++    0x30488388,
+++    0x30490397,
+++    0x304983af,
+++    0x304a03c1,
+++    0x304a83d5,
+++    0x304b03ed,
+++    0x304b8400,
+++    0x304c040b,
+++    0x304c841c,
+++    0x304d0428,
+++    0x304d843e,
+++    0x304e044c,
+++    0x304e8462,
+++    0x304f0474,
+++    0x304f8486,
+++    0x305004a9,
+++    0x305084bc,
+++    0x305104cd,
+++    0x305184dd,
+++    0x305204f5,
+++    0x3052850a,
+++    0x30530522,
+++    0x30538536,
+++    0x3054054e,
+++    0x30548567,
+++    0x30550580,
+++    0x3055859d,
+++    0x305605a8,
+++    0x305685c0,
+++    0x305705d0,
+++    0x305785e1,
+++    0x305805f4,
+++    0x3058860a,
+++    0x30590613,
+++    0x30598628,
+++    0x305a063b,
+++    0x305a864a,
+++    0x305b066a,
+++    0x305b8679,
+++    0x305c069a,
+++    0x305c86b6,
+++    0x305d06c2,
+++    0x305d86e2,
+++    0x305e06fe,
+++    0x305e870f,
+++    0x305f0725,
+++    0x305f872f,
+++    0x30600499,
+++    0x3060804a,
+++    0x34320b72,
+++    0x34328b86,
+++    0x34330ba3,
+++    0x34338bb6,
+++    0x34340bc5,
+++    0x34348bfe,
+++    0x34350be2,
+++    0x3c320090,
+++    0x3c328cc9,
+++    0x3c330ce2,
+++    0x3c338cfd,
+++    0x3c340d1a,
+++    0x3c348d44,
+++    0x3c350d5f,
+++    0x3c358d85,
+++    0x3c360d9e,
+++    0x3c368db6,
+++    0x3c370dc7,
+++    0x3c378dd5,
+++    0x3c380de2,
+++    0x3c388df6,
+++    0x3c390c8c,
+++    0x3c398e19,
+++    0x3c3a0e2d,
+++    0x3c3a891c,
+++    0x3c3b0e3d,
+++    0x3c3b8e58,
+++    0x3c3c0e6a,
+++    0x3c3c8e9d,
+++    0x3c3d0ea7,
+++    0x3c3d8ebb,
+++    0x3c3e0ec9,
+++    0x3c3e8eee,
+++    0x3c3f0cb5,
+++    0x3c3f8ed7,
+++    0x3c4000b9,
+++    0x3c4080f7,
+++    0x3c410d35,
+++    0x3c418d74,
+++    0x3c420e80,
+++    0x3c428e0a,
+++    0x403219b8,
+++    0x403299ce,
+++    0x403319fc,
+++    0x40339a06,
+++    0x40341a1d,
+++    0x40349a3b,
+++    0x40351a4b,
+++    0x40359a5d,
+++    0x40361a6a,
+++    0x40369a76,
+++    0x40371a8b,
+++    0x40379a9d,
+++    0x40381aa8,
+++    0x40389aba,
+++    0x40390f34,
+++    0x40399aca,
+++    0x403a1add,
+++    0x403a9afe,
+++    0x403b1b0f,
+++    0x403b9b1f,
+++    0x403c0071,
+++    0x403c8090,
+++    0x403d1b80,
+++    0x403d9b96,
+++    0x403e1ba5,
+++    0x403e9bdd,
+++    0x403f1bf7,
+++    0x403f9c1f,
+++    0x40401c34,
+++    0x40409c48,
+++    0x40411c83,
+++    0x40419c9e,
+++    0x40421cb7,
+++    0x40429cca,
+++    0x40431cde,
+++    0x40439d0c,
+++    0x40441d23,
+++    0x404480b9,
+++    0x40451d38,
+++    0x40459d4a,
+++    0x40461d6e,
+++    0x40469d8e,
+++    0x40471d9c,
+++    0x40479dc3,
+++    0x40481e34,
+++    0x40489ee1,
+++    0x40491ef8,
+++    0x40499f12,
+++    0x404a1f29,
+++    0x404a9f47,
+++    0x404b1f5f,
+++    0x404b9f8c,
+++    0x404c1fa2,
+++    0x404c9fb4,
+++    0x404d1fd5,
+++    0x404da00e,
+++    0x404e2022,
+++    0x404ea02f,
+++    0x404f20ac,
+++    0x404fa0f2,
+++    0x40502149,
+++    0x4050a15d,
+++    0x40512190,
+++    0x405221a0,
+++    0x4052a1c4,
+++    0x405321dc,
+++    0x4053a1ef,
+++    0x40542204,
+++    0x4054a227,
+++    0x40552252,
+++    0x4055a28f,
+++    0x405622b4,
+++    0x4056a2cd,
+++    0x405722e5,
+++    0x4057a2f8,
+++    0x4058230d,
+++    0x4058a334,
+++    0x40592363,
+++    0x4059a390,
+++    0x405a23a4,
+++    0x405aa3b4,
+++    0x405b23cc,
+++    0x405ba3dd,
+++    0x405c23f0,
+++    0x405ca42f,
+++    0x405d243c,
+++    0x405da461,
+++    0x405e249f,
+++    0x405e8ac0,
+++    0x405f24c0,
+++    0x405fa4cd,
+++    0x406024db,
+++    0x4060a4fd,
+++    0x4061255e,
+++    0x4061a596,
+++    0x406225ad,
+++    0x4062a5be,
+++    0x4063260b,
+++    0x4063a620,
+++    0x40642637,
+++    0x4064a663,
+++    0x4065267e,
+++    0x4065a695,
+++    0x406626ad,
+++    0x4066a6d7,
+++    0x40672702,
+++    0x4067a747,
+++    0x4068278f,
+++    0x4068a7b0,
+++    0x406927e2,
+++    0x4069a810,
+++    0x406a2831,
+++    0x406aa851,
+++    0x406b29d9,
+++    0x406ba9fc,
+++    0x406c2a12,
+++    0x406cad03,
+++    0x406d2d32,
+++    0x406dad5a,
+++    0x406e2d88,
+++    0x406eadd5,
+++    0x406f2e2e,
+++    0x406fae66,
+++    0x40702e79,
+++    0x4070ae96,
+++    0x4071080f,
+++    0x4071aea8,
+++    0x40722ebb,
+++    0x4072aef1,
+++    0x40732f09,
+++    0x40739525,
+++    0x40742f1d,
+++    0x4074af37,
+++    0x40752f48,
+++    0x4075af5c,
+++    0x40762f6a,
+++    0x407692e9,
+++    0x40772f8f,
+++    0x4077afcf,
+++    0x40782fea,
+++    0x4078b023,
+++    0x4079303a,
+++    0x4079b050,
+++    0x407a307c,
+++    0x407ab08f,
+++    0x407b30a4,
+++    0x407bb0b6,
+++    0x407c30e7,
+++    0x407cb0f0,
+++    0x407d27cb,
+++    0x407da102,
+++    0x407e2fff,
+++    0x407ea344,
+++    0x407f1db0,
+++    0x407f9f76,
+++    0x408020bc,
+++    0x40809dd8,
+++    0x408121b2,
+++    0x4081a060,
+++    0x40822d73,
+++    0x40829b2b,
+++    0x4083231f,
+++    0x4083a648,
+++    0x40841dec,
+++    0x4084a37c,
+++    0x40852401,
+++    0x4085a525,
+++    0x40862481,
+++    0x4086a11c,
+++    0x40872db9,
+++    0x4087a573,
+++    0x40881b69,
+++    0x4088a75a,
+++    0x40891bb8,
+++    0x40899b45,
+++    0x408a2a4a,
+++    0x408a993d,
+++    0x408b30cb,
+++    0x408bae43,
+++    0x408c2411,
+++    0x408c9975,
+++    0x408d1ec7,
+++    0x408d9e1e,
+++    0x408e1ff7,
+++    0x408ea26f,
+++    0x408f276e,
+++    0x408fa541,
+++    0x40902723,
+++    0x4090a453,
+++    0x40912a32,
+++    0x4091999b,
+++    0x40921c05,
+++    0x4092adf4,
+++    0x40932ed4,
+++    0x4093a12d,
+++    0x40941e00,
+++    0x4094aa63,
+++    0x409525cf,
+++    0x4095b05c,
+++    0x40962da0,
+++    0x4096a0d5,
+++    0x40972178,
+++    0x4097a046,
+++    0x40981c65,
+++    0x4098a5e3,
+++    0x40992e10,
+++    0x4099a29c,
+++    0x409a2235,
+++    0x409a9959,
+++    0x409b1e4d,
+++    0x409b9e78,
+++    0x409c2fb1,
+++    0x409c9ea0,
+++    0x409d2091,
+++    0x409da076,
+++    0x409e1cf6,
+++    0x41f42904,
+++    0x41f92996,
+++    0x41fe2889,
+++    0x41feab3f,
+++    0x41ff2c54,
+++    0x4203291d,
+++    0x4208293f,
+++    0x4208a97b,
+++    0x4209286d,
+++    0x4209a9b5,
+++    0x420a28c4,
+++    0x420aa8a4,
+++    0x420b28e4,
+++    0x420ba95d,
+++    0x420c2c70,
+++    0x420caa73,
+++    0x420d2b26,
+++    0x420dab5d,
+++    0x42122b77,
+++    0x42172c37,
+++    0x4217abb9,
+++    0x421c2bdb,
+++    0x421f2b96,
+++    0x42212ce8,
+++    0x42262c1a,
+++    0x422b2cc6,
+++    0x422bab01,
+++    0x422c2ca8,
+++    0x422caab4,
+++    0x422d2a8d,
+++    0x422dac87,
+++    0x422e2ae0,
+++    0x42302bf6,
+++    0x4432073a,
+++    0x44328749,
+++    0x44330755,
+++    0x44338763,
+++    0x44340776,
+++    0x44348787,
+++    0x4435078e,
+++    0x44358798,
+++    0x443607ab,
+++    0x443687c1,
+++    0x443707d3,
+++    0x443787e0,
+++    0x443807ef,
+++    0x443887f7,
+++    0x4439080f,
+++    0x4439881d,
+++    0x443a0830,
+++    0x48321313,
+++    0x48329325,
+++    0x4833133b,
+++    0x48339354,
+++    0x4c321379,
+++    0x4c329389,
+++    0x4c33139c,
+++    0x4c3393bc,
+++    0x4c3400b9,
+++    0x4c3480f7,
+++    0x4c3513c8,
+++    0x4c3593d6,
+++    0x4c3613f2,
+++    0x4c369418,
+++    0x4c371427,
+++    0x4c379435,
+++    0x4c38144a,
+++    0x4c389456,
+++    0x4c391476,
+++    0x4c3994a0,
+++    0x4c3a14b9,
+++    0x4c3a94d2,
+++    0x4c3b060a,
+++    0x4c3b94eb,
+++    0x4c3c14fd,
+++    0x4c3c950c,
+++    0x4c3d1525,
+++    0x4c3d8c54,
+++    0x4c3e1592,
+++    0x4c3e9534,
+++    0x4c3f15b4,
+++    0x4c3f92e9,
+++    0x4c40154a,
+++    0x4c409365,
+++    0x4c411582,
+++    0x4c419405,
+++    0x4c42156e,
+++    0x5032345a,
+++    0x5032b469,
+++    0x50333474,
+++    0x5033b484,
+++    0x5034349d,
+++    0x5034b4b7,
+++    0x503534c5,
+++    0x5035b4db,
+++    0x503634ed,
+++    0x5036b503,
+++    0x5037351c,
+++    0x5037b52f,
+++    0x50383547,
+++    0x5038b558,
+++    0x5039356d,
+++    0x5039b581,
+++    0x503a35a1,
+++    0x503ab5b7,
+++    0x503b35cf,
+++    0x503bb5e1,
+++    0x503c35fd,
+++    0x503cb614,
+++    0x503d362d,
+++    0x503db643,
+++    0x503e3650,
+++    0x503eb666,
+++    0x503f3678,
+++    0x503f8388,
+++    0x5040368b,
+++    0x5040b69b,
+++    0x504136b5,
+++    0x5041b6c4,
+++    0x504236de,
+++    0x5042b6fb,
+++    0x5043370b,
+++    0x5043b71b,
+++    0x5044372a,
+++    0x5044843e,
+++    0x5045373e,
+++    0x5045b75c,
+++    0x5046376f,
+++    0x5046b785,
+++    0x50473797,
+++    0x5047b7ac,
+++    0x504837d2,
+++    0x5048b7e0,
+++    0x504937f3,
+++    0x5049b808,
+++    0x504a381e,
+++    0x504ab82e,
+++    0x504b384e,
+++    0x504bb861,
+++    0x504c3884,
+++    0x504cb8b2,
+++    0x504d38c4,
+++    0x504db8e1,
+++    0x504e38fc,
+++    0x504eb918,
+++    0x504f392a,
+++    0x504fb941,
+++    0x50503950,
+++    0x505086fe,
+++    0x50513963,
+++    0x58320f72,
+++    0x68320f34,
+++    0x68328c8c,
+++    0x68330c9f,
+++    0x68338f42,
+++    0x68340f52,
+++    0x683480f7,
+++    0x6c320efa,
+++    0x6c328c43,
+++    0x6c330f05,
+++    0x6c338f1e,
+++    0x74320a28,
+++    0x743280b9,
+++    0x74330c54,
+++    0x7832098d,
+++    0x783289a2,
+++    0x783309ae,
+++    0x78338090,
+++    0x783409bd,
+++    0x783489d2,
+++    0x783509f1,
+++    0x78358a13,
+++    0x78360a28,
+++    0x78368a3e,
+++    0x78370a4e,
+++    0x78378a6f,
+++    0x78380a82,
+++    0x78388a94,
+++    0x78390aa1,
+++    0x78398ac0,
+++    0x783a0ad5,
+++    0x783a8ae3,
+++    0x783b0aed,
+++    0x783b8b01,
+++    0x783c0b18,
+++    0x783c8b2d,
+++    0x783d0b44,
+++    0x783d8b59,
+++    0x783e0aaf,
+++    0x783e8a61,
+++    0x7c321202,
+++    0x80321418,
+++    0x80328090,
+++    0x803331ad,
+++    0x803380b9,
+++    0x803431bc,
+++    0x8034b124,
+++    0x80353142,
+++    0x8035b1d0,
+++    0x80363184,
+++    0x8036b133,
+++    0x80373176,
+++    0x8037b111,
+++    0x80383197,
+++    0x8038b153,
+++    0x80393168,
+++};
+++
+++const size_t kOpenSSLReasonValuesLen = sizeof(kOpenSSLReasonValues) / sizeof(kOpenSSLReasonValues[0]);
+++
+++const char kOpenSSLReasonStringData[] =
+++    "ASN1_LENGTH_MISMATCH\0"
+++    "AUX_ERROR\0"
+++    "BAD_GET_ASN1_OBJECT_CALL\0"
+++    "BAD_OBJECT_HEADER\0"
+++    "BAD_TEMPLATE\0"
+++    "BMPSTRING_IS_WRONG_LENGTH\0"
+++    "BN_LIB\0"
+++    "BOOLEAN_IS_WRONG_LENGTH\0"
+++    "BUFFER_TOO_SMALL\0"
+++    "CONTEXT_NOT_INITIALISED\0"
+++    "DECODE_ERROR\0"
+++    "DEPTH_EXCEEDED\0"
+++    "DIGEST_AND_KEY_TYPE_NOT_SUPPORTED\0"
+++    "ENCODE_ERROR\0"
+++    "ERROR_GETTING_TIME\0"
+++    "EXPECTING_AN_ASN1_SEQUENCE\0"
+++    "EXPECTING_AN_INTEGER\0"
+++    "EXPECTING_AN_OBJECT\0"
+++    "EXPECTING_A_BOOLEAN\0"
+++    "EXPECTING_A_TIME\0"
+++    "EXPLICIT_LENGTH_MISMATCH\0"
+++    "EXPLICIT_TAG_NOT_CONSTRUCTED\0"
+++    "FIELD_MISSING\0"
+++    "FIRST_NUM_TOO_LARGE\0"
+++    "HEADER_TOO_LONG\0"
+++    "ILLEGAL_BITSTRING_FORMAT\0"
+++    "ILLEGAL_BOOLEAN\0"
+++    "ILLEGAL_CHARACTERS\0"
+++    "ILLEGAL_FORMAT\0"
+++    "ILLEGAL_HEX\0"
+++    "ILLEGAL_IMPLICIT_TAG\0"
+++    "ILLEGAL_INTEGER\0"
+++    "ILLEGAL_NESTED_TAGGING\0"
+++    "ILLEGAL_NULL\0"
+++    "ILLEGAL_NULL_VALUE\0"
+++    "ILLEGAL_OBJECT\0"
+++    "ILLEGAL_OPTIONAL_ANY\0"
+++    "ILLEGAL_OPTIONS_ON_ITEM_TEMPLATE\0"
+++    "ILLEGAL_TAGGED_ANY\0"
+++    "ILLEGAL_TIME_VALUE\0"
+++    "INTEGER_NOT_ASCII_FORMAT\0"
+++    "INTEGER_TOO_LARGE_FOR_LONG\0"
+++    "INVALID_BIT_STRING_BITS_LEFT\0"
+++    "INVALID_BMPSTRING\0"
+++    "INVALID_DIGIT\0"
+++    "INVALID_MODIFIER\0"
+++    "INVALID_NUMBER\0"
+++    "INVALID_OBJECT_ENCODING\0"
+++    "INVALID_SEPARATOR\0"
+++    "INVALID_TIME_FORMAT\0"
+++    "INVALID_UNIVERSALSTRING\0"
+++    "INVALID_UTF8STRING\0"
+++    "LIST_ERROR\0"
+++    "MISSING_ASN1_EOS\0"
+++    "MISSING_EOC\0"
+++    "MISSING_SECOND_NUMBER\0"
+++    "MISSING_VALUE\0"
+++    "MSTRING_NOT_UNIVERSAL\0"
+++    "MSTRING_WRONG_TAG\0"
+++    "NESTED_ASN1_ERROR\0"
+++    "NESTED_ASN1_STRING\0"
+++    "NESTED_TOO_DEEP\0"
+++    "NON_HEX_CHARACTERS\0"
+++    "NOT_ASCII_FORMAT\0"
+++    "NOT_ENOUGH_DATA\0"
+++    "NO_MATCHING_CHOICE_TYPE\0"
+++    "NULL_IS_WRONG_LENGTH\0"
+++    "OBJECT_NOT_ASCII_FORMAT\0"
+++    "ODD_NUMBER_OF_CHARS\0"
+++    "SECOND_NUMBER_TOO_LARGE\0"
+++    "SEQUENCE_LENGTH_MISMATCH\0"
+++    "SEQUENCE_NOT_CONSTRUCTED\0"
+++    "SEQUENCE_OR_SET_NEEDS_CONFIG\0"
+++    "SHORT_LINE\0"
+++    "STREAMING_NOT_SUPPORTED\0"
+++    "STRING_TOO_LONG\0"
+++    "STRING_TOO_SHORT\0"
+++    "TAG_VALUE_TOO_HIGH\0"
+++    "TIME_NOT_ASCII_FORMAT\0"
+++    "TOO_LONG\0"
+++    "TYPE_NOT_CONSTRUCTED\0"
+++    "TYPE_NOT_PRIMITIVE\0"
+++    "UNEXPECTED_EOC\0"
+++    "UNIVERSALSTRING_IS_WRONG_LENGTH\0"
+++    "UNKNOWN_FORMAT\0"
+++    "UNKNOWN_MESSAGE_DIGEST_ALGORITHM\0"
+++    "UNKNOWN_SIGNATURE_ALGORITHM\0"
+++    "UNKNOWN_TAG\0"
+++    "UNSUPPORTED_ANY_DEFINED_BY_TYPE\0"
+++    "UNSUPPORTED_PUBLIC_KEY_TYPE\0"
+++    "UNSUPPORTED_TYPE\0"
+++    "WRONG_PUBLIC_KEY_TYPE\0"
+++    "WRONG_TAG\0"
+++    "WRONG_TYPE\0"
+++    "BAD_FOPEN_MODE\0"
+++    "BROKEN_PIPE\0"
+++    "CONNECT_ERROR\0"
+++    "ERROR_SETTING_NBIO\0"
+++    "INVALID_ARGUMENT\0"
+++    "IN_USE\0"
+++    "KEEPALIVE\0"
+++    "NBIO_CONNECT_ERROR\0"
+++    "NO_HOSTNAME_SPECIFIED\0"
+++    "NO_PORT_SPECIFIED\0"
+++    "NO_SUCH_FILE\0"
+++    "NULL_PARAMETER\0"
+++    "SYS_LIB\0"
+++    "UNABLE_TO_CREATE_SOCKET\0"
+++    "UNINITIALIZED\0"
+++    "UNSUPPORTED_METHOD\0"
+++    "WRITE_TO_READ_ONLY_BIO\0"
+++    "ARG2_LT_ARG3\0"
+++    "BAD_ENCODING\0"
+++    "BAD_RECIPROCAL\0"
+++    "BIGNUM_TOO_LONG\0"
+++    "BITS_TOO_SMALL\0"
+++    "CALLED_WITH_EVEN_MODULUS\0"
+++    "DIV_BY_ZERO\0"
+++    "EXPAND_ON_STATIC_BIGNUM_DATA\0"
+++    "INPUT_NOT_REDUCED\0"
+++    "INVALID_INPUT\0"
+++    "INVALID_RANGE\0"
+++    "NEGATIVE_NUMBER\0"
+++    "NOT_A_SQUARE\0"
+++    "NOT_INITIALIZED\0"
+++    "NO_INVERSE\0"
+++    "PRIVATE_KEY_TOO_LARGE\0"
+++    "P_IS_NOT_PRIME\0"
+++    "TOO_MANY_ITERATIONS\0"
+++    "TOO_MANY_TEMPORARY_VARIABLES\0"
+++    "AES_KEY_SETUP_FAILED\0"
+++    "BAD_DECRYPT\0"
+++    "BAD_KEY_LENGTH\0"
+++    "CTRL_NOT_IMPLEMENTED\0"
+++    "CTRL_OPERATION_NOT_IMPLEMENTED\0"
+++    "DATA_NOT_MULTIPLE_OF_BLOCK_LENGTH\0"
+++    "INITIALIZATION_ERROR\0"
+++    "INPUT_NOT_INITIALIZED\0"
+++    "INVALID_AD_SIZE\0"
+++    "INVALID_KEY_LENGTH\0"
+++    "INVALID_NONCE\0"
+++    "INVALID_NONCE_SIZE\0"
+++    "INVALID_OPERATION\0"
+++    "IV_TOO_LARGE\0"
+++    "NO_CIPHER_SET\0"
+++    "NO_DIRECTION_SET\0"
+++    "OUTPUT_ALIASES_INPUT\0"
+++    "TAG_TOO_LARGE\0"
+++    "TOO_LARGE\0"
+++    "UNSUPPORTED_AD_SIZE\0"
+++    "UNSUPPORTED_INPUT_SIZE\0"
+++    "UNSUPPORTED_KEY_SIZE\0"
+++    "UNSUPPORTED_NONCE_SIZE\0"
+++    "UNSUPPORTED_TAG_SIZE\0"
+++    "WRONG_FINAL_BLOCK_LENGTH\0"
+++    "LIST_CANNOT_BE_NULL\0"
+++    "MISSING_CLOSE_SQUARE_BRACKET\0"
+++    "MISSING_EQUAL_SIGN\0"
+++    "NO_CLOSE_BRACE\0"
+++    "UNABLE_TO_CREATE_NEW_SECTION\0"
+++    "VARIABLE_EXPANSION_TOO_LONG\0"
+++    "VARIABLE_HAS_NO_VALUE\0"
+++    "BAD_GENERATOR\0"
+++    "INVALID_PUBKEY\0"
+++    "MODULUS_TOO_LARGE\0"
+++    "NO_PRIVATE_VALUE\0"
+++    "UNKNOWN_HASH\0"
+++    "BAD_Q_VALUE\0"
+++    "BAD_VERSION\0"
+++    "INVALID_PARAMETERS\0"
+++    "MISSING_PARAMETERS\0"
+++    "NEED_NEW_SETUP_VALUES\0"
+++    "BIGNUM_OUT_OF_RANGE\0"
+++    "COORDINATES_OUT_OF_RANGE\0"
+++    "D2I_ECPKPARAMETERS_FAILURE\0"
+++    "EC_GROUP_NEW_BY_NAME_FAILURE\0"
+++    "GROUP2PKPARAMETERS_FAILURE\0"
+++    "GROUP_MISMATCH\0"
+++    "I2D_ECPKPARAMETERS_FAILURE\0"
+++    "INCOMPATIBLE_OBJECTS\0"
+++    "INVALID_COFACTOR\0"
+++    "INVALID_COMPRESSED_POINT\0"
+++    "INVALID_COMPRESSION_BIT\0"
+++    "INVALID_ENCODING\0"
+++    "INVALID_FIELD\0"
+++    "INVALID_FORM\0"
+++    "INVALID_GROUP_ORDER\0"
+++    "INVALID_PRIVATE_KEY\0"
+++    "INVALID_SCALAR\0"
+++    "MISSING_PRIVATE_KEY\0"
+++    "NON_NAMED_CURVE\0"
+++    "PKPARAMETERS2GROUP_FAILURE\0"
+++    "POINT_AT_INFINITY\0"
+++    "POINT_IS_NOT_ON_CURVE\0"
+++    "PUBLIC_KEY_VALIDATION_FAILED\0"
+++    "SLOT_FULL\0"
+++    "UNDEFINED_GENERATOR\0"
+++    "UNKNOWN_GROUP\0"
+++    "UNKNOWN_ORDER\0"
+++    "WRONG_CURVE_PARAMETERS\0"
+++    "WRONG_ORDER\0"
+++    "KDF_FAILED\0"
+++    "POINT_ARITHMETIC_FAILURE\0"
+++    "UNKNOWN_DIGEST_LENGTH\0"
+++    "BAD_SIGNATURE\0"
+++    "NOT_IMPLEMENTED\0"
+++    "RANDOM_NUMBER_GENERATION_FAILED\0"
+++    "OPERATION_NOT_SUPPORTED\0"
+++    "COMMAND_NOT_SUPPORTED\0"
+++    "DIFFERENT_KEY_TYPES\0"
+++    "DIFFERENT_PARAMETERS\0"
+++    "EMPTY_PSK\0"
+++    "EXPECTING_AN_EC_KEY_KEY\0"
+++    "EXPECTING_AN_RSA_KEY\0"
+++    "EXPECTING_A_DSA_KEY\0"
+++    "ILLEGAL_OR_UNSUPPORTED_PADDING_MODE\0"
+++    "INVALID_BUFFER_SIZE\0"
+++    "INVALID_DIGEST_LENGTH\0"
+++    "INVALID_DIGEST_TYPE\0"
+++    "INVALID_KEYBITS\0"
+++    "INVALID_MGF1_MD\0"
+++    "INVALID_PADDING_MODE\0"
+++    "INVALID_PEER_KEY\0"
+++    "INVALID_PSS_SALTLEN\0"
+++    "INVALID_SIGNATURE\0"
+++    "KEYS_NOT_SET\0"
+++    "MEMORY_LIMIT_EXCEEDED\0"
+++    "NOT_A_PRIVATE_KEY\0"
+++    "NOT_XOF_OR_INVALID_LENGTH\0"
+++    "NO_DEFAULT_DIGEST\0"
+++    "NO_KEY_SET\0"
+++    "NO_MDC2_SUPPORT\0"
+++    "NO_NID_FOR_CURVE\0"
+++    "NO_OPERATION_SET\0"
+++    "NO_PARAMETERS_SET\0"
+++    "OPERATION_NOT_SUPPORTED_FOR_THIS_KEYTYPE\0"
+++    "OPERATON_NOT_INITIALIZED\0"
+++    "UNKNOWN_PUBLIC_KEY_TYPE\0"
+++    "UNSUPPORTED_ALGORITHM\0"
+++    "OUTPUT_TOO_LARGE\0"
+++    "INVALID_OID_STRING\0"
+++    "UNKNOWN_NID\0"
+++    "BAD_BASE64_DECODE\0"
+++    "BAD_END_LINE\0"
+++    "BAD_IV_CHARS\0"
+++    "BAD_PASSWORD_READ\0"
+++    "CIPHER_IS_NULL\0"
+++    "ERROR_CONVERTING_PRIVATE_KEY\0"
+++    "NOT_DEK_INFO\0"
+++    "NOT_ENCRYPTED\0"
+++    "NOT_PROC_TYPE\0"
+++    "NO_START_LINE\0"
+++    "READ_KEY\0"
+++    "SHORT_HEADER\0"
+++    "UNSUPPORTED_CIPHER\0"
+++    "UNSUPPORTED_ENCRYPTION\0"
+++    "BAD_PKCS7_VERSION\0"
+++    "NOT_PKCS7_SIGNED_DATA\0"
+++    "NO_CERTIFICATES_INCLUDED\0"
+++    "NO_CRLS_INCLUDED\0"
+++    "BAD_ITERATION_COUNT\0"
+++    "BAD_PKCS12_DATA\0"
+++    "BAD_PKCS12_VERSION\0"
+++    "CIPHER_HAS_NO_OBJECT_IDENTIFIER\0"
+++    "CRYPT_ERROR\0"
+++    "ENCRYPT_ERROR\0"
+++    "ERROR_SETTING_CIPHER_PARAMS\0"
+++    "INCORRECT_PASSWORD\0"
+++    "INVALID_CHARACTERS\0"
+++    "KEYGEN_FAILURE\0"
+++    "KEY_GEN_ERROR\0"
+++    "METHOD_NOT_SUPPORTED\0"
+++    "MISSING_MAC\0"
+++    "MULTIPLE_PRIVATE_KEYS_IN_PKCS12\0"
+++    "PKCS12_PUBLIC_KEY_INTEGRITY_NOT_SUPPORTED\0"
+++    "PKCS12_TOO_DEEPLY_NESTED\0"
+++    "PRIVATE_KEY_DECODE_ERROR\0"
+++    "PRIVATE_KEY_ENCODE_ERROR\0"
+++    "UNKNOWN_ALGORITHM\0"
+++    "UNKNOWN_CIPHER\0"
+++    "UNKNOWN_CIPHER_ALGORITHM\0"
+++    "UNKNOWN_DIGEST\0"
+++    "UNSUPPORTED_KEYLENGTH\0"
+++    "UNSUPPORTED_KEY_DERIVATION_FUNCTION\0"
+++    "UNSUPPORTED_OPTIONS\0"
+++    "UNSUPPORTED_PRF\0"
+++    "UNSUPPORTED_PRIVATE_KEY_ALGORITHM\0"
+++    "UNSUPPORTED_SALT_TYPE\0"
+++    "BAD_E_VALUE\0"
+++    "BAD_FIXED_HEADER_DECRYPT\0"
+++    "BAD_PAD_BYTE_COUNT\0"
+++    "BAD_RSA_PARAMETERS\0"
+++    "BLOCK_TYPE_IS_NOT_01\0"
+++    "BLOCK_TYPE_IS_NOT_02\0"
+++    "BN_NOT_INITIALIZED\0"
+++    "CANNOT_RECOVER_MULTI_PRIME_KEY\0"
+++    "CRT_PARAMS_ALREADY_GIVEN\0"
+++    "CRT_VALUES_INCORRECT\0"
+++    "DATA_LEN_NOT_EQUAL_TO_MOD_LEN\0"
+++    "DATA_TOO_LARGE\0"
+++    "DATA_TOO_LARGE_FOR_KEY_SIZE\0"
+++    "DATA_TOO_LARGE_FOR_MODULUS\0"
+++    "DATA_TOO_SMALL\0"
+++    "DATA_TOO_SMALL_FOR_KEY_SIZE\0"
+++    "DIGEST_TOO_BIG_FOR_RSA_KEY\0"
+++    "D_E_NOT_CONGRUENT_TO_1\0"
+++    "D_OUT_OF_RANGE\0"
+++    "EMPTY_PUBLIC_KEY\0"
+++    "FIRST_OCTET_INVALID\0"
+++    "INCONSISTENT_SET_OF_CRT_VALUES\0"
+++    "INTERNAL_ERROR\0"
+++    "INVALID_MESSAGE_LENGTH\0"
+++    "KEY_SIZE_TOO_SMALL\0"
+++    "LAST_OCTET_INVALID\0"
+++    "MUST_HAVE_AT_LEAST_TWO_PRIMES\0"
+++    "NO_PUBLIC_EXPONENT\0"
+++    "NULL_BEFORE_BLOCK_MISSING\0"
+++    "N_NOT_EQUAL_P_Q\0"
+++    "OAEP_DECODING_ERROR\0"
+++    "ONLY_ONE_OF_P_Q_GIVEN\0"
+++    "OUTPUT_BUFFER_TOO_SMALL\0"
+++    "PADDING_CHECK_FAILED\0"
+++    "PKCS_DECODING_ERROR\0"
+++    "SLEN_CHECK_FAILED\0"
+++    "SLEN_RECOVERY_FAILED\0"
+++    "UNKNOWN_ALGORITHM_TYPE\0"
+++    "UNKNOWN_PADDING_TYPE\0"
+++    "VALUE_MISSING\0"
+++    "WRONG_SIGNATURE_LENGTH\0"
+++    "ALPN_MISMATCH_ON_EARLY_DATA\0"
+++    "ALPS_MISMATCH_ON_EARLY_DATA\0"
+++    "APPLICATION_DATA_INSTEAD_OF_HANDSHAKE\0"
+++    "APPLICATION_DATA_ON_SHUTDOWN\0"
+++    "APP_DATA_IN_HANDSHAKE\0"
+++    "ATTEMPT_TO_REUSE_SESSION_IN_DIFFERENT_CONTEXT\0"
+++    "BAD_ALERT\0"
+++    "BAD_CHANGE_CIPHER_SPEC\0"
+++    "BAD_DATA_RETURNED_BY_CALLBACK\0"
+++    "BAD_DH_P_LENGTH\0"
+++    "BAD_DIGEST_LENGTH\0"
+++    "BAD_ECC_CERT\0"
+++    "BAD_ECPOINT\0"
+++    "BAD_HANDSHAKE_RECORD\0"
+++    "BAD_HELLO_REQUEST\0"
+++    "BAD_LENGTH\0"
+++    "BAD_PACKET_LENGTH\0"
+++    "BAD_RSA_ENCRYPT\0"
+++    "BAD_SRTP_MKI_VALUE\0"
+++    "BAD_SRTP_PROTECTION_PROFILE_LIST\0"
+++    "BAD_SSL_FILETYPE\0"
+++    "BAD_WRITE_RETRY\0"
+++    "BIO_NOT_SET\0"
+++    "BLOCK_CIPHER_PAD_IS_WRONG\0"
+++    "CANNOT_HAVE_BOTH_PRIVKEY_AND_METHOD\0"
+++    "CANNOT_PARSE_LEAF_CERT\0"
+++    "CA_DN_LENGTH_MISMATCH\0"
+++    "CA_DN_TOO_LONG\0"
+++    "CCS_RECEIVED_EARLY\0"
+++    "CERTIFICATE_AND_PRIVATE_KEY_MISMATCH\0"
+++    "CERTIFICATE_VERIFY_FAILED\0"
+++    "CERT_CB_ERROR\0"
+++    "CERT_DECOMPRESSION_FAILED\0"
+++    "CERT_LENGTH_MISMATCH\0"
+++    "CHANNEL_ID_NOT_P256\0"
+++    "CHANNEL_ID_SIGNATURE_INVALID\0"
+++    "CIPHER_MISMATCH_ON_EARLY_DATA\0"
+++    "CIPHER_OR_HASH_UNAVAILABLE\0"
+++    "CLIENTHELLO_PARSE_FAILED\0"
+++    "CLIENTHELLO_TLSEXT\0"
+++    "CONNECTION_REJECTED\0"
+++    "CONNECTION_TYPE_NOT_SET\0"
+++    "COULD_NOT_PARSE_HINTS\0"
+++    "CUSTOM_EXTENSION_ERROR\0"
+++    "DATA_LENGTH_TOO_LONG\0"
+++    "DECRYPTION_FAILED\0"
+++    "DECRYPTION_FAILED_OR_BAD_RECORD_MAC\0"
+++    "DH_PUBLIC_VALUE_LENGTH_IS_WRONG\0"
+++    "DH_P_TOO_LONG\0"
+++    "DIGEST_CHECK_FAILED\0"
+++    "DOWNGRADE_DETECTED\0"
+++    "DTLS_MESSAGE_TOO_BIG\0"
+++    "DUPLICATE_EXTENSION\0"
+++    "DUPLICATE_KEY_SHARE\0"
+++    "DUPLICATE_SIGNATURE_ALGORITHM\0"
+++    "EARLY_DATA_NOT_IN_USE\0"
+++    "ECC_CERT_NOT_FOR_SIGNING\0"
+++    "ECH_SERVER_CONFIG_AND_PRIVATE_KEY_MISMATCH\0"
+++    "ECH_SERVER_CONFIG_UNSUPPORTED_EXTENSION\0"
+++    "ECH_SERVER_WOULD_HAVE_NO_RETRY_CONFIGS\0"
+++    "EMPTY_HELLO_RETRY_REQUEST\0"
+++    "EMS_STATE_INCONSISTENT\0"
+++    "ENCRYPTED_LENGTH_TOO_LONG\0"
+++    "ERROR_ADDING_EXTENSION\0"
+++    "ERROR_IN_RECEIVED_CIPHER_LIST\0"
+++    "ERROR_PARSING_EXTENSION\0"
+++    "EXCESSIVE_MESSAGE_SIZE\0"
+++    "EXCESS_HANDSHAKE_DATA\0"
+++    "EXTRA_DATA_IN_MESSAGE\0"
+++    "FRAGMENT_MISMATCH\0"
+++    "GOT_NEXT_PROTO_WITHOUT_EXTENSION\0"
+++    "HANDSHAKE_FAILURE_ON_CLIENT_HELLO\0"
+++    "HANDSHAKE_NOT_COMPLETE\0"
+++    "HTTPS_PROXY_REQUEST\0"
+++    "HTTP_REQUEST\0"
+++    "INAPPROPRIATE_FALLBACK\0"
+++    "INCONSISTENT_CLIENT_HELLO\0"
+++    "INVALID_ALPN_PROTOCOL\0"
+++    "INVALID_ALPN_PROTOCOL_LIST\0"
+++    "INVALID_CLIENT_HELLO_INNER\0"
+++    "INVALID_COMMAND\0"
+++    "INVALID_COMPRESSION_LIST\0"
+++    "INVALID_DELEGATED_CREDENTIAL\0"
+++    "INVALID_MESSAGE\0"
+++    "INVALID_OUTER_RECORD_TYPE\0"
+++    "INVALID_SCT_LIST\0"
+++    "INVALID_SIGNATURE_ALGORITHM\0"
+++    "INVALID_SSL_SESSION\0"
+++    "INVALID_TICKET_KEYS_LENGTH\0"
+++    "KEY_USAGE_BIT_INCORRECT\0"
+++    "LENGTH_MISMATCH\0"
+++    "MISSING_EXTENSION\0"
+++    "MISSING_KEY_SHARE\0"
+++    "MISSING_RSA_CERTIFICATE\0"
+++    "MISSING_TMP_DH_KEY\0"
+++    "MISSING_TMP_ECDH_KEY\0"
+++    "MIXED_SPECIAL_OPERATOR_WITH_GROUPS\0"
+++    "MTU_TOO_SMALL\0"
+++    "NEGOTIATED_ALPS_WITHOUT_ALPN\0"
+++    "NEGOTIATED_BOTH_NPN_AND_ALPN\0"
+++    "NEGOTIATED_TB_WITHOUT_EMS_OR_RI\0"
+++    "NESTED_GROUP\0"
+++    "NO_APPLICATION_PROTOCOL\0"
+++    "NO_CERTIFICATES_RETURNED\0"
+++    "NO_CERTIFICATE_ASSIGNED\0"
+++    "NO_CERTIFICATE_SET\0"
+++    "NO_CIPHERS_AVAILABLE\0"
+++    "NO_CIPHERS_PASSED\0"
+++    "NO_CIPHERS_SPECIFIED\0"
+++    "NO_CIPHER_MATCH\0"
+++    "NO_COMMON_SIGNATURE_ALGORITHMS\0"
+++    "NO_COMPRESSION_SPECIFIED\0"
+++    "NO_GROUPS_SPECIFIED\0"
+++    "NO_METHOD_SPECIFIED\0"
+++    "NO_P256_SUPPORT\0"
+++    "NO_PRIVATE_KEY_ASSIGNED\0"
+++    "NO_RENEGOTIATION\0"
+++    "NO_REQUIRED_DIGEST\0"
+++    "NO_SHARED_CIPHER\0"
+++    "NO_SHARED_GROUP\0"
+++    "NO_SUPPORTED_VERSIONS_ENABLED\0"
+++    "NULL_SSL_CTX\0"
+++    "NULL_SSL_METHOD_PASSED\0"
+++    "OCSP_CB_ERROR\0"
+++    "OLD_SESSION_CIPHER_NOT_RETURNED\0"
+++    "OLD_SESSION_PRF_HASH_MISMATCH\0"
+++    "OLD_SESSION_VERSION_NOT_RETURNED\0"
+++    "PARSE_TLSEXT\0"
+++    "PATH_TOO_LONG\0"
+++    "PEER_DID_NOT_RETURN_A_CERTIFICATE\0"
+++    "PEER_ERROR_UNSUPPORTED_CERTIFICATE_TYPE\0"
+++    "PRE_SHARED_KEY_MUST_BE_LAST\0"
+++    "PRIVATE_KEY_OPERATION_FAILED\0"
+++    "PROTOCOL_IS_SHUTDOWN\0"
+++    "PSK_IDENTITY_BINDER_COUNT_MISMATCH\0"
+++    "PSK_IDENTITY_NOT_FOUND\0"
+++    "PSK_NO_CLIENT_CB\0"
+++    "PSK_NO_SERVER_CB\0"
+++    "QUIC_INTERNAL_ERROR\0"
+++    "QUIC_TRANSPORT_PARAMETERS_MISCONFIGURED\0"
+++    "READ_TIMEOUT_EXPIRED\0"
+++    "RECORD_LENGTH_MISMATCH\0"
+++    "RECORD_TOO_LARGE\0"
+++    "RENEGOTIATION_EMS_MISMATCH\0"
+++    "RENEGOTIATION_ENCODING_ERR\0"
+++    "RENEGOTIATION_MISMATCH\0"
+++    "REQUIRED_CIPHER_MISSING\0"
+++    "RESUMED_EMS_SESSION_WITHOUT_EMS_EXTENSION\0"
+++    "RESUMED_NON_EMS_SESSION_WITH_EMS_EXTENSION\0"
+++    "SCSV_RECEIVED_WHEN_RENEGOTIATING\0"
+++    "SECOND_SERVERHELLO_VERSION_MISMATCH\0"
+++    "SERVERHELLO_TLSEXT\0"
+++    "SERVER_CERT_CHANGED\0"
+++    "SERVER_ECHOED_INVALID_SESSION_ID\0"
+++    "SESSION_ID_CONTEXT_UNINITIALIZED\0"
+++    "SESSION_MAY_NOT_BE_CREATED\0"
+++    "SHUTDOWN_WHILE_IN_INIT\0"
+++    "SIGNATURE_ALGORITHMS_EXTENSION_SENT_BY_SERVER\0"
+++    "SRTP_COULD_NOT_ALLOCATE_PROFILES\0"
+++    "SRTP_UNKNOWN_PROTECTION_PROFILE\0"
+++    "SSL3_EXT_INVALID_SERVERNAME\0"
+++    "SSLV3_ALERT_BAD_CERTIFICATE\0"
+++    "SSLV3_ALERT_BAD_RECORD_MAC\0"
+++    "SSLV3_ALERT_CERTIFICATE_EXPIRED\0"
+++    "SSLV3_ALERT_CERTIFICATE_REVOKED\0"
+++    "SSLV3_ALERT_CERTIFICATE_UNKNOWN\0"
+++    "SSLV3_ALERT_CLOSE_NOTIFY\0"
+++    "SSLV3_ALERT_DECOMPRESSION_FAILURE\0"
+++    "SSLV3_ALERT_HANDSHAKE_FAILURE\0"
+++    "SSLV3_ALERT_ILLEGAL_PARAMETER\0"
+++    "SSLV3_ALERT_NO_CERTIFICATE\0"
+++    "SSLV3_ALERT_UNEXPECTED_MESSAGE\0"
+++    "SSLV3_ALERT_UNSUPPORTED_CERTIFICATE\0"
+++    "SSL_CTX_HAS_NO_DEFAULT_SSL_VERSION\0"
+++    "SSL_HANDSHAKE_FAILURE\0"
+++    "SSL_SESSION_ID_CONTEXT_TOO_LONG\0"
+++    "SSL_SESSION_ID_TOO_LONG\0"
+++    "TICKET_ENCRYPTION_FAILED\0"
+++    "TLS13_DOWNGRADE\0"
+++    "TLSV1_ALERT_ACCESS_DENIED\0"
+++    "TLSV1_ALERT_BAD_CERTIFICATE_HASH_VALUE\0"
+++    "TLSV1_ALERT_BAD_CERTIFICATE_STATUS_RESPONSE\0"
+++    "TLSV1_ALERT_CERTIFICATE_REQUIRED\0"
+++    "TLSV1_ALERT_CERTIFICATE_UNOBTAINABLE\0"
+++    "TLSV1_ALERT_DECODE_ERROR\0"
+++    "TLSV1_ALERT_DECRYPTION_FAILED\0"
+++    "TLSV1_ALERT_DECRYPT_ERROR\0"
+++    "TLSV1_ALERT_EXPORT_RESTRICTION\0"
+++    "TLSV1_ALERT_INAPPROPRIATE_FALLBACK\0"
+++    "TLSV1_ALERT_INSUFFICIENT_SECURITY\0"
+++    "TLSV1_ALERT_INTERNAL_ERROR\0"
+++    "TLSV1_ALERT_NO_APPLICATION_PROTOCOL\0"
+++    "TLSV1_ALERT_NO_RENEGOTIATION\0"
+++    "TLSV1_ALERT_PROTOCOL_VERSION\0"
+++    "TLSV1_ALERT_RECORD_OVERFLOW\0"
+++    "TLSV1_ALERT_UNKNOWN_CA\0"
+++    "TLSV1_ALERT_UNKNOWN_PSK_IDENTITY\0"
+++    "TLSV1_ALERT_UNRECOGNIZED_NAME\0"
+++    "TLSV1_ALERT_UNSUPPORTED_EXTENSION\0"
+++    "TLSV1_ALERT_USER_CANCELLED\0"
+++    "TLS_PEER_DID_NOT_RESPOND_WITH_CERTIFICATE_LIST\0"
+++    "TLS_RSA_ENCRYPTED_VALUE_LENGTH_IS_WRONG\0"
+++    "TOO_MANY_EMPTY_FRAGMENTS\0"
+++    "TOO_MANY_KEY_UPDATES\0"
+++    "TOO_MANY_WARNING_ALERTS\0"
+++    "TOO_MUCH_READ_EARLY_DATA\0"
+++    "TOO_MUCH_SKIPPED_EARLY_DATA\0"
+++    "UNABLE_TO_FIND_ECDH_PARAMETERS\0"
+++    "UNCOMPRESSED_CERT_TOO_LARGE\0"
+++    "UNEXPECTED_COMPATIBILITY_MODE\0"
+++    "UNEXPECTED_EXTENSION\0"
+++    "UNEXPECTED_EXTENSION_ON_EARLY_DATA\0"
+++    "UNEXPECTED_MESSAGE\0"
+++    "UNEXPECTED_OPERATOR_IN_GROUP\0"
+++    "UNEXPECTED_RECORD\0"
+++    "UNKNOWN_ALERT_TYPE\0"
+++    "UNKNOWN_CERTIFICATE_TYPE\0"
+++    "UNKNOWN_CERT_COMPRESSION_ALG\0"
+++    "UNKNOWN_CIPHER_RETURNED\0"
+++    "UNKNOWN_CIPHER_TYPE\0"
+++    "UNKNOWN_KEY_EXCHANGE_TYPE\0"
+++    "UNKNOWN_PROTOCOL\0"
+++    "UNKNOWN_SSL_VERSION\0"
+++    "UNKNOWN_STATE\0"
+++    "UNSAFE_LEGACY_RENEGOTIATION_DISABLED\0"
+++    "UNSUPPORTED_COMPRESSION_ALGORITHM\0"
+++    "UNSUPPORTED_ECH_SERVER_CONFIG\0"
+++    "UNSUPPORTED_ELLIPTIC_CURVE\0"
+++    "UNSUPPORTED_PROTOCOL\0"
+++    "UNSUPPORTED_PROTOCOL_FOR_CUSTOM_KEY\0"
+++    "WRONG_CERTIFICATE_TYPE\0"
+++    "WRONG_CIPHER_RETURNED\0"
+++    "WRONG_CURVE\0"
+++    "WRONG_ENCRYPTION_LEVEL_RECEIVED\0"
+++    "WRONG_MESSAGE_TYPE\0"
+++    "WRONG_SIGNATURE_TYPE\0"
+++    "WRONG_SSL_VERSION\0"
+++    "WRONG_VERSION_NUMBER\0"
+++    "WRONG_VERSION_ON_EARLY_DATA\0"
+++    "X509_LIB\0"
+++    "X509_VERIFICATION_SETUP_PROBLEMS\0"
+++    "BAD_VALIDITY_CHECK\0"
+++    "DECODE_FAILURE\0"
+++    "INVALID_KEY_ID\0"
+++    "INVALID_METADATA\0"
+++    "INVALID_METADATA_KEY\0"
+++    "INVALID_PROOF\0"
+++    "INVALID_TOKEN\0"
+++    "NO_KEYS_CONFIGURED\0"
+++    "NO_SRR_KEY_CONFIGURED\0"
+++    "OVER_BATCHSIZE\0"
+++    "SRR_SIGNATURE_ERROR\0"
+++    "TOO_MANY_KEYS\0"
+++    "AKID_MISMATCH\0"
+++    "BAD_X509_FILETYPE\0"
+++    "BASE64_DECODE_ERROR\0"
+++    "CANT_CHECK_DH_KEY\0"
+++    "CERT_ALREADY_IN_HASH_TABLE\0"
+++    "CRL_ALREADY_DELTA\0"
+++    "CRL_VERIFY_FAILURE\0"
+++    "DELTA_CRL_WITHOUT_CRL_NUMBER\0"
+++    "IDP_MISMATCH\0"
+++    "INVALID_DIRECTORY\0"
+++    "INVALID_FIELD_FOR_VERSION\0"
+++    "INVALID_FIELD_NAME\0"
+++    "INVALID_PARAMETER\0"
+++    "INVALID_PSS_PARAMETERS\0"
+++    "INVALID_TRUST\0"
+++    "INVALID_VERSION\0"
+++    "ISSUER_MISMATCH\0"
+++    "KEY_TYPE_MISMATCH\0"
+++    "KEY_VALUES_MISMATCH\0"
+++    "LOADING_CERT_DIR\0"
+++    "LOADING_DEFAULTS\0"
+++    "NAME_TOO_LONG\0"
+++    "NEWER_CRL_NOT_NEWER\0"
+++    "NO_CERT_SET_FOR_US_TO_VERIFY\0"
+++    "NO_CRL_NUMBER\0"
+++    "PUBLIC_KEY_DECODE_ERROR\0"
+++    "PUBLIC_KEY_ENCODE_ERROR\0"
+++    "SHOULD_RETRY\0"
+++    "SIGNATURE_ALGORITHM_MISMATCH\0"
+++    "UNKNOWN_KEY_TYPE\0"
+++    "UNKNOWN_PURPOSE_ID\0"
+++    "UNKNOWN_TRUST_ID\0"
+++    "WRONG_LOOKUP_TYPE\0"
+++    "BAD_IP_ADDRESS\0"
+++    "BAD_OBJECT\0"
+++    "BN_DEC2BN_ERROR\0"
+++    "BN_TO_ASN1_INTEGER_ERROR\0"
+++    "CANNOT_FIND_FREE_FUNCTION\0"
+++    "DIRNAME_ERROR\0"
+++    "DISTPOINT_ALREADY_SET\0"
+++    "DUPLICATE_ZONE_ID\0"
+++    "ERROR_CONVERTING_ZONE\0"
+++    "ERROR_CREATING_EXTENSION\0"
+++    "ERROR_IN_EXTENSION\0"
+++    "EXPECTED_A_SECTION_NAME\0"
+++    "EXTENSION_EXISTS\0"
+++    "EXTENSION_NAME_ERROR\0"
+++    "EXTENSION_NOT_FOUND\0"
+++    "EXTENSION_SETTING_NOT_SUPPORTED\0"
+++    "EXTENSION_VALUE_ERROR\0"
+++    "ILLEGAL_EMPTY_EXTENSION\0"
+++    "ILLEGAL_HEX_DIGIT\0"
+++    "INCORRECT_POLICY_SYNTAX_TAG\0"
+++    "INVALID_BOOLEAN_STRING\0"
+++    "INVALID_EXTENSION_STRING\0"
+++    "INVALID_MULTIPLE_RDNS\0"
+++    "INVALID_NAME\0"
+++    "INVALID_NULL_ARGUMENT\0"
+++    "INVALID_NULL_NAME\0"
+++    "INVALID_NULL_VALUE\0"
+++    "INVALID_NUMBERS\0"
+++    "INVALID_OBJECT_IDENTIFIER\0"
+++    "INVALID_OPTION\0"
+++    "INVALID_POLICY_IDENTIFIER\0"
+++    "INVALID_PROXY_POLICY_SETTING\0"
+++    "INVALID_PURPOSE\0"
+++    "INVALID_SECTION\0"
+++    "INVALID_SYNTAX\0"
+++    "ISSUER_DECODE_ERROR\0"
+++    "NEED_ORGANIZATION_AND_NUMBERS\0"
+++    "NO_CONFIG_DATABASE\0"
+++    "NO_ISSUER_CERTIFICATE\0"
+++    "NO_ISSUER_DETAILS\0"
+++    "NO_POLICY_IDENTIFIER\0"
+++    "NO_PROXY_CERT_POLICY_LANGUAGE_DEFINED\0"
+++    "NO_PUBLIC_KEY\0"
+++    "NO_SUBJECT_DETAILS\0"
+++    "ODD_NUMBER_OF_DIGITS\0"
+++    "OPERATION_NOT_DEFINED\0"
+++    "OTHERNAME_ERROR\0"
+++    "POLICY_LANGUAGE_ALREADY_DEFINED\0"
+++    "POLICY_PATH_LENGTH\0"
+++    "POLICY_PATH_LENGTH_ALREADY_DEFINED\0"
+++    "POLICY_WHEN_PROXY_LANGUAGE_REQUIRES_NO_POLICY\0"
+++    "SECTION_NOT_FOUND\0"
+++    "UNABLE_TO_GET_ISSUER_DETAILS\0"
+++    "UNABLE_TO_GET_ISSUER_KEYID\0"
+++    "UNKNOWN_BIT_STRING_ARGUMENT\0"
+++    "UNKNOWN_EXTENSION\0"
+++    "UNKNOWN_EXTENSION_NAME\0"
+++    "UNKNOWN_OPTION\0"
+++    "UNSUPPORTED_OPTION\0"
+++    "USER_TOO_LONG\0"
+++    "";
+++
++diff --git a/linux-ppc64le/ypto/fipsmodule/aesp8-ppc.S b/linux-ppc64le/ypto/fipsmodule/aesp8-ppc.S
++new file mode 100644
++index 000000000..86b06fc2e
++--- /dev/null
+++++ b/linux-ppc64le/ypto/fipsmodule/aesp8-ppc.S
++@@ -0,0 +1,3670 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if !defined(OPENSSL_NO_ASM) && defined(__powerpc64__)
+++.machine	"any"
+++
+++.abiversion	2
+++.text
+++
+++.align	7
+++.Lrcon:
+++.byte	0x00,0x00,0x00,0x01,0x00,0x00,0x00,0x01,0x00,0x00,0x00,0x01,0x00,0x00,0x00,0x01
+++.byte	0x00,0x00,0x00,0x1b,0x00,0x00,0x00,0x1b,0x00,0x00,0x00,0x1b,0x00,0x00,0x00,0x1b
+++.byte	0x0c,0x0f,0x0e,0x0d,0x0c,0x0f,0x0e,0x0d,0x0c,0x0f,0x0e,0x0d,0x0c,0x0f,0x0e,0x0d
+++.byte	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00
+++.Lconsts:
+++	mflr	0
+++	bcl	20,31,$+4
+++	mflr	6
+++	addi	6,6,-0x48
+++	mtlr	0
+++	blr	
+++.long	0
+++.byte	0,12,0x14,0,0,0,0,0
+++.byte	65,69,83,32,102,111,114,32,80,111,119,101,114,73,83,65,32,50,46,48,55,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
+++.align	2
+++
+++.globl	aes_hw_set_encrypt_key
+++.type	aes_hw_set_encrypt_key,@function
+++.align	5
+++aes_hw_set_encrypt_key:
+++.localentry	aes_hw_set_encrypt_key,0
+++
+++.Lset_encrypt_key:
+++	mflr	11
+++	std	11,16(1)
+++
+++	li	6,-1
+++	cmpldi	3,0
+++	beq-	.Lenc_key_abort
+++	cmpldi	5,0
+++	beq-	.Lenc_key_abort
+++	li	6,-2
+++	cmpwi	4,128
+++	blt-	.Lenc_key_abort
+++	cmpwi	4,256
+++	bgt-	.Lenc_key_abort
+++	andi.	0,4,0x3f
+++	bne-	.Lenc_key_abort
+++
+++	lis	0,0xfff0
+++	li	12,-1
+++	or	0,0,0
+++
+++	bl	.Lconsts
+++	mtlr	11
+++
+++	neg	9,3
+++	lvx	1,0,3
+++	addi	3,3,15
+++	lvsr	3,0,9
+++	li	8,0x20
+++	cmpwi	4,192
+++	lvx	2,0,3
+++	vspltisb	5,0x0f
+++	lvx	4,0,6
+++	vxor	3,3,5
+++	lvx	5,8,6
+++	addi	6,6,0x10
+++	vperm	1,1,2,3
+++	li	7,8
+++	vxor	0,0,0
+++	mtctr	7
+++
+++	lvsl	8,0,5
+++	vspltisb	9,-1
+++	lvx	10,0,5
+++	vperm	9,9,0,8
+++
+++	blt	.Loop128
+++	addi	3,3,8
+++	beq	.L192
+++	addi	3,3,8
+++	b	.L256
+++
+++.align	4
+++.Loop128:
+++	vperm	3,1,1,5
+++	vsldoi	6,0,1,12
+++	vperm	11,1,1,8
+++	vsel	7,10,11,9
+++	vor	10,11,11
+++	.long	0x10632509
+++	stvx	7,0,5
+++	addi	5,5,16
+++
+++	vxor	1,1,6
+++	vsldoi	6,0,6,12
+++	vxor	1,1,6
+++	vsldoi	6,0,6,12
+++	vxor	1,1,6
+++	vadduwm	4,4,4
+++	vxor	1,1,3
+++	bdnz	.Loop128
+++
+++	lvx	4,0,6
+++
+++	vperm	3,1,1,5
+++	vsldoi	6,0,1,12
+++	vperm	11,1,1,8
+++	vsel	7,10,11,9
+++	vor	10,11,11
+++	.long	0x10632509
+++	stvx	7,0,5
+++	addi	5,5,16
+++
+++	vxor	1,1,6
+++	vsldoi	6,0,6,12
+++	vxor	1,1,6
+++	vsldoi	6,0,6,12
+++	vxor	1,1,6
+++	vadduwm	4,4,4
+++	vxor	1,1,3
+++
+++	vperm	3,1,1,5
+++	vsldoi	6,0,1,12
+++	vperm	11,1,1,8
+++	vsel	7,10,11,9
+++	vor	10,11,11
+++	.long	0x10632509
+++	stvx	7,0,5
+++	addi	5,5,16
+++
+++	vxor	1,1,6
+++	vsldoi	6,0,6,12
+++	vxor	1,1,6
+++	vsldoi	6,0,6,12
+++	vxor	1,1,6
+++	vxor	1,1,3
+++	vperm	11,1,1,8
+++	vsel	7,10,11,9
+++	vor	10,11,11
+++	stvx	7,0,5
+++
+++	addi	3,5,15
+++	addi	5,5,0x50
+++
+++	li	8,10
+++	b	.Ldone
+++
+++.align	4
+++.L192:
+++	lvx	6,0,3
+++	li	7,4
+++	vperm	11,1,1,8
+++	vsel	7,10,11,9
+++	vor	10,11,11
+++	stvx	7,0,5
+++	addi	5,5,16
+++	vperm	2,2,6,3
+++	vspltisb	3,8
+++	mtctr	7
+++	vsububm	5,5,3
+++
+++.Loop192:
+++	vperm	3,2,2,5
+++	vsldoi	6,0,1,12
+++	.long	0x10632509
+++
+++	vxor	1,1,6
+++	vsldoi	6,0,6,12
+++	vxor	1,1,6
+++	vsldoi	6,0,6,12
+++	vxor	1,1,6
+++
+++	vsldoi	7,0,2,8
+++	vspltw	6,1,3
+++	vxor	6,6,2
+++	vsldoi	2,0,2,12
+++	vadduwm	4,4,4
+++	vxor	2,2,6
+++	vxor	1,1,3
+++	vxor	2,2,3
+++	vsldoi	7,7,1,8
+++
+++	vperm	3,2,2,5
+++	vsldoi	6,0,1,12
+++	vperm	11,7,7,8
+++	vsel	7,10,11,9
+++	vor	10,11,11
+++	.long	0x10632509
+++	stvx	7,0,5
+++	addi	5,5,16
+++
+++	vsldoi	7,1,2,8
+++	vxor	1,1,6
+++	vsldoi	6,0,6,12
+++	vperm	11,7,7,8
+++	vsel	7,10,11,9
+++	vor	10,11,11
+++	vxor	1,1,6
+++	vsldoi	6,0,6,12
+++	vxor	1,1,6
+++	stvx	7,0,5
+++	addi	5,5,16
+++
+++	vspltw	6,1,3
+++	vxor	6,6,2
+++	vsldoi	2,0,2,12
+++	vadduwm	4,4,4
+++	vxor	2,2,6
+++	vxor	1,1,3
+++	vxor	2,2,3
+++	vperm	11,1,1,8
+++	vsel	7,10,11,9
+++	vor	10,11,11
+++	stvx	7,0,5
+++	addi	3,5,15
+++	addi	5,5,16
+++	bdnz	.Loop192
+++
+++	li	8,12
+++	addi	5,5,0x20
+++	b	.Ldone
+++
+++.align	4
+++.L256:
+++	lvx	6,0,3
+++	li	7,7
+++	li	8,14
+++	vperm	11,1,1,8
+++	vsel	7,10,11,9
+++	vor	10,11,11
+++	stvx	7,0,5
+++	addi	5,5,16
+++	vperm	2,2,6,3
+++	mtctr	7
+++
+++.Loop256:
+++	vperm	3,2,2,5
+++	vsldoi	6,0,1,12
+++	vperm	11,2,2,8
+++	vsel	7,10,11,9
+++	vor	10,11,11
+++	.long	0x10632509
+++	stvx	7,0,5
+++	addi	5,5,16
+++
+++	vxor	1,1,6
+++	vsldoi	6,0,6,12
+++	vxor	1,1,6
+++	vsldoi	6,0,6,12
+++	vxor	1,1,6
+++	vadduwm	4,4,4
+++	vxor	1,1,3
+++	vperm	11,1,1,8
+++	vsel	7,10,11,9
+++	vor	10,11,11
+++	stvx	7,0,5
+++	addi	3,5,15
+++	addi	5,5,16
+++	bdz	.Ldone
+++
+++	vspltw	3,1,3
+++	vsldoi	6,0,2,12
+++	.long	0x106305C8
+++
+++	vxor	2,2,6
+++	vsldoi	6,0,6,12
+++	vxor	2,2,6
+++	vsldoi	6,0,6,12
+++	vxor	2,2,6
+++
+++	vxor	2,2,3
+++	b	.Loop256
+++
+++.align	4
+++.Ldone:
+++	lvx	2,0,3
+++	vsel	2,10,2,9
+++	stvx	2,0,3
+++	li	6,0
+++	or	12,12,12
+++	stw	8,0(5)
+++
+++.Lenc_key_abort:
+++	mr	3,6
+++	blr	
+++.long	0
+++.byte	0,12,0x14,1,0,0,3,0
+++.long	0
+++.size	aes_hw_set_encrypt_key,.-aes_hw_set_encrypt_key
+++
+++.globl	aes_hw_set_decrypt_key
+++.type	aes_hw_set_decrypt_key,@function
+++.align	5
+++aes_hw_set_decrypt_key:
+++.localentry	aes_hw_set_decrypt_key,0
+++
+++	stdu	1,-64(1)
+++	mflr	10
+++	std	10,80(1)
+++	bl	.Lset_encrypt_key
+++	mtlr	10
+++
+++	cmpwi	3,0
+++	bne-	.Ldec_key_abort
+++
+++	slwi	7,8,4
+++	subi	3,5,240
+++	srwi	8,8,1
+++	add	5,3,7
+++	mtctr	8
+++
+++.Ldeckey:
+++	lwz	0, 0(3)
+++	lwz	6, 4(3)
+++	lwz	7, 8(3)
+++	lwz	8, 12(3)
+++	addi	3,3,16
+++	lwz	9, 0(5)
+++	lwz	10,4(5)
+++	lwz	11,8(5)
+++	lwz	12,12(5)
+++	stw	0, 0(5)
+++	stw	6, 4(5)
+++	stw	7, 8(5)
+++	stw	8, 12(5)
+++	subi	5,5,16
+++	stw	9, -16(3)
+++	stw	10,-12(3)
+++	stw	11,-8(3)
+++	stw	12,-4(3)
+++	bdnz	.Ldeckey
+++
+++	xor	3,3,3
+++.Ldec_key_abort:
+++	addi	1,1,64
+++	blr	
+++.long	0
+++.byte	0,12,4,1,0x80,0,3,0
+++.long	0
+++.size	aes_hw_set_decrypt_key,.-aes_hw_set_decrypt_key
+++.globl	aes_hw_encrypt
+++.type	aes_hw_encrypt,@function
+++.align	5
+++aes_hw_encrypt:
+++.localentry	aes_hw_encrypt,0
+++
+++	lwz	6,240(5)
+++	lis	0,0xfc00
+++	li	12,-1
+++	li	7,15
+++	or	0,0,0
+++
+++	lvx	0,0,3
+++	neg	11,4
+++	lvx	1,7,3
+++	lvsl	2,0,3
+++	vspltisb	4,0x0f
+++	lvsr	3,0,11
+++	vxor	2,2,4
+++	li	7,16
+++	vperm	0,0,1,2
+++	lvx	1,0,5
+++	lvsr	5,0,5
+++	srwi	6,6,1
+++	lvx	2,7,5
+++	addi	7,7,16
+++	subi	6,6,1
+++	vperm	1,2,1,5
+++
+++	vxor	0,0,1
+++	lvx	1,7,5
+++	addi	7,7,16
+++	mtctr	6
+++
+++.Loop_enc:
+++	vperm	2,1,2,5
+++	.long	0x10001508
+++	lvx	2,7,5
+++	addi	7,7,16
+++	vperm	1,2,1,5
+++	.long	0x10000D08
+++	lvx	1,7,5
+++	addi	7,7,16
+++	bdnz	.Loop_enc
+++
+++	vperm	2,1,2,5
+++	.long	0x10001508
+++	lvx	2,7,5
+++	vperm	1,2,1,5
+++	.long	0x10000D09
+++
+++	vspltisb	2,-1
+++	vxor	1,1,1
+++	li	7,15
+++	vperm	2,2,1,3
+++	vxor	3,3,4
+++	lvx	1,0,4
+++	vperm	0,0,0,3
+++	vsel	1,1,0,2
+++	lvx	4,7,4
+++	stvx	1,0,4
+++	vsel	0,0,4,2
+++	stvx	0,7,4
+++
+++	or	12,12,12
+++	blr	
+++.long	0
+++.byte	0,12,0x14,0,0,0,3,0
+++.long	0
+++.size	aes_hw_encrypt,.-aes_hw_encrypt
+++.globl	aes_hw_decrypt
+++.type	aes_hw_decrypt,@function
+++.align	5
+++aes_hw_decrypt:
+++.localentry	aes_hw_decrypt,0
+++
+++	lwz	6,240(5)
+++	lis	0,0xfc00
+++	li	12,-1
+++	li	7,15
+++	or	0,0,0
+++
+++	lvx	0,0,3
+++	neg	11,4
+++	lvx	1,7,3
+++	lvsl	2,0,3
+++	vspltisb	4,0x0f
+++	lvsr	3,0,11
+++	vxor	2,2,4
+++	li	7,16
+++	vperm	0,0,1,2
+++	lvx	1,0,5
+++	lvsr	5,0,5
+++	srwi	6,6,1
+++	lvx	2,7,5
+++	addi	7,7,16
+++	subi	6,6,1
+++	vperm	1,2,1,5
+++
+++	vxor	0,0,1
+++	lvx	1,7,5
+++	addi	7,7,16
+++	mtctr	6
+++
+++.Loop_dec:
+++	vperm	2,1,2,5
+++	.long	0x10001548
+++	lvx	2,7,5
+++	addi	7,7,16
+++	vperm	1,2,1,5
+++	.long	0x10000D48
+++	lvx	1,7,5
+++	addi	7,7,16
+++	bdnz	.Loop_dec
+++
+++	vperm	2,1,2,5
+++	.long	0x10001548
+++	lvx	2,7,5
+++	vperm	1,2,1,5
+++	.long	0x10000D49
+++
+++	vspltisb	2,-1
+++	vxor	1,1,1
+++	li	7,15
+++	vperm	2,2,1,3
+++	vxor	3,3,4
+++	lvx	1,0,4
+++	vperm	0,0,0,3
+++	vsel	1,1,0,2
+++	lvx	4,7,4
+++	stvx	1,0,4
+++	vsel	0,0,4,2
+++	stvx	0,7,4
+++
+++	or	12,12,12
+++	blr	
+++.long	0
+++.byte	0,12,0x14,0,0,0,3,0
+++.long	0
+++.size	aes_hw_decrypt,.-aes_hw_decrypt
+++.globl	aes_hw_cbc_encrypt
+++.type	aes_hw_cbc_encrypt,@function
+++.align	5
+++aes_hw_cbc_encrypt:
+++.localentry	aes_hw_cbc_encrypt,0
+++
+++	cmpldi	5,16
+++	.long	0x4dc00020
+++
+++	cmpwi	8,0
+++	lis	0,0xffe0
+++	li	12,-1
+++	or	0,0,0
+++
+++	li	10,15
+++	vxor	0,0,0
+++	vspltisb	3,0x0f
+++
+++	lvx	4,0,7
+++	lvsl	6,0,7
+++	lvx	5,10,7
+++	vxor	6,6,3
+++	vperm	4,4,5,6
+++
+++	neg	11,3
+++	lvsr	10,0,6
+++	lwz	9,240(6)
+++
+++	lvsr	6,0,11
+++	lvx	5,0,3
+++	addi	3,3,15
+++	vxor	6,6,3
+++
+++	lvsl	8,0,4
+++	vspltisb	9,-1
+++	lvx	7,0,4
+++	vperm	9,9,0,8
+++	vxor	8,8,3
+++
+++	srwi	9,9,1
+++	li	10,16
+++	subi	9,9,1
+++	beq	.Lcbc_dec
+++
+++.Lcbc_enc:
+++	vor	2,5,5
+++	lvx	5,0,3
+++	addi	3,3,16
+++	mtctr	9
+++	subi	5,5,16
+++
+++	lvx	0,0,6
+++	vperm	2,2,5,6
+++	lvx	1,10,6
+++	addi	10,10,16
+++	vperm	0,1,0,10
+++	vxor	2,2,0
+++	lvx	0,10,6
+++	addi	10,10,16
+++	vxor	2,2,4
+++
+++.Loop_cbc_enc:
+++	vperm	1,0,1,10
+++	.long	0x10420D08
+++	lvx	1,10,6
+++	addi	10,10,16
+++	vperm	0,1,0,10
+++	.long	0x10420508
+++	lvx	0,10,6
+++	addi	10,10,16
+++	bdnz	.Loop_cbc_enc
+++
+++	vperm	1,0,1,10
+++	.long	0x10420D08
+++	lvx	1,10,6
+++	li	10,16
+++	vperm	0,1,0,10
+++	.long	0x10820509
+++	cmpldi	5,16
+++
+++	vperm	3,4,4,8
+++	vsel	2,7,3,9
+++	vor	7,3,3
+++	stvx	2,0,4
+++	addi	4,4,16
+++	bge	.Lcbc_enc
+++
+++	b	.Lcbc_done
+++
+++.align	4
+++.Lcbc_dec:
+++	cmpldi	5,128
+++	bge	_aesp8_cbc_decrypt8x
+++	vor	3,5,5
+++	lvx	5,0,3
+++	addi	3,3,16
+++	mtctr	9
+++	subi	5,5,16
+++
+++	lvx	0,0,6
+++	vperm	3,3,5,6
+++	lvx	1,10,6
+++	addi	10,10,16
+++	vperm	0,1,0,10
+++	vxor	2,3,0
+++	lvx	0,10,6
+++	addi	10,10,16
+++
+++.Loop_cbc_dec:
+++	vperm	1,0,1,10
+++	.long	0x10420D48
+++	lvx	1,10,6
+++	addi	10,10,16
+++	vperm	0,1,0,10
+++	.long	0x10420548
+++	lvx	0,10,6
+++	addi	10,10,16
+++	bdnz	.Loop_cbc_dec
+++
+++	vperm	1,0,1,10
+++	.long	0x10420D48
+++	lvx	1,10,6
+++	li	10,16
+++	vperm	0,1,0,10
+++	.long	0x10420549
+++	cmpldi	5,16
+++
+++	vxor	2,2,4
+++	vor	4,3,3
+++	vperm	3,2,2,8
+++	vsel	2,7,3,9
+++	vor	7,3,3
+++	stvx	2,0,4
+++	addi	4,4,16
+++	bge	.Lcbc_dec
+++
+++.Lcbc_done:
+++	addi	4,4,-1
+++	lvx	2,0,4
+++	vsel	2,7,2,9
+++	stvx	2,0,4
+++
+++	neg	8,7
+++	li	10,15
+++	vxor	0,0,0
+++	vspltisb	9,-1
+++	vspltisb	3,0x0f
+++	lvsr	8,0,8
+++	vperm	9,9,0,8
+++	vxor	8,8,3
+++	lvx	7,0,7
+++	vperm	4,4,4,8
+++	vsel	2,7,4,9
+++	lvx	5,10,7
+++	stvx	2,0,7
+++	vsel	2,4,5,9
+++	stvx	2,10,7
+++
+++	or	12,12,12
+++	blr	
+++.long	0
+++.byte	0,12,0x14,0,0,0,6,0
+++.long	0
+++.align	5
+++_aesp8_cbc_decrypt8x:
+++	stdu	1,-448(1)
+++	li	10,207
+++	li	11,223
+++	stvx	20,10,1
+++	addi	10,10,32
+++	stvx	21,11,1
+++	addi	11,11,32
+++	stvx	22,10,1
+++	addi	10,10,32
+++	stvx	23,11,1
+++	addi	11,11,32
+++	stvx	24,10,1
+++	addi	10,10,32
+++	stvx	25,11,1
+++	addi	11,11,32
+++	stvx	26,10,1
+++	addi	10,10,32
+++	stvx	27,11,1
+++	addi	11,11,32
+++	stvx	28,10,1
+++	addi	10,10,32
+++	stvx	29,11,1
+++	addi	11,11,32
+++	stvx	30,10,1
+++	stvx	31,11,1
+++	li	0,-1
+++	stw	12,396(1)
+++	li	8,0x10
+++	std	26,400(1)
+++	li	26,0x20
+++	std	27,408(1)
+++	li	27,0x30
+++	std	28,416(1)
+++	li	28,0x40
+++	std	29,424(1)
+++	li	29,0x50
+++	std	30,432(1)
+++	li	30,0x60
+++	std	31,440(1)
+++	li	31,0x70
+++	or	0,0,0
+++
+++	subi	9,9,3
+++	subi	5,5,128
+++
+++	lvx	23,0,6
+++	lvx	30,8,6
+++	addi	6,6,0x20
+++	lvx	31,0,6
+++	vperm	23,30,23,10
+++	addi	11,1,79
+++	mtctr	9
+++
+++.Load_cbc_dec_key:
+++	vperm	24,31,30,10
+++	lvx	30,8,6
+++	addi	6,6,0x20
+++	stvx	24,0,11
+++	vperm	25,30,31,10
+++	lvx	31,0,6
+++	stvx	25,8,11
+++	addi	11,11,0x20
+++	bdnz	.Load_cbc_dec_key
+++
+++	lvx	26,8,6
+++	vperm	24,31,30,10
+++	lvx	27,26,6
+++	stvx	24,0,11
+++	vperm	25,26,31,10
+++	lvx	28,27,6
+++	stvx	25,8,11
+++	addi	11,1,79
+++	vperm	26,27,26,10
+++	lvx	29,28,6
+++	vperm	27,28,27,10
+++	lvx	30,29,6
+++	vperm	28,29,28,10
+++	lvx	31,30,6
+++	vperm	29,30,29,10
+++	lvx	14,31,6
+++	vperm	30,31,30,10
+++	lvx	24,0,11
+++	vperm	31,14,31,10
+++	lvx	25,8,11
+++
+++
+++
+++	subi	3,3,15
+++
+++	li	10,8
+++	.long	0x7C001E99
+++	lvsl	6,0,10
+++	vspltisb	3,0x0f
+++	.long	0x7C281E99
+++	vxor	6,6,3
+++	.long	0x7C5A1E99
+++	vperm	0,0,0,6
+++	.long	0x7C7B1E99
+++	vperm	1,1,1,6
+++	.long	0x7D5C1E99
+++	vperm	2,2,2,6
+++	vxor	14,0,23
+++	.long	0x7D7D1E99
+++	vperm	3,3,3,6
+++	vxor	15,1,23
+++	.long	0x7D9E1E99
+++	vperm	10,10,10,6
+++	vxor	16,2,23
+++	.long	0x7DBF1E99
+++	addi	3,3,0x80
+++	vperm	11,11,11,6
+++	vxor	17,3,23
+++	vperm	12,12,12,6
+++	vxor	18,10,23
+++	vperm	13,13,13,6
+++	vxor	19,11,23
+++	vxor	20,12,23
+++	vxor	21,13,23
+++
+++	mtctr	9
+++	b	.Loop_cbc_dec8x
+++.align	5
+++.Loop_cbc_dec8x:
+++	.long	0x11CEC548
+++	.long	0x11EFC548
+++	.long	0x1210C548
+++	.long	0x1231C548
+++	.long	0x1252C548
+++	.long	0x1273C548
+++	.long	0x1294C548
+++	.long	0x12B5C548
+++	lvx	24,26,11
+++	addi	11,11,0x20
+++
+++	.long	0x11CECD48
+++	.long	0x11EFCD48
+++	.long	0x1210CD48
+++	.long	0x1231CD48
+++	.long	0x1252CD48
+++	.long	0x1273CD48
+++	.long	0x1294CD48
+++	.long	0x12B5CD48
+++	lvx	25,8,11
+++	bdnz	.Loop_cbc_dec8x
+++
+++	subic	5,5,128
+++	.long	0x11CEC548
+++	.long	0x11EFC548
+++	.long	0x1210C548
+++	.long	0x1231C548
+++	.long	0x1252C548
+++	.long	0x1273C548
+++	.long	0x1294C548
+++	.long	0x12B5C548
+++
+++	subfe.	0,0,0
+++	.long	0x11CECD48
+++	.long	0x11EFCD48
+++	.long	0x1210CD48
+++	.long	0x1231CD48
+++	.long	0x1252CD48
+++	.long	0x1273CD48
+++	.long	0x1294CD48
+++	.long	0x12B5CD48
+++
+++	and	0,0,5
+++	.long	0x11CED548
+++	.long	0x11EFD548
+++	.long	0x1210D548
+++	.long	0x1231D548
+++	.long	0x1252D548
+++	.long	0x1273D548
+++	.long	0x1294D548
+++	.long	0x12B5D548
+++
+++	add	3,3,0
+++
+++
+++
+++	.long	0x11CEDD48
+++	.long	0x11EFDD48
+++	.long	0x1210DD48
+++	.long	0x1231DD48
+++	.long	0x1252DD48
+++	.long	0x1273DD48
+++	.long	0x1294DD48
+++	.long	0x12B5DD48
+++
+++	addi	11,1,79
+++	.long	0x11CEE548
+++	.long	0x11EFE548
+++	.long	0x1210E548
+++	.long	0x1231E548
+++	.long	0x1252E548
+++	.long	0x1273E548
+++	.long	0x1294E548
+++	.long	0x12B5E548
+++	lvx	24,0,11
+++
+++	.long	0x11CEED48
+++	.long	0x11EFED48
+++	.long	0x1210ED48
+++	.long	0x1231ED48
+++	.long	0x1252ED48
+++	.long	0x1273ED48
+++	.long	0x1294ED48
+++	.long	0x12B5ED48
+++	lvx	25,8,11
+++
+++	.long	0x11CEF548
+++	vxor	4,4,31
+++	.long	0x11EFF548
+++	vxor	0,0,31
+++	.long	0x1210F548
+++	vxor	1,1,31
+++	.long	0x1231F548
+++	vxor	2,2,31
+++	.long	0x1252F548
+++	vxor	3,3,31
+++	.long	0x1273F548
+++	vxor	10,10,31
+++	.long	0x1294F548
+++	vxor	11,11,31
+++	.long	0x12B5F548
+++	vxor	12,12,31
+++
+++	.long	0x11CE2549
+++	.long	0x11EF0549
+++	.long	0x7C001E99
+++	.long	0x12100D49
+++	.long	0x7C281E99
+++	.long	0x12311549
+++	vperm	0,0,0,6
+++	.long	0x7C5A1E99
+++	.long	0x12521D49
+++	vperm	1,1,1,6
+++	.long	0x7C7B1E99
+++	.long	0x12735549
+++	vperm	2,2,2,6
+++	.long	0x7D5C1E99
+++	.long	0x12945D49
+++	vperm	3,3,3,6
+++	.long	0x7D7D1E99
+++	.long	0x12B56549
+++	vperm	10,10,10,6
+++	.long	0x7D9E1E99
+++	vor	4,13,13
+++	vperm	11,11,11,6
+++	.long	0x7DBF1E99
+++	addi	3,3,0x80
+++
+++	vperm	14,14,14,6
+++	vperm	15,15,15,6
+++	.long	0x7DC02799
+++	vperm	12,12,12,6
+++	vxor	14,0,23
+++	vperm	16,16,16,6
+++	.long	0x7DE82799
+++	vperm	13,13,13,6
+++	vxor	15,1,23
+++	vperm	17,17,17,6
+++	.long	0x7E1A2799
+++	vxor	16,2,23
+++	vperm	18,18,18,6
+++	.long	0x7E3B2799
+++	vxor	17,3,23
+++	vperm	19,19,19,6
+++	.long	0x7E5C2799
+++	vxor	18,10,23
+++	vperm	20,20,20,6
+++	.long	0x7E7D2799
+++	vxor	19,11,23
+++	vperm	21,21,21,6
+++	.long	0x7E9E2799
+++	vxor	20,12,23
+++	.long	0x7EBF2799
+++	addi	4,4,0x80
+++	vxor	21,13,23
+++
+++	mtctr	9
+++	beq	.Loop_cbc_dec8x
+++
+++	addic.	5,5,128
+++	beq	.Lcbc_dec8x_done
+++	nop	
+++	nop	
+++
+++.Loop_cbc_dec8x_tail:
+++	.long	0x11EFC548
+++	.long	0x1210C548
+++	.long	0x1231C548
+++	.long	0x1252C548
+++	.long	0x1273C548
+++	.long	0x1294C548
+++	.long	0x12B5C548
+++	lvx	24,26,11
+++	addi	11,11,0x20
+++
+++	.long	0x11EFCD48
+++	.long	0x1210CD48
+++	.long	0x1231CD48
+++	.long	0x1252CD48
+++	.long	0x1273CD48
+++	.long	0x1294CD48
+++	.long	0x12B5CD48
+++	lvx	25,8,11
+++	bdnz	.Loop_cbc_dec8x_tail
+++
+++	.long	0x11EFC548
+++	.long	0x1210C548
+++	.long	0x1231C548
+++	.long	0x1252C548
+++	.long	0x1273C548
+++	.long	0x1294C548
+++	.long	0x12B5C548
+++
+++	.long	0x11EFCD48
+++	.long	0x1210CD48
+++	.long	0x1231CD48
+++	.long	0x1252CD48
+++	.long	0x1273CD48
+++	.long	0x1294CD48
+++	.long	0x12B5CD48
+++
+++	.long	0x11EFD548
+++	.long	0x1210D548
+++	.long	0x1231D548
+++	.long	0x1252D548
+++	.long	0x1273D548
+++	.long	0x1294D548
+++	.long	0x12B5D548
+++
+++	.long	0x11EFDD48
+++	.long	0x1210DD48
+++	.long	0x1231DD48
+++	.long	0x1252DD48
+++	.long	0x1273DD48
+++	.long	0x1294DD48
+++	.long	0x12B5DD48
+++
+++	.long	0x11EFE548
+++	.long	0x1210E548
+++	.long	0x1231E548
+++	.long	0x1252E548
+++	.long	0x1273E548
+++	.long	0x1294E548
+++	.long	0x12B5E548
+++
+++	.long	0x11EFED48
+++	.long	0x1210ED48
+++	.long	0x1231ED48
+++	.long	0x1252ED48
+++	.long	0x1273ED48
+++	.long	0x1294ED48
+++	.long	0x12B5ED48
+++
+++	.long	0x11EFF548
+++	vxor	4,4,31
+++	.long	0x1210F548
+++	vxor	1,1,31
+++	.long	0x1231F548
+++	vxor	2,2,31
+++	.long	0x1252F548
+++	vxor	3,3,31
+++	.long	0x1273F548
+++	vxor	10,10,31
+++	.long	0x1294F548
+++	vxor	11,11,31
+++	.long	0x12B5F548
+++	vxor	12,12,31
+++
+++	cmplwi	5,32
+++	blt	.Lcbc_dec8x_one
+++	nop	
+++	beq	.Lcbc_dec8x_two
+++	cmplwi	5,64
+++	blt	.Lcbc_dec8x_three
+++	nop	
+++	beq	.Lcbc_dec8x_four
+++	cmplwi	5,96
+++	blt	.Lcbc_dec8x_five
+++	nop	
+++	beq	.Lcbc_dec8x_six
+++
+++.Lcbc_dec8x_seven:
+++	.long	0x11EF2549
+++	.long	0x12100D49
+++	.long	0x12311549
+++	.long	0x12521D49
+++	.long	0x12735549
+++	.long	0x12945D49
+++	.long	0x12B56549
+++	vor	4,13,13
+++
+++	vperm	15,15,15,6
+++	vperm	16,16,16,6
+++	.long	0x7DE02799
+++	vperm	17,17,17,6
+++	.long	0x7E082799
+++	vperm	18,18,18,6
+++	.long	0x7E3A2799
+++	vperm	19,19,19,6
+++	.long	0x7E5B2799
+++	vperm	20,20,20,6
+++	.long	0x7E7C2799
+++	vperm	21,21,21,6
+++	.long	0x7E9D2799
+++	.long	0x7EBE2799
+++	addi	4,4,0x70
+++	b	.Lcbc_dec8x_done
+++
+++.align	5
+++.Lcbc_dec8x_six:
+++	.long	0x12102549
+++	.long	0x12311549
+++	.long	0x12521D49
+++	.long	0x12735549
+++	.long	0x12945D49
+++	.long	0x12B56549
+++	vor	4,13,13
+++
+++	vperm	16,16,16,6
+++	vperm	17,17,17,6
+++	.long	0x7E002799
+++	vperm	18,18,18,6
+++	.long	0x7E282799
+++	vperm	19,19,19,6
+++	.long	0x7E5A2799
+++	vperm	20,20,20,6
+++	.long	0x7E7B2799
+++	vperm	21,21,21,6
+++	.long	0x7E9C2799
+++	.long	0x7EBD2799
+++	addi	4,4,0x60
+++	b	.Lcbc_dec8x_done
+++
+++.align	5
+++.Lcbc_dec8x_five:
+++	.long	0x12312549
+++	.long	0x12521D49
+++	.long	0x12735549
+++	.long	0x12945D49
+++	.long	0x12B56549
+++	vor	4,13,13
+++
+++	vperm	17,17,17,6
+++	vperm	18,18,18,6
+++	.long	0x7E202799
+++	vperm	19,19,19,6
+++	.long	0x7E482799
+++	vperm	20,20,20,6
+++	.long	0x7E7A2799
+++	vperm	21,21,21,6
+++	.long	0x7E9B2799
+++	.long	0x7EBC2799
+++	addi	4,4,0x50
+++	b	.Lcbc_dec8x_done
+++
+++.align	5
+++.Lcbc_dec8x_four:
+++	.long	0x12522549
+++	.long	0x12735549
+++	.long	0x12945D49
+++	.long	0x12B56549
+++	vor	4,13,13
+++
+++	vperm	18,18,18,6
+++	vperm	19,19,19,6
+++	.long	0x7E402799
+++	vperm	20,20,20,6
+++	.long	0x7E682799
+++	vperm	21,21,21,6
+++	.long	0x7E9A2799
+++	.long	0x7EBB2799
+++	addi	4,4,0x40
+++	b	.Lcbc_dec8x_done
+++
+++.align	5
+++.Lcbc_dec8x_three:
+++	.long	0x12732549
+++	.long	0x12945D49
+++	.long	0x12B56549
+++	vor	4,13,13
+++
+++	vperm	19,19,19,6
+++	vperm	20,20,20,6
+++	.long	0x7E602799
+++	vperm	21,21,21,6
+++	.long	0x7E882799
+++	.long	0x7EBA2799
+++	addi	4,4,0x30
+++	b	.Lcbc_dec8x_done
+++
+++.align	5
+++.Lcbc_dec8x_two:
+++	.long	0x12942549
+++	.long	0x12B56549
+++	vor	4,13,13
+++
+++	vperm	20,20,20,6
+++	vperm	21,21,21,6
+++	.long	0x7E802799
+++	.long	0x7EA82799
+++	addi	4,4,0x20
+++	b	.Lcbc_dec8x_done
+++
+++.align	5
+++.Lcbc_dec8x_one:
+++	.long	0x12B52549
+++	vor	4,13,13
+++
+++	vperm	21,21,21,6
+++	.long	0x7EA02799
+++	addi	4,4,0x10
+++
+++.Lcbc_dec8x_done:
+++	vperm	4,4,4,6
+++	.long	0x7C803F99
+++
+++	li	10,79
+++	li	11,95
+++	stvx	6,10,1
+++	addi	10,10,32
+++	stvx	6,11,1
+++	addi	11,11,32
+++	stvx	6,10,1
+++	addi	10,10,32
+++	stvx	6,11,1
+++	addi	11,11,32
+++	stvx	6,10,1
+++	addi	10,10,32
+++	stvx	6,11,1
+++	addi	11,11,32
+++	stvx	6,10,1
+++	addi	10,10,32
+++	stvx	6,11,1
+++	addi	11,11,32
+++
+++	or	12,12,12
+++	lvx	20,10,1
+++	addi	10,10,32
+++	lvx	21,11,1
+++	addi	11,11,32
+++	lvx	22,10,1
+++	addi	10,10,32
+++	lvx	23,11,1
+++	addi	11,11,32
+++	lvx	24,10,1
+++	addi	10,10,32
+++	lvx	25,11,1
+++	addi	11,11,32
+++	lvx	26,10,1
+++	addi	10,10,32
+++	lvx	27,11,1
+++	addi	11,11,32
+++	lvx	28,10,1
+++	addi	10,10,32
+++	lvx	29,11,1
+++	addi	11,11,32
+++	lvx	30,10,1
+++	lvx	31,11,1
+++	ld	26,400(1)
+++	ld	27,408(1)
+++	ld	28,416(1)
+++	ld	29,424(1)
+++	ld	30,432(1)
+++	ld	31,440(1)
+++	addi	1,1,448
+++	blr	
+++.long	0
+++.byte	0,12,0x04,0,0x80,6,6,0
+++.long	0
+++.size	aes_hw_cbc_encrypt,.-aes_hw_cbc_encrypt
+++.globl	aes_hw_ctr32_encrypt_blocks
+++.type	aes_hw_ctr32_encrypt_blocks,@function
+++.align	5
+++aes_hw_ctr32_encrypt_blocks:
+++.localentry	aes_hw_ctr32_encrypt_blocks,0
+++
+++	cmpldi	5,1
+++	.long	0x4dc00020
+++
+++	lis	0,0xfff0
+++	li	12,-1
+++	or	0,0,0
+++
+++	li	10,15
+++	vxor	0,0,0
+++	vspltisb	3,0x0f
+++
+++	lvx	4,0,7
+++	lvsl	6,0,7
+++	lvx	5,10,7
+++	vspltisb	11,1
+++	vxor	6,6,3
+++	vperm	4,4,5,6
+++	vsldoi	11,0,11,1
+++
+++	neg	11,3
+++	lvsr	10,0,6
+++	lwz	9,240(6)
+++
+++	lvsr	6,0,11
+++	lvx	5,0,3
+++	addi	3,3,15
+++	vxor	6,6,3
+++
+++	srwi	9,9,1
+++	li	10,16
+++	subi	9,9,1
+++
+++	cmpldi	5,8
+++	bge	_aesp8_ctr32_encrypt8x
+++
+++	lvsl	8,0,4
+++	vspltisb	9,-1
+++	lvx	7,0,4
+++	vperm	9,9,0,8
+++	vxor	8,8,3
+++
+++	lvx	0,0,6
+++	mtctr	9
+++	lvx	1,10,6
+++	addi	10,10,16
+++	vperm	0,1,0,10
+++	vxor	2,4,0
+++	lvx	0,10,6
+++	addi	10,10,16
+++	b	.Loop_ctr32_enc
+++
+++.align	5
+++.Loop_ctr32_enc:
+++	vperm	1,0,1,10
+++	.long	0x10420D08
+++	lvx	1,10,6
+++	addi	10,10,16
+++	vperm	0,1,0,10
+++	.long	0x10420508
+++	lvx	0,10,6
+++	addi	10,10,16
+++	bdnz	.Loop_ctr32_enc
+++
+++	vadduwm	4,4,11
+++	vor	3,5,5
+++	lvx	5,0,3
+++	addi	3,3,16
+++	subic.	5,5,1
+++
+++	vperm	1,0,1,10
+++	.long	0x10420D08
+++	lvx	1,10,6
+++	vperm	3,3,5,6
+++	li	10,16
+++	vperm	1,1,0,10
+++	lvx	0,0,6
+++	vxor	3,3,1
+++	.long	0x10421D09
+++
+++	lvx	1,10,6
+++	addi	10,10,16
+++	vperm	2,2,2,8
+++	vsel	3,7,2,9
+++	mtctr	9
+++	vperm	0,1,0,10
+++	vor	7,2,2
+++	vxor	2,4,0
+++	lvx	0,10,6
+++	addi	10,10,16
+++	stvx	3,0,4
+++	addi	4,4,16
+++	bne	.Loop_ctr32_enc
+++
+++	addi	4,4,-1
+++	lvx	2,0,4
+++	vsel	2,7,2,9
+++	stvx	2,0,4
+++
+++	or	12,12,12
+++	blr	
+++.long	0
+++.byte	0,12,0x14,0,0,0,6,0
+++.long	0
+++.align	5
+++_aesp8_ctr32_encrypt8x:
+++	stdu	1,-448(1)
+++	li	10,207
+++	li	11,223
+++	stvx	20,10,1
+++	addi	10,10,32
+++	stvx	21,11,1
+++	addi	11,11,32
+++	stvx	22,10,1
+++	addi	10,10,32
+++	stvx	23,11,1
+++	addi	11,11,32
+++	stvx	24,10,1
+++	addi	10,10,32
+++	stvx	25,11,1
+++	addi	11,11,32
+++	stvx	26,10,1
+++	addi	10,10,32
+++	stvx	27,11,1
+++	addi	11,11,32
+++	stvx	28,10,1
+++	addi	10,10,32
+++	stvx	29,11,1
+++	addi	11,11,32
+++	stvx	30,10,1
+++	stvx	31,11,1
+++	li	0,-1
+++	stw	12,396(1)
+++	li	8,0x10
+++	std	26,400(1)
+++	li	26,0x20
+++	std	27,408(1)
+++	li	27,0x30
+++	std	28,416(1)
+++	li	28,0x40
+++	std	29,424(1)
+++	li	29,0x50
+++	std	30,432(1)
+++	li	30,0x60
+++	std	31,440(1)
+++	li	31,0x70
+++	or	0,0,0
+++
+++	subi	9,9,3
+++
+++	lvx	23,0,6
+++	lvx	30,8,6
+++	addi	6,6,0x20
+++	lvx	31,0,6
+++	vperm	23,30,23,10
+++	addi	11,1,79
+++	mtctr	9
+++
+++.Load_ctr32_enc_key:
+++	vperm	24,31,30,10
+++	lvx	30,8,6
+++	addi	6,6,0x20
+++	stvx	24,0,11
+++	vperm	25,30,31,10
+++	lvx	31,0,6
+++	stvx	25,8,11
+++	addi	11,11,0x20
+++	bdnz	.Load_ctr32_enc_key
+++
+++	lvx	26,8,6
+++	vperm	24,31,30,10
+++	lvx	27,26,6
+++	stvx	24,0,11
+++	vperm	25,26,31,10
+++	lvx	28,27,6
+++	stvx	25,8,11
+++	addi	11,1,79
+++	vperm	26,27,26,10
+++	lvx	29,28,6
+++	vperm	27,28,27,10
+++	lvx	30,29,6
+++	vperm	28,29,28,10
+++	lvx	31,30,6
+++	vperm	29,30,29,10
+++	lvx	15,31,6
+++	vperm	30,31,30,10
+++	lvx	24,0,11
+++	vperm	31,15,31,10
+++	lvx	25,8,11
+++
+++	vadduwm	7,11,11
+++	subi	3,3,15
+++	sldi	5,5,4
+++
+++	vadduwm	16,4,11
+++	vadduwm	17,4,7
+++	vxor	15,4,23
+++	li	10,8
+++	vadduwm	18,16,7
+++	vxor	16,16,23
+++	lvsl	6,0,10
+++	vadduwm	19,17,7
+++	vxor	17,17,23
+++	vspltisb	3,0x0f
+++	vadduwm	20,18,7
+++	vxor	18,18,23
+++	vxor	6,6,3
+++	vadduwm	21,19,7
+++	vxor	19,19,23
+++	vadduwm	22,20,7
+++	vxor	20,20,23
+++	vadduwm	4,21,7
+++	vxor	21,21,23
+++	vxor	22,22,23
+++
+++	mtctr	9
+++	b	.Loop_ctr32_enc8x
+++.align	5
+++.Loop_ctr32_enc8x:
+++	.long	0x11EFC508
+++	.long	0x1210C508
+++	.long	0x1231C508
+++	.long	0x1252C508
+++	.long	0x1273C508
+++	.long	0x1294C508
+++	.long	0x12B5C508
+++	.long	0x12D6C508
+++.Loop_ctr32_enc8x_middle:
+++	lvx	24,26,11
+++	addi	11,11,0x20
+++
+++	.long	0x11EFCD08
+++	.long	0x1210CD08
+++	.long	0x1231CD08
+++	.long	0x1252CD08
+++	.long	0x1273CD08
+++	.long	0x1294CD08
+++	.long	0x12B5CD08
+++	.long	0x12D6CD08
+++	lvx	25,8,11
+++	bdnz	.Loop_ctr32_enc8x
+++
+++	subic	11,5,256
+++	.long	0x11EFC508
+++	.long	0x1210C508
+++	.long	0x1231C508
+++	.long	0x1252C508
+++	.long	0x1273C508
+++	.long	0x1294C508
+++	.long	0x12B5C508
+++	.long	0x12D6C508
+++
+++	subfe	0,0,0
+++	.long	0x11EFCD08
+++	.long	0x1210CD08
+++	.long	0x1231CD08
+++	.long	0x1252CD08
+++	.long	0x1273CD08
+++	.long	0x1294CD08
+++	.long	0x12B5CD08
+++	.long	0x12D6CD08
+++
+++	and	0,0,11
+++	addi	11,1,79
+++	.long	0x11EFD508
+++	.long	0x1210D508
+++	.long	0x1231D508
+++	.long	0x1252D508
+++	.long	0x1273D508
+++	.long	0x1294D508
+++	.long	0x12B5D508
+++	.long	0x12D6D508
+++	lvx	24,0,11
+++
+++	subic	5,5,129
+++	.long	0x11EFDD08
+++	addi	5,5,1
+++	.long	0x1210DD08
+++	.long	0x1231DD08
+++	.long	0x1252DD08
+++	.long	0x1273DD08
+++	.long	0x1294DD08
+++	.long	0x12B5DD08
+++	.long	0x12D6DD08
+++	lvx	25,8,11
+++
+++	.long	0x11EFE508
+++	.long	0x7C001E99
+++	.long	0x1210E508
+++	.long	0x7C281E99
+++	.long	0x1231E508
+++	.long	0x7C5A1E99
+++	.long	0x1252E508
+++	.long	0x7C7B1E99
+++	.long	0x1273E508
+++	.long	0x7D5C1E99
+++	.long	0x1294E508
+++	.long	0x7D9D1E99
+++	.long	0x12B5E508
+++	.long	0x7DBE1E99
+++	.long	0x12D6E508
+++	.long	0x7DDF1E99
+++	addi	3,3,0x80
+++
+++	.long	0x11EFED08
+++	vperm	0,0,0,6
+++	.long	0x1210ED08
+++	vperm	1,1,1,6
+++	.long	0x1231ED08
+++	vperm	2,2,2,6
+++	.long	0x1252ED08
+++	vperm	3,3,3,6
+++	.long	0x1273ED08
+++	vperm	10,10,10,6
+++	.long	0x1294ED08
+++	vperm	12,12,12,6
+++	.long	0x12B5ED08
+++	vperm	13,13,13,6
+++	.long	0x12D6ED08
+++	vperm	14,14,14,6
+++
+++	add	3,3,0
+++
+++
+++
+++	subfe.	0,0,0
+++	.long	0x11EFF508
+++	vxor	0,0,31
+++	.long	0x1210F508
+++	vxor	1,1,31
+++	.long	0x1231F508
+++	vxor	2,2,31
+++	.long	0x1252F508
+++	vxor	3,3,31
+++	.long	0x1273F508
+++	vxor	10,10,31
+++	.long	0x1294F508
+++	vxor	12,12,31
+++	.long	0x12B5F508
+++	vxor	13,13,31
+++	.long	0x12D6F508
+++	vxor	14,14,31
+++
+++	bne	.Lctr32_enc8x_break
+++
+++	.long	0x100F0509
+++	.long	0x10300D09
+++	vadduwm	16,4,11
+++	.long	0x10511509
+++	vadduwm	17,4,7
+++	vxor	15,4,23
+++	.long	0x10721D09
+++	vadduwm	18,16,7
+++	vxor	16,16,23
+++	.long	0x11535509
+++	vadduwm	19,17,7
+++	vxor	17,17,23
+++	.long	0x11946509
+++	vadduwm	20,18,7
+++	vxor	18,18,23
+++	.long	0x11B56D09
+++	vadduwm	21,19,7
+++	vxor	19,19,23
+++	.long	0x11D67509
+++	vadduwm	22,20,7
+++	vxor	20,20,23
+++	vperm	0,0,0,6
+++	vadduwm	4,21,7
+++	vxor	21,21,23
+++	vperm	1,1,1,6
+++	vxor	22,22,23
+++	mtctr	9
+++
+++	.long	0x11EFC508
+++	.long	0x7C002799
+++	vperm	2,2,2,6
+++	.long	0x1210C508
+++	.long	0x7C282799
+++	vperm	3,3,3,6
+++	.long	0x1231C508
+++	.long	0x7C5A2799
+++	vperm	10,10,10,6
+++	.long	0x1252C508
+++	.long	0x7C7B2799
+++	vperm	12,12,12,6
+++	.long	0x1273C508
+++	.long	0x7D5C2799
+++	vperm	13,13,13,6
+++	.long	0x1294C508
+++	.long	0x7D9D2799
+++	vperm	14,14,14,6
+++	.long	0x12B5C508
+++	.long	0x7DBE2799
+++	.long	0x12D6C508
+++	.long	0x7DDF2799
+++	addi	4,4,0x80
+++
+++	b	.Loop_ctr32_enc8x_middle
+++
+++.align	5
+++.Lctr32_enc8x_break:
+++	cmpwi	5,-0x60
+++	blt	.Lctr32_enc8x_one
+++	nop	
+++	beq	.Lctr32_enc8x_two
+++	cmpwi	5,-0x40
+++	blt	.Lctr32_enc8x_three
+++	nop	
+++	beq	.Lctr32_enc8x_four
+++	cmpwi	5,-0x20
+++	blt	.Lctr32_enc8x_five
+++	nop	
+++	beq	.Lctr32_enc8x_six
+++	cmpwi	5,0x00
+++	blt	.Lctr32_enc8x_seven
+++
+++.Lctr32_enc8x_eight:
+++	.long	0x11EF0509
+++	.long	0x12100D09
+++	.long	0x12311509
+++	.long	0x12521D09
+++	.long	0x12735509
+++	.long	0x12946509
+++	.long	0x12B56D09
+++	.long	0x12D67509
+++
+++	vperm	15,15,15,6
+++	vperm	16,16,16,6
+++	.long	0x7DE02799
+++	vperm	17,17,17,6
+++	.long	0x7E082799
+++	vperm	18,18,18,6
+++	.long	0x7E3A2799
+++	vperm	19,19,19,6
+++	.long	0x7E5B2799
+++	vperm	20,20,20,6
+++	.long	0x7E7C2799
+++	vperm	21,21,21,6
+++	.long	0x7E9D2799
+++	vperm	22,22,22,6
+++	.long	0x7EBE2799
+++	.long	0x7EDF2799
+++	addi	4,4,0x80
+++	b	.Lctr32_enc8x_done
+++
+++.align	5
+++.Lctr32_enc8x_seven:
+++	.long	0x11EF0D09
+++	.long	0x12101509
+++	.long	0x12311D09
+++	.long	0x12525509
+++	.long	0x12736509
+++	.long	0x12946D09
+++	.long	0x12B57509
+++
+++	vperm	15,15,15,6
+++	vperm	16,16,16,6
+++	.long	0x7DE02799
+++	vperm	17,17,17,6
+++	.long	0x7E082799
+++	vperm	18,18,18,6
+++	.long	0x7E3A2799
+++	vperm	19,19,19,6
+++	.long	0x7E5B2799
+++	vperm	20,20,20,6
+++	.long	0x7E7C2799
+++	vperm	21,21,21,6
+++	.long	0x7E9D2799
+++	.long	0x7EBE2799
+++	addi	4,4,0x70
+++	b	.Lctr32_enc8x_done
+++
+++.align	5
+++.Lctr32_enc8x_six:
+++	.long	0x11EF1509
+++	.long	0x12101D09
+++	.long	0x12315509
+++	.long	0x12526509
+++	.long	0x12736D09
+++	.long	0x12947509
+++
+++	vperm	15,15,15,6
+++	vperm	16,16,16,6
+++	.long	0x7DE02799
+++	vperm	17,17,17,6
+++	.long	0x7E082799
+++	vperm	18,18,18,6
+++	.long	0x7E3A2799
+++	vperm	19,19,19,6
+++	.long	0x7E5B2799
+++	vperm	20,20,20,6
+++	.long	0x7E7C2799
+++	.long	0x7E9D2799
+++	addi	4,4,0x60
+++	b	.Lctr32_enc8x_done
+++
+++.align	5
+++.Lctr32_enc8x_five:
+++	.long	0x11EF1D09
+++	.long	0x12105509
+++	.long	0x12316509
+++	.long	0x12526D09
+++	.long	0x12737509
+++
+++	vperm	15,15,15,6
+++	vperm	16,16,16,6
+++	.long	0x7DE02799
+++	vperm	17,17,17,6
+++	.long	0x7E082799
+++	vperm	18,18,18,6
+++	.long	0x7E3A2799
+++	vperm	19,19,19,6
+++	.long	0x7E5B2799
+++	.long	0x7E7C2799
+++	addi	4,4,0x50
+++	b	.Lctr32_enc8x_done
+++
+++.align	5
+++.Lctr32_enc8x_four:
+++	.long	0x11EF5509
+++	.long	0x12106509
+++	.long	0x12316D09
+++	.long	0x12527509
+++
+++	vperm	15,15,15,6
+++	vperm	16,16,16,6
+++	.long	0x7DE02799
+++	vperm	17,17,17,6
+++	.long	0x7E082799
+++	vperm	18,18,18,6
+++	.long	0x7E3A2799
+++	.long	0x7E5B2799
+++	addi	4,4,0x40
+++	b	.Lctr32_enc8x_done
+++
+++.align	5
+++.Lctr32_enc8x_three:
+++	.long	0x11EF6509
+++	.long	0x12106D09
+++	.long	0x12317509
+++
+++	vperm	15,15,15,6
+++	vperm	16,16,16,6
+++	.long	0x7DE02799
+++	vperm	17,17,17,6
+++	.long	0x7E082799
+++	.long	0x7E3A2799
+++	addi	4,4,0x30
+++	b	.Lctr32_enc8x_done
+++
+++.align	5
+++.Lctr32_enc8x_two:
+++	.long	0x11EF6D09
+++	.long	0x12107509
+++
+++	vperm	15,15,15,6
+++	vperm	16,16,16,6
+++	.long	0x7DE02799
+++	.long	0x7E082799
+++	addi	4,4,0x20
+++	b	.Lctr32_enc8x_done
+++
+++.align	5
+++.Lctr32_enc8x_one:
+++	.long	0x11EF7509
+++
+++	vperm	15,15,15,6
+++	.long	0x7DE02799
+++	addi	4,4,0x10
+++
+++.Lctr32_enc8x_done:
+++	li	10,79
+++	li	11,95
+++	stvx	6,10,1
+++	addi	10,10,32
+++	stvx	6,11,1
+++	addi	11,11,32
+++	stvx	6,10,1
+++	addi	10,10,32
+++	stvx	6,11,1
+++	addi	11,11,32
+++	stvx	6,10,1
+++	addi	10,10,32
+++	stvx	6,11,1
+++	addi	11,11,32
+++	stvx	6,10,1
+++	addi	10,10,32
+++	stvx	6,11,1
+++	addi	11,11,32
+++
+++	or	12,12,12
+++	lvx	20,10,1
+++	addi	10,10,32
+++	lvx	21,11,1
+++	addi	11,11,32
+++	lvx	22,10,1
+++	addi	10,10,32
+++	lvx	23,11,1
+++	addi	11,11,32
+++	lvx	24,10,1
+++	addi	10,10,32
+++	lvx	25,11,1
+++	addi	11,11,32
+++	lvx	26,10,1
+++	addi	10,10,32
+++	lvx	27,11,1
+++	addi	11,11,32
+++	lvx	28,10,1
+++	addi	10,10,32
+++	lvx	29,11,1
+++	addi	11,11,32
+++	lvx	30,10,1
+++	lvx	31,11,1
+++	ld	26,400(1)
+++	ld	27,408(1)
+++	ld	28,416(1)
+++	ld	29,424(1)
+++	ld	30,432(1)
+++	ld	31,440(1)
+++	addi	1,1,448
+++	blr	
+++.long	0
+++.byte	0,12,0x04,0,0x80,6,6,0
+++.long	0
+++.size	aes_hw_ctr32_encrypt_blocks,.-aes_hw_ctr32_encrypt_blocks
+++.globl	aes_hw_xts_encrypt
+++.type	aes_hw_xts_encrypt,@function
+++.align	5
+++aes_hw_xts_encrypt:
+++.localentry	aes_hw_xts_encrypt,0
+++
+++	mr	10,3
+++	li	3,-1
+++	cmpldi	5,16
+++	.long	0x4dc00020
+++
+++	lis	0,0xfff0
+++	li	12,-1
+++	li	11,0
+++	or	0,0,0
+++
+++	vspltisb	9,0x07
+++	lvsl	6,11,11
+++	vspltisb	11,0x0f
+++	vxor	6,6,9
+++
+++	li	3,15
+++	lvx	8,0,8
+++	lvsl	5,0,8
+++	lvx	4,3,8
+++	vxor	5,5,11
+++	vperm	8,8,4,5
+++
+++	neg	11,10
+++	lvsr	5,0,11
+++	lvx	2,0,10
+++	addi	10,10,15
+++	vxor	5,5,11
+++
+++	cmpldi	7,0
+++	beq	.Lxts_enc_no_key2
+++
+++	lvsr	7,0,7
+++	lwz	9,240(7)
+++	srwi	9,9,1
+++	subi	9,9,1
+++	li	3,16
+++
+++	lvx	0,0,7
+++	lvx	1,3,7
+++	addi	3,3,16
+++	vperm	0,1,0,7
+++	vxor	8,8,0
+++	lvx	0,3,7
+++	addi	3,3,16
+++	mtctr	9
+++
+++.Ltweak_xts_enc:
+++	vperm	1,0,1,7
+++	.long	0x11080D08
+++	lvx	1,3,7
+++	addi	3,3,16
+++	vperm	0,1,0,7
+++	.long	0x11080508
+++	lvx	0,3,7
+++	addi	3,3,16
+++	bdnz	.Ltweak_xts_enc
+++
+++	vperm	1,0,1,7
+++	.long	0x11080D08
+++	lvx	1,3,7
+++	vperm	0,1,0,7
+++	.long	0x11080509
+++
+++	li	8,0
+++	b	.Lxts_enc
+++
+++.Lxts_enc_no_key2:
+++	li	3,-16
+++	and	5,5,3
+++
+++
+++.Lxts_enc:
+++	lvx	4,0,10
+++	addi	10,10,16
+++
+++	lvsr	7,0,6
+++	lwz	9,240(6)
+++	srwi	9,9,1
+++	subi	9,9,1
+++	li	3,16
+++
+++	vslb	10,9,9
+++	vor	10,10,9
+++	vspltisb	11,1
+++	vsldoi	10,10,11,15
+++
+++	cmpldi	5,96
+++	bge	_aesp8_xts_encrypt6x
+++
+++	andi.	7,5,15
+++	subic	0,5,32
+++	subi	7,7,16
+++	subfe	0,0,0
+++	and	0,0,7
+++	add	10,10,0
+++
+++	lvx	0,0,6
+++	lvx	1,3,6
+++	addi	3,3,16
+++	vperm	2,2,4,5
+++	vperm	0,1,0,7
+++	vxor	2,2,8
+++	vxor	2,2,0
+++	lvx	0,3,6
+++	addi	3,3,16
+++	mtctr	9
+++	b	.Loop_xts_enc
+++
+++.align	5
+++.Loop_xts_enc:
+++	vperm	1,0,1,7
+++	.long	0x10420D08
+++	lvx	1,3,6
+++	addi	3,3,16
+++	vperm	0,1,0,7
+++	.long	0x10420508
+++	lvx	0,3,6
+++	addi	3,3,16
+++	bdnz	.Loop_xts_enc
+++
+++	vperm	1,0,1,7
+++	.long	0x10420D08
+++	lvx	1,3,6
+++	li	3,16
+++	vperm	0,1,0,7
+++	vxor	0,0,8
+++	.long	0x10620509
+++
+++	vperm	11,3,3,6
+++
+++	.long	0x7D602799
+++
+++	addi	4,4,16
+++
+++	subic.	5,5,16
+++	beq	.Lxts_enc_done
+++
+++	vor	2,4,4
+++	lvx	4,0,10
+++	addi	10,10,16
+++	lvx	0,0,6
+++	lvx	1,3,6
+++	addi	3,3,16
+++
+++	subic	0,5,32
+++	subfe	0,0,0
+++	and	0,0,7
+++	add	10,10,0
+++
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vand	11,11,10
+++	vxor	8,8,11
+++
+++	vperm	2,2,4,5
+++	vperm	0,1,0,7
+++	vxor	2,2,8
+++	vxor	3,3,0
+++	vxor	2,2,0
+++	lvx	0,3,6
+++	addi	3,3,16
+++
+++	mtctr	9
+++	cmpldi	5,16
+++	bge	.Loop_xts_enc
+++
+++	vxor	3,3,8
+++	lvsr	5,0,5
+++	vxor	4,4,4
+++	vspltisb	11,-1
+++	vperm	4,4,11,5
+++	vsel	2,2,3,4
+++
+++	subi	11,4,17
+++	subi	4,4,16
+++	mtctr	5
+++	li	5,16
+++.Loop_xts_enc_steal:
+++	lbzu	0,1(11)
+++	stb	0,16(11)
+++	bdnz	.Loop_xts_enc_steal
+++
+++	mtctr	9
+++	b	.Loop_xts_enc
+++
+++.Lxts_enc_done:
+++	cmpldi	8,0
+++	beq	.Lxts_enc_ret
+++
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vand	11,11,10
+++	vxor	8,8,11
+++
+++	vperm	8,8,8,6
+++	.long	0x7D004799
+++
+++.Lxts_enc_ret:
+++	or	12,12,12
+++	li	3,0
+++	blr	
+++.long	0
+++.byte	0,12,0x04,0,0x80,6,6,0
+++.long	0
+++.size	aes_hw_xts_encrypt,.-aes_hw_xts_encrypt
+++
+++.globl	aes_hw_xts_decrypt
+++.type	aes_hw_xts_decrypt,@function
+++.align	5
+++aes_hw_xts_decrypt:
+++.localentry	aes_hw_xts_decrypt,0
+++
+++	mr	10,3
+++	li	3,-1
+++	cmpldi	5,16
+++	.long	0x4dc00020
+++
+++	lis	0,0xfff8
+++	li	12,-1
+++	li	11,0
+++	or	0,0,0
+++
+++	andi.	0,5,15
+++	neg	0,0
+++	andi.	0,0,16
+++	sub	5,5,0
+++
+++	vspltisb	9,0x07
+++	lvsl	6,11,11
+++	vspltisb	11,0x0f
+++	vxor	6,6,9
+++
+++	li	3,15
+++	lvx	8,0,8
+++	lvsl	5,0,8
+++	lvx	4,3,8
+++	vxor	5,5,11
+++	vperm	8,8,4,5
+++
+++	neg	11,10
+++	lvsr	5,0,11
+++	lvx	2,0,10
+++	addi	10,10,15
+++	vxor	5,5,11
+++
+++	cmpldi	7,0
+++	beq	.Lxts_dec_no_key2
+++
+++	lvsr	7,0,7
+++	lwz	9,240(7)
+++	srwi	9,9,1
+++	subi	9,9,1
+++	li	3,16
+++
+++	lvx	0,0,7
+++	lvx	1,3,7
+++	addi	3,3,16
+++	vperm	0,1,0,7
+++	vxor	8,8,0
+++	lvx	0,3,7
+++	addi	3,3,16
+++	mtctr	9
+++
+++.Ltweak_xts_dec:
+++	vperm	1,0,1,7
+++	.long	0x11080D08
+++	lvx	1,3,7
+++	addi	3,3,16
+++	vperm	0,1,0,7
+++	.long	0x11080508
+++	lvx	0,3,7
+++	addi	3,3,16
+++	bdnz	.Ltweak_xts_dec
+++
+++	vperm	1,0,1,7
+++	.long	0x11080D08
+++	lvx	1,3,7
+++	vperm	0,1,0,7
+++	.long	0x11080509
+++
+++	li	8,0
+++	b	.Lxts_dec
+++
+++.Lxts_dec_no_key2:
+++	neg	3,5
+++	andi.	3,3,15
+++	add	5,5,3
+++
+++
+++.Lxts_dec:
+++	lvx	4,0,10
+++	addi	10,10,16
+++
+++	lvsr	7,0,6
+++	lwz	9,240(6)
+++	srwi	9,9,1
+++	subi	9,9,1
+++	li	3,16
+++
+++	vslb	10,9,9
+++	vor	10,10,9
+++	vspltisb	11,1
+++	vsldoi	10,10,11,15
+++
+++	cmpldi	5,96
+++	bge	_aesp8_xts_decrypt6x
+++
+++	lvx	0,0,6
+++	lvx	1,3,6
+++	addi	3,3,16
+++	vperm	2,2,4,5
+++	vperm	0,1,0,7
+++	vxor	2,2,8
+++	vxor	2,2,0
+++	lvx	0,3,6
+++	addi	3,3,16
+++	mtctr	9
+++
+++	cmpldi	5,16
+++	blt	.Ltail_xts_dec
+++
+++
+++.align	5
+++.Loop_xts_dec:
+++	vperm	1,0,1,7
+++	.long	0x10420D48
+++	lvx	1,3,6
+++	addi	3,3,16
+++	vperm	0,1,0,7
+++	.long	0x10420548
+++	lvx	0,3,6
+++	addi	3,3,16
+++	bdnz	.Loop_xts_dec
+++
+++	vperm	1,0,1,7
+++	.long	0x10420D48
+++	lvx	1,3,6
+++	li	3,16
+++	vperm	0,1,0,7
+++	vxor	0,0,8
+++	.long	0x10620549
+++
+++	vperm	11,3,3,6
+++
+++	.long	0x7D602799
+++
+++	addi	4,4,16
+++
+++	subic.	5,5,16
+++	beq	.Lxts_dec_done
+++
+++	vor	2,4,4
+++	lvx	4,0,10
+++	addi	10,10,16
+++	lvx	0,0,6
+++	lvx	1,3,6
+++	addi	3,3,16
+++
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vand	11,11,10
+++	vxor	8,8,11
+++
+++	vperm	2,2,4,5
+++	vperm	0,1,0,7
+++	vxor	2,2,8
+++	vxor	2,2,0
+++	lvx	0,3,6
+++	addi	3,3,16
+++
+++	mtctr	9
+++	cmpldi	5,16
+++	bge	.Loop_xts_dec
+++
+++.Ltail_xts_dec:
+++	vsrab	11,8,9
+++	vaddubm	12,8,8
+++	vsldoi	11,11,11,15
+++	vand	11,11,10
+++	vxor	12,12,11
+++
+++	subi	10,10,16
+++	add	10,10,5
+++
+++	vxor	2,2,8
+++	vxor	2,2,12
+++
+++.Loop_xts_dec_short:
+++	vperm	1,0,1,7
+++	.long	0x10420D48
+++	lvx	1,3,6
+++	addi	3,3,16
+++	vperm	0,1,0,7
+++	.long	0x10420548
+++	lvx	0,3,6
+++	addi	3,3,16
+++	bdnz	.Loop_xts_dec_short
+++
+++	vperm	1,0,1,7
+++	.long	0x10420D48
+++	lvx	1,3,6
+++	li	3,16
+++	vperm	0,1,0,7
+++	vxor	0,0,12
+++	.long	0x10620549
+++
+++	vperm	11,3,3,6
+++
+++	.long	0x7D602799
+++
+++
+++	vor	2,4,4
+++	lvx	4,0,10
+++
+++	lvx	0,0,6
+++	lvx	1,3,6
+++	addi	3,3,16
+++	vperm	2,2,4,5
+++	vperm	0,1,0,7
+++
+++	lvsr	5,0,5
+++	vxor	4,4,4
+++	vspltisb	11,-1
+++	vperm	4,4,11,5
+++	vsel	2,2,3,4
+++
+++	vxor	0,0,8
+++	vxor	2,2,0
+++	lvx	0,3,6
+++	addi	3,3,16
+++
+++	subi	11,4,1
+++	mtctr	5
+++	li	5,16
+++.Loop_xts_dec_steal:
+++	lbzu	0,1(11)
+++	stb	0,16(11)
+++	bdnz	.Loop_xts_dec_steal
+++
+++	mtctr	9
+++	b	.Loop_xts_dec
+++
+++.Lxts_dec_done:
+++	cmpldi	8,0
+++	beq	.Lxts_dec_ret
+++
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vand	11,11,10
+++	vxor	8,8,11
+++
+++	vperm	8,8,8,6
+++	.long	0x7D004799
+++
+++.Lxts_dec_ret:
+++	or	12,12,12
+++	li	3,0
+++	blr	
+++.long	0
+++.byte	0,12,0x04,0,0x80,6,6,0
+++.long	0
+++.size	aes_hw_xts_decrypt,.-aes_hw_xts_decrypt
+++.align	5
+++_aesp8_xts_encrypt6x:
+++	stdu	1,-448(1)
+++	mflr	11
+++	li	7,207
+++	li	3,223
+++	std	11,464(1)
+++	stvx	20,7,1
+++	addi	7,7,32
+++	stvx	21,3,1
+++	addi	3,3,32
+++	stvx	22,7,1
+++	addi	7,7,32
+++	stvx	23,3,1
+++	addi	3,3,32
+++	stvx	24,7,1
+++	addi	7,7,32
+++	stvx	25,3,1
+++	addi	3,3,32
+++	stvx	26,7,1
+++	addi	7,7,32
+++	stvx	27,3,1
+++	addi	3,3,32
+++	stvx	28,7,1
+++	addi	7,7,32
+++	stvx	29,3,1
+++	addi	3,3,32
+++	stvx	30,7,1
+++	stvx	31,3,1
+++	li	0,-1
+++	stw	12,396(1)
+++	li	3,0x10
+++	std	26,400(1)
+++	li	26,0x20
+++	std	27,408(1)
+++	li	27,0x30
+++	std	28,416(1)
+++	li	28,0x40
+++	std	29,424(1)
+++	li	29,0x50
+++	std	30,432(1)
+++	li	30,0x60
+++	std	31,440(1)
+++	li	31,0x70
+++	or	0,0,0
+++
+++	subi	9,9,3
+++
+++	lvx	23,0,6
+++	lvx	30,3,6
+++	addi	6,6,0x20
+++	lvx	31,0,6
+++	vperm	23,30,23,7
+++	addi	7,1,79
+++	mtctr	9
+++
+++.Load_xts_enc_key:
+++	vperm	24,31,30,7
+++	lvx	30,3,6
+++	addi	6,6,0x20
+++	stvx	24,0,7
+++	vperm	25,30,31,7
+++	lvx	31,0,6
+++	stvx	25,3,7
+++	addi	7,7,0x20
+++	bdnz	.Load_xts_enc_key
+++
+++	lvx	26,3,6
+++	vperm	24,31,30,7
+++	lvx	27,26,6
+++	stvx	24,0,7
+++	vperm	25,26,31,7
+++	lvx	28,27,6
+++	stvx	25,3,7
+++	addi	7,1,79
+++	vperm	26,27,26,7
+++	lvx	29,28,6
+++	vperm	27,28,27,7
+++	lvx	30,29,6
+++	vperm	28,29,28,7
+++	lvx	31,30,6
+++	vperm	29,30,29,7
+++	lvx	22,31,6
+++	vperm	30,31,30,7
+++	lvx	24,0,7
+++	vperm	31,22,31,7
+++	lvx	25,3,7
+++
+++	vperm	0,2,4,5
+++	subi	10,10,31
+++	vxor	17,8,23
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vand	11,11,10
+++	vxor	7,0,17
+++	vxor	8,8,11
+++
+++	.long	0x7C235699
+++	vxor	18,8,23
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vperm	1,1,1,6
+++	vand	11,11,10
+++	vxor	12,1,18
+++	vxor	8,8,11
+++
+++	.long	0x7C5A5699
+++	andi.	31,5,15
+++	vxor	19,8,23
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vperm	2,2,2,6
+++	vand	11,11,10
+++	vxor	13,2,19
+++	vxor	8,8,11
+++
+++	.long	0x7C7B5699
+++	sub	5,5,31
+++	vxor	20,8,23
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vperm	3,3,3,6
+++	vand	11,11,10
+++	vxor	14,3,20
+++	vxor	8,8,11
+++
+++	.long	0x7C9C5699
+++	subi	5,5,0x60
+++	vxor	21,8,23
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vperm	4,4,4,6
+++	vand	11,11,10
+++	vxor	15,4,21
+++	vxor	8,8,11
+++
+++	.long	0x7CBD5699
+++	addi	10,10,0x60
+++	vxor	22,8,23
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vperm	5,5,5,6
+++	vand	11,11,10
+++	vxor	16,5,22
+++	vxor	8,8,11
+++
+++	vxor	31,31,23
+++	mtctr	9
+++	b	.Loop_xts_enc6x
+++
+++.align	5
+++.Loop_xts_enc6x:
+++	.long	0x10E7C508
+++	.long	0x118CC508
+++	.long	0x11ADC508
+++	.long	0x11CEC508
+++	.long	0x11EFC508
+++	.long	0x1210C508
+++	lvx	24,26,7
+++	addi	7,7,0x20
+++
+++	.long	0x10E7CD08
+++	.long	0x118CCD08
+++	.long	0x11ADCD08
+++	.long	0x11CECD08
+++	.long	0x11EFCD08
+++	.long	0x1210CD08
+++	lvx	25,3,7
+++	bdnz	.Loop_xts_enc6x
+++
+++	subic	5,5,96
+++	vxor	0,17,31
+++	.long	0x10E7C508
+++	.long	0x118CC508
+++	vsrab	11,8,9
+++	vxor	17,8,23
+++	vaddubm	8,8,8
+++	.long	0x11ADC508
+++	.long	0x11CEC508
+++	vsldoi	11,11,11,15
+++	.long	0x11EFC508
+++	.long	0x1210C508
+++
+++	subfe.	0,0,0
+++	vand	11,11,10
+++	.long	0x10E7CD08
+++	.long	0x118CCD08
+++	vxor	8,8,11
+++	.long	0x11ADCD08
+++	.long	0x11CECD08
+++	vxor	1,18,31
+++	vsrab	11,8,9
+++	vxor	18,8,23
+++	.long	0x11EFCD08
+++	.long	0x1210CD08
+++
+++	and	0,0,5
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	.long	0x10E7D508
+++	.long	0x118CD508
+++	vand	11,11,10
+++	.long	0x11ADD508
+++	.long	0x11CED508
+++	vxor	8,8,11
+++	.long	0x11EFD508
+++	.long	0x1210D508
+++
+++	add	10,10,0
+++
+++
+++
+++	vxor	2,19,31
+++	vsrab	11,8,9
+++	vxor	19,8,23
+++	vaddubm	8,8,8
+++	.long	0x10E7DD08
+++	.long	0x118CDD08
+++	vsldoi	11,11,11,15
+++	.long	0x11ADDD08
+++	.long	0x11CEDD08
+++	vand	11,11,10
+++	.long	0x11EFDD08
+++	.long	0x1210DD08
+++
+++	addi	7,1,79
+++	vxor	8,8,11
+++	.long	0x10E7E508
+++	.long	0x118CE508
+++	vxor	3,20,31
+++	vsrab	11,8,9
+++	vxor	20,8,23
+++	.long	0x11ADE508
+++	.long	0x11CEE508
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	.long	0x11EFE508
+++	.long	0x1210E508
+++	lvx	24,0,7
+++	vand	11,11,10
+++
+++	.long	0x10E7ED08
+++	.long	0x118CED08
+++	vxor	8,8,11
+++	.long	0x11ADED08
+++	.long	0x11CEED08
+++	vxor	4,21,31
+++	vsrab	11,8,9
+++	vxor	21,8,23
+++	.long	0x11EFED08
+++	.long	0x1210ED08
+++	lvx	25,3,7
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++
+++	.long	0x10E7F508
+++	.long	0x118CF508
+++	vand	11,11,10
+++	.long	0x11ADF508
+++	.long	0x11CEF508
+++	vxor	8,8,11
+++	.long	0x11EFF508
+++	.long	0x1210F508
+++	vxor	5,22,31
+++	vsrab	11,8,9
+++	vxor	22,8,23
+++
+++	.long	0x10E70509
+++	.long	0x7C005699
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	.long	0x118C0D09
+++	.long	0x7C235699
+++	.long	0x11AD1509
+++	vperm	0,0,0,6
+++	.long	0x7C5A5699
+++	vand	11,11,10
+++	.long	0x11CE1D09
+++	vperm	1,1,1,6
+++	.long	0x7C7B5699
+++	.long	0x11EF2509
+++	vperm	2,2,2,6
+++	.long	0x7C9C5699
+++	vxor	8,8,11
+++	.long	0x11702D09
+++
+++	vperm	3,3,3,6
+++	.long	0x7CBD5699
+++	addi	10,10,0x60
+++	vperm	4,4,4,6
+++	vperm	5,5,5,6
+++
+++	vperm	7,7,7,6
+++	vperm	12,12,12,6
+++	.long	0x7CE02799
+++	vxor	7,0,17
+++	vperm	13,13,13,6
+++	.long	0x7D832799
+++	vxor	12,1,18
+++	vperm	14,14,14,6
+++	.long	0x7DBA2799
+++	vxor	13,2,19
+++	vperm	15,15,15,6
+++	.long	0x7DDB2799
+++	vxor	14,3,20
+++	vperm	16,11,11,6
+++	.long	0x7DFC2799
+++	vxor	15,4,21
+++	.long	0x7E1D2799
+++
+++	vxor	16,5,22
+++	addi	4,4,0x60
+++
+++	mtctr	9
+++	beq	.Loop_xts_enc6x
+++
+++	addic.	5,5,0x60
+++	beq	.Lxts_enc6x_zero
+++	cmpwi	5,0x20
+++	blt	.Lxts_enc6x_one
+++	nop	
+++	beq	.Lxts_enc6x_two
+++	cmpwi	5,0x40
+++	blt	.Lxts_enc6x_three
+++	nop	
+++	beq	.Lxts_enc6x_four
+++
+++.Lxts_enc6x_five:
+++	vxor	7,1,17
+++	vxor	12,2,18
+++	vxor	13,3,19
+++	vxor	14,4,20
+++	vxor	15,5,21
+++
+++	bl	_aesp8_xts_enc5x
+++
+++	vperm	7,7,7,6
+++	vor	17,22,22
+++	vperm	12,12,12,6
+++	.long	0x7CE02799
+++	vperm	13,13,13,6
+++	.long	0x7D832799
+++	vperm	14,14,14,6
+++	.long	0x7DBA2799
+++	vxor	11,15,22
+++	vperm	15,15,15,6
+++	.long	0x7DDB2799
+++	.long	0x7DFC2799
+++	addi	4,4,0x50
+++	bne	.Lxts_enc6x_steal
+++	b	.Lxts_enc6x_done
+++
+++.align	4
+++.Lxts_enc6x_four:
+++	vxor	7,2,17
+++	vxor	12,3,18
+++	vxor	13,4,19
+++	vxor	14,5,20
+++	vxor	15,15,15
+++
+++	bl	_aesp8_xts_enc5x
+++
+++	vperm	7,7,7,6
+++	vor	17,21,21
+++	vperm	12,12,12,6
+++	.long	0x7CE02799
+++	vperm	13,13,13,6
+++	.long	0x7D832799
+++	vxor	11,14,21
+++	vperm	14,14,14,6
+++	.long	0x7DBA2799
+++	.long	0x7DDB2799
+++	addi	4,4,0x40
+++	bne	.Lxts_enc6x_steal
+++	b	.Lxts_enc6x_done
+++
+++.align	4
+++.Lxts_enc6x_three:
+++	vxor	7,3,17
+++	vxor	12,4,18
+++	vxor	13,5,19
+++	vxor	14,14,14
+++	vxor	15,15,15
+++
+++	bl	_aesp8_xts_enc5x
+++
+++	vperm	7,7,7,6
+++	vor	17,20,20
+++	vperm	12,12,12,6
+++	.long	0x7CE02799
+++	vxor	11,13,20
+++	vperm	13,13,13,6
+++	.long	0x7D832799
+++	.long	0x7DBA2799
+++	addi	4,4,0x30
+++	bne	.Lxts_enc6x_steal
+++	b	.Lxts_enc6x_done
+++
+++.align	4
+++.Lxts_enc6x_two:
+++	vxor	7,4,17
+++	vxor	12,5,18
+++	vxor	13,13,13
+++	vxor	14,14,14
+++	vxor	15,15,15
+++
+++	bl	_aesp8_xts_enc5x
+++
+++	vperm	7,7,7,6
+++	vor	17,19,19
+++	vxor	11,12,19
+++	vperm	12,12,12,6
+++	.long	0x7CE02799
+++	.long	0x7D832799
+++	addi	4,4,0x20
+++	bne	.Lxts_enc6x_steal
+++	b	.Lxts_enc6x_done
+++
+++.align	4
+++.Lxts_enc6x_one:
+++	vxor	7,5,17
+++	nop	
+++.Loop_xts_enc1x:
+++	.long	0x10E7C508
+++	lvx	24,26,7
+++	addi	7,7,0x20
+++
+++	.long	0x10E7CD08
+++	lvx	25,3,7
+++	bdnz	.Loop_xts_enc1x
+++
+++	add	10,10,31
+++	cmpwi	31,0
+++	.long	0x10E7C508
+++
+++	subi	10,10,16
+++	.long	0x10E7CD08
+++
+++	lvsr	5,0,31
+++	.long	0x10E7D508
+++
+++	.long	0x7C005699
+++	.long	0x10E7DD08
+++
+++	addi	7,1,79
+++	.long	0x10E7E508
+++	lvx	24,0,7
+++
+++	.long	0x10E7ED08
+++	lvx	25,3,7
+++	vxor	17,17,31
+++
+++	vperm	0,0,0,6
+++	.long	0x10E7F508
+++
+++	vperm	0,0,0,5
+++	.long	0x10E78D09
+++
+++	vor	17,18,18
+++	vxor	11,7,18
+++	vperm	7,7,7,6
+++	.long	0x7CE02799
+++	addi	4,4,0x10
+++	bne	.Lxts_enc6x_steal
+++	b	.Lxts_enc6x_done
+++
+++.align	4
+++.Lxts_enc6x_zero:
+++	cmpwi	31,0
+++	beq	.Lxts_enc6x_done
+++
+++	add	10,10,31
+++	subi	10,10,16
+++	.long	0x7C005699
+++	lvsr	5,0,31
+++	vperm	0,0,0,6
+++	vperm	0,0,0,5
+++	vxor	11,11,17
+++.Lxts_enc6x_steal:
+++	vxor	0,0,17
+++	vxor	7,7,7
+++	vspltisb	12,-1
+++	vperm	7,7,12,5
+++	vsel	7,0,11,7
+++
+++	subi	30,4,17
+++	subi	4,4,16
+++	mtctr	31
+++.Loop_xts_enc6x_steal:
+++	lbzu	0,1(30)
+++	stb	0,16(30)
+++	bdnz	.Loop_xts_enc6x_steal
+++
+++	li	31,0
+++	mtctr	9
+++	b	.Loop_xts_enc1x
+++
+++.align	4
+++.Lxts_enc6x_done:
+++	cmpldi	8,0
+++	beq	.Lxts_enc6x_ret
+++
+++	vxor	8,17,23
+++	vperm	8,8,8,6
+++	.long	0x7D004799
+++
+++.Lxts_enc6x_ret:
+++	mtlr	11
+++	li	10,79
+++	li	11,95
+++	stvx	9,10,1
+++	addi	10,10,32
+++	stvx	9,11,1
+++	addi	11,11,32
+++	stvx	9,10,1
+++	addi	10,10,32
+++	stvx	9,11,1
+++	addi	11,11,32
+++	stvx	9,10,1
+++	addi	10,10,32
+++	stvx	9,11,1
+++	addi	11,11,32
+++	stvx	9,10,1
+++	addi	10,10,32
+++	stvx	9,11,1
+++	addi	11,11,32
+++
+++	or	12,12,12
+++	lvx	20,10,1
+++	addi	10,10,32
+++	lvx	21,11,1
+++	addi	11,11,32
+++	lvx	22,10,1
+++	addi	10,10,32
+++	lvx	23,11,1
+++	addi	11,11,32
+++	lvx	24,10,1
+++	addi	10,10,32
+++	lvx	25,11,1
+++	addi	11,11,32
+++	lvx	26,10,1
+++	addi	10,10,32
+++	lvx	27,11,1
+++	addi	11,11,32
+++	lvx	28,10,1
+++	addi	10,10,32
+++	lvx	29,11,1
+++	addi	11,11,32
+++	lvx	30,10,1
+++	lvx	31,11,1
+++	ld	26,400(1)
+++	ld	27,408(1)
+++	ld	28,416(1)
+++	ld	29,424(1)
+++	ld	30,432(1)
+++	ld	31,440(1)
+++	addi	1,1,448
+++	blr	
+++.long	0
+++.byte	0,12,0x04,1,0x80,6,6,0
+++.long	0
+++
+++.align	5
+++_aesp8_xts_enc5x:
+++	.long	0x10E7C508
+++	.long	0x118CC508
+++	.long	0x11ADC508
+++	.long	0x11CEC508
+++	.long	0x11EFC508
+++	lvx	24,26,7
+++	addi	7,7,0x20
+++
+++	.long	0x10E7CD08
+++	.long	0x118CCD08
+++	.long	0x11ADCD08
+++	.long	0x11CECD08
+++	.long	0x11EFCD08
+++	lvx	25,3,7
+++	bdnz	_aesp8_xts_enc5x
+++
+++	add	10,10,31
+++	cmpwi	31,0
+++	.long	0x10E7C508
+++	.long	0x118CC508
+++	.long	0x11ADC508
+++	.long	0x11CEC508
+++	.long	0x11EFC508
+++
+++	subi	10,10,16
+++	.long	0x10E7CD08
+++	.long	0x118CCD08
+++	.long	0x11ADCD08
+++	.long	0x11CECD08
+++	.long	0x11EFCD08
+++	vxor	17,17,31
+++
+++	.long	0x10E7D508
+++	lvsr	5,0,31
+++	.long	0x118CD508
+++	.long	0x11ADD508
+++	.long	0x11CED508
+++	.long	0x11EFD508
+++	vxor	1,18,31
+++
+++	.long	0x10E7DD08
+++	.long	0x7C005699
+++	.long	0x118CDD08
+++	.long	0x11ADDD08
+++	.long	0x11CEDD08
+++	.long	0x11EFDD08
+++	vxor	2,19,31
+++
+++	addi	7,1,79
+++	.long	0x10E7E508
+++	.long	0x118CE508
+++	.long	0x11ADE508
+++	.long	0x11CEE508
+++	.long	0x11EFE508
+++	lvx	24,0,7
+++	vxor	3,20,31
+++
+++	.long	0x10E7ED08
+++	vperm	0,0,0,6
+++	.long	0x118CED08
+++	.long	0x11ADED08
+++	.long	0x11CEED08
+++	.long	0x11EFED08
+++	lvx	25,3,7
+++	vxor	4,21,31
+++
+++	.long	0x10E7F508
+++	vperm	0,0,0,5
+++	.long	0x118CF508
+++	.long	0x11ADF508
+++	.long	0x11CEF508
+++	.long	0x11EFF508
+++
+++	.long	0x10E78D09
+++	.long	0x118C0D09
+++	.long	0x11AD1509
+++	.long	0x11CE1D09
+++	.long	0x11EF2509
+++	blr	
+++.long	0
+++.byte	0,12,0x14,0,0,0,0,0
+++
+++.align	5
+++_aesp8_xts_decrypt6x:
+++	stdu	1,-448(1)
+++	mflr	11
+++	li	7,207
+++	li	3,223
+++	std	11,464(1)
+++	stvx	20,7,1
+++	addi	7,7,32
+++	stvx	21,3,1
+++	addi	3,3,32
+++	stvx	22,7,1
+++	addi	7,7,32
+++	stvx	23,3,1
+++	addi	3,3,32
+++	stvx	24,7,1
+++	addi	7,7,32
+++	stvx	25,3,1
+++	addi	3,3,32
+++	stvx	26,7,1
+++	addi	7,7,32
+++	stvx	27,3,1
+++	addi	3,3,32
+++	stvx	28,7,1
+++	addi	7,7,32
+++	stvx	29,3,1
+++	addi	3,3,32
+++	stvx	30,7,1
+++	stvx	31,3,1
+++	li	0,-1
+++	stw	12,396(1)
+++	li	3,0x10
+++	std	26,400(1)
+++	li	26,0x20
+++	std	27,408(1)
+++	li	27,0x30
+++	std	28,416(1)
+++	li	28,0x40
+++	std	29,424(1)
+++	li	29,0x50
+++	std	30,432(1)
+++	li	30,0x60
+++	std	31,440(1)
+++	li	31,0x70
+++	or	0,0,0
+++
+++	subi	9,9,3
+++
+++	lvx	23,0,6
+++	lvx	30,3,6
+++	addi	6,6,0x20
+++	lvx	31,0,6
+++	vperm	23,30,23,7
+++	addi	7,1,79
+++	mtctr	9
+++
+++.Load_xts_dec_key:
+++	vperm	24,31,30,7
+++	lvx	30,3,6
+++	addi	6,6,0x20
+++	stvx	24,0,7
+++	vperm	25,30,31,7
+++	lvx	31,0,6
+++	stvx	25,3,7
+++	addi	7,7,0x20
+++	bdnz	.Load_xts_dec_key
+++
+++	lvx	26,3,6
+++	vperm	24,31,30,7
+++	lvx	27,26,6
+++	stvx	24,0,7
+++	vperm	25,26,31,7
+++	lvx	28,27,6
+++	stvx	25,3,7
+++	addi	7,1,79
+++	vperm	26,27,26,7
+++	lvx	29,28,6
+++	vperm	27,28,27,7
+++	lvx	30,29,6
+++	vperm	28,29,28,7
+++	lvx	31,30,6
+++	vperm	29,30,29,7
+++	lvx	22,31,6
+++	vperm	30,31,30,7
+++	lvx	24,0,7
+++	vperm	31,22,31,7
+++	lvx	25,3,7
+++
+++	vperm	0,2,4,5
+++	subi	10,10,31
+++	vxor	17,8,23
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vand	11,11,10
+++	vxor	7,0,17
+++	vxor	8,8,11
+++
+++	.long	0x7C235699
+++	vxor	18,8,23
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vperm	1,1,1,6
+++	vand	11,11,10
+++	vxor	12,1,18
+++	vxor	8,8,11
+++
+++	.long	0x7C5A5699
+++	andi.	31,5,15
+++	vxor	19,8,23
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vperm	2,2,2,6
+++	vand	11,11,10
+++	vxor	13,2,19
+++	vxor	8,8,11
+++
+++	.long	0x7C7B5699
+++	sub	5,5,31
+++	vxor	20,8,23
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vperm	3,3,3,6
+++	vand	11,11,10
+++	vxor	14,3,20
+++	vxor	8,8,11
+++
+++	.long	0x7C9C5699
+++	subi	5,5,0x60
+++	vxor	21,8,23
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vperm	4,4,4,6
+++	vand	11,11,10
+++	vxor	15,4,21
+++	vxor	8,8,11
+++
+++	.long	0x7CBD5699
+++	addi	10,10,0x60
+++	vxor	22,8,23
+++	vsrab	11,8,9
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	vperm	5,5,5,6
+++	vand	11,11,10
+++	vxor	16,5,22
+++	vxor	8,8,11
+++
+++	vxor	31,31,23
+++	mtctr	9
+++	b	.Loop_xts_dec6x
+++
+++.align	5
+++.Loop_xts_dec6x:
+++	.long	0x10E7C548
+++	.long	0x118CC548
+++	.long	0x11ADC548
+++	.long	0x11CEC548
+++	.long	0x11EFC548
+++	.long	0x1210C548
+++	lvx	24,26,7
+++	addi	7,7,0x20
+++
+++	.long	0x10E7CD48
+++	.long	0x118CCD48
+++	.long	0x11ADCD48
+++	.long	0x11CECD48
+++	.long	0x11EFCD48
+++	.long	0x1210CD48
+++	lvx	25,3,7
+++	bdnz	.Loop_xts_dec6x
+++
+++	subic	5,5,96
+++	vxor	0,17,31
+++	.long	0x10E7C548
+++	.long	0x118CC548
+++	vsrab	11,8,9
+++	vxor	17,8,23
+++	vaddubm	8,8,8
+++	.long	0x11ADC548
+++	.long	0x11CEC548
+++	vsldoi	11,11,11,15
+++	.long	0x11EFC548
+++	.long	0x1210C548
+++
+++	subfe.	0,0,0
+++	vand	11,11,10
+++	.long	0x10E7CD48
+++	.long	0x118CCD48
+++	vxor	8,8,11
+++	.long	0x11ADCD48
+++	.long	0x11CECD48
+++	vxor	1,18,31
+++	vsrab	11,8,9
+++	vxor	18,8,23
+++	.long	0x11EFCD48
+++	.long	0x1210CD48
+++
+++	and	0,0,5
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	.long	0x10E7D548
+++	.long	0x118CD548
+++	vand	11,11,10
+++	.long	0x11ADD548
+++	.long	0x11CED548
+++	vxor	8,8,11
+++	.long	0x11EFD548
+++	.long	0x1210D548
+++
+++	add	10,10,0
+++
+++
+++
+++	vxor	2,19,31
+++	vsrab	11,8,9
+++	vxor	19,8,23
+++	vaddubm	8,8,8
+++	.long	0x10E7DD48
+++	.long	0x118CDD48
+++	vsldoi	11,11,11,15
+++	.long	0x11ADDD48
+++	.long	0x11CEDD48
+++	vand	11,11,10
+++	.long	0x11EFDD48
+++	.long	0x1210DD48
+++
+++	addi	7,1,79
+++	vxor	8,8,11
+++	.long	0x10E7E548
+++	.long	0x118CE548
+++	vxor	3,20,31
+++	vsrab	11,8,9
+++	vxor	20,8,23
+++	.long	0x11ADE548
+++	.long	0x11CEE548
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	.long	0x11EFE548
+++	.long	0x1210E548
+++	lvx	24,0,7
+++	vand	11,11,10
+++
+++	.long	0x10E7ED48
+++	.long	0x118CED48
+++	vxor	8,8,11
+++	.long	0x11ADED48
+++	.long	0x11CEED48
+++	vxor	4,21,31
+++	vsrab	11,8,9
+++	vxor	21,8,23
+++	.long	0x11EFED48
+++	.long	0x1210ED48
+++	lvx	25,3,7
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++
+++	.long	0x10E7F548
+++	.long	0x118CF548
+++	vand	11,11,10
+++	.long	0x11ADF548
+++	.long	0x11CEF548
+++	vxor	8,8,11
+++	.long	0x11EFF548
+++	.long	0x1210F548
+++	vxor	5,22,31
+++	vsrab	11,8,9
+++	vxor	22,8,23
+++
+++	.long	0x10E70549
+++	.long	0x7C005699
+++	vaddubm	8,8,8
+++	vsldoi	11,11,11,15
+++	.long	0x118C0D49
+++	.long	0x7C235699
+++	.long	0x11AD1549
+++	vperm	0,0,0,6
+++	.long	0x7C5A5699
+++	vand	11,11,10
+++	.long	0x11CE1D49
+++	vperm	1,1,1,6
+++	.long	0x7C7B5699
+++	.long	0x11EF2549
+++	vperm	2,2,2,6
+++	.long	0x7C9C5699
+++	vxor	8,8,11
+++	.long	0x12102D49
+++	vperm	3,3,3,6
+++	.long	0x7CBD5699
+++	addi	10,10,0x60
+++	vperm	4,4,4,6
+++	vperm	5,5,5,6
+++
+++	vperm	7,7,7,6
+++	vperm	12,12,12,6
+++	.long	0x7CE02799
+++	vxor	7,0,17
+++	vperm	13,13,13,6
+++	.long	0x7D832799
+++	vxor	12,1,18
+++	vperm	14,14,14,6
+++	.long	0x7DBA2799
+++	vxor	13,2,19
+++	vperm	15,15,15,6
+++	.long	0x7DDB2799
+++	vxor	14,3,20
+++	vperm	16,16,16,6
+++	.long	0x7DFC2799
+++	vxor	15,4,21
+++	.long	0x7E1D2799
+++	vxor	16,5,22
+++	addi	4,4,0x60
+++
+++	mtctr	9
+++	beq	.Loop_xts_dec6x
+++
+++	addic.	5,5,0x60
+++	beq	.Lxts_dec6x_zero
+++	cmpwi	5,0x20
+++	blt	.Lxts_dec6x_one
+++	nop	
+++	beq	.Lxts_dec6x_two
+++	cmpwi	5,0x40
+++	blt	.Lxts_dec6x_three
+++	nop	
+++	beq	.Lxts_dec6x_four
+++
+++.Lxts_dec6x_five:
+++	vxor	7,1,17
+++	vxor	12,2,18
+++	vxor	13,3,19
+++	vxor	14,4,20
+++	vxor	15,5,21
+++
+++	bl	_aesp8_xts_dec5x
+++
+++	vperm	7,7,7,6
+++	vor	17,22,22
+++	vxor	18,8,23
+++	vperm	12,12,12,6
+++	.long	0x7CE02799
+++	vxor	7,0,18
+++	vperm	13,13,13,6
+++	.long	0x7D832799
+++	vperm	14,14,14,6
+++	.long	0x7DBA2799
+++	vperm	15,15,15,6
+++	.long	0x7DDB2799
+++	.long	0x7DFC2799
+++	addi	4,4,0x50
+++	bne	.Lxts_dec6x_steal
+++	b	.Lxts_dec6x_done
+++
+++.align	4
+++.Lxts_dec6x_four:
+++	vxor	7,2,17
+++	vxor	12,3,18
+++	vxor	13,4,19
+++	vxor	14,5,20
+++	vxor	15,15,15
+++
+++	bl	_aesp8_xts_dec5x
+++
+++	vperm	7,7,7,6
+++	vor	17,21,21
+++	vor	18,22,22
+++	vperm	12,12,12,6
+++	.long	0x7CE02799
+++	vxor	7,0,22
+++	vperm	13,13,13,6
+++	.long	0x7D832799
+++	vperm	14,14,14,6
+++	.long	0x7DBA2799
+++	.long	0x7DDB2799
+++	addi	4,4,0x40
+++	bne	.Lxts_dec6x_steal
+++	b	.Lxts_dec6x_done
+++
+++.align	4
+++.Lxts_dec6x_three:
+++	vxor	7,3,17
+++	vxor	12,4,18
+++	vxor	13,5,19
+++	vxor	14,14,14
+++	vxor	15,15,15
+++
+++	bl	_aesp8_xts_dec5x
+++
+++	vperm	7,7,7,6
+++	vor	17,20,20
+++	vor	18,21,21
+++	vperm	12,12,12,6
+++	.long	0x7CE02799
+++	vxor	7,0,21
+++	vperm	13,13,13,6
+++	.long	0x7D832799
+++	.long	0x7DBA2799
+++	addi	4,4,0x30
+++	bne	.Lxts_dec6x_steal
+++	b	.Lxts_dec6x_done
+++
+++.align	4
+++.Lxts_dec6x_two:
+++	vxor	7,4,17
+++	vxor	12,5,18
+++	vxor	13,13,13
+++	vxor	14,14,14
+++	vxor	15,15,15
+++
+++	bl	_aesp8_xts_dec5x
+++
+++	vperm	7,7,7,6
+++	vor	17,19,19
+++	vor	18,20,20
+++	vperm	12,12,12,6
+++	.long	0x7CE02799
+++	vxor	7,0,20
+++	.long	0x7D832799
+++	addi	4,4,0x20
+++	bne	.Lxts_dec6x_steal
+++	b	.Lxts_dec6x_done
+++
+++.align	4
+++.Lxts_dec6x_one:
+++	vxor	7,5,17
+++	nop	
+++.Loop_xts_dec1x:
+++	.long	0x10E7C548
+++	lvx	24,26,7
+++	addi	7,7,0x20
+++
+++	.long	0x10E7CD48
+++	lvx	25,3,7
+++	bdnz	.Loop_xts_dec1x
+++
+++	subi	0,31,1
+++	.long	0x10E7C548
+++
+++	andi.	0,0,16
+++	cmpwi	31,0
+++	.long	0x10E7CD48
+++
+++	sub	10,10,0
+++	.long	0x10E7D548
+++
+++	.long	0x7C005699
+++	.long	0x10E7DD48
+++
+++	addi	7,1,79
+++	.long	0x10E7E548
+++	lvx	24,0,7
+++
+++	.long	0x10E7ED48
+++	lvx	25,3,7
+++	vxor	17,17,31
+++
+++	vperm	0,0,0,6
+++	.long	0x10E7F548
+++
+++	mtctr	9
+++	.long	0x10E78D49
+++
+++	vor	17,18,18
+++	vor	18,19,19
+++	vperm	7,7,7,6
+++	.long	0x7CE02799
+++	addi	4,4,0x10
+++	vxor	7,0,19
+++	bne	.Lxts_dec6x_steal
+++	b	.Lxts_dec6x_done
+++
+++.align	4
+++.Lxts_dec6x_zero:
+++	cmpwi	31,0
+++	beq	.Lxts_dec6x_done
+++
+++	.long	0x7C005699
+++	vperm	0,0,0,6
+++	vxor	7,0,18
+++.Lxts_dec6x_steal:
+++	.long	0x10E7C548
+++	lvx	24,26,7
+++	addi	7,7,0x20
+++
+++	.long	0x10E7CD48
+++	lvx	25,3,7
+++	bdnz	.Lxts_dec6x_steal
+++
+++	add	10,10,31
+++	.long	0x10E7C548
+++
+++	cmpwi	31,0
+++	.long	0x10E7CD48
+++
+++	.long	0x7C005699
+++	.long	0x10E7D548
+++
+++	lvsr	5,0,31
+++	.long	0x10E7DD48
+++
+++	addi	7,1,79
+++	.long	0x10E7E548
+++	lvx	24,0,7
+++
+++	.long	0x10E7ED48
+++	lvx	25,3,7
+++	vxor	18,18,31
+++
+++	vperm	0,0,0,6
+++	.long	0x10E7F548
+++
+++	vperm	0,0,0,5
+++	.long	0x11679549
+++
+++	vperm	7,11,11,6
+++	.long	0x7CE02799
+++
+++
+++	vxor	7,7,7
+++	vspltisb	12,-1
+++	vperm	7,7,12,5
+++	vsel	7,0,11,7
+++	vxor	7,7,17
+++
+++	subi	30,4,1
+++	mtctr	31
+++.Loop_xts_dec6x_steal:
+++	lbzu	0,1(30)
+++	stb	0,16(30)
+++	bdnz	.Loop_xts_dec6x_steal
+++
+++	li	31,0
+++	mtctr	9
+++	b	.Loop_xts_dec1x
+++
+++.align	4
+++.Lxts_dec6x_done:
+++	cmpldi	8,0
+++	beq	.Lxts_dec6x_ret
+++
+++	vxor	8,17,23
+++	vperm	8,8,8,6
+++	.long	0x7D004799
+++
+++.Lxts_dec6x_ret:
+++	mtlr	11
+++	li	10,79
+++	li	11,95
+++	stvx	9,10,1
+++	addi	10,10,32
+++	stvx	9,11,1
+++	addi	11,11,32
+++	stvx	9,10,1
+++	addi	10,10,32
+++	stvx	9,11,1
+++	addi	11,11,32
+++	stvx	9,10,1
+++	addi	10,10,32
+++	stvx	9,11,1
+++	addi	11,11,32
+++	stvx	9,10,1
+++	addi	10,10,32
+++	stvx	9,11,1
+++	addi	11,11,32
+++
+++	or	12,12,12
+++	lvx	20,10,1
+++	addi	10,10,32
+++	lvx	21,11,1
+++	addi	11,11,32
+++	lvx	22,10,1
+++	addi	10,10,32
+++	lvx	23,11,1
+++	addi	11,11,32
+++	lvx	24,10,1
+++	addi	10,10,32
+++	lvx	25,11,1
+++	addi	11,11,32
+++	lvx	26,10,1
+++	addi	10,10,32
+++	lvx	27,11,1
+++	addi	11,11,32
+++	lvx	28,10,1
+++	addi	10,10,32
+++	lvx	29,11,1
+++	addi	11,11,32
+++	lvx	30,10,1
+++	lvx	31,11,1
+++	ld	26,400(1)
+++	ld	27,408(1)
+++	ld	28,416(1)
+++	ld	29,424(1)
+++	ld	30,432(1)
+++	ld	31,440(1)
+++	addi	1,1,448
+++	blr	
+++.long	0
+++.byte	0,12,0x04,1,0x80,6,6,0
+++.long	0
+++
+++.align	5
+++_aesp8_xts_dec5x:
+++	.long	0x10E7C548
+++	.long	0x118CC548
+++	.long	0x11ADC548
+++	.long	0x11CEC548
+++	.long	0x11EFC548
+++	lvx	24,26,7
+++	addi	7,7,0x20
+++
+++	.long	0x10E7CD48
+++	.long	0x118CCD48
+++	.long	0x11ADCD48
+++	.long	0x11CECD48
+++	.long	0x11EFCD48
+++	lvx	25,3,7
+++	bdnz	_aesp8_xts_dec5x
+++
+++	subi	0,31,1
+++	.long	0x10E7C548
+++	.long	0x118CC548
+++	.long	0x11ADC548
+++	.long	0x11CEC548
+++	.long	0x11EFC548
+++
+++	andi.	0,0,16
+++	cmpwi	31,0
+++	.long	0x10E7CD48
+++	.long	0x118CCD48
+++	.long	0x11ADCD48
+++	.long	0x11CECD48
+++	.long	0x11EFCD48
+++	vxor	17,17,31
+++
+++	sub	10,10,0
+++	.long	0x10E7D548
+++	.long	0x118CD548
+++	.long	0x11ADD548
+++	.long	0x11CED548
+++	.long	0x11EFD548
+++	vxor	1,18,31
+++
+++	.long	0x10E7DD48
+++	.long	0x7C005699
+++	.long	0x118CDD48
+++	.long	0x11ADDD48
+++	.long	0x11CEDD48
+++	.long	0x11EFDD48
+++	vxor	2,19,31
+++
+++	addi	7,1,79
+++	.long	0x10E7E548
+++	.long	0x118CE548
+++	.long	0x11ADE548
+++	.long	0x11CEE548
+++	.long	0x11EFE548
+++	lvx	24,0,7
+++	vxor	3,20,31
+++
+++	.long	0x10E7ED48
+++	vperm	0,0,0,6
+++	.long	0x118CED48
+++	.long	0x11ADED48
+++	.long	0x11CEED48
+++	.long	0x11EFED48
+++	lvx	25,3,7
+++	vxor	4,21,31
+++
+++	.long	0x10E7F548
+++	.long	0x118CF548
+++	.long	0x11ADF548
+++	.long	0x11CEF548
+++	.long	0x11EFF548
+++
+++	.long	0x10E78D49
+++	.long	0x118C0D49
+++	.long	0x11AD1549
+++	.long	0x11CE1D49
+++	.long	0x11EF2549
+++	mtctr	9
+++	blr	
+++.long	0
+++.byte	0,12,0x14,0,0,0,0,0
+++#endif  // !OPENSSL_NO_ASM && __powerpc64__
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-ppc64le/ypto/fipsmodule/ghashp8-ppc.S b/linux-ppc64le/ypto/fipsmodule/ghashp8-ppc.S
++new file mode 100644
++index 000000000..5b909a38d
++--- /dev/null
+++++ b/linux-ppc64le/ypto/fipsmodule/ghashp8-ppc.S
++@@ -0,0 +1,587 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if !defined(OPENSSL_NO_ASM) && defined(__powerpc64__)
+++.machine	"any"
+++
+++.abiversion	2
+++.text
+++
+++.globl	gcm_init_p8
+++.type	gcm_init_p8,@function
+++.align	5
+++gcm_init_p8:
+++.localentry	gcm_init_p8,0
+++
+++	li	0,-4096
+++	li	8,0x10
+++	li	12,-1
+++	li	9,0x20
+++	or	0,0,0
+++	li	10,0x30
+++	.long	0x7D202699
+++
+++	vspltisb	8,-16
+++	vspltisb	5,1
+++	vaddubm	8,8,8
+++	vxor	4,4,4
+++	vor	8,8,5
+++	vsldoi	8,8,4,15
+++	vsldoi	6,4,5,1
+++	vaddubm	8,8,8
+++	vspltisb	7,7
+++	vor	8,8,6
+++	vspltb	6,9,0
+++	vsl	9,9,5
+++	vsrab	6,6,7
+++	vand	6,6,8
+++	vxor	3,9,6
+++
+++	vsldoi	9,3,3,8
+++	vsldoi	8,4,8,8
+++	vsldoi	11,4,9,8
+++	vsldoi	10,9,4,8
+++
+++	.long	0x7D001F99
+++	.long	0x7D681F99
+++	li	8,0x40
+++	.long	0x7D291F99
+++	li	9,0x50
+++	.long	0x7D4A1F99
+++	li	10,0x60
+++
+++	.long	0x10035CC8
+++	.long	0x10234CC8
+++	.long	0x104354C8
+++
+++	.long	0x10E044C8
+++
+++	vsldoi	5,1,4,8
+++	vsldoi	6,4,1,8
+++	vxor	0,0,5
+++	vxor	2,2,6
+++
+++	vsldoi	0,0,0,8
+++	vxor	0,0,7
+++
+++	vsldoi	6,0,0,8
+++	.long	0x100044C8
+++	vxor	6,6,2
+++	vxor	16,0,6
+++
+++	vsldoi	17,16,16,8
+++	vsldoi	19,4,17,8
+++	vsldoi	18,17,4,8
+++
+++	.long	0x7E681F99
+++	li	8,0x70
+++	.long	0x7E291F99
+++	li	9,0x80
+++	.long	0x7E4A1F99
+++	li	10,0x90
+++	.long	0x10039CC8
+++	.long	0x11B09CC8
+++	.long	0x10238CC8
+++	.long	0x11D08CC8
+++	.long	0x104394C8
+++	.long	0x11F094C8
+++
+++	.long	0x10E044C8
+++	.long	0x114D44C8
+++
+++	vsldoi	5,1,4,8
+++	vsldoi	6,4,1,8
+++	vsldoi	11,14,4,8
+++	vsldoi	9,4,14,8
+++	vxor	0,0,5
+++	vxor	2,2,6
+++	vxor	13,13,11
+++	vxor	15,15,9
+++
+++	vsldoi	0,0,0,8
+++	vsldoi	13,13,13,8
+++	vxor	0,0,7
+++	vxor	13,13,10
+++
+++	vsldoi	6,0,0,8
+++	vsldoi	9,13,13,8
+++	.long	0x100044C8
+++	.long	0x11AD44C8
+++	vxor	6,6,2
+++	vxor	9,9,15
+++	vxor	0,0,6
+++	vxor	13,13,9
+++
+++	vsldoi	9,0,0,8
+++	vsldoi	17,13,13,8
+++	vsldoi	11,4,9,8
+++	vsldoi	10,9,4,8
+++	vsldoi	19,4,17,8
+++	vsldoi	18,17,4,8
+++
+++	.long	0x7D681F99
+++	li	8,0xa0
+++	.long	0x7D291F99
+++	li	9,0xb0
+++	.long	0x7D4A1F99
+++	li	10,0xc0
+++	.long	0x7E681F99
+++	.long	0x7E291F99
+++	.long	0x7E4A1F99
+++
+++	or	12,12,12
+++	blr	
+++.long	0
+++.byte	0,12,0x14,0,0,0,2,0
+++.long	0
+++.size	gcm_init_p8,.-gcm_init_p8
+++.globl	gcm_gmult_p8
+++.type	gcm_gmult_p8,@function
+++.align	5
+++gcm_gmult_p8:
+++.localentry	gcm_gmult_p8,0
+++
+++	lis	0,0xfff8
+++	li	8,0x10
+++	li	12,-1
+++	li	9,0x20
+++	or	0,0,0
+++	li	10,0x30
+++	.long	0x7C601E99
+++
+++	.long	0x7D682699
+++	lvsl	12,0,0
+++	.long	0x7D292699
+++	vspltisb	5,0x07
+++	.long	0x7D4A2699
+++	vxor	12,12,5
+++	.long	0x7D002699
+++	vperm	3,3,3,12
+++	vxor	4,4,4
+++
+++	.long	0x10035CC8
+++	.long	0x10234CC8
+++	.long	0x104354C8
+++
+++	.long	0x10E044C8
+++
+++	vsldoi	5,1,4,8
+++	vsldoi	6,4,1,8
+++	vxor	0,0,5
+++	vxor	2,2,6
+++
+++	vsldoi	0,0,0,8
+++	vxor	0,0,7
+++
+++	vsldoi	6,0,0,8
+++	.long	0x100044C8
+++	vxor	6,6,2
+++	vxor	0,0,6
+++
+++	vperm	0,0,0,12
+++	.long	0x7C001F99
+++
+++	or	12,12,12
+++	blr	
+++.long	0
+++.byte	0,12,0x14,0,0,0,2,0
+++.long	0
+++.size	gcm_gmult_p8,.-gcm_gmult_p8
+++
+++.globl	gcm_ghash_p8
+++.type	gcm_ghash_p8,@function
+++.align	5
+++gcm_ghash_p8:
+++.localentry	gcm_ghash_p8,0
+++
+++	li	0,-4096
+++	li	8,0x10
+++	li	12,-1
+++	li	9,0x20
+++	or	0,0,0
+++	li	10,0x30
+++	.long	0x7C001E99
+++
+++	.long	0x7D682699
+++	li	8,0x40
+++	lvsl	12,0,0
+++	.long	0x7D292699
+++	li	9,0x50
+++	vspltisb	5,0x07
+++	.long	0x7D4A2699
+++	li	10,0x60
+++	vxor	12,12,5
+++	.long	0x7D002699
+++	vperm	0,0,0,12
+++	vxor	4,4,4
+++
+++	cmpldi	6,64
+++	bge	.Lgcm_ghash_p8_4x
+++
+++	.long	0x7C602E99
+++	addi	5,5,16
+++	subic.	6,6,16
+++	vperm	3,3,3,12
+++	vxor	3,3,0
+++	beq	.Lshort
+++
+++	.long	0x7E682699
+++	li	8,16
+++	.long	0x7E292699
+++	add	9,5,6
+++	.long	0x7E4A2699
+++
+++
+++.align	5
+++.Loop_2x:
+++	.long	0x7E002E99
+++	vperm	16,16,16,12
+++
+++	subic	6,6,32
+++	.long	0x10039CC8
+++	.long	0x11B05CC8
+++	subfe	0,0,0
+++	.long	0x10238CC8
+++	.long	0x11D04CC8
+++	and	0,0,6
+++	.long	0x104394C8
+++	.long	0x11F054C8
+++	add	5,5,0
+++
+++	vxor	0,0,13
+++	vxor	1,1,14
+++
+++	.long	0x10E044C8
+++
+++	vsldoi	5,1,4,8
+++	vsldoi	6,4,1,8
+++	vxor	2,2,15
+++	vxor	0,0,5
+++	vxor	2,2,6
+++
+++	vsldoi	0,0,0,8
+++	vxor	0,0,7
+++	.long	0x7C682E99
+++	addi	5,5,32
+++
+++	vsldoi	6,0,0,8
+++	.long	0x100044C8
+++	vperm	3,3,3,12
+++	vxor	6,6,2
+++	vxor	3,3,6
+++	vxor	3,3,0
+++	cmpld	9,5
+++	bgt	.Loop_2x
+++
+++	cmplwi	6,0
+++	bne	.Leven
+++
+++.Lshort:
+++	.long	0x10035CC8
+++	.long	0x10234CC8
+++	.long	0x104354C8
+++
+++	.long	0x10E044C8
+++
+++	vsldoi	5,1,4,8
+++	vsldoi	6,4,1,8
+++	vxor	0,0,5
+++	vxor	2,2,6
+++
+++	vsldoi	0,0,0,8
+++	vxor	0,0,7
+++
+++	vsldoi	6,0,0,8
+++	.long	0x100044C8
+++	vxor	6,6,2
+++
+++.Leven:
+++	vxor	0,0,6
+++	vperm	0,0,0,12
+++	.long	0x7C001F99
+++
+++	or	12,12,12
+++	blr	
+++.long	0
+++.byte	0,12,0x14,0,0,0,4,0
+++.long	0
+++.align	5
+++.gcm_ghash_p8_4x:
+++.Lgcm_ghash_p8_4x:
+++	stdu	1,-256(1)
+++	li	10,63
+++	li	11,79
+++	stvx	20,10,1
+++	addi	10,10,32
+++	stvx	21,11,1
+++	addi	11,11,32
+++	stvx	22,10,1
+++	addi	10,10,32
+++	stvx	23,11,1
+++	addi	11,11,32
+++	stvx	24,10,1
+++	addi	10,10,32
+++	stvx	25,11,1
+++	addi	11,11,32
+++	stvx	26,10,1
+++	addi	10,10,32
+++	stvx	27,11,1
+++	addi	11,11,32
+++	stvx	28,10,1
+++	addi	10,10,32
+++	stvx	29,11,1
+++	addi	11,11,32
+++	stvx	30,10,1
+++	li	10,0x60
+++	stvx	31,11,1
+++	li	0,-1
+++	stw	12,252(1)
+++	or	0,0,0
+++
+++	lvsl	5,0,8
+++
+++	li	8,0x70
+++	.long	0x7E292699
+++	li	9,0x80
+++	vspltisb	6,8
+++
+++	li	10,0x90
+++	.long	0x7EE82699
+++	li	8,0xa0
+++	.long	0x7F092699
+++	li	9,0xb0
+++	.long	0x7F2A2699
+++	li	10,0xc0
+++	.long	0x7FA82699
+++	li	8,0x10
+++	.long	0x7FC92699
+++	li	9,0x20
+++	.long	0x7FEA2699
+++	li	10,0x30
+++
+++	vsldoi	7,4,6,8
+++	vaddubm	18,5,7
+++	vaddubm	19,6,18
+++
+++	srdi	6,6,4
+++
+++	.long	0x7C602E99
+++	.long	0x7E082E99
+++	subic.	6,6,8
+++	.long	0x7EC92E99
+++	.long	0x7F8A2E99
+++	addi	5,5,0x40
+++	vperm	3,3,3,12
+++	vperm	16,16,16,12
+++	vperm	22,22,22,12
+++	vperm	28,28,28,12
+++
+++	vxor	2,3,0
+++
+++	.long	0x11B0BCC8
+++	.long	0x11D0C4C8
+++	.long	0x11F0CCC8
+++
+++	vperm	11,17,9,18
+++	vperm	5,22,28,19
+++	vperm	10,17,9,19
+++	vperm	6,22,28,18
+++	.long	0x12B68CC8
+++	.long	0x12855CC8
+++	.long	0x137C4CC8
+++	.long	0x134654C8
+++
+++	vxor	21,21,14
+++	vxor	20,20,13
+++	vxor	27,27,21
+++	vxor	26,26,15
+++
+++	blt	.Ltail_4x
+++
+++.Loop_4x:
+++	.long	0x7C602E99
+++	.long	0x7E082E99
+++	subic.	6,6,4
+++	.long	0x7EC92E99
+++	.long	0x7F8A2E99
+++	addi	5,5,0x40
+++	vperm	16,16,16,12
+++	vperm	22,22,22,12
+++	vperm	28,28,28,12
+++	vperm	3,3,3,12
+++
+++	.long	0x1002ECC8
+++	.long	0x1022F4C8
+++	.long	0x1042FCC8
+++	.long	0x11B0BCC8
+++	.long	0x11D0C4C8
+++	.long	0x11F0CCC8
+++
+++	vxor	0,0,20
+++	vxor	1,1,27
+++	vxor	2,2,26
+++	vperm	5,22,28,19
+++	vperm	6,22,28,18
+++
+++	.long	0x10E044C8
+++	.long	0x12855CC8
+++	.long	0x134654C8
+++
+++	vsldoi	5,1,4,8
+++	vsldoi	6,4,1,8
+++	vxor	0,0,5
+++	vxor	2,2,6
+++
+++	vsldoi	0,0,0,8
+++	vxor	0,0,7
+++
+++	vsldoi	6,0,0,8
+++	.long	0x12B68CC8
+++	.long	0x137C4CC8
+++	.long	0x100044C8
+++
+++	vxor	20,20,13
+++	vxor	26,26,15
+++	vxor	2,2,3
+++	vxor	21,21,14
+++	vxor	2,2,6
+++	vxor	27,27,21
+++	vxor	2,2,0
+++	bge	.Loop_4x
+++
+++.Ltail_4x:
+++	.long	0x1002ECC8
+++	.long	0x1022F4C8
+++	.long	0x1042FCC8
+++
+++	vxor	0,0,20
+++	vxor	1,1,27
+++
+++	.long	0x10E044C8
+++
+++	vsldoi	5,1,4,8
+++	vsldoi	6,4,1,8
+++	vxor	2,2,26
+++	vxor	0,0,5
+++	vxor	2,2,6
+++
+++	vsldoi	0,0,0,8
+++	vxor	0,0,7
+++
+++	vsldoi	6,0,0,8
+++	.long	0x100044C8
+++	vxor	6,6,2
+++	vxor	0,0,6
+++
+++	addic.	6,6,4
+++	beq	.Ldone_4x
+++
+++	.long	0x7C602E99
+++	cmpldi	6,2
+++	li	6,-4
+++	blt	.Lone
+++	.long	0x7E082E99
+++	beq	.Ltwo
+++
+++.Lthree:
+++	.long	0x7EC92E99
+++	vperm	3,3,3,12
+++	vperm	16,16,16,12
+++	vperm	22,22,22,12
+++
+++	vxor	2,3,0
+++	vor	29,23,23
+++	vor	30, 24, 24
+++	vor	31,25,25
+++
+++	vperm	5,16,22,19
+++	vperm	6,16,22,18
+++	.long	0x12B08CC8
+++	.long	0x13764CC8
+++	.long	0x12855CC8
+++	.long	0x134654C8
+++
+++	vxor	27,27,21
+++	b	.Ltail_4x
+++
+++.align	4
+++.Ltwo:
+++	vperm	3,3,3,12
+++	vperm	16,16,16,12
+++
+++	vxor	2,3,0
+++	vperm	5,4,16,19
+++	vperm	6,4,16,18
+++
+++	vsldoi	29,4,17,8
+++	vor	30, 17, 17
+++	vsldoi	31,17,4,8
+++
+++	.long	0x12855CC8
+++	.long	0x13704CC8
+++	.long	0x134654C8
+++
+++	b	.Ltail_4x
+++
+++.align	4
+++.Lone:
+++	vperm	3,3,3,12
+++
+++	vsldoi	29,4,9,8
+++	vor	30, 9, 9
+++	vsldoi	31,9,4,8
+++
+++	vxor	2,3,0
+++	vxor	20,20,20
+++	vxor	27,27,27
+++	vxor	26,26,26
+++
+++	b	.Ltail_4x
+++
+++.Ldone_4x:
+++	vperm	0,0,0,12
+++	.long	0x7C001F99
+++
+++	li	10,63
+++	li	11,79
+++	or	12,12,12
+++	lvx	20,10,1
+++	addi	10,10,32
+++	lvx	21,11,1
+++	addi	11,11,32
+++	lvx	22,10,1
+++	addi	10,10,32
+++	lvx	23,11,1
+++	addi	11,11,32
+++	lvx	24,10,1
+++	addi	10,10,32
+++	lvx	25,11,1
+++	addi	11,11,32
+++	lvx	26,10,1
+++	addi	10,10,32
+++	lvx	27,11,1
+++	addi	11,11,32
+++	lvx	28,10,1
+++	addi	10,10,32
+++	lvx	29,11,1
+++	addi	11,11,32
+++	lvx	30,10,1
+++	lvx	31,11,1
+++	addi	1,1,256
+++	blr	
+++.long	0
+++.byte	0,12,0x04,0,0x80,0,4,0
+++.long	0
+++.size	gcm_ghash_p8,.-gcm_ghash_p8
+++
+++.byte	71,72,65,83,72,32,102,111,114,32,80,111,119,101,114,73,83,65,32,50,46,48,55,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
+++.align	2
+++.align	2
+++#endif  // !OPENSSL_NO_ASM && __powerpc64__
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-ppc64le/ypto/test/trampoline-ppc.S b/linux-ppc64le/ypto/test/trampoline-ppc.S
++new file mode 100644
++index 000000000..7271090ca
++--- /dev/null
+++++ b/linux-ppc64le/ypto/test/trampoline-ppc.S
++@@ -0,0 +1,1410 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if !defined(OPENSSL_NO_ASM) && defined(__powerpc64__)
+++.machine	"any"
+++.abiversion	2
+++.text
+++
+++
+++
+++
+++
+++
+++
+++.globl	abi_test_trampoline
+++.type	abi_test_trampoline,@function
+++.align	5
+++abi_test_trampoline:
+++.localentry	abi_test_trampoline,0
+++
+++
+++	mflr	0
+++	std	0, 16(1)
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	stdu	1, -528(1)
+++
+++	mfcr	0
+++	std	0, 8(1)
+++	std	2, 24(1)
+++	std	4, 32(1)
+++	li	11, 48
+++	stvx	20, 11, 1
+++	li	11, 64
+++	stvx	21, 11, 1
+++	li	11, 80
+++	stvx	22, 11, 1
+++	li	11, 96
+++	stvx	23, 11, 1
+++	li	11, 112
+++	stvx	24, 11, 1
+++	li	11, 128
+++	stvx	25, 11, 1
+++	li	11, 144
+++	stvx	26, 11, 1
+++	li	11, 160
+++	stvx	27, 11, 1
+++	li	11, 176
+++	stvx	28, 11, 1
+++	li	11, 192
+++	stvx	29, 11, 1
+++	li	11, 208
+++	stvx	30, 11, 1
+++	li	11, 224
+++	stvx	31, 11, 1
+++	std	14, 240(1)
+++	std	15, 248(1)
+++	std	16, 256(1)
+++	std	17, 264(1)
+++	std	18, 272(1)
+++	std	19, 280(1)
+++	std	20, 288(1)
+++	std	21, 296(1)
+++	std	22, 304(1)
+++	std	23, 312(1)
+++	std	24, 320(1)
+++	std	25, 328(1)
+++	std	26, 336(1)
+++	std	27, 344(1)
+++	std	28, 352(1)
+++	std	29, 360(1)
+++	std	30, 368(1)
+++	std	31, 376(1)
+++	stfd	14, 384(1)
+++	stfd	15, 392(1)
+++	stfd	16, 400(1)
+++	stfd	17, 408(1)
+++	stfd	18, 416(1)
+++	stfd	19, 424(1)
+++	stfd	20, 432(1)
+++	stfd	21, 440(1)
+++	stfd	22, 448(1)
+++	stfd	23, 456(1)
+++	stfd	24, 464(1)
+++	stfd	25, 472(1)
+++	stfd	26, 480(1)
+++	stfd	27, 488(1)
+++	stfd	28, 496(1)
+++	stfd	29, 504(1)
+++	stfd	30, 512(1)
+++	stfd	31, 520(1)
+++	li	11, 0
+++	lvx	20, 11, 4
+++	li	11, 16
+++	lvx	21, 11, 4
+++	li	11, 32
+++	lvx	22, 11, 4
+++	li	11, 48
+++	lvx	23, 11, 4
+++	li	11, 64
+++	lvx	24, 11, 4
+++	li	11, 80
+++	lvx	25, 11, 4
+++	li	11, 96
+++	lvx	26, 11, 4
+++	li	11, 112
+++	lvx	27, 11, 4
+++	li	11, 128
+++	lvx	28, 11, 4
+++	li	11, 144
+++	lvx	29, 11, 4
+++	li	11, 160
+++	lvx	30, 11, 4
+++	li	11, 176
+++	lvx	31, 11, 4
+++	ld	14, 192(4)
+++	ld	15, 200(4)
+++	ld	16, 208(4)
+++	ld	17, 216(4)
+++	ld	18, 224(4)
+++	ld	19, 232(4)
+++	ld	20, 240(4)
+++	ld	21, 248(4)
+++	ld	22, 256(4)
+++	ld	23, 264(4)
+++	ld	24, 272(4)
+++	ld	25, 280(4)
+++	ld	26, 288(4)
+++	ld	27, 296(4)
+++	ld	28, 304(4)
+++	ld	29, 312(4)
+++	ld	30, 320(4)
+++	ld	31, 328(4)
+++	lfd	14, 336(4)
+++	lfd	15, 344(4)
+++	lfd	16, 352(4)
+++	lfd	17, 360(4)
+++	lfd	18, 368(4)
+++	lfd	19, 376(4)
+++	lfd	20, 384(4)
+++	lfd	21, 392(4)
+++	lfd	22, 400(4)
+++	lfd	23, 408(4)
+++	lfd	24, 416(4)
+++	lfd	25, 424(4)
+++	lfd	26, 432(4)
+++	lfd	27, 440(4)
+++	lfd	28, 448(4)
+++	lfd	29, 456(4)
+++	lfd	30, 464(4)
+++	lfd	31, 472(4)
+++
+++	ld	0, 480(4)
+++	mtcr	0
+++
+++
+++	addi	11, 5, -8
+++	mr	12, 3
+++
+++
+++	cmpdi	6, 0
+++	beq	.Largs_done
+++	mtctr	6
+++	ldu	3, 8(11)
+++	bdz	.Largs_done
+++	ldu	4, 8(11)
+++	bdz	.Largs_done
+++	ldu	5, 8(11)
+++	bdz	.Largs_done
+++	ldu	6, 8(11)
+++	bdz	.Largs_done
+++	ldu	7, 8(11)
+++	bdz	.Largs_done
+++	ldu	8, 8(11)
+++	bdz	.Largs_done
+++	ldu	9, 8(11)
+++	bdz	.Largs_done
+++	ldu	10, 8(11)
+++
+++.Largs_done:
+++	li	2, 0
+++	mtctr	12
+++	bctrl	
+++	ld	2, 24(1)
+++
+++	ld	4, 32(1)
+++	li	11, 0
+++	stvx	20, 11, 4
+++	li	11, 16
+++	stvx	21, 11, 4
+++	li	11, 32
+++	stvx	22, 11, 4
+++	li	11, 48
+++	stvx	23, 11, 4
+++	li	11, 64
+++	stvx	24, 11, 4
+++	li	11, 80
+++	stvx	25, 11, 4
+++	li	11, 96
+++	stvx	26, 11, 4
+++	li	11, 112
+++	stvx	27, 11, 4
+++	li	11, 128
+++	stvx	28, 11, 4
+++	li	11, 144
+++	stvx	29, 11, 4
+++	li	11, 160
+++	stvx	30, 11, 4
+++	li	11, 176
+++	stvx	31, 11, 4
+++	std	14, 192(4)
+++	std	15, 200(4)
+++	std	16, 208(4)
+++	std	17, 216(4)
+++	std	18, 224(4)
+++	std	19, 232(4)
+++	std	20, 240(4)
+++	std	21, 248(4)
+++	std	22, 256(4)
+++	std	23, 264(4)
+++	std	24, 272(4)
+++	std	25, 280(4)
+++	std	26, 288(4)
+++	std	27, 296(4)
+++	std	28, 304(4)
+++	std	29, 312(4)
+++	std	30, 320(4)
+++	std	31, 328(4)
+++	stfd	14, 336(4)
+++	stfd	15, 344(4)
+++	stfd	16, 352(4)
+++	stfd	17, 360(4)
+++	stfd	18, 368(4)
+++	stfd	19, 376(4)
+++	stfd	20, 384(4)
+++	stfd	21, 392(4)
+++	stfd	22, 400(4)
+++	stfd	23, 408(4)
+++	stfd	24, 416(4)
+++	stfd	25, 424(4)
+++	stfd	26, 432(4)
+++	stfd	27, 440(4)
+++	stfd	28, 448(4)
+++	stfd	29, 456(4)
+++	stfd	30, 464(4)
+++	stfd	31, 472(4)
+++	li	11, 48
+++	lvx	20, 11, 1
+++	li	11, 64
+++	lvx	21, 11, 1
+++	li	11, 80
+++	lvx	22, 11, 1
+++	li	11, 96
+++	lvx	23, 11, 1
+++	li	11, 112
+++	lvx	24, 11, 1
+++	li	11, 128
+++	lvx	25, 11, 1
+++	li	11, 144
+++	lvx	26, 11, 1
+++	li	11, 160
+++	lvx	27, 11, 1
+++	li	11, 176
+++	lvx	28, 11, 1
+++	li	11, 192
+++	lvx	29, 11, 1
+++	li	11, 208
+++	lvx	30, 11, 1
+++	li	11, 224
+++	lvx	31, 11, 1
+++	ld	14, 240(1)
+++	ld	15, 248(1)
+++	ld	16, 256(1)
+++	ld	17, 264(1)
+++	ld	18, 272(1)
+++	ld	19, 280(1)
+++	ld	20, 288(1)
+++	ld	21, 296(1)
+++	ld	22, 304(1)
+++	ld	23, 312(1)
+++	ld	24, 320(1)
+++	ld	25, 328(1)
+++	ld	26, 336(1)
+++	ld	27, 344(1)
+++	ld	28, 352(1)
+++	ld	29, 360(1)
+++	ld	30, 368(1)
+++	ld	31, 376(1)
+++	lfd	14, 384(1)
+++	lfd	15, 392(1)
+++	lfd	16, 400(1)
+++	lfd	17, 408(1)
+++	lfd	18, 416(1)
+++	lfd	19, 424(1)
+++	lfd	20, 432(1)
+++	lfd	21, 440(1)
+++	lfd	22, 448(1)
+++	lfd	23, 456(1)
+++	lfd	24, 464(1)
+++	lfd	25, 472(1)
+++	lfd	26, 480(1)
+++	lfd	27, 488(1)
+++	lfd	28, 496(1)
+++	lfd	29, 504(1)
+++	lfd	30, 512(1)
+++	lfd	31, 520(1)
+++	mfcr	0
+++	std	0, 480(4)
+++	ld	0, 8(1)
+++	mtcrf	0b00111000, 0
+++	addi	1, 1, 528
+++	ld	0, 16(1)
+++	mtlr	0
+++	blr	
+++.size	abi_test_trampoline,.-abi_test_trampoline
+++.globl	abi_test_clobber_r0
+++.type	abi_test_clobber_r0,@function
+++.align	5
+++abi_test_clobber_r0:
+++.localentry	abi_test_clobber_r0,0
+++
+++	li	0, 0
+++	blr	
+++.size	abi_test_clobber_r0,.-abi_test_clobber_r0
+++.globl	abi_test_clobber_r2
+++.type	abi_test_clobber_r2,@function
+++.align	5
+++abi_test_clobber_r2:
+++.localentry	abi_test_clobber_r2,0
+++
+++	li	2, 0
+++	blr	
+++.size	abi_test_clobber_r2,.-abi_test_clobber_r2
+++.globl	abi_test_clobber_r3
+++.type	abi_test_clobber_r3,@function
+++.align	5
+++abi_test_clobber_r3:
+++.localentry	abi_test_clobber_r3,0
+++
+++	li	3, 0
+++	blr	
+++.size	abi_test_clobber_r3,.-abi_test_clobber_r3
+++.globl	abi_test_clobber_r4
+++.type	abi_test_clobber_r4,@function
+++.align	5
+++abi_test_clobber_r4:
+++.localentry	abi_test_clobber_r4,0
+++
+++	li	4, 0
+++	blr	
+++.size	abi_test_clobber_r4,.-abi_test_clobber_r4
+++.globl	abi_test_clobber_r5
+++.type	abi_test_clobber_r5,@function
+++.align	5
+++abi_test_clobber_r5:
+++.localentry	abi_test_clobber_r5,0
+++
+++	li	5, 0
+++	blr	
+++.size	abi_test_clobber_r5,.-abi_test_clobber_r5
+++.globl	abi_test_clobber_r6
+++.type	abi_test_clobber_r6,@function
+++.align	5
+++abi_test_clobber_r6:
+++.localentry	abi_test_clobber_r6,0
+++
+++	li	6, 0
+++	blr	
+++.size	abi_test_clobber_r6,.-abi_test_clobber_r6
+++.globl	abi_test_clobber_r7
+++.type	abi_test_clobber_r7,@function
+++.align	5
+++abi_test_clobber_r7:
+++.localentry	abi_test_clobber_r7,0
+++
+++	li	7, 0
+++	blr	
+++.size	abi_test_clobber_r7,.-abi_test_clobber_r7
+++.globl	abi_test_clobber_r8
+++.type	abi_test_clobber_r8,@function
+++.align	5
+++abi_test_clobber_r8:
+++.localentry	abi_test_clobber_r8,0
+++
+++	li	8, 0
+++	blr	
+++.size	abi_test_clobber_r8,.-abi_test_clobber_r8
+++.globl	abi_test_clobber_r9
+++.type	abi_test_clobber_r9,@function
+++.align	5
+++abi_test_clobber_r9:
+++.localentry	abi_test_clobber_r9,0
+++
+++	li	9, 0
+++	blr	
+++.size	abi_test_clobber_r9,.-abi_test_clobber_r9
+++.globl	abi_test_clobber_r10
+++.type	abi_test_clobber_r10,@function
+++.align	5
+++abi_test_clobber_r10:
+++.localentry	abi_test_clobber_r10,0
+++
+++	li	10, 0
+++	blr	
+++.size	abi_test_clobber_r10,.-abi_test_clobber_r10
+++.globl	abi_test_clobber_r11
+++.type	abi_test_clobber_r11,@function
+++.align	5
+++abi_test_clobber_r11:
+++.localentry	abi_test_clobber_r11,0
+++
+++	li	11, 0
+++	blr	
+++.size	abi_test_clobber_r11,.-abi_test_clobber_r11
+++.globl	abi_test_clobber_r12
+++.type	abi_test_clobber_r12,@function
+++.align	5
+++abi_test_clobber_r12:
+++.localentry	abi_test_clobber_r12,0
+++
+++	li	12, 0
+++	blr	
+++.size	abi_test_clobber_r12,.-abi_test_clobber_r12
+++.globl	abi_test_clobber_r14
+++.type	abi_test_clobber_r14,@function
+++.align	5
+++abi_test_clobber_r14:
+++.localentry	abi_test_clobber_r14,0
+++
+++	li	14, 0
+++	blr	
+++.size	abi_test_clobber_r14,.-abi_test_clobber_r14
+++.globl	abi_test_clobber_r15
+++.type	abi_test_clobber_r15,@function
+++.align	5
+++abi_test_clobber_r15:
+++.localentry	abi_test_clobber_r15,0
+++
+++	li	15, 0
+++	blr	
+++.size	abi_test_clobber_r15,.-abi_test_clobber_r15
+++.globl	abi_test_clobber_r16
+++.type	abi_test_clobber_r16,@function
+++.align	5
+++abi_test_clobber_r16:
+++.localentry	abi_test_clobber_r16,0
+++
+++	li	16, 0
+++	blr	
+++.size	abi_test_clobber_r16,.-abi_test_clobber_r16
+++.globl	abi_test_clobber_r17
+++.type	abi_test_clobber_r17,@function
+++.align	5
+++abi_test_clobber_r17:
+++.localentry	abi_test_clobber_r17,0
+++
+++	li	17, 0
+++	blr	
+++.size	abi_test_clobber_r17,.-abi_test_clobber_r17
+++.globl	abi_test_clobber_r18
+++.type	abi_test_clobber_r18,@function
+++.align	5
+++abi_test_clobber_r18:
+++.localentry	abi_test_clobber_r18,0
+++
+++	li	18, 0
+++	blr	
+++.size	abi_test_clobber_r18,.-abi_test_clobber_r18
+++.globl	abi_test_clobber_r19
+++.type	abi_test_clobber_r19,@function
+++.align	5
+++abi_test_clobber_r19:
+++.localentry	abi_test_clobber_r19,0
+++
+++	li	19, 0
+++	blr	
+++.size	abi_test_clobber_r19,.-abi_test_clobber_r19
+++.globl	abi_test_clobber_r20
+++.type	abi_test_clobber_r20,@function
+++.align	5
+++abi_test_clobber_r20:
+++.localentry	abi_test_clobber_r20,0
+++
+++	li	20, 0
+++	blr	
+++.size	abi_test_clobber_r20,.-abi_test_clobber_r20
+++.globl	abi_test_clobber_r21
+++.type	abi_test_clobber_r21,@function
+++.align	5
+++abi_test_clobber_r21:
+++.localentry	abi_test_clobber_r21,0
+++
+++	li	21, 0
+++	blr	
+++.size	abi_test_clobber_r21,.-abi_test_clobber_r21
+++.globl	abi_test_clobber_r22
+++.type	abi_test_clobber_r22,@function
+++.align	5
+++abi_test_clobber_r22:
+++.localentry	abi_test_clobber_r22,0
+++
+++	li	22, 0
+++	blr	
+++.size	abi_test_clobber_r22,.-abi_test_clobber_r22
+++.globl	abi_test_clobber_r23
+++.type	abi_test_clobber_r23,@function
+++.align	5
+++abi_test_clobber_r23:
+++.localentry	abi_test_clobber_r23,0
+++
+++	li	23, 0
+++	blr	
+++.size	abi_test_clobber_r23,.-abi_test_clobber_r23
+++.globl	abi_test_clobber_r24
+++.type	abi_test_clobber_r24,@function
+++.align	5
+++abi_test_clobber_r24:
+++.localentry	abi_test_clobber_r24,0
+++
+++	li	24, 0
+++	blr	
+++.size	abi_test_clobber_r24,.-abi_test_clobber_r24
+++.globl	abi_test_clobber_r25
+++.type	abi_test_clobber_r25,@function
+++.align	5
+++abi_test_clobber_r25:
+++.localentry	abi_test_clobber_r25,0
+++
+++	li	25, 0
+++	blr	
+++.size	abi_test_clobber_r25,.-abi_test_clobber_r25
+++.globl	abi_test_clobber_r26
+++.type	abi_test_clobber_r26,@function
+++.align	5
+++abi_test_clobber_r26:
+++.localentry	abi_test_clobber_r26,0
+++
+++	li	26, 0
+++	blr	
+++.size	abi_test_clobber_r26,.-abi_test_clobber_r26
+++.globl	abi_test_clobber_r27
+++.type	abi_test_clobber_r27,@function
+++.align	5
+++abi_test_clobber_r27:
+++.localentry	abi_test_clobber_r27,0
+++
+++	li	27, 0
+++	blr	
+++.size	abi_test_clobber_r27,.-abi_test_clobber_r27
+++.globl	abi_test_clobber_r28
+++.type	abi_test_clobber_r28,@function
+++.align	5
+++abi_test_clobber_r28:
+++.localentry	abi_test_clobber_r28,0
+++
+++	li	28, 0
+++	blr	
+++.size	abi_test_clobber_r28,.-abi_test_clobber_r28
+++.globl	abi_test_clobber_r29
+++.type	abi_test_clobber_r29,@function
+++.align	5
+++abi_test_clobber_r29:
+++.localentry	abi_test_clobber_r29,0
+++
+++	li	29, 0
+++	blr	
+++.size	abi_test_clobber_r29,.-abi_test_clobber_r29
+++.globl	abi_test_clobber_r30
+++.type	abi_test_clobber_r30,@function
+++.align	5
+++abi_test_clobber_r30:
+++.localentry	abi_test_clobber_r30,0
+++
+++	li	30, 0
+++	blr	
+++.size	abi_test_clobber_r30,.-abi_test_clobber_r30
+++.globl	abi_test_clobber_r31
+++.type	abi_test_clobber_r31,@function
+++.align	5
+++abi_test_clobber_r31:
+++.localentry	abi_test_clobber_r31,0
+++
+++	li	31, 0
+++	blr	
+++.size	abi_test_clobber_r31,.-abi_test_clobber_r31
+++.globl	abi_test_clobber_f0
+++.type	abi_test_clobber_f0,@function
+++.align	4
+++abi_test_clobber_f0:
+++.localentry	abi_test_clobber_f0,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	0, -8(1)
+++	blr	
+++.size	abi_test_clobber_f0,.-abi_test_clobber_f0
+++.globl	abi_test_clobber_f1
+++.type	abi_test_clobber_f1,@function
+++.align	4
+++abi_test_clobber_f1:
+++.localentry	abi_test_clobber_f1,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	1, -8(1)
+++	blr	
+++.size	abi_test_clobber_f1,.-abi_test_clobber_f1
+++.globl	abi_test_clobber_f2
+++.type	abi_test_clobber_f2,@function
+++.align	4
+++abi_test_clobber_f2:
+++.localentry	abi_test_clobber_f2,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	2, -8(1)
+++	blr	
+++.size	abi_test_clobber_f2,.-abi_test_clobber_f2
+++.globl	abi_test_clobber_f3
+++.type	abi_test_clobber_f3,@function
+++.align	4
+++abi_test_clobber_f3:
+++.localentry	abi_test_clobber_f3,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	3, -8(1)
+++	blr	
+++.size	abi_test_clobber_f3,.-abi_test_clobber_f3
+++.globl	abi_test_clobber_f4
+++.type	abi_test_clobber_f4,@function
+++.align	4
+++abi_test_clobber_f4:
+++.localentry	abi_test_clobber_f4,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	4, -8(1)
+++	blr	
+++.size	abi_test_clobber_f4,.-abi_test_clobber_f4
+++.globl	abi_test_clobber_f5
+++.type	abi_test_clobber_f5,@function
+++.align	4
+++abi_test_clobber_f5:
+++.localentry	abi_test_clobber_f5,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	5, -8(1)
+++	blr	
+++.size	abi_test_clobber_f5,.-abi_test_clobber_f5
+++.globl	abi_test_clobber_f6
+++.type	abi_test_clobber_f6,@function
+++.align	4
+++abi_test_clobber_f6:
+++.localentry	abi_test_clobber_f6,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	6, -8(1)
+++	blr	
+++.size	abi_test_clobber_f6,.-abi_test_clobber_f6
+++.globl	abi_test_clobber_f7
+++.type	abi_test_clobber_f7,@function
+++.align	4
+++abi_test_clobber_f7:
+++.localentry	abi_test_clobber_f7,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	7, -8(1)
+++	blr	
+++.size	abi_test_clobber_f7,.-abi_test_clobber_f7
+++.globl	abi_test_clobber_f8
+++.type	abi_test_clobber_f8,@function
+++.align	4
+++abi_test_clobber_f8:
+++.localentry	abi_test_clobber_f8,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	8, -8(1)
+++	blr	
+++.size	abi_test_clobber_f8,.-abi_test_clobber_f8
+++.globl	abi_test_clobber_f9
+++.type	abi_test_clobber_f9,@function
+++.align	4
+++abi_test_clobber_f9:
+++.localentry	abi_test_clobber_f9,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	9, -8(1)
+++	blr	
+++.size	abi_test_clobber_f9,.-abi_test_clobber_f9
+++.globl	abi_test_clobber_f10
+++.type	abi_test_clobber_f10,@function
+++.align	4
+++abi_test_clobber_f10:
+++.localentry	abi_test_clobber_f10,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	10, -8(1)
+++	blr	
+++.size	abi_test_clobber_f10,.-abi_test_clobber_f10
+++.globl	abi_test_clobber_f11
+++.type	abi_test_clobber_f11,@function
+++.align	4
+++abi_test_clobber_f11:
+++.localentry	abi_test_clobber_f11,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	11, -8(1)
+++	blr	
+++.size	abi_test_clobber_f11,.-abi_test_clobber_f11
+++.globl	abi_test_clobber_f12
+++.type	abi_test_clobber_f12,@function
+++.align	4
+++abi_test_clobber_f12:
+++.localentry	abi_test_clobber_f12,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	12, -8(1)
+++	blr	
+++.size	abi_test_clobber_f12,.-abi_test_clobber_f12
+++.globl	abi_test_clobber_f13
+++.type	abi_test_clobber_f13,@function
+++.align	4
+++abi_test_clobber_f13:
+++.localentry	abi_test_clobber_f13,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	13, -8(1)
+++	blr	
+++.size	abi_test_clobber_f13,.-abi_test_clobber_f13
+++.globl	abi_test_clobber_f14
+++.type	abi_test_clobber_f14,@function
+++.align	4
+++abi_test_clobber_f14:
+++.localentry	abi_test_clobber_f14,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	14, -8(1)
+++	blr	
+++.size	abi_test_clobber_f14,.-abi_test_clobber_f14
+++.globl	abi_test_clobber_f15
+++.type	abi_test_clobber_f15,@function
+++.align	4
+++abi_test_clobber_f15:
+++.localentry	abi_test_clobber_f15,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	15, -8(1)
+++	blr	
+++.size	abi_test_clobber_f15,.-abi_test_clobber_f15
+++.globl	abi_test_clobber_f16
+++.type	abi_test_clobber_f16,@function
+++.align	4
+++abi_test_clobber_f16:
+++.localentry	abi_test_clobber_f16,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	16, -8(1)
+++	blr	
+++.size	abi_test_clobber_f16,.-abi_test_clobber_f16
+++.globl	abi_test_clobber_f17
+++.type	abi_test_clobber_f17,@function
+++.align	4
+++abi_test_clobber_f17:
+++.localentry	abi_test_clobber_f17,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	17, -8(1)
+++	blr	
+++.size	abi_test_clobber_f17,.-abi_test_clobber_f17
+++.globl	abi_test_clobber_f18
+++.type	abi_test_clobber_f18,@function
+++.align	4
+++abi_test_clobber_f18:
+++.localentry	abi_test_clobber_f18,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	18, -8(1)
+++	blr	
+++.size	abi_test_clobber_f18,.-abi_test_clobber_f18
+++.globl	abi_test_clobber_f19
+++.type	abi_test_clobber_f19,@function
+++.align	4
+++abi_test_clobber_f19:
+++.localentry	abi_test_clobber_f19,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	19, -8(1)
+++	blr	
+++.size	abi_test_clobber_f19,.-abi_test_clobber_f19
+++.globl	abi_test_clobber_f20
+++.type	abi_test_clobber_f20,@function
+++.align	4
+++abi_test_clobber_f20:
+++.localentry	abi_test_clobber_f20,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	20, -8(1)
+++	blr	
+++.size	abi_test_clobber_f20,.-abi_test_clobber_f20
+++.globl	abi_test_clobber_f21
+++.type	abi_test_clobber_f21,@function
+++.align	4
+++abi_test_clobber_f21:
+++.localentry	abi_test_clobber_f21,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	21, -8(1)
+++	blr	
+++.size	abi_test_clobber_f21,.-abi_test_clobber_f21
+++.globl	abi_test_clobber_f22
+++.type	abi_test_clobber_f22,@function
+++.align	4
+++abi_test_clobber_f22:
+++.localentry	abi_test_clobber_f22,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	22, -8(1)
+++	blr	
+++.size	abi_test_clobber_f22,.-abi_test_clobber_f22
+++.globl	abi_test_clobber_f23
+++.type	abi_test_clobber_f23,@function
+++.align	4
+++abi_test_clobber_f23:
+++.localentry	abi_test_clobber_f23,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	23, -8(1)
+++	blr	
+++.size	abi_test_clobber_f23,.-abi_test_clobber_f23
+++.globl	abi_test_clobber_f24
+++.type	abi_test_clobber_f24,@function
+++.align	4
+++abi_test_clobber_f24:
+++.localentry	abi_test_clobber_f24,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	24, -8(1)
+++	blr	
+++.size	abi_test_clobber_f24,.-abi_test_clobber_f24
+++.globl	abi_test_clobber_f25
+++.type	abi_test_clobber_f25,@function
+++.align	4
+++abi_test_clobber_f25:
+++.localentry	abi_test_clobber_f25,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	25, -8(1)
+++	blr	
+++.size	abi_test_clobber_f25,.-abi_test_clobber_f25
+++.globl	abi_test_clobber_f26
+++.type	abi_test_clobber_f26,@function
+++.align	4
+++abi_test_clobber_f26:
+++.localentry	abi_test_clobber_f26,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	26, -8(1)
+++	blr	
+++.size	abi_test_clobber_f26,.-abi_test_clobber_f26
+++.globl	abi_test_clobber_f27
+++.type	abi_test_clobber_f27,@function
+++.align	4
+++abi_test_clobber_f27:
+++.localentry	abi_test_clobber_f27,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	27, -8(1)
+++	blr	
+++.size	abi_test_clobber_f27,.-abi_test_clobber_f27
+++.globl	abi_test_clobber_f28
+++.type	abi_test_clobber_f28,@function
+++.align	4
+++abi_test_clobber_f28:
+++.localentry	abi_test_clobber_f28,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	28, -8(1)
+++	blr	
+++.size	abi_test_clobber_f28,.-abi_test_clobber_f28
+++.globl	abi_test_clobber_f29
+++.type	abi_test_clobber_f29,@function
+++.align	4
+++abi_test_clobber_f29:
+++.localentry	abi_test_clobber_f29,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	29, -8(1)
+++	blr	
+++.size	abi_test_clobber_f29,.-abi_test_clobber_f29
+++.globl	abi_test_clobber_f30
+++.type	abi_test_clobber_f30,@function
+++.align	4
+++abi_test_clobber_f30:
+++.localentry	abi_test_clobber_f30,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	30, -8(1)
+++	blr	
+++.size	abi_test_clobber_f30,.-abi_test_clobber_f30
+++.globl	abi_test_clobber_f31
+++.type	abi_test_clobber_f31,@function
+++.align	4
+++abi_test_clobber_f31:
+++.localentry	abi_test_clobber_f31,0
+++
+++	li	0, 0
+++
+++	std	0, -8(1)
+++	lfd	31, -8(1)
+++	blr	
+++.size	abi_test_clobber_f31,.-abi_test_clobber_f31
+++.globl	abi_test_clobber_v0
+++.type	abi_test_clobber_v0,@function
+++.align	4
+++abi_test_clobber_v0:
+++.localentry	abi_test_clobber_v0,0
+++
+++	vxor	0, 0, 0
+++	blr	
+++.size	abi_test_clobber_v0,.-abi_test_clobber_v0
+++.globl	abi_test_clobber_v1
+++.type	abi_test_clobber_v1,@function
+++.align	4
+++abi_test_clobber_v1:
+++.localentry	abi_test_clobber_v1,0
+++
+++	vxor	1, 1, 1
+++	blr	
+++.size	abi_test_clobber_v1,.-abi_test_clobber_v1
+++.globl	abi_test_clobber_v2
+++.type	abi_test_clobber_v2,@function
+++.align	4
+++abi_test_clobber_v2:
+++.localentry	abi_test_clobber_v2,0
+++
+++	vxor	2, 2, 2
+++	blr	
+++.size	abi_test_clobber_v2,.-abi_test_clobber_v2
+++.globl	abi_test_clobber_v3
+++.type	abi_test_clobber_v3,@function
+++.align	4
+++abi_test_clobber_v3:
+++.localentry	abi_test_clobber_v3,0
+++
+++	vxor	3, 3, 3
+++	blr	
+++.size	abi_test_clobber_v3,.-abi_test_clobber_v3
+++.globl	abi_test_clobber_v4
+++.type	abi_test_clobber_v4,@function
+++.align	4
+++abi_test_clobber_v4:
+++.localentry	abi_test_clobber_v4,0
+++
+++	vxor	4, 4, 4
+++	blr	
+++.size	abi_test_clobber_v4,.-abi_test_clobber_v4
+++.globl	abi_test_clobber_v5
+++.type	abi_test_clobber_v5,@function
+++.align	4
+++abi_test_clobber_v5:
+++.localentry	abi_test_clobber_v5,0
+++
+++	vxor	5, 5, 5
+++	blr	
+++.size	abi_test_clobber_v5,.-abi_test_clobber_v5
+++.globl	abi_test_clobber_v6
+++.type	abi_test_clobber_v6,@function
+++.align	4
+++abi_test_clobber_v6:
+++.localentry	abi_test_clobber_v6,0
+++
+++	vxor	6, 6, 6
+++	blr	
+++.size	abi_test_clobber_v6,.-abi_test_clobber_v6
+++.globl	abi_test_clobber_v7
+++.type	abi_test_clobber_v7,@function
+++.align	4
+++abi_test_clobber_v7:
+++.localentry	abi_test_clobber_v7,0
+++
+++	vxor	7, 7, 7
+++	blr	
+++.size	abi_test_clobber_v7,.-abi_test_clobber_v7
+++.globl	abi_test_clobber_v8
+++.type	abi_test_clobber_v8,@function
+++.align	4
+++abi_test_clobber_v8:
+++.localentry	abi_test_clobber_v8,0
+++
+++	vxor	8, 8, 8
+++	blr	
+++.size	abi_test_clobber_v8,.-abi_test_clobber_v8
+++.globl	abi_test_clobber_v9
+++.type	abi_test_clobber_v9,@function
+++.align	4
+++abi_test_clobber_v9:
+++.localentry	abi_test_clobber_v9,0
+++
+++	vxor	9, 9, 9
+++	blr	
+++.size	abi_test_clobber_v9,.-abi_test_clobber_v9
+++.globl	abi_test_clobber_v10
+++.type	abi_test_clobber_v10,@function
+++.align	4
+++abi_test_clobber_v10:
+++.localentry	abi_test_clobber_v10,0
+++
+++	vxor	10, 10, 10
+++	blr	
+++.size	abi_test_clobber_v10,.-abi_test_clobber_v10
+++.globl	abi_test_clobber_v11
+++.type	abi_test_clobber_v11,@function
+++.align	4
+++abi_test_clobber_v11:
+++.localentry	abi_test_clobber_v11,0
+++
+++	vxor	11, 11, 11
+++	blr	
+++.size	abi_test_clobber_v11,.-abi_test_clobber_v11
+++.globl	abi_test_clobber_v12
+++.type	abi_test_clobber_v12,@function
+++.align	4
+++abi_test_clobber_v12:
+++.localentry	abi_test_clobber_v12,0
+++
+++	vxor	12, 12, 12
+++	blr	
+++.size	abi_test_clobber_v12,.-abi_test_clobber_v12
+++.globl	abi_test_clobber_v13
+++.type	abi_test_clobber_v13,@function
+++.align	4
+++abi_test_clobber_v13:
+++.localentry	abi_test_clobber_v13,0
+++
+++	vxor	13, 13, 13
+++	blr	
+++.size	abi_test_clobber_v13,.-abi_test_clobber_v13
+++.globl	abi_test_clobber_v14
+++.type	abi_test_clobber_v14,@function
+++.align	4
+++abi_test_clobber_v14:
+++.localentry	abi_test_clobber_v14,0
+++
+++	vxor	14, 14, 14
+++	blr	
+++.size	abi_test_clobber_v14,.-abi_test_clobber_v14
+++.globl	abi_test_clobber_v15
+++.type	abi_test_clobber_v15,@function
+++.align	4
+++abi_test_clobber_v15:
+++.localentry	abi_test_clobber_v15,0
+++
+++	vxor	15, 15, 15
+++	blr	
+++.size	abi_test_clobber_v15,.-abi_test_clobber_v15
+++.globl	abi_test_clobber_v16
+++.type	abi_test_clobber_v16,@function
+++.align	4
+++abi_test_clobber_v16:
+++.localentry	abi_test_clobber_v16,0
+++
+++	vxor	16, 16, 16
+++	blr	
+++.size	abi_test_clobber_v16,.-abi_test_clobber_v16
+++.globl	abi_test_clobber_v17
+++.type	abi_test_clobber_v17,@function
+++.align	4
+++abi_test_clobber_v17:
+++.localentry	abi_test_clobber_v17,0
+++
+++	vxor	17, 17, 17
+++	blr	
+++.size	abi_test_clobber_v17,.-abi_test_clobber_v17
+++.globl	abi_test_clobber_v18
+++.type	abi_test_clobber_v18,@function
+++.align	4
+++abi_test_clobber_v18:
+++.localentry	abi_test_clobber_v18,0
+++
+++	vxor	18, 18, 18
+++	blr	
+++.size	abi_test_clobber_v18,.-abi_test_clobber_v18
+++.globl	abi_test_clobber_v19
+++.type	abi_test_clobber_v19,@function
+++.align	4
+++abi_test_clobber_v19:
+++.localentry	abi_test_clobber_v19,0
+++
+++	vxor	19, 19, 19
+++	blr	
+++.size	abi_test_clobber_v19,.-abi_test_clobber_v19
+++.globl	abi_test_clobber_v20
+++.type	abi_test_clobber_v20,@function
+++.align	4
+++abi_test_clobber_v20:
+++.localentry	abi_test_clobber_v20,0
+++
+++	vxor	20, 20, 20
+++	blr	
+++.size	abi_test_clobber_v20,.-abi_test_clobber_v20
+++.globl	abi_test_clobber_v21
+++.type	abi_test_clobber_v21,@function
+++.align	4
+++abi_test_clobber_v21:
+++.localentry	abi_test_clobber_v21,0
+++
+++	vxor	21, 21, 21
+++	blr	
+++.size	abi_test_clobber_v21,.-abi_test_clobber_v21
+++.globl	abi_test_clobber_v22
+++.type	abi_test_clobber_v22,@function
+++.align	4
+++abi_test_clobber_v22:
+++.localentry	abi_test_clobber_v22,0
+++
+++	vxor	22, 22, 22
+++	blr	
+++.size	abi_test_clobber_v22,.-abi_test_clobber_v22
+++.globl	abi_test_clobber_v23
+++.type	abi_test_clobber_v23,@function
+++.align	4
+++abi_test_clobber_v23:
+++.localentry	abi_test_clobber_v23,0
+++
+++	vxor	23, 23, 23
+++	blr	
+++.size	abi_test_clobber_v23,.-abi_test_clobber_v23
+++.globl	abi_test_clobber_v24
+++.type	abi_test_clobber_v24,@function
+++.align	4
+++abi_test_clobber_v24:
+++.localentry	abi_test_clobber_v24,0
+++
+++	vxor	24, 24, 24
+++	blr	
+++.size	abi_test_clobber_v24,.-abi_test_clobber_v24
+++.globl	abi_test_clobber_v25
+++.type	abi_test_clobber_v25,@function
+++.align	4
+++abi_test_clobber_v25:
+++.localentry	abi_test_clobber_v25,0
+++
+++	vxor	25, 25, 25
+++	blr	
+++.size	abi_test_clobber_v25,.-abi_test_clobber_v25
+++.globl	abi_test_clobber_v26
+++.type	abi_test_clobber_v26,@function
+++.align	4
+++abi_test_clobber_v26:
+++.localentry	abi_test_clobber_v26,0
+++
+++	vxor	26, 26, 26
+++	blr	
+++.size	abi_test_clobber_v26,.-abi_test_clobber_v26
+++.globl	abi_test_clobber_v27
+++.type	abi_test_clobber_v27,@function
+++.align	4
+++abi_test_clobber_v27:
+++.localentry	abi_test_clobber_v27,0
+++
+++	vxor	27, 27, 27
+++	blr	
+++.size	abi_test_clobber_v27,.-abi_test_clobber_v27
+++.globl	abi_test_clobber_v28
+++.type	abi_test_clobber_v28,@function
+++.align	4
+++abi_test_clobber_v28:
+++.localentry	abi_test_clobber_v28,0
+++
+++	vxor	28, 28, 28
+++	blr	
+++.size	abi_test_clobber_v28,.-abi_test_clobber_v28
+++.globl	abi_test_clobber_v29
+++.type	abi_test_clobber_v29,@function
+++.align	4
+++abi_test_clobber_v29:
+++.localentry	abi_test_clobber_v29,0
+++
+++	vxor	29, 29, 29
+++	blr	
+++.size	abi_test_clobber_v29,.-abi_test_clobber_v29
+++.globl	abi_test_clobber_v30
+++.type	abi_test_clobber_v30,@function
+++.align	4
+++abi_test_clobber_v30:
+++.localentry	abi_test_clobber_v30,0
+++
+++	vxor	30, 30, 30
+++	blr	
+++.size	abi_test_clobber_v30,.-abi_test_clobber_v30
+++.globl	abi_test_clobber_v31
+++.type	abi_test_clobber_v31,@function
+++.align	4
+++abi_test_clobber_v31:
+++.localentry	abi_test_clobber_v31,0
+++
+++	vxor	31, 31, 31
+++	blr	
+++.size	abi_test_clobber_v31,.-abi_test_clobber_v31
+++.globl	abi_test_clobber_cr0
+++.type	abi_test_clobber_cr0,@function
+++.align	4
+++abi_test_clobber_cr0:
+++.localentry	abi_test_clobber_cr0,0
+++
+++
+++
+++	mfcr	0
+++	not	0, 0
+++	mtcrf	128, 0
+++	blr	
+++.size	abi_test_clobber_cr0,.-abi_test_clobber_cr0
+++.globl	abi_test_clobber_cr1
+++.type	abi_test_clobber_cr1,@function
+++.align	4
+++abi_test_clobber_cr1:
+++.localentry	abi_test_clobber_cr1,0
+++
+++
+++
+++	mfcr	0
+++	not	0, 0
+++	mtcrf	64, 0
+++	blr	
+++.size	abi_test_clobber_cr1,.-abi_test_clobber_cr1
+++.globl	abi_test_clobber_cr2
+++.type	abi_test_clobber_cr2,@function
+++.align	4
+++abi_test_clobber_cr2:
+++.localentry	abi_test_clobber_cr2,0
+++
+++
+++
+++	mfcr	0
+++	not	0, 0
+++	mtcrf	32, 0
+++	blr	
+++.size	abi_test_clobber_cr2,.-abi_test_clobber_cr2
+++.globl	abi_test_clobber_cr3
+++.type	abi_test_clobber_cr3,@function
+++.align	4
+++abi_test_clobber_cr3:
+++.localentry	abi_test_clobber_cr3,0
+++
+++
+++
+++	mfcr	0
+++	not	0, 0
+++	mtcrf	16, 0
+++	blr	
+++.size	abi_test_clobber_cr3,.-abi_test_clobber_cr3
+++.globl	abi_test_clobber_cr4
+++.type	abi_test_clobber_cr4,@function
+++.align	4
+++abi_test_clobber_cr4:
+++.localentry	abi_test_clobber_cr4,0
+++
+++
+++
+++	mfcr	0
+++	not	0, 0
+++	mtcrf	8, 0
+++	blr	
+++.size	abi_test_clobber_cr4,.-abi_test_clobber_cr4
+++.globl	abi_test_clobber_cr5
+++.type	abi_test_clobber_cr5,@function
+++.align	4
+++abi_test_clobber_cr5:
+++.localentry	abi_test_clobber_cr5,0
+++
+++
+++
+++	mfcr	0
+++	not	0, 0
+++	mtcrf	4, 0
+++	blr	
+++.size	abi_test_clobber_cr5,.-abi_test_clobber_cr5
+++.globl	abi_test_clobber_cr6
+++.type	abi_test_clobber_cr6,@function
+++.align	4
+++abi_test_clobber_cr6:
+++.localentry	abi_test_clobber_cr6,0
+++
+++
+++
+++	mfcr	0
+++	not	0, 0
+++	mtcrf	2, 0
+++	blr	
+++.size	abi_test_clobber_cr6,.-abi_test_clobber_cr6
+++.globl	abi_test_clobber_cr7
+++.type	abi_test_clobber_cr7,@function
+++.align	4
+++abi_test_clobber_cr7:
+++.localentry	abi_test_clobber_cr7,0
+++
+++
+++
+++	mfcr	0
+++	not	0, 0
+++	mtcrf	1, 0
+++	blr	
+++.size	abi_test_clobber_cr7,.-abi_test_clobber_cr7
+++.globl	abi_test_clobber_ctr
+++.type	abi_test_clobber_ctr,@function
+++.align	4
+++abi_test_clobber_ctr:
+++.localentry	abi_test_clobber_ctr,0
+++
+++	li	0, 0
+++	mtctr	0
+++	blr	
+++.size	abi_test_clobber_ctr,.-abi_test_clobber_ctr
+++
+++.globl	abi_test_clobber_lr
+++.type	abi_test_clobber_lr,@function
+++.align	4
+++abi_test_clobber_lr:
+++.localentry	abi_test_clobber_lr,0
+++
+++	mflr	0
+++	mtctr	0
+++	li	0, 0
+++	mtlr	0
+++	bctr	
+++.size	abi_test_clobber_lr,.-abi_test_clobber_lr
+++
+++#endif  // !OPENSSL_NO_ASM && __powerpc64__
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/chacha/chacha-x86_64.S b/linux-x86_64/ypto/chacha/chacha-x86_64.S
++new file mode 100644
++index 000000000..b76713398
++--- /dev/null
+++++ b/linux-x86_64/ypto/chacha/chacha-x86_64.S
++@@ -0,0 +1,1633 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++
+++.extern	OPENSSL_ia32cap_P
+++.hidden OPENSSL_ia32cap_P
+++
+++.align	64
+++.Lzero:
+++.long	0,0,0,0
+++.Lone:
+++.long	1,0,0,0
+++.Linc:
+++.long	0,1,2,3
+++.Lfour:
+++.long	4,4,4,4
+++.Lincy:
+++.long	0,2,4,6,1,3,5,7
+++.Leight:
+++.long	8,8,8,8,8,8,8,8
+++.Lrot16:
+++.byte	0x2,0x3,0x0,0x1, 0x6,0x7,0x4,0x5, 0xa,0xb,0x8,0x9, 0xe,0xf,0xc,0xd
+++.Lrot24:
+++.byte	0x3,0x0,0x1,0x2, 0x7,0x4,0x5,0x6, 0xb,0x8,0x9,0xa, 0xf,0xc,0xd,0xe
+++.Lsigma:
+++.byte	101,120,112,97,110,100,32,51,50,45,98,121,116,101,32,107,0
+++.align	64
+++.Lzeroz:
+++.long	0,0,0,0, 1,0,0,0, 2,0,0,0, 3,0,0,0
+++.Lfourz:
+++.long	4,0,0,0, 4,0,0,0, 4,0,0,0, 4,0,0,0
+++.Lincz:
+++.long	0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
+++.Lsixteen:
+++.long	16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16
+++.byte	67,104,97,67,104,97,50,48,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
+++.globl	ChaCha20_ctr32
+++.hidden ChaCha20_ctr32
+++.type	ChaCha20_ctr32,@function
+++.align	64
+++ChaCha20_ctr32:
+++.cfi_startproc	
+++	cmpq	$0,%rdx
+++	je	.Lno_data
+++	movq	OPENSSL_ia32cap_P+4(%rip),%r10
+++	testl	$512,%r10d
+++	jnz	.LChaCha20_ssse3
+++
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	rbx,-16
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	rbp,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	r15,-56
+++	subq	$64+24,%rsp
+++.cfi_adjust_cfa_offset	88
+++.Lctr32_body:
+++
+++
+++	movdqu	(%rcx),%xmm1
+++	movdqu	16(%rcx),%xmm2
+++	movdqu	(%r8),%xmm3
+++	movdqa	.Lone(%rip),%xmm4
+++
+++
+++	movdqa	%xmm1,16(%rsp)
+++	movdqa	%xmm2,32(%rsp)
+++	movdqa	%xmm3,48(%rsp)
+++	movq	%rdx,%rbp
+++	jmp	.Loop_outer
+++
+++.align	32
+++.Loop_outer:
+++	movl	$0x61707865,%eax
+++	movl	$0x3320646e,%ebx
+++	movl	$0x79622d32,%ecx
+++	movl	$0x6b206574,%edx
+++	movl	16(%rsp),%r8d
+++	movl	20(%rsp),%r9d
+++	movl	24(%rsp),%r10d
+++	movl	28(%rsp),%r11d
+++	movd	%xmm3,%r12d
+++	movl	52(%rsp),%r13d
+++	movl	56(%rsp),%r14d
+++	movl	60(%rsp),%r15d
+++
+++	movq	%rbp,64+0(%rsp)
+++	movl	$10,%ebp
+++	movq	%rsi,64+8(%rsp)
+++.byte	102,72,15,126,214
+++	movq	%rdi,64+16(%rsp)
+++	movq	%rsi,%rdi
+++	shrq	$32,%rdi
+++	jmp	.Loop
+++
+++.align	32
+++.Loop:
+++	addl	%r8d,%eax
+++	xorl	%eax,%r12d
+++	roll	$16,%r12d
+++	addl	%r9d,%ebx
+++	xorl	%ebx,%r13d
+++	roll	$16,%r13d
+++	addl	%r12d,%esi
+++	xorl	%esi,%r8d
+++	roll	$12,%r8d
+++	addl	%r13d,%edi
+++	xorl	%edi,%r9d
+++	roll	$12,%r9d
+++	addl	%r8d,%eax
+++	xorl	%eax,%r12d
+++	roll	$8,%r12d
+++	addl	%r9d,%ebx
+++	xorl	%ebx,%r13d
+++	roll	$8,%r13d
+++	addl	%r12d,%esi
+++	xorl	%esi,%r8d
+++	roll	$7,%r8d
+++	addl	%r13d,%edi
+++	xorl	%edi,%r9d
+++	roll	$7,%r9d
+++	movl	%esi,32(%rsp)
+++	movl	%edi,36(%rsp)
+++	movl	40(%rsp),%esi
+++	movl	44(%rsp),%edi
+++	addl	%r10d,%ecx
+++	xorl	%ecx,%r14d
+++	roll	$16,%r14d
+++	addl	%r11d,%edx
+++	xorl	%edx,%r15d
+++	roll	$16,%r15d
+++	addl	%r14d,%esi
+++	xorl	%esi,%r10d
+++	roll	$12,%r10d
+++	addl	%r15d,%edi
+++	xorl	%edi,%r11d
+++	roll	$12,%r11d
+++	addl	%r10d,%ecx
+++	xorl	%ecx,%r14d
+++	roll	$8,%r14d
+++	addl	%r11d,%edx
+++	xorl	%edx,%r15d
+++	roll	$8,%r15d
+++	addl	%r14d,%esi
+++	xorl	%esi,%r10d
+++	roll	$7,%r10d
+++	addl	%r15d,%edi
+++	xorl	%edi,%r11d
+++	roll	$7,%r11d
+++	addl	%r9d,%eax
+++	xorl	%eax,%r15d
+++	roll	$16,%r15d
+++	addl	%r10d,%ebx
+++	xorl	%ebx,%r12d
+++	roll	$16,%r12d
+++	addl	%r15d,%esi
+++	xorl	%esi,%r9d
+++	roll	$12,%r9d
+++	addl	%r12d,%edi
+++	xorl	%edi,%r10d
+++	roll	$12,%r10d
+++	addl	%r9d,%eax
+++	xorl	%eax,%r15d
+++	roll	$8,%r15d
+++	addl	%r10d,%ebx
+++	xorl	%ebx,%r12d
+++	roll	$8,%r12d
+++	addl	%r15d,%esi
+++	xorl	%esi,%r9d
+++	roll	$7,%r9d
+++	addl	%r12d,%edi
+++	xorl	%edi,%r10d
+++	roll	$7,%r10d
+++	movl	%esi,40(%rsp)
+++	movl	%edi,44(%rsp)
+++	movl	32(%rsp),%esi
+++	movl	36(%rsp),%edi
+++	addl	%r11d,%ecx
+++	xorl	%ecx,%r13d
+++	roll	$16,%r13d
+++	addl	%r8d,%edx
+++	xorl	%edx,%r14d
+++	roll	$16,%r14d
+++	addl	%r13d,%esi
+++	xorl	%esi,%r11d
+++	roll	$12,%r11d
+++	addl	%r14d,%edi
+++	xorl	%edi,%r8d
+++	roll	$12,%r8d
+++	addl	%r11d,%ecx
+++	xorl	%ecx,%r13d
+++	roll	$8,%r13d
+++	addl	%r8d,%edx
+++	xorl	%edx,%r14d
+++	roll	$8,%r14d
+++	addl	%r13d,%esi
+++	xorl	%esi,%r11d
+++	roll	$7,%r11d
+++	addl	%r14d,%edi
+++	xorl	%edi,%r8d
+++	roll	$7,%r8d
+++	decl	%ebp
+++	jnz	.Loop
+++	movl	%edi,36(%rsp)
+++	movl	%esi,32(%rsp)
+++	movq	64(%rsp),%rbp
+++	movdqa	%xmm2,%xmm1
+++	movq	64+8(%rsp),%rsi
+++	paddd	%xmm4,%xmm3
+++	movq	64+16(%rsp),%rdi
+++
+++	addl	$0x61707865,%eax
+++	addl	$0x3320646e,%ebx
+++	addl	$0x79622d32,%ecx
+++	addl	$0x6b206574,%edx
+++	addl	16(%rsp),%r8d
+++	addl	20(%rsp),%r9d
+++	addl	24(%rsp),%r10d
+++	addl	28(%rsp),%r11d
+++	addl	48(%rsp),%r12d
+++	addl	52(%rsp),%r13d
+++	addl	56(%rsp),%r14d
+++	addl	60(%rsp),%r15d
+++	paddd	32(%rsp),%xmm1
+++
+++	cmpq	$64,%rbp
+++	jb	.Ltail
+++
+++	xorl	0(%rsi),%eax
+++	xorl	4(%rsi),%ebx
+++	xorl	8(%rsi),%ecx
+++	xorl	12(%rsi),%edx
+++	xorl	16(%rsi),%r8d
+++	xorl	20(%rsi),%r9d
+++	xorl	24(%rsi),%r10d
+++	xorl	28(%rsi),%r11d
+++	movdqu	32(%rsi),%xmm0
+++	xorl	48(%rsi),%r12d
+++	xorl	52(%rsi),%r13d
+++	xorl	56(%rsi),%r14d
+++	xorl	60(%rsi),%r15d
+++	leaq	64(%rsi),%rsi
+++	pxor	%xmm1,%xmm0
+++
+++	movdqa	%xmm2,32(%rsp)
+++	movd	%xmm3,48(%rsp)
+++
+++	movl	%eax,0(%rdi)
+++	movl	%ebx,4(%rdi)
+++	movl	%ecx,8(%rdi)
+++	movl	%edx,12(%rdi)
+++	movl	%r8d,16(%rdi)
+++	movl	%r9d,20(%rdi)
+++	movl	%r10d,24(%rdi)
+++	movl	%r11d,28(%rdi)
+++	movdqu	%xmm0,32(%rdi)
+++	movl	%r12d,48(%rdi)
+++	movl	%r13d,52(%rdi)
+++	movl	%r14d,56(%rdi)
+++	movl	%r15d,60(%rdi)
+++	leaq	64(%rdi),%rdi
+++
+++	subq	$64,%rbp
+++	jnz	.Loop_outer
+++
+++	jmp	.Ldone
+++
+++.align	16
+++.Ltail:
+++	movl	%eax,0(%rsp)
+++	movl	%ebx,4(%rsp)
+++	xorq	%rbx,%rbx
+++	movl	%ecx,8(%rsp)
+++	movl	%edx,12(%rsp)
+++	movl	%r8d,16(%rsp)
+++	movl	%r9d,20(%rsp)
+++	movl	%r10d,24(%rsp)
+++	movl	%r11d,28(%rsp)
+++	movdqa	%xmm1,32(%rsp)
+++	movl	%r12d,48(%rsp)
+++	movl	%r13d,52(%rsp)
+++	movl	%r14d,56(%rsp)
+++	movl	%r15d,60(%rsp)
+++
+++.Loop_tail:
+++	movzbl	(%rsi,%rbx,1),%eax
+++	movzbl	(%rsp,%rbx,1),%edx
+++	leaq	1(%rbx),%rbx
+++	xorl	%edx,%eax
+++	movb	%al,-1(%rdi,%rbx,1)
+++	decq	%rbp
+++	jnz	.Loop_tail
+++
+++.Ldone:
+++	leaq	64+24+48(%rsp),%rsi
+++	movq	-48(%rsi),%r15
+++.cfi_restore	r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	rbx
+++	leaq	(%rsi),%rsp
+++.cfi_adjust_cfa_offset	-136
+++.Lno_data:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ChaCha20_ctr32,.-ChaCha20_ctr32
+++.type	ChaCha20_ssse3,@function
+++.align	32
+++ChaCha20_ssse3:
+++.LChaCha20_ssse3:
+++.cfi_startproc	
+++	movq	%rsp,%r9
+++.cfi_def_cfa_register	r9
+++	cmpq	$128,%rdx
+++	ja	.LChaCha20_4x
+++
+++.Ldo_sse3_after_all:
+++	subq	$64+8,%rsp
+++	movdqa	.Lsigma(%rip),%xmm0
+++	movdqu	(%rcx),%xmm1
+++	movdqu	16(%rcx),%xmm2
+++	movdqu	(%r8),%xmm3
+++	movdqa	.Lrot16(%rip),%xmm6
+++	movdqa	.Lrot24(%rip),%xmm7
+++
+++	movdqa	%xmm0,0(%rsp)
+++	movdqa	%xmm1,16(%rsp)
+++	movdqa	%xmm2,32(%rsp)
+++	movdqa	%xmm3,48(%rsp)
+++	movq	$10,%r8
+++	jmp	.Loop_ssse3
+++
+++.align	32
+++.Loop_outer_ssse3:
+++	movdqa	.Lone(%rip),%xmm3
+++	movdqa	0(%rsp),%xmm0
+++	movdqa	16(%rsp),%xmm1
+++	movdqa	32(%rsp),%xmm2
+++	paddd	48(%rsp),%xmm3
+++	movq	$10,%r8
+++	movdqa	%xmm3,48(%rsp)
+++	jmp	.Loop_ssse3
+++
+++.align	32
+++.Loop_ssse3:
+++	paddd	%xmm1,%xmm0
+++	pxor	%xmm0,%xmm3
+++.byte	102,15,56,0,222
+++	paddd	%xmm3,%xmm2
+++	pxor	%xmm2,%xmm1
+++	movdqa	%xmm1,%xmm4
+++	psrld	$20,%xmm1
+++	pslld	$12,%xmm4
+++	por	%xmm4,%xmm1
+++	paddd	%xmm1,%xmm0
+++	pxor	%xmm0,%xmm3
+++.byte	102,15,56,0,223
+++	paddd	%xmm3,%xmm2
+++	pxor	%xmm2,%xmm1
+++	movdqa	%xmm1,%xmm4
+++	psrld	$25,%xmm1
+++	pslld	$7,%xmm4
+++	por	%xmm4,%xmm1
+++	pshufd	$78,%xmm2,%xmm2
+++	pshufd	$57,%xmm1,%xmm1
+++	pshufd	$147,%xmm3,%xmm3
+++	nop
+++	paddd	%xmm1,%xmm0
+++	pxor	%xmm0,%xmm3
+++.byte	102,15,56,0,222
+++	paddd	%xmm3,%xmm2
+++	pxor	%xmm2,%xmm1
+++	movdqa	%xmm1,%xmm4
+++	psrld	$20,%xmm1
+++	pslld	$12,%xmm4
+++	por	%xmm4,%xmm1
+++	paddd	%xmm1,%xmm0
+++	pxor	%xmm0,%xmm3
+++.byte	102,15,56,0,223
+++	paddd	%xmm3,%xmm2
+++	pxor	%xmm2,%xmm1
+++	movdqa	%xmm1,%xmm4
+++	psrld	$25,%xmm1
+++	pslld	$7,%xmm4
+++	por	%xmm4,%xmm1
+++	pshufd	$78,%xmm2,%xmm2
+++	pshufd	$147,%xmm1,%xmm1
+++	pshufd	$57,%xmm3,%xmm3
+++	decq	%r8
+++	jnz	.Loop_ssse3
+++	paddd	0(%rsp),%xmm0
+++	paddd	16(%rsp),%xmm1
+++	paddd	32(%rsp),%xmm2
+++	paddd	48(%rsp),%xmm3
+++
+++	cmpq	$64,%rdx
+++	jb	.Ltail_ssse3
+++
+++	movdqu	0(%rsi),%xmm4
+++	movdqu	16(%rsi),%xmm5
+++	pxor	%xmm4,%xmm0
+++	movdqu	32(%rsi),%xmm4
+++	pxor	%xmm5,%xmm1
+++	movdqu	48(%rsi),%xmm5
+++	leaq	64(%rsi),%rsi
+++	pxor	%xmm4,%xmm2
+++	pxor	%xmm5,%xmm3
+++
+++	movdqu	%xmm0,0(%rdi)
+++	movdqu	%xmm1,16(%rdi)
+++	movdqu	%xmm2,32(%rdi)
+++	movdqu	%xmm3,48(%rdi)
+++	leaq	64(%rdi),%rdi
+++
+++	subq	$64,%rdx
+++	jnz	.Loop_outer_ssse3
+++
+++	jmp	.Ldone_ssse3
+++
+++.align	16
+++.Ltail_ssse3:
+++	movdqa	%xmm0,0(%rsp)
+++	movdqa	%xmm1,16(%rsp)
+++	movdqa	%xmm2,32(%rsp)
+++	movdqa	%xmm3,48(%rsp)
+++	xorq	%r8,%r8
+++
+++.Loop_tail_ssse3:
+++	movzbl	(%rsi,%r8,1),%eax
+++	movzbl	(%rsp,%r8,1),%ecx
+++	leaq	1(%r8),%r8
+++	xorl	%ecx,%eax
+++	movb	%al,-1(%rdi,%r8,1)
+++	decq	%rdx
+++	jnz	.Loop_tail_ssse3
+++
+++.Ldone_ssse3:
+++	leaq	(%r9),%rsp
+++.cfi_def_cfa_register	rsp
+++.Lssse3_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ChaCha20_ssse3,.-ChaCha20_ssse3
+++.type	ChaCha20_4x,@function
+++.align	32
+++ChaCha20_4x:
+++.LChaCha20_4x:
+++.cfi_startproc	
+++	movq	%rsp,%r9
+++.cfi_def_cfa_register	r9
+++	movq	%r10,%r11
+++	shrq	$32,%r10
+++	testq	$32,%r10
+++	jnz	.LChaCha20_8x
+++	cmpq	$192,%rdx
+++	ja	.Lproceed4x
+++
+++	andq	$71303168,%r11
+++	cmpq	$4194304,%r11
+++	je	.Ldo_sse3_after_all
+++
+++.Lproceed4x:
+++	subq	$0x140+8,%rsp
+++	movdqa	.Lsigma(%rip),%xmm11
+++	movdqu	(%rcx),%xmm15
+++	movdqu	16(%rcx),%xmm7
+++	movdqu	(%r8),%xmm3
+++	leaq	256(%rsp),%rcx
+++	leaq	.Lrot16(%rip),%r10
+++	leaq	.Lrot24(%rip),%r11
+++
+++	pshufd	$0x00,%xmm11,%xmm8
+++	pshufd	$0x55,%xmm11,%xmm9
+++	movdqa	%xmm8,64(%rsp)
+++	pshufd	$0xaa,%xmm11,%xmm10
+++	movdqa	%xmm9,80(%rsp)
+++	pshufd	$0xff,%xmm11,%xmm11
+++	movdqa	%xmm10,96(%rsp)
+++	movdqa	%xmm11,112(%rsp)
+++
+++	pshufd	$0x00,%xmm15,%xmm12
+++	pshufd	$0x55,%xmm15,%xmm13
+++	movdqa	%xmm12,128-256(%rcx)
+++	pshufd	$0xaa,%xmm15,%xmm14
+++	movdqa	%xmm13,144-256(%rcx)
+++	pshufd	$0xff,%xmm15,%xmm15
+++	movdqa	%xmm14,160-256(%rcx)
+++	movdqa	%xmm15,176-256(%rcx)
+++
+++	pshufd	$0x00,%xmm7,%xmm4
+++	pshufd	$0x55,%xmm7,%xmm5
+++	movdqa	%xmm4,192-256(%rcx)
+++	pshufd	$0xaa,%xmm7,%xmm6
+++	movdqa	%xmm5,208-256(%rcx)
+++	pshufd	$0xff,%xmm7,%xmm7
+++	movdqa	%xmm6,224-256(%rcx)
+++	movdqa	%xmm7,240-256(%rcx)
+++
+++	pshufd	$0x00,%xmm3,%xmm0
+++	pshufd	$0x55,%xmm3,%xmm1
+++	paddd	.Linc(%rip),%xmm0
+++	pshufd	$0xaa,%xmm3,%xmm2
+++	movdqa	%xmm1,272-256(%rcx)
+++	pshufd	$0xff,%xmm3,%xmm3
+++	movdqa	%xmm2,288-256(%rcx)
+++	movdqa	%xmm3,304-256(%rcx)
+++
+++	jmp	.Loop_enter4x
+++
+++.align	32
+++.Loop_outer4x:
+++	movdqa	64(%rsp),%xmm8
+++	movdqa	80(%rsp),%xmm9
+++	movdqa	96(%rsp),%xmm10
+++	movdqa	112(%rsp),%xmm11
+++	movdqa	128-256(%rcx),%xmm12
+++	movdqa	144-256(%rcx),%xmm13
+++	movdqa	160-256(%rcx),%xmm14
+++	movdqa	176-256(%rcx),%xmm15
+++	movdqa	192-256(%rcx),%xmm4
+++	movdqa	208-256(%rcx),%xmm5
+++	movdqa	224-256(%rcx),%xmm6
+++	movdqa	240-256(%rcx),%xmm7
+++	movdqa	256-256(%rcx),%xmm0
+++	movdqa	272-256(%rcx),%xmm1
+++	movdqa	288-256(%rcx),%xmm2
+++	movdqa	304-256(%rcx),%xmm3
+++	paddd	.Lfour(%rip),%xmm0
+++
+++.Loop_enter4x:
+++	movdqa	%xmm6,32(%rsp)
+++	movdqa	%xmm7,48(%rsp)
+++	movdqa	(%r10),%xmm7
+++	movl	$10,%eax
+++	movdqa	%xmm0,256-256(%rcx)
+++	jmp	.Loop4x
+++
+++.align	32
+++.Loop4x:
+++	paddd	%xmm12,%xmm8
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm8,%xmm0
+++	pxor	%xmm9,%xmm1
+++.byte	102,15,56,0,199
+++.byte	102,15,56,0,207
+++	paddd	%xmm0,%xmm4
+++	paddd	%xmm1,%xmm5
+++	pxor	%xmm4,%xmm12
+++	pxor	%xmm5,%xmm13
+++	movdqa	%xmm12,%xmm6
+++	pslld	$12,%xmm12
+++	psrld	$20,%xmm6
+++	movdqa	%xmm13,%xmm7
+++	pslld	$12,%xmm13
+++	por	%xmm6,%xmm12
+++	psrld	$20,%xmm7
+++	movdqa	(%r11),%xmm6
+++	por	%xmm7,%xmm13
+++	paddd	%xmm12,%xmm8
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm8,%xmm0
+++	pxor	%xmm9,%xmm1
+++.byte	102,15,56,0,198
+++.byte	102,15,56,0,206
+++	paddd	%xmm0,%xmm4
+++	paddd	%xmm1,%xmm5
+++	pxor	%xmm4,%xmm12
+++	pxor	%xmm5,%xmm13
+++	movdqa	%xmm12,%xmm7
+++	pslld	$7,%xmm12
+++	psrld	$25,%xmm7
+++	movdqa	%xmm13,%xmm6
+++	pslld	$7,%xmm13
+++	por	%xmm7,%xmm12
+++	psrld	$25,%xmm6
+++	movdqa	(%r10),%xmm7
+++	por	%xmm6,%xmm13
+++	movdqa	%xmm4,0(%rsp)
+++	movdqa	%xmm5,16(%rsp)
+++	movdqa	32(%rsp),%xmm4
+++	movdqa	48(%rsp),%xmm5
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm15,%xmm11
+++	pxor	%xmm10,%xmm2
+++	pxor	%xmm11,%xmm3
+++.byte	102,15,56,0,215
+++.byte	102,15,56,0,223
+++	paddd	%xmm2,%xmm4
+++	paddd	%xmm3,%xmm5
+++	pxor	%xmm4,%xmm14
+++	pxor	%xmm5,%xmm15
+++	movdqa	%xmm14,%xmm6
+++	pslld	$12,%xmm14
+++	psrld	$20,%xmm6
+++	movdqa	%xmm15,%xmm7
+++	pslld	$12,%xmm15
+++	por	%xmm6,%xmm14
+++	psrld	$20,%xmm7
+++	movdqa	(%r11),%xmm6
+++	por	%xmm7,%xmm15
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm15,%xmm11
+++	pxor	%xmm10,%xmm2
+++	pxor	%xmm11,%xmm3
+++.byte	102,15,56,0,214
+++.byte	102,15,56,0,222
+++	paddd	%xmm2,%xmm4
+++	paddd	%xmm3,%xmm5
+++	pxor	%xmm4,%xmm14
+++	pxor	%xmm5,%xmm15
+++	movdqa	%xmm14,%xmm7
+++	pslld	$7,%xmm14
+++	psrld	$25,%xmm7
+++	movdqa	%xmm15,%xmm6
+++	pslld	$7,%xmm15
+++	por	%xmm7,%xmm14
+++	psrld	$25,%xmm6
+++	movdqa	(%r10),%xmm7
+++	por	%xmm6,%xmm15
+++	paddd	%xmm13,%xmm8
+++	paddd	%xmm14,%xmm9
+++	pxor	%xmm8,%xmm3
+++	pxor	%xmm9,%xmm0
+++.byte	102,15,56,0,223
+++.byte	102,15,56,0,199
+++	paddd	%xmm3,%xmm4
+++	paddd	%xmm0,%xmm5
+++	pxor	%xmm4,%xmm13
+++	pxor	%xmm5,%xmm14
+++	movdqa	%xmm13,%xmm6
+++	pslld	$12,%xmm13
+++	psrld	$20,%xmm6
+++	movdqa	%xmm14,%xmm7
+++	pslld	$12,%xmm14
+++	por	%xmm6,%xmm13
+++	psrld	$20,%xmm7
+++	movdqa	(%r11),%xmm6
+++	por	%xmm7,%xmm14
+++	paddd	%xmm13,%xmm8
+++	paddd	%xmm14,%xmm9
+++	pxor	%xmm8,%xmm3
+++	pxor	%xmm9,%xmm0
+++.byte	102,15,56,0,222
+++.byte	102,15,56,0,198
+++	paddd	%xmm3,%xmm4
+++	paddd	%xmm0,%xmm5
+++	pxor	%xmm4,%xmm13
+++	pxor	%xmm5,%xmm14
+++	movdqa	%xmm13,%xmm7
+++	pslld	$7,%xmm13
+++	psrld	$25,%xmm7
+++	movdqa	%xmm14,%xmm6
+++	pslld	$7,%xmm14
+++	por	%xmm7,%xmm13
+++	psrld	$25,%xmm6
+++	movdqa	(%r10),%xmm7
+++	por	%xmm6,%xmm14
+++	movdqa	%xmm4,32(%rsp)
+++	movdqa	%xmm5,48(%rsp)
+++	movdqa	0(%rsp),%xmm4
+++	movdqa	16(%rsp),%xmm5
+++	paddd	%xmm15,%xmm10
+++	paddd	%xmm12,%xmm11
+++	pxor	%xmm10,%xmm1
+++	pxor	%xmm11,%xmm2
+++.byte	102,15,56,0,207
+++.byte	102,15,56,0,215
+++	paddd	%xmm1,%xmm4
+++	paddd	%xmm2,%xmm5
+++	pxor	%xmm4,%xmm15
+++	pxor	%xmm5,%xmm12
+++	movdqa	%xmm15,%xmm6
+++	pslld	$12,%xmm15
+++	psrld	$20,%xmm6
+++	movdqa	%xmm12,%xmm7
+++	pslld	$12,%xmm12
+++	por	%xmm6,%xmm15
+++	psrld	$20,%xmm7
+++	movdqa	(%r11),%xmm6
+++	por	%xmm7,%xmm12
+++	paddd	%xmm15,%xmm10
+++	paddd	%xmm12,%xmm11
+++	pxor	%xmm10,%xmm1
+++	pxor	%xmm11,%xmm2
+++.byte	102,15,56,0,206
+++.byte	102,15,56,0,214
+++	paddd	%xmm1,%xmm4
+++	paddd	%xmm2,%xmm5
+++	pxor	%xmm4,%xmm15
+++	pxor	%xmm5,%xmm12
+++	movdqa	%xmm15,%xmm7
+++	pslld	$7,%xmm15
+++	psrld	$25,%xmm7
+++	movdqa	%xmm12,%xmm6
+++	pslld	$7,%xmm12
+++	por	%xmm7,%xmm15
+++	psrld	$25,%xmm6
+++	movdqa	(%r10),%xmm7
+++	por	%xmm6,%xmm12
+++	decl	%eax
+++	jnz	.Loop4x
+++
+++	paddd	64(%rsp),%xmm8
+++	paddd	80(%rsp),%xmm9
+++	paddd	96(%rsp),%xmm10
+++	paddd	112(%rsp),%xmm11
+++
+++	movdqa	%xmm8,%xmm6
+++	punpckldq	%xmm9,%xmm8
+++	movdqa	%xmm10,%xmm7
+++	punpckldq	%xmm11,%xmm10
+++	punpckhdq	%xmm9,%xmm6
+++	punpckhdq	%xmm11,%xmm7
+++	movdqa	%xmm8,%xmm9
+++	punpcklqdq	%xmm10,%xmm8
+++	movdqa	%xmm6,%xmm11
+++	punpcklqdq	%xmm7,%xmm6
+++	punpckhqdq	%xmm10,%xmm9
+++	punpckhqdq	%xmm7,%xmm11
+++	paddd	128-256(%rcx),%xmm12
+++	paddd	144-256(%rcx),%xmm13
+++	paddd	160-256(%rcx),%xmm14
+++	paddd	176-256(%rcx),%xmm15
+++
+++	movdqa	%xmm8,0(%rsp)
+++	movdqa	%xmm9,16(%rsp)
+++	movdqa	32(%rsp),%xmm8
+++	movdqa	48(%rsp),%xmm9
+++
+++	movdqa	%xmm12,%xmm10
+++	punpckldq	%xmm13,%xmm12
+++	movdqa	%xmm14,%xmm7
+++	punpckldq	%xmm15,%xmm14
+++	punpckhdq	%xmm13,%xmm10
+++	punpckhdq	%xmm15,%xmm7
+++	movdqa	%xmm12,%xmm13
+++	punpcklqdq	%xmm14,%xmm12
+++	movdqa	%xmm10,%xmm15
+++	punpcklqdq	%xmm7,%xmm10
+++	punpckhqdq	%xmm14,%xmm13
+++	punpckhqdq	%xmm7,%xmm15
+++	paddd	192-256(%rcx),%xmm4
+++	paddd	208-256(%rcx),%xmm5
+++	paddd	224-256(%rcx),%xmm8
+++	paddd	240-256(%rcx),%xmm9
+++
+++	movdqa	%xmm6,32(%rsp)
+++	movdqa	%xmm11,48(%rsp)
+++
+++	movdqa	%xmm4,%xmm14
+++	punpckldq	%xmm5,%xmm4
+++	movdqa	%xmm8,%xmm7
+++	punpckldq	%xmm9,%xmm8
+++	punpckhdq	%xmm5,%xmm14
+++	punpckhdq	%xmm9,%xmm7
+++	movdqa	%xmm4,%xmm5
+++	punpcklqdq	%xmm8,%xmm4
+++	movdqa	%xmm14,%xmm9
+++	punpcklqdq	%xmm7,%xmm14
+++	punpckhqdq	%xmm8,%xmm5
+++	punpckhqdq	%xmm7,%xmm9
+++	paddd	256-256(%rcx),%xmm0
+++	paddd	272-256(%rcx),%xmm1
+++	paddd	288-256(%rcx),%xmm2
+++	paddd	304-256(%rcx),%xmm3
+++
+++	movdqa	%xmm0,%xmm8
+++	punpckldq	%xmm1,%xmm0
+++	movdqa	%xmm2,%xmm7
+++	punpckldq	%xmm3,%xmm2
+++	punpckhdq	%xmm1,%xmm8
+++	punpckhdq	%xmm3,%xmm7
+++	movdqa	%xmm0,%xmm1
+++	punpcklqdq	%xmm2,%xmm0
+++	movdqa	%xmm8,%xmm3
+++	punpcklqdq	%xmm7,%xmm8
+++	punpckhqdq	%xmm2,%xmm1
+++	punpckhqdq	%xmm7,%xmm3
+++	cmpq	$256,%rdx
+++	jb	.Ltail4x
+++
+++	movdqu	0(%rsi),%xmm6
+++	movdqu	16(%rsi),%xmm11
+++	movdqu	32(%rsi),%xmm2
+++	movdqu	48(%rsi),%xmm7
+++	pxor	0(%rsp),%xmm6
+++	pxor	%xmm12,%xmm11
+++	pxor	%xmm4,%xmm2
+++	pxor	%xmm0,%xmm7
+++
+++	movdqu	%xmm6,0(%rdi)
+++	movdqu	64(%rsi),%xmm6
+++	movdqu	%xmm11,16(%rdi)
+++	movdqu	80(%rsi),%xmm11
+++	movdqu	%xmm2,32(%rdi)
+++	movdqu	96(%rsi),%xmm2
+++	movdqu	%xmm7,48(%rdi)
+++	movdqu	112(%rsi),%xmm7
+++	leaq	128(%rsi),%rsi
+++	pxor	16(%rsp),%xmm6
+++	pxor	%xmm13,%xmm11
+++	pxor	%xmm5,%xmm2
+++	pxor	%xmm1,%xmm7
+++
+++	movdqu	%xmm6,64(%rdi)
+++	movdqu	0(%rsi),%xmm6
+++	movdqu	%xmm11,80(%rdi)
+++	movdqu	16(%rsi),%xmm11
+++	movdqu	%xmm2,96(%rdi)
+++	movdqu	32(%rsi),%xmm2
+++	movdqu	%xmm7,112(%rdi)
+++	leaq	128(%rdi),%rdi
+++	movdqu	48(%rsi),%xmm7
+++	pxor	32(%rsp),%xmm6
+++	pxor	%xmm10,%xmm11
+++	pxor	%xmm14,%xmm2
+++	pxor	%xmm8,%xmm7
+++
+++	movdqu	%xmm6,0(%rdi)
+++	movdqu	64(%rsi),%xmm6
+++	movdqu	%xmm11,16(%rdi)
+++	movdqu	80(%rsi),%xmm11
+++	movdqu	%xmm2,32(%rdi)
+++	movdqu	96(%rsi),%xmm2
+++	movdqu	%xmm7,48(%rdi)
+++	movdqu	112(%rsi),%xmm7
+++	leaq	128(%rsi),%rsi
+++	pxor	48(%rsp),%xmm6
+++	pxor	%xmm15,%xmm11
+++	pxor	%xmm9,%xmm2
+++	pxor	%xmm3,%xmm7
+++	movdqu	%xmm6,64(%rdi)
+++	movdqu	%xmm11,80(%rdi)
+++	movdqu	%xmm2,96(%rdi)
+++	movdqu	%xmm7,112(%rdi)
+++	leaq	128(%rdi),%rdi
+++
+++	subq	$256,%rdx
+++	jnz	.Loop_outer4x
+++
+++	jmp	.Ldone4x
+++
+++.Ltail4x:
+++	cmpq	$192,%rdx
+++	jae	.L192_or_more4x
+++	cmpq	$128,%rdx
+++	jae	.L128_or_more4x
+++	cmpq	$64,%rdx
+++	jae	.L64_or_more4x
+++
+++
+++	xorq	%r10,%r10
+++
+++	movdqa	%xmm12,16(%rsp)
+++	movdqa	%xmm4,32(%rsp)
+++	movdqa	%xmm0,48(%rsp)
+++	jmp	.Loop_tail4x
+++
+++.align	32
+++.L64_or_more4x:
+++	movdqu	0(%rsi),%xmm6
+++	movdqu	16(%rsi),%xmm11
+++	movdqu	32(%rsi),%xmm2
+++	movdqu	48(%rsi),%xmm7
+++	pxor	0(%rsp),%xmm6
+++	pxor	%xmm12,%xmm11
+++	pxor	%xmm4,%xmm2
+++	pxor	%xmm0,%xmm7
+++	movdqu	%xmm6,0(%rdi)
+++	movdqu	%xmm11,16(%rdi)
+++	movdqu	%xmm2,32(%rdi)
+++	movdqu	%xmm7,48(%rdi)
+++	je	.Ldone4x
+++
+++	movdqa	16(%rsp),%xmm6
+++	leaq	64(%rsi),%rsi
+++	xorq	%r10,%r10
+++	movdqa	%xmm6,0(%rsp)
+++	movdqa	%xmm13,16(%rsp)
+++	leaq	64(%rdi),%rdi
+++	movdqa	%xmm5,32(%rsp)
+++	subq	$64,%rdx
+++	movdqa	%xmm1,48(%rsp)
+++	jmp	.Loop_tail4x
+++
+++.align	32
+++.L128_or_more4x:
+++	movdqu	0(%rsi),%xmm6
+++	movdqu	16(%rsi),%xmm11
+++	movdqu	32(%rsi),%xmm2
+++	movdqu	48(%rsi),%xmm7
+++	pxor	0(%rsp),%xmm6
+++	pxor	%xmm12,%xmm11
+++	pxor	%xmm4,%xmm2
+++	pxor	%xmm0,%xmm7
+++
+++	movdqu	%xmm6,0(%rdi)
+++	movdqu	64(%rsi),%xmm6
+++	movdqu	%xmm11,16(%rdi)
+++	movdqu	80(%rsi),%xmm11
+++	movdqu	%xmm2,32(%rdi)
+++	movdqu	96(%rsi),%xmm2
+++	movdqu	%xmm7,48(%rdi)
+++	movdqu	112(%rsi),%xmm7
+++	pxor	16(%rsp),%xmm6
+++	pxor	%xmm13,%xmm11
+++	pxor	%xmm5,%xmm2
+++	pxor	%xmm1,%xmm7
+++	movdqu	%xmm6,64(%rdi)
+++	movdqu	%xmm11,80(%rdi)
+++	movdqu	%xmm2,96(%rdi)
+++	movdqu	%xmm7,112(%rdi)
+++	je	.Ldone4x
+++
+++	movdqa	32(%rsp),%xmm6
+++	leaq	128(%rsi),%rsi
+++	xorq	%r10,%r10
+++	movdqa	%xmm6,0(%rsp)
+++	movdqa	%xmm10,16(%rsp)
+++	leaq	128(%rdi),%rdi
+++	movdqa	%xmm14,32(%rsp)
+++	subq	$128,%rdx
+++	movdqa	%xmm8,48(%rsp)
+++	jmp	.Loop_tail4x
+++
+++.align	32
+++.L192_or_more4x:
+++	movdqu	0(%rsi),%xmm6
+++	movdqu	16(%rsi),%xmm11
+++	movdqu	32(%rsi),%xmm2
+++	movdqu	48(%rsi),%xmm7
+++	pxor	0(%rsp),%xmm6
+++	pxor	%xmm12,%xmm11
+++	pxor	%xmm4,%xmm2
+++	pxor	%xmm0,%xmm7
+++
+++	movdqu	%xmm6,0(%rdi)
+++	movdqu	64(%rsi),%xmm6
+++	movdqu	%xmm11,16(%rdi)
+++	movdqu	80(%rsi),%xmm11
+++	movdqu	%xmm2,32(%rdi)
+++	movdqu	96(%rsi),%xmm2
+++	movdqu	%xmm7,48(%rdi)
+++	movdqu	112(%rsi),%xmm7
+++	leaq	128(%rsi),%rsi
+++	pxor	16(%rsp),%xmm6
+++	pxor	%xmm13,%xmm11
+++	pxor	%xmm5,%xmm2
+++	pxor	%xmm1,%xmm7
+++
+++	movdqu	%xmm6,64(%rdi)
+++	movdqu	0(%rsi),%xmm6
+++	movdqu	%xmm11,80(%rdi)
+++	movdqu	16(%rsi),%xmm11
+++	movdqu	%xmm2,96(%rdi)
+++	movdqu	32(%rsi),%xmm2
+++	movdqu	%xmm7,112(%rdi)
+++	leaq	128(%rdi),%rdi
+++	movdqu	48(%rsi),%xmm7
+++	pxor	32(%rsp),%xmm6
+++	pxor	%xmm10,%xmm11
+++	pxor	%xmm14,%xmm2
+++	pxor	%xmm8,%xmm7
+++	movdqu	%xmm6,0(%rdi)
+++	movdqu	%xmm11,16(%rdi)
+++	movdqu	%xmm2,32(%rdi)
+++	movdqu	%xmm7,48(%rdi)
+++	je	.Ldone4x
+++
+++	movdqa	48(%rsp),%xmm6
+++	leaq	64(%rsi),%rsi
+++	xorq	%r10,%r10
+++	movdqa	%xmm6,0(%rsp)
+++	movdqa	%xmm15,16(%rsp)
+++	leaq	64(%rdi),%rdi
+++	movdqa	%xmm9,32(%rsp)
+++	subq	$192,%rdx
+++	movdqa	%xmm3,48(%rsp)
+++
+++.Loop_tail4x:
+++	movzbl	(%rsi,%r10,1),%eax
+++	movzbl	(%rsp,%r10,1),%ecx
+++	leaq	1(%r10),%r10
+++	xorl	%ecx,%eax
+++	movb	%al,-1(%rdi,%r10,1)
+++	decq	%rdx
+++	jnz	.Loop_tail4x
+++
+++.Ldone4x:
+++	leaq	(%r9),%rsp
+++.cfi_def_cfa_register	rsp
+++.L4x_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ChaCha20_4x,.-ChaCha20_4x
+++.type	ChaCha20_8x,@function
+++.align	32
+++ChaCha20_8x:
+++.LChaCha20_8x:
+++.cfi_startproc	
+++	movq	%rsp,%r9
+++.cfi_def_cfa_register	r9
+++	subq	$0x280+8,%rsp
+++	andq	$-32,%rsp
+++	vzeroupper
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	vbroadcasti128	.Lsigma(%rip),%ymm11
+++	vbroadcasti128	(%rcx),%ymm3
+++	vbroadcasti128	16(%rcx),%ymm15
+++	vbroadcasti128	(%r8),%ymm7
+++	leaq	256(%rsp),%rcx
+++	leaq	512(%rsp),%rax
+++	leaq	.Lrot16(%rip),%r10
+++	leaq	.Lrot24(%rip),%r11
+++
+++	vpshufd	$0x00,%ymm11,%ymm8
+++	vpshufd	$0x55,%ymm11,%ymm9
+++	vmovdqa	%ymm8,128-256(%rcx)
+++	vpshufd	$0xaa,%ymm11,%ymm10
+++	vmovdqa	%ymm9,160-256(%rcx)
+++	vpshufd	$0xff,%ymm11,%ymm11
+++	vmovdqa	%ymm10,192-256(%rcx)
+++	vmovdqa	%ymm11,224-256(%rcx)
+++
+++	vpshufd	$0x00,%ymm3,%ymm0
+++	vpshufd	$0x55,%ymm3,%ymm1
+++	vmovdqa	%ymm0,256-256(%rcx)
+++	vpshufd	$0xaa,%ymm3,%ymm2
+++	vmovdqa	%ymm1,288-256(%rcx)
+++	vpshufd	$0xff,%ymm3,%ymm3
+++	vmovdqa	%ymm2,320-256(%rcx)
+++	vmovdqa	%ymm3,352-256(%rcx)
+++
+++	vpshufd	$0x00,%ymm15,%ymm12
+++	vpshufd	$0x55,%ymm15,%ymm13
+++	vmovdqa	%ymm12,384-512(%rax)
+++	vpshufd	$0xaa,%ymm15,%ymm14
+++	vmovdqa	%ymm13,416-512(%rax)
+++	vpshufd	$0xff,%ymm15,%ymm15
+++	vmovdqa	%ymm14,448-512(%rax)
+++	vmovdqa	%ymm15,480-512(%rax)
+++
+++	vpshufd	$0x00,%ymm7,%ymm4
+++	vpshufd	$0x55,%ymm7,%ymm5
+++	vpaddd	.Lincy(%rip),%ymm4,%ymm4
+++	vpshufd	$0xaa,%ymm7,%ymm6
+++	vmovdqa	%ymm5,544-512(%rax)
+++	vpshufd	$0xff,%ymm7,%ymm7
+++	vmovdqa	%ymm6,576-512(%rax)
+++	vmovdqa	%ymm7,608-512(%rax)
+++
+++	jmp	.Loop_enter8x
+++
+++.align	32
+++.Loop_outer8x:
+++	vmovdqa	128-256(%rcx),%ymm8
+++	vmovdqa	160-256(%rcx),%ymm9
+++	vmovdqa	192-256(%rcx),%ymm10
+++	vmovdqa	224-256(%rcx),%ymm11
+++	vmovdqa	256-256(%rcx),%ymm0
+++	vmovdqa	288-256(%rcx),%ymm1
+++	vmovdqa	320-256(%rcx),%ymm2
+++	vmovdqa	352-256(%rcx),%ymm3
+++	vmovdqa	384-512(%rax),%ymm12
+++	vmovdqa	416-512(%rax),%ymm13
+++	vmovdqa	448-512(%rax),%ymm14
+++	vmovdqa	480-512(%rax),%ymm15
+++	vmovdqa	512-512(%rax),%ymm4
+++	vmovdqa	544-512(%rax),%ymm5
+++	vmovdqa	576-512(%rax),%ymm6
+++	vmovdqa	608-512(%rax),%ymm7
+++	vpaddd	.Leight(%rip),%ymm4,%ymm4
+++
+++.Loop_enter8x:
+++	vmovdqa	%ymm14,64(%rsp)
+++	vmovdqa	%ymm15,96(%rsp)
+++	vbroadcasti128	(%r10),%ymm15
+++	vmovdqa	%ymm4,512-512(%rax)
+++	movl	$10,%eax
+++	jmp	.Loop8x
+++
+++.align	32
+++.Loop8x:
+++	vpaddd	%ymm0,%ymm8,%ymm8
+++	vpxor	%ymm4,%ymm8,%ymm4
+++	vpshufb	%ymm15,%ymm4,%ymm4
+++	vpaddd	%ymm1,%ymm9,%ymm9
+++	vpxor	%ymm5,%ymm9,%ymm5
+++	vpshufb	%ymm15,%ymm5,%ymm5
+++	vpaddd	%ymm4,%ymm12,%ymm12
+++	vpxor	%ymm0,%ymm12,%ymm0
+++	vpslld	$12,%ymm0,%ymm14
+++	vpsrld	$20,%ymm0,%ymm0
+++	vpor	%ymm0,%ymm14,%ymm0
+++	vbroadcasti128	(%r11),%ymm14
+++	vpaddd	%ymm5,%ymm13,%ymm13
+++	vpxor	%ymm1,%ymm13,%ymm1
+++	vpslld	$12,%ymm1,%ymm15
+++	vpsrld	$20,%ymm1,%ymm1
+++	vpor	%ymm1,%ymm15,%ymm1
+++	vpaddd	%ymm0,%ymm8,%ymm8
+++	vpxor	%ymm4,%ymm8,%ymm4
+++	vpshufb	%ymm14,%ymm4,%ymm4
+++	vpaddd	%ymm1,%ymm9,%ymm9
+++	vpxor	%ymm5,%ymm9,%ymm5
+++	vpshufb	%ymm14,%ymm5,%ymm5
+++	vpaddd	%ymm4,%ymm12,%ymm12
+++	vpxor	%ymm0,%ymm12,%ymm0
+++	vpslld	$7,%ymm0,%ymm15
+++	vpsrld	$25,%ymm0,%ymm0
+++	vpor	%ymm0,%ymm15,%ymm0
+++	vbroadcasti128	(%r10),%ymm15
+++	vpaddd	%ymm5,%ymm13,%ymm13
+++	vpxor	%ymm1,%ymm13,%ymm1
+++	vpslld	$7,%ymm1,%ymm14
+++	vpsrld	$25,%ymm1,%ymm1
+++	vpor	%ymm1,%ymm14,%ymm1
+++	vmovdqa	%ymm12,0(%rsp)
+++	vmovdqa	%ymm13,32(%rsp)
+++	vmovdqa	64(%rsp),%ymm12
+++	vmovdqa	96(%rsp),%ymm13
+++	vpaddd	%ymm2,%ymm10,%ymm10
+++	vpxor	%ymm6,%ymm10,%ymm6
+++	vpshufb	%ymm15,%ymm6,%ymm6
+++	vpaddd	%ymm3,%ymm11,%ymm11
+++	vpxor	%ymm7,%ymm11,%ymm7
+++	vpshufb	%ymm15,%ymm7,%ymm7
+++	vpaddd	%ymm6,%ymm12,%ymm12
+++	vpxor	%ymm2,%ymm12,%ymm2
+++	vpslld	$12,%ymm2,%ymm14
+++	vpsrld	$20,%ymm2,%ymm2
+++	vpor	%ymm2,%ymm14,%ymm2
+++	vbroadcasti128	(%r11),%ymm14
+++	vpaddd	%ymm7,%ymm13,%ymm13
+++	vpxor	%ymm3,%ymm13,%ymm3
+++	vpslld	$12,%ymm3,%ymm15
+++	vpsrld	$20,%ymm3,%ymm3
+++	vpor	%ymm3,%ymm15,%ymm3
+++	vpaddd	%ymm2,%ymm10,%ymm10
+++	vpxor	%ymm6,%ymm10,%ymm6
+++	vpshufb	%ymm14,%ymm6,%ymm6
+++	vpaddd	%ymm3,%ymm11,%ymm11
+++	vpxor	%ymm7,%ymm11,%ymm7
+++	vpshufb	%ymm14,%ymm7,%ymm7
+++	vpaddd	%ymm6,%ymm12,%ymm12
+++	vpxor	%ymm2,%ymm12,%ymm2
+++	vpslld	$7,%ymm2,%ymm15
+++	vpsrld	$25,%ymm2,%ymm2
+++	vpor	%ymm2,%ymm15,%ymm2
+++	vbroadcasti128	(%r10),%ymm15
+++	vpaddd	%ymm7,%ymm13,%ymm13
+++	vpxor	%ymm3,%ymm13,%ymm3
+++	vpslld	$7,%ymm3,%ymm14
+++	vpsrld	$25,%ymm3,%ymm3
+++	vpor	%ymm3,%ymm14,%ymm3
+++	vpaddd	%ymm1,%ymm8,%ymm8
+++	vpxor	%ymm7,%ymm8,%ymm7
+++	vpshufb	%ymm15,%ymm7,%ymm7
+++	vpaddd	%ymm2,%ymm9,%ymm9
+++	vpxor	%ymm4,%ymm9,%ymm4
+++	vpshufb	%ymm15,%ymm4,%ymm4
+++	vpaddd	%ymm7,%ymm12,%ymm12
+++	vpxor	%ymm1,%ymm12,%ymm1
+++	vpslld	$12,%ymm1,%ymm14
+++	vpsrld	$20,%ymm1,%ymm1
+++	vpor	%ymm1,%ymm14,%ymm1
+++	vbroadcasti128	(%r11),%ymm14
+++	vpaddd	%ymm4,%ymm13,%ymm13
+++	vpxor	%ymm2,%ymm13,%ymm2
+++	vpslld	$12,%ymm2,%ymm15
+++	vpsrld	$20,%ymm2,%ymm2
+++	vpor	%ymm2,%ymm15,%ymm2
+++	vpaddd	%ymm1,%ymm8,%ymm8
+++	vpxor	%ymm7,%ymm8,%ymm7
+++	vpshufb	%ymm14,%ymm7,%ymm7
+++	vpaddd	%ymm2,%ymm9,%ymm9
+++	vpxor	%ymm4,%ymm9,%ymm4
+++	vpshufb	%ymm14,%ymm4,%ymm4
+++	vpaddd	%ymm7,%ymm12,%ymm12
+++	vpxor	%ymm1,%ymm12,%ymm1
+++	vpslld	$7,%ymm1,%ymm15
+++	vpsrld	$25,%ymm1,%ymm1
+++	vpor	%ymm1,%ymm15,%ymm1
+++	vbroadcasti128	(%r10),%ymm15
+++	vpaddd	%ymm4,%ymm13,%ymm13
+++	vpxor	%ymm2,%ymm13,%ymm2
+++	vpslld	$7,%ymm2,%ymm14
+++	vpsrld	$25,%ymm2,%ymm2
+++	vpor	%ymm2,%ymm14,%ymm2
+++	vmovdqa	%ymm12,64(%rsp)
+++	vmovdqa	%ymm13,96(%rsp)
+++	vmovdqa	0(%rsp),%ymm12
+++	vmovdqa	32(%rsp),%ymm13
+++	vpaddd	%ymm3,%ymm10,%ymm10
+++	vpxor	%ymm5,%ymm10,%ymm5
+++	vpshufb	%ymm15,%ymm5,%ymm5
+++	vpaddd	%ymm0,%ymm11,%ymm11
+++	vpxor	%ymm6,%ymm11,%ymm6
+++	vpshufb	%ymm15,%ymm6,%ymm6
+++	vpaddd	%ymm5,%ymm12,%ymm12
+++	vpxor	%ymm3,%ymm12,%ymm3
+++	vpslld	$12,%ymm3,%ymm14
+++	vpsrld	$20,%ymm3,%ymm3
+++	vpor	%ymm3,%ymm14,%ymm3
+++	vbroadcasti128	(%r11),%ymm14
+++	vpaddd	%ymm6,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm13,%ymm0
+++	vpslld	$12,%ymm0,%ymm15
+++	vpsrld	$20,%ymm0,%ymm0
+++	vpor	%ymm0,%ymm15,%ymm0
+++	vpaddd	%ymm3,%ymm10,%ymm10
+++	vpxor	%ymm5,%ymm10,%ymm5
+++	vpshufb	%ymm14,%ymm5,%ymm5
+++	vpaddd	%ymm0,%ymm11,%ymm11
+++	vpxor	%ymm6,%ymm11,%ymm6
+++	vpshufb	%ymm14,%ymm6,%ymm6
+++	vpaddd	%ymm5,%ymm12,%ymm12
+++	vpxor	%ymm3,%ymm12,%ymm3
+++	vpslld	$7,%ymm3,%ymm15
+++	vpsrld	$25,%ymm3,%ymm3
+++	vpor	%ymm3,%ymm15,%ymm3
+++	vbroadcasti128	(%r10),%ymm15
+++	vpaddd	%ymm6,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm13,%ymm0
+++	vpslld	$7,%ymm0,%ymm14
+++	vpsrld	$25,%ymm0,%ymm0
+++	vpor	%ymm0,%ymm14,%ymm0
+++	decl	%eax
+++	jnz	.Loop8x
+++
+++	leaq	512(%rsp),%rax
+++	vpaddd	128-256(%rcx),%ymm8,%ymm8
+++	vpaddd	160-256(%rcx),%ymm9,%ymm9
+++	vpaddd	192-256(%rcx),%ymm10,%ymm10
+++	vpaddd	224-256(%rcx),%ymm11,%ymm11
+++
+++	vpunpckldq	%ymm9,%ymm8,%ymm14
+++	vpunpckldq	%ymm11,%ymm10,%ymm15
+++	vpunpckhdq	%ymm9,%ymm8,%ymm8
+++	vpunpckhdq	%ymm11,%ymm10,%ymm10
+++	vpunpcklqdq	%ymm15,%ymm14,%ymm9
+++	vpunpckhqdq	%ymm15,%ymm14,%ymm14
+++	vpunpcklqdq	%ymm10,%ymm8,%ymm11
+++	vpunpckhqdq	%ymm10,%ymm8,%ymm8
+++	vpaddd	256-256(%rcx),%ymm0,%ymm0
+++	vpaddd	288-256(%rcx),%ymm1,%ymm1
+++	vpaddd	320-256(%rcx),%ymm2,%ymm2
+++	vpaddd	352-256(%rcx),%ymm3,%ymm3
+++
+++	vpunpckldq	%ymm1,%ymm0,%ymm10
+++	vpunpckldq	%ymm3,%ymm2,%ymm15
+++	vpunpckhdq	%ymm1,%ymm0,%ymm0
+++	vpunpckhdq	%ymm3,%ymm2,%ymm2
+++	vpunpcklqdq	%ymm15,%ymm10,%ymm1
+++	vpunpckhqdq	%ymm15,%ymm10,%ymm10
+++	vpunpcklqdq	%ymm2,%ymm0,%ymm3
+++	vpunpckhqdq	%ymm2,%ymm0,%ymm0
+++	vperm2i128	$0x20,%ymm1,%ymm9,%ymm15
+++	vperm2i128	$0x31,%ymm1,%ymm9,%ymm1
+++	vperm2i128	$0x20,%ymm10,%ymm14,%ymm9
+++	vperm2i128	$0x31,%ymm10,%ymm14,%ymm10
+++	vperm2i128	$0x20,%ymm3,%ymm11,%ymm14
+++	vperm2i128	$0x31,%ymm3,%ymm11,%ymm3
+++	vperm2i128	$0x20,%ymm0,%ymm8,%ymm11
+++	vperm2i128	$0x31,%ymm0,%ymm8,%ymm0
+++	vmovdqa	%ymm15,0(%rsp)
+++	vmovdqa	%ymm9,32(%rsp)
+++	vmovdqa	64(%rsp),%ymm15
+++	vmovdqa	96(%rsp),%ymm9
+++
+++	vpaddd	384-512(%rax),%ymm12,%ymm12
+++	vpaddd	416-512(%rax),%ymm13,%ymm13
+++	vpaddd	448-512(%rax),%ymm15,%ymm15
+++	vpaddd	480-512(%rax),%ymm9,%ymm9
+++
+++	vpunpckldq	%ymm13,%ymm12,%ymm2
+++	vpunpckldq	%ymm9,%ymm15,%ymm8
+++	vpunpckhdq	%ymm13,%ymm12,%ymm12
+++	vpunpckhdq	%ymm9,%ymm15,%ymm15
+++	vpunpcklqdq	%ymm8,%ymm2,%ymm13
+++	vpunpckhqdq	%ymm8,%ymm2,%ymm2
+++	vpunpcklqdq	%ymm15,%ymm12,%ymm9
+++	vpunpckhqdq	%ymm15,%ymm12,%ymm12
+++	vpaddd	512-512(%rax),%ymm4,%ymm4
+++	vpaddd	544-512(%rax),%ymm5,%ymm5
+++	vpaddd	576-512(%rax),%ymm6,%ymm6
+++	vpaddd	608-512(%rax),%ymm7,%ymm7
+++
+++	vpunpckldq	%ymm5,%ymm4,%ymm15
+++	vpunpckldq	%ymm7,%ymm6,%ymm8
+++	vpunpckhdq	%ymm5,%ymm4,%ymm4
+++	vpunpckhdq	%ymm7,%ymm6,%ymm6
+++	vpunpcklqdq	%ymm8,%ymm15,%ymm5
+++	vpunpckhqdq	%ymm8,%ymm15,%ymm15
+++	vpunpcklqdq	%ymm6,%ymm4,%ymm7
+++	vpunpckhqdq	%ymm6,%ymm4,%ymm4
+++	vperm2i128	$0x20,%ymm5,%ymm13,%ymm8
+++	vperm2i128	$0x31,%ymm5,%ymm13,%ymm5
+++	vperm2i128	$0x20,%ymm15,%ymm2,%ymm13
+++	vperm2i128	$0x31,%ymm15,%ymm2,%ymm15
+++	vperm2i128	$0x20,%ymm7,%ymm9,%ymm2
+++	vperm2i128	$0x31,%ymm7,%ymm9,%ymm7
+++	vperm2i128	$0x20,%ymm4,%ymm12,%ymm9
+++	vperm2i128	$0x31,%ymm4,%ymm12,%ymm4
+++	vmovdqa	0(%rsp),%ymm6
+++	vmovdqa	32(%rsp),%ymm12
+++
+++	cmpq	$512,%rdx
+++	jb	.Ltail8x
+++
+++	vpxor	0(%rsi),%ymm6,%ymm6
+++	vpxor	32(%rsi),%ymm8,%ymm8
+++	vpxor	64(%rsi),%ymm1,%ymm1
+++	vpxor	96(%rsi),%ymm5,%ymm5
+++	leaq	128(%rsi),%rsi
+++	vmovdqu	%ymm6,0(%rdi)
+++	vmovdqu	%ymm8,32(%rdi)
+++	vmovdqu	%ymm1,64(%rdi)
+++	vmovdqu	%ymm5,96(%rdi)
+++	leaq	128(%rdi),%rdi
+++
+++	vpxor	0(%rsi),%ymm12,%ymm12
+++	vpxor	32(%rsi),%ymm13,%ymm13
+++	vpxor	64(%rsi),%ymm10,%ymm10
+++	vpxor	96(%rsi),%ymm15,%ymm15
+++	leaq	128(%rsi),%rsi
+++	vmovdqu	%ymm12,0(%rdi)
+++	vmovdqu	%ymm13,32(%rdi)
+++	vmovdqu	%ymm10,64(%rdi)
+++	vmovdqu	%ymm15,96(%rdi)
+++	leaq	128(%rdi),%rdi
+++
+++	vpxor	0(%rsi),%ymm14,%ymm14
+++	vpxor	32(%rsi),%ymm2,%ymm2
+++	vpxor	64(%rsi),%ymm3,%ymm3
+++	vpxor	96(%rsi),%ymm7,%ymm7
+++	leaq	128(%rsi),%rsi
+++	vmovdqu	%ymm14,0(%rdi)
+++	vmovdqu	%ymm2,32(%rdi)
+++	vmovdqu	%ymm3,64(%rdi)
+++	vmovdqu	%ymm7,96(%rdi)
+++	leaq	128(%rdi),%rdi
+++
+++	vpxor	0(%rsi),%ymm11,%ymm11
+++	vpxor	32(%rsi),%ymm9,%ymm9
+++	vpxor	64(%rsi),%ymm0,%ymm0
+++	vpxor	96(%rsi),%ymm4,%ymm4
+++	leaq	128(%rsi),%rsi
+++	vmovdqu	%ymm11,0(%rdi)
+++	vmovdqu	%ymm9,32(%rdi)
+++	vmovdqu	%ymm0,64(%rdi)
+++	vmovdqu	%ymm4,96(%rdi)
+++	leaq	128(%rdi),%rdi
+++
+++	subq	$512,%rdx
+++	jnz	.Loop_outer8x
+++
+++	jmp	.Ldone8x
+++
+++.Ltail8x:
+++	cmpq	$448,%rdx
+++	jae	.L448_or_more8x
+++	cmpq	$384,%rdx
+++	jae	.L384_or_more8x
+++	cmpq	$320,%rdx
+++	jae	.L320_or_more8x
+++	cmpq	$256,%rdx
+++	jae	.L256_or_more8x
+++	cmpq	$192,%rdx
+++	jae	.L192_or_more8x
+++	cmpq	$128,%rdx
+++	jae	.L128_or_more8x
+++	cmpq	$64,%rdx
+++	jae	.L64_or_more8x
+++
+++	xorq	%r10,%r10
+++	vmovdqa	%ymm6,0(%rsp)
+++	vmovdqa	%ymm8,32(%rsp)
+++	jmp	.Loop_tail8x
+++
+++.align	32
+++.L64_or_more8x:
+++	vpxor	0(%rsi),%ymm6,%ymm6
+++	vpxor	32(%rsi),%ymm8,%ymm8
+++	vmovdqu	%ymm6,0(%rdi)
+++	vmovdqu	%ymm8,32(%rdi)
+++	je	.Ldone8x
+++
+++	leaq	64(%rsi),%rsi
+++	xorq	%r10,%r10
+++	vmovdqa	%ymm1,0(%rsp)
+++	leaq	64(%rdi),%rdi
+++	subq	$64,%rdx
+++	vmovdqa	%ymm5,32(%rsp)
+++	jmp	.Loop_tail8x
+++
+++.align	32
+++.L128_or_more8x:
+++	vpxor	0(%rsi),%ymm6,%ymm6
+++	vpxor	32(%rsi),%ymm8,%ymm8
+++	vpxor	64(%rsi),%ymm1,%ymm1
+++	vpxor	96(%rsi),%ymm5,%ymm5
+++	vmovdqu	%ymm6,0(%rdi)
+++	vmovdqu	%ymm8,32(%rdi)
+++	vmovdqu	%ymm1,64(%rdi)
+++	vmovdqu	%ymm5,96(%rdi)
+++	je	.Ldone8x
+++
+++	leaq	128(%rsi),%rsi
+++	xorq	%r10,%r10
+++	vmovdqa	%ymm12,0(%rsp)
+++	leaq	128(%rdi),%rdi
+++	subq	$128,%rdx
+++	vmovdqa	%ymm13,32(%rsp)
+++	jmp	.Loop_tail8x
+++
+++.align	32
+++.L192_or_more8x:
+++	vpxor	0(%rsi),%ymm6,%ymm6
+++	vpxor	32(%rsi),%ymm8,%ymm8
+++	vpxor	64(%rsi),%ymm1,%ymm1
+++	vpxor	96(%rsi),%ymm5,%ymm5
+++	vpxor	128(%rsi),%ymm12,%ymm12
+++	vpxor	160(%rsi),%ymm13,%ymm13
+++	vmovdqu	%ymm6,0(%rdi)
+++	vmovdqu	%ymm8,32(%rdi)
+++	vmovdqu	%ymm1,64(%rdi)
+++	vmovdqu	%ymm5,96(%rdi)
+++	vmovdqu	%ymm12,128(%rdi)
+++	vmovdqu	%ymm13,160(%rdi)
+++	je	.Ldone8x
+++
+++	leaq	192(%rsi),%rsi
+++	xorq	%r10,%r10
+++	vmovdqa	%ymm10,0(%rsp)
+++	leaq	192(%rdi),%rdi
+++	subq	$192,%rdx
+++	vmovdqa	%ymm15,32(%rsp)
+++	jmp	.Loop_tail8x
+++
+++.align	32
+++.L256_or_more8x:
+++	vpxor	0(%rsi),%ymm6,%ymm6
+++	vpxor	32(%rsi),%ymm8,%ymm8
+++	vpxor	64(%rsi),%ymm1,%ymm1
+++	vpxor	96(%rsi),%ymm5,%ymm5
+++	vpxor	128(%rsi),%ymm12,%ymm12
+++	vpxor	160(%rsi),%ymm13,%ymm13
+++	vpxor	192(%rsi),%ymm10,%ymm10
+++	vpxor	224(%rsi),%ymm15,%ymm15
+++	vmovdqu	%ymm6,0(%rdi)
+++	vmovdqu	%ymm8,32(%rdi)
+++	vmovdqu	%ymm1,64(%rdi)
+++	vmovdqu	%ymm5,96(%rdi)
+++	vmovdqu	%ymm12,128(%rdi)
+++	vmovdqu	%ymm13,160(%rdi)
+++	vmovdqu	%ymm10,192(%rdi)
+++	vmovdqu	%ymm15,224(%rdi)
+++	je	.Ldone8x
+++
+++	leaq	256(%rsi),%rsi
+++	xorq	%r10,%r10
+++	vmovdqa	%ymm14,0(%rsp)
+++	leaq	256(%rdi),%rdi
+++	subq	$256,%rdx
+++	vmovdqa	%ymm2,32(%rsp)
+++	jmp	.Loop_tail8x
+++
+++.align	32
+++.L320_or_more8x:
+++	vpxor	0(%rsi),%ymm6,%ymm6
+++	vpxor	32(%rsi),%ymm8,%ymm8
+++	vpxor	64(%rsi),%ymm1,%ymm1
+++	vpxor	96(%rsi),%ymm5,%ymm5
+++	vpxor	128(%rsi),%ymm12,%ymm12
+++	vpxor	160(%rsi),%ymm13,%ymm13
+++	vpxor	192(%rsi),%ymm10,%ymm10
+++	vpxor	224(%rsi),%ymm15,%ymm15
+++	vpxor	256(%rsi),%ymm14,%ymm14
+++	vpxor	288(%rsi),%ymm2,%ymm2
+++	vmovdqu	%ymm6,0(%rdi)
+++	vmovdqu	%ymm8,32(%rdi)
+++	vmovdqu	%ymm1,64(%rdi)
+++	vmovdqu	%ymm5,96(%rdi)
+++	vmovdqu	%ymm12,128(%rdi)
+++	vmovdqu	%ymm13,160(%rdi)
+++	vmovdqu	%ymm10,192(%rdi)
+++	vmovdqu	%ymm15,224(%rdi)
+++	vmovdqu	%ymm14,256(%rdi)
+++	vmovdqu	%ymm2,288(%rdi)
+++	je	.Ldone8x
+++
+++	leaq	320(%rsi),%rsi
+++	xorq	%r10,%r10
+++	vmovdqa	%ymm3,0(%rsp)
+++	leaq	320(%rdi),%rdi
+++	subq	$320,%rdx
+++	vmovdqa	%ymm7,32(%rsp)
+++	jmp	.Loop_tail8x
+++
+++.align	32
+++.L384_or_more8x:
+++	vpxor	0(%rsi),%ymm6,%ymm6
+++	vpxor	32(%rsi),%ymm8,%ymm8
+++	vpxor	64(%rsi),%ymm1,%ymm1
+++	vpxor	96(%rsi),%ymm5,%ymm5
+++	vpxor	128(%rsi),%ymm12,%ymm12
+++	vpxor	160(%rsi),%ymm13,%ymm13
+++	vpxor	192(%rsi),%ymm10,%ymm10
+++	vpxor	224(%rsi),%ymm15,%ymm15
+++	vpxor	256(%rsi),%ymm14,%ymm14
+++	vpxor	288(%rsi),%ymm2,%ymm2
+++	vpxor	320(%rsi),%ymm3,%ymm3
+++	vpxor	352(%rsi),%ymm7,%ymm7
+++	vmovdqu	%ymm6,0(%rdi)
+++	vmovdqu	%ymm8,32(%rdi)
+++	vmovdqu	%ymm1,64(%rdi)
+++	vmovdqu	%ymm5,96(%rdi)
+++	vmovdqu	%ymm12,128(%rdi)
+++	vmovdqu	%ymm13,160(%rdi)
+++	vmovdqu	%ymm10,192(%rdi)
+++	vmovdqu	%ymm15,224(%rdi)
+++	vmovdqu	%ymm14,256(%rdi)
+++	vmovdqu	%ymm2,288(%rdi)
+++	vmovdqu	%ymm3,320(%rdi)
+++	vmovdqu	%ymm7,352(%rdi)
+++	je	.Ldone8x
+++
+++	leaq	384(%rsi),%rsi
+++	xorq	%r10,%r10
+++	vmovdqa	%ymm11,0(%rsp)
+++	leaq	384(%rdi),%rdi
+++	subq	$384,%rdx
+++	vmovdqa	%ymm9,32(%rsp)
+++	jmp	.Loop_tail8x
+++
+++.align	32
+++.L448_or_more8x:
+++	vpxor	0(%rsi),%ymm6,%ymm6
+++	vpxor	32(%rsi),%ymm8,%ymm8
+++	vpxor	64(%rsi),%ymm1,%ymm1
+++	vpxor	96(%rsi),%ymm5,%ymm5
+++	vpxor	128(%rsi),%ymm12,%ymm12
+++	vpxor	160(%rsi),%ymm13,%ymm13
+++	vpxor	192(%rsi),%ymm10,%ymm10
+++	vpxor	224(%rsi),%ymm15,%ymm15
+++	vpxor	256(%rsi),%ymm14,%ymm14
+++	vpxor	288(%rsi),%ymm2,%ymm2
+++	vpxor	320(%rsi),%ymm3,%ymm3
+++	vpxor	352(%rsi),%ymm7,%ymm7
+++	vpxor	384(%rsi),%ymm11,%ymm11
+++	vpxor	416(%rsi),%ymm9,%ymm9
+++	vmovdqu	%ymm6,0(%rdi)
+++	vmovdqu	%ymm8,32(%rdi)
+++	vmovdqu	%ymm1,64(%rdi)
+++	vmovdqu	%ymm5,96(%rdi)
+++	vmovdqu	%ymm12,128(%rdi)
+++	vmovdqu	%ymm13,160(%rdi)
+++	vmovdqu	%ymm10,192(%rdi)
+++	vmovdqu	%ymm15,224(%rdi)
+++	vmovdqu	%ymm14,256(%rdi)
+++	vmovdqu	%ymm2,288(%rdi)
+++	vmovdqu	%ymm3,320(%rdi)
+++	vmovdqu	%ymm7,352(%rdi)
+++	vmovdqu	%ymm11,384(%rdi)
+++	vmovdqu	%ymm9,416(%rdi)
+++	je	.Ldone8x
+++
+++	leaq	448(%rsi),%rsi
+++	xorq	%r10,%r10
+++	vmovdqa	%ymm0,0(%rsp)
+++	leaq	448(%rdi),%rdi
+++	subq	$448,%rdx
+++	vmovdqa	%ymm4,32(%rsp)
+++
+++.Loop_tail8x:
+++	movzbl	(%rsi,%r10,1),%eax
+++	movzbl	(%rsp,%r10,1),%ecx
+++	leaq	1(%r10),%r10
+++	xorl	%ecx,%eax
+++	movb	%al,-1(%rdi,%r10,1)
+++	decq	%rdx
+++	jnz	.Loop_tail8x
+++
+++.Ldone8x:
+++	vzeroall
+++	leaq	(%r9),%rsp
+++.cfi_def_cfa_register	rsp
+++.L8x_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ChaCha20_8x,.-ChaCha20_8x
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/cipher_extra/aes128gcmsiv-x86_64.S b/linux-x86_64/ypto/cipher_extra/aes128gcmsiv-x86_64.S
++new file mode 100644
++index 000000000..a22bee8fc
++--- /dev/null
+++++ b/linux-x86_64/ypto/cipher_extra/aes128gcmsiv-x86_64.S
++@@ -0,0 +1,3079 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.data	
+++
+++.align	16
+++one:
+++.quad	1,0
+++two:
+++.quad	2,0
+++three:
+++.quad	3,0
+++four:
+++.quad	4,0
+++five:
+++.quad	5,0
+++six:
+++.quad	6,0
+++seven:
+++.quad	7,0
+++eight:
+++.quad	8,0
+++
+++OR_MASK:
+++.long	0x00000000,0x00000000,0x00000000,0x80000000
+++poly:
+++.quad	0x1, 0xc200000000000000
+++mask:
+++.long	0x0c0f0e0d,0x0c0f0e0d,0x0c0f0e0d,0x0c0f0e0d
+++con1:
+++.long	1,1,1,1
+++con2:
+++.long	0x1b,0x1b,0x1b,0x1b
+++con3:
+++.byte	-1,-1,-1,-1,-1,-1,-1,-1,4,5,6,7,4,5,6,7
+++and_mask:
+++.long	0,0xffffffff, 0xffffffff, 0xffffffff
+++.text	
+++.type	GFMUL,@function
+++.align	16
+++GFMUL:
+++.cfi_startproc	
+++	vpclmulqdq	$0x00,%xmm1,%xmm0,%xmm2
+++	vpclmulqdq	$0x11,%xmm1,%xmm0,%xmm5
+++	vpclmulqdq	$0x10,%xmm1,%xmm0,%xmm3
+++	vpclmulqdq	$0x01,%xmm1,%xmm0,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpslldq	$8,%xmm3,%xmm4
+++	vpsrldq	$8,%xmm3,%xmm3
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpxor	%xmm3,%xmm5,%xmm5
+++
+++	vpclmulqdq	$0x10,poly(%rip),%xmm2,%xmm3
+++	vpshufd	$78,%xmm2,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm2
+++
+++	vpclmulqdq	$0x10,poly(%rip),%xmm2,%xmm3
+++	vpshufd	$78,%xmm2,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm2
+++
+++	vpxor	%xmm5,%xmm2,%xmm0
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	GFMUL, .-GFMUL
+++.globl	aesgcmsiv_htable_init
+++.hidden aesgcmsiv_htable_init
+++.type	aesgcmsiv_htable_init,@function
+++.align	16
+++aesgcmsiv_htable_init:
+++.cfi_startproc	
+++	vmovdqa	(%rsi),%xmm0
+++	vmovdqa	%xmm0,%xmm1
+++	vmovdqa	%xmm0,(%rdi)
+++	call	GFMUL
+++	vmovdqa	%xmm0,16(%rdi)
+++	call	GFMUL
+++	vmovdqa	%xmm0,32(%rdi)
+++	call	GFMUL
+++	vmovdqa	%xmm0,48(%rdi)
+++	call	GFMUL
+++	vmovdqa	%xmm0,64(%rdi)
+++	call	GFMUL
+++	vmovdqa	%xmm0,80(%rdi)
+++	call	GFMUL
+++	vmovdqa	%xmm0,96(%rdi)
+++	call	GFMUL
+++	vmovdqa	%xmm0,112(%rdi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aesgcmsiv_htable_init, .-aesgcmsiv_htable_init
+++.globl	aesgcmsiv_htable6_init
+++.hidden aesgcmsiv_htable6_init
+++.type	aesgcmsiv_htable6_init,@function
+++.align	16
+++aesgcmsiv_htable6_init:
+++.cfi_startproc	
+++	vmovdqa	(%rsi),%xmm0
+++	vmovdqa	%xmm0,%xmm1
+++	vmovdqa	%xmm0,(%rdi)
+++	call	GFMUL
+++	vmovdqa	%xmm0,16(%rdi)
+++	call	GFMUL
+++	vmovdqa	%xmm0,32(%rdi)
+++	call	GFMUL
+++	vmovdqa	%xmm0,48(%rdi)
+++	call	GFMUL
+++	vmovdqa	%xmm0,64(%rdi)
+++	call	GFMUL
+++	vmovdqa	%xmm0,80(%rdi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aesgcmsiv_htable6_init, .-aesgcmsiv_htable6_init
+++.globl	aesgcmsiv_htable_polyval
+++.hidden aesgcmsiv_htable_polyval
+++.type	aesgcmsiv_htable_polyval,@function
+++.align	16
+++aesgcmsiv_htable_polyval:
+++.cfi_startproc	
+++	testq	%rdx,%rdx
+++	jnz	.Lhtable_polyval_start
+++	.byte	0xf3,0xc3
+++
+++.Lhtable_polyval_start:
+++	vzeroall
+++
+++
+++
+++	movq	%rdx,%r11
+++	andq	$127,%r11
+++
+++	jz	.Lhtable_polyval_no_prefix
+++
+++	vpxor	%xmm9,%xmm9,%xmm9
+++	vmovdqa	(%rcx),%xmm1
+++	subq	%r11,%rdx
+++
+++	subq	$16,%r11
+++
+++
+++	vmovdqu	(%rsi),%xmm0
+++	vpxor	%xmm1,%xmm0,%xmm0
+++
+++	vpclmulqdq	$0x01,(%rdi,%r11,1),%xmm0,%xmm5
+++	vpclmulqdq	$0x00,(%rdi,%r11,1),%xmm0,%xmm3
+++	vpclmulqdq	$0x11,(%rdi,%r11,1),%xmm0,%xmm4
+++	vpclmulqdq	$0x10,(%rdi,%r11,1),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++
+++	leaq	16(%rsi),%rsi
+++	testq	%r11,%r11
+++	jnz	.Lhtable_polyval_prefix_loop
+++	jmp	.Lhtable_polyval_prefix_complete
+++
+++
+++.align	64
+++.Lhtable_polyval_prefix_loop:
+++	subq	$16,%r11
+++
+++	vmovdqu	(%rsi),%xmm0
+++
+++	vpclmulqdq	$0x00,(%rdi,%r11,1),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm3,%xmm3
+++	vpclmulqdq	$0x11,(%rdi,%r11,1),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm4,%xmm4
+++	vpclmulqdq	$0x01,(%rdi,%r11,1),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++	vpclmulqdq	$0x10,(%rdi,%r11,1),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++
+++	testq	%r11,%r11
+++
+++	leaq	16(%rsi),%rsi
+++
+++	jnz	.Lhtable_polyval_prefix_loop
+++
+++.Lhtable_polyval_prefix_complete:
+++	vpsrldq	$8,%xmm5,%xmm6
+++	vpslldq	$8,%xmm5,%xmm5
+++
+++	vpxor	%xmm6,%xmm4,%xmm9
+++	vpxor	%xmm5,%xmm3,%xmm1
+++
+++	jmp	.Lhtable_polyval_main_loop
+++
+++.Lhtable_polyval_no_prefix:
+++
+++
+++
+++
+++	vpxor	%xmm1,%xmm1,%xmm1
+++	vmovdqa	(%rcx),%xmm9
+++
+++.align	64
+++.Lhtable_polyval_main_loop:
+++	subq	$0x80,%rdx
+++	jb	.Lhtable_polyval_out
+++
+++	vmovdqu	112(%rsi),%xmm0
+++
+++	vpclmulqdq	$0x01,(%rdi),%xmm0,%xmm5
+++	vpclmulqdq	$0x00,(%rdi),%xmm0,%xmm3
+++	vpclmulqdq	$0x11,(%rdi),%xmm0,%xmm4
+++	vpclmulqdq	$0x10,(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++
+++
+++	vmovdqu	96(%rsi),%xmm0
+++	vpclmulqdq	$0x01,16(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,16(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm3,%xmm3
+++	vpclmulqdq	$0x11,16(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm4,%xmm4
+++	vpclmulqdq	$0x10,16(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++
+++
+++
+++	vmovdqu	80(%rsi),%xmm0
+++
+++	vpclmulqdq	$0x10,poly(%rip),%xmm1,%xmm7
+++	vpalignr	$8,%xmm1,%xmm1,%xmm1
+++
+++	vpclmulqdq	$0x01,32(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,32(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm3,%xmm3
+++	vpclmulqdq	$0x11,32(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm4,%xmm4
+++	vpclmulqdq	$0x10,32(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++
+++
+++	vpxor	%xmm7,%xmm1,%xmm1
+++
+++	vmovdqu	64(%rsi),%xmm0
+++
+++	vpclmulqdq	$0x01,48(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,48(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm3,%xmm3
+++	vpclmulqdq	$0x11,48(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm4,%xmm4
+++	vpclmulqdq	$0x10,48(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++
+++
+++	vmovdqu	48(%rsi),%xmm0
+++
+++	vpclmulqdq	$0x10,poly(%rip),%xmm1,%xmm7
+++	vpalignr	$8,%xmm1,%xmm1,%xmm1
+++
+++	vpclmulqdq	$0x01,64(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,64(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm3,%xmm3
+++	vpclmulqdq	$0x11,64(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm4,%xmm4
+++	vpclmulqdq	$0x10,64(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++
+++
+++	vpxor	%xmm7,%xmm1,%xmm1
+++
+++	vmovdqu	32(%rsi),%xmm0
+++
+++	vpclmulqdq	$0x01,80(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,80(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm3,%xmm3
+++	vpclmulqdq	$0x11,80(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm4,%xmm4
+++	vpclmulqdq	$0x10,80(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++
+++
+++	vpxor	%xmm9,%xmm1,%xmm1
+++
+++	vmovdqu	16(%rsi),%xmm0
+++
+++	vpclmulqdq	$0x01,96(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,96(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm3,%xmm3
+++	vpclmulqdq	$0x11,96(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm4,%xmm4
+++	vpclmulqdq	$0x10,96(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++
+++
+++	vmovdqu	0(%rsi),%xmm0
+++	vpxor	%xmm1,%xmm0,%xmm0
+++
+++	vpclmulqdq	$0x01,112(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,112(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm3,%xmm3
+++	vpclmulqdq	$0x11,112(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm4,%xmm4
+++	vpclmulqdq	$0x10,112(%rdi),%xmm0,%xmm6
+++	vpxor	%xmm6,%xmm5,%xmm5
+++
+++
+++	vpsrldq	$8,%xmm5,%xmm6
+++	vpslldq	$8,%xmm5,%xmm5
+++
+++	vpxor	%xmm6,%xmm4,%xmm9
+++	vpxor	%xmm5,%xmm3,%xmm1
+++
+++	leaq	128(%rsi),%rsi
+++	jmp	.Lhtable_polyval_main_loop
+++
+++
+++
+++.Lhtable_polyval_out:
+++	vpclmulqdq	$0x10,poly(%rip),%xmm1,%xmm6
+++	vpalignr	$8,%xmm1,%xmm1,%xmm1
+++	vpxor	%xmm6,%xmm1,%xmm1
+++
+++	vpclmulqdq	$0x10,poly(%rip),%xmm1,%xmm6
+++	vpalignr	$8,%xmm1,%xmm1,%xmm1
+++	vpxor	%xmm6,%xmm1,%xmm1
+++	vpxor	%xmm9,%xmm1,%xmm1
+++
+++	vmovdqu	%xmm1,(%rcx)
+++	vzeroupper
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aesgcmsiv_htable_polyval,.-aesgcmsiv_htable_polyval
+++.globl	aesgcmsiv_polyval_horner
+++.hidden aesgcmsiv_polyval_horner
+++.type	aesgcmsiv_polyval_horner,@function
+++.align	16
+++aesgcmsiv_polyval_horner:
+++.cfi_startproc	
+++	testq	%rcx,%rcx
+++	jnz	.Lpolyval_horner_start
+++	.byte	0xf3,0xc3
+++
+++.Lpolyval_horner_start:
+++
+++
+++
+++	xorq	%r10,%r10
+++	shlq	$4,%rcx
+++
+++	vmovdqa	(%rsi),%xmm1
+++	vmovdqa	(%rdi),%xmm0
+++
+++.Lpolyval_horner_loop:
+++	vpxor	(%rdx,%r10,1),%xmm0,%xmm0
+++	call	GFMUL
+++
+++	addq	$16,%r10
+++	cmpq	%r10,%rcx
+++	jne	.Lpolyval_horner_loop
+++
+++
+++	vmovdqa	%xmm0,(%rdi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aesgcmsiv_polyval_horner,.-aesgcmsiv_polyval_horner
+++.globl	aes128gcmsiv_aes_ks
+++.hidden aes128gcmsiv_aes_ks
+++.type	aes128gcmsiv_aes_ks,@function
+++.align	16
+++aes128gcmsiv_aes_ks:
+++.cfi_startproc	
+++	vmovdqu	(%rdi),%xmm1
+++	vmovdqa	%xmm1,(%rsi)
+++
+++	vmovdqa	con1(%rip),%xmm0
+++	vmovdqa	mask(%rip),%xmm15
+++
+++	movq	$8,%rax
+++
+++.Lks128_loop:
+++	addq	$16,%rsi
+++	subq	$1,%rax
+++	vpshufb	%xmm15,%xmm1,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpslldq	$4,%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpslldq	$4,%xmm3,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpslldq	$4,%xmm3,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	vmovdqa	%xmm1,(%rsi)
+++	jne	.Lks128_loop
+++
+++	vmovdqa	con2(%rip),%xmm0
+++	vpshufb	%xmm15,%xmm1,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpslldq	$4,%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpslldq	$4,%xmm3,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpslldq	$4,%xmm3,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	vmovdqa	%xmm1,16(%rsi)
+++
+++	vpshufb	%xmm15,%xmm1,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslldq	$4,%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpslldq	$4,%xmm3,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpslldq	$4,%xmm3,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	vmovdqa	%xmm1,32(%rsi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes128gcmsiv_aes_ks,.-aes128gcmsiv_aes_ks
+++.globl	aes256gcmsiv_aes_ks
+++.hidden aes256gcmsiv_aes_ks
+++.type	aes256gcmsiv_aes_ks,@function
+++.align	16
+++aes256gcmsiv_aes_ks:
+++.cfi_startproc	
+++	vmovdqu	(%rdi),%xmm1
+++	vmovdqu	16(%rdi),%xmm3
+++	vmovdqa	%xmm1,(%rsi)
+++	vmovdqa	%xmm3,16(%rsi)
+++	vmovdqa	con1(%rip),%xmm0
+++	vmovdqa	mask(%rip),%xmm15
+++	vpxor	%xmm14,%xmm14,%xmm14
+++	movq	$6,%rax
+++
+++.Lks256_loop:
+++	addq	$32,%rsi
+++	subq	$1,%rax
+++	vpshufb	%xmm15,%xmm3,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpsllq	$32,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpshufb	con3(%rip),%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	vmovdqa	%xmm1,(%rsi)
+++	vpshufd	$0xff,%xmm1,%xmm2
+++	vaesenclast	%xmm14,%xmm2,%xmm2
+++	vpsllq	$32,%xmm3,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpshufb	con3(%rip),%xmm3,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpxor	%xmm2,%xmm3,%xmm3
+++	vmovdqa	%xmm3,16(%rsi)
+++	jne	.Lks256_loop
+++
+++	vpshufb	%xmm15,%xmm3,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpsllq	$32,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpshufb	con3(%rip),%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	vmovdqa	%xmm1,32(%rsi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.globl	aes128gcmsiv_aes_ks_enc_x1
+++.hidden aes128gcmsiv_aes_ks_enc_x1
+++.type	aes128gcmsiv_aes_ks_enc_x1,@function
+++.align	16
+++aes128gcmsiv_aes_ks_enc_x1:
+++.cfi_startproc	
+++	vmovdqa	(%rcx),%xmm1
+++	vmovdqa	0(%rdi),%xmm4
+++
+++	vmovdqa	%xmm1,(%rdx)
+++	vpxor	%xmm1,%xmm4,%xmm4
+++
+++	vmovdqa	con1(%rip),%xmm0
+++	vmovdqa	mask(%rip),%xmm15
+++
+++	vpshufb	%xmm15,%xmm1,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpsllq	$32,%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpshufb	con3(%rip),%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vmovdqa	%xmm1,16(%rdx)
+++
+++	vpshufb	%xmm15,%xmm1,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpsllq	$32,%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpshufb	con3(%rip),%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vmovdqa	%xmm1,32(%rdx)
+++
+++	vpshufb	%xmm15,%xmm1,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpsllq	$32,%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpshufb	con3(%rip),%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vmovdqa	%xmm1,48(%rdx)
+++
+++	vpshufb	%xmm15,%xmm1,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpsllq	$32,%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpshufb	con3(%rip),%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vmovdqa	%xmm1,64(%rdx)
+++
+++	vpshufb	%xmm15,%xmm1,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpsllq	$32,%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpshufb	con3(%rip),%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vmovdqa	%xmm1,80(%rdx)
+++
+++	vpshufb	%xmm15,%xmm1,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpsllq	$32,%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpshufb	con3(%rip),%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vmovdqa	%xmm1,96(%rdx)
+++
+++	vpshufb	%xmm15,%xmm1,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpsllq	$32,%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpshufb	con3(%rip),%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vmovdqa	%xmm1,112(%rdx)
+++
+++	vpshufb	%xmm15,%xmm1,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpsllq	$32,%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpshufb	con3(%rip),%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vmovdqa	%xmm1,128(%rdx)
+++
+++
+++	vmovdqa	con2(%rip),%xmm0
+++
+++	vpshufb	%xmm15,%xmm1,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpsllq	$32,%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpshufb	con3(%rip),%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vmovdqa	%xmm1,144(%rdx)
+++
+++	vpshufb	%xmm15,%xmm1,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpsllq	$32,%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpshufb	con3(%rip),%xmm1,%xmm3
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++
+++	vaesenclast	%xmm1,%xmm4,%xmm4
+++	vmovdqa	%xmm1,160(%rdx)
+++
+++
+++	vmovdqa	%xmm4,0(%rsi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes128gcmsiv_aes_ks_enc_x1,.-aes128gcmsiv_aes_ks_enc_x1
+++.globl	aes128gcmsiv_kdf
+++.hidden aes128gcmsiv_kdf
+++.type	aes128gcmsiv_kdf,@function
+++.align	16
+++aes128gcmsiv_kdf:
+++.cfi_startproc	
+++
+++
+++
+++
+++	vmovdqa	(%rdx),%xmm1
+++	vmovdqa	0(%rdi),%xmm9
+++	vmovdqa	and_mask(%rip),%xmm12
+++	vmovdqa	one(%rip),%xmm13
+++	vpshufd	$0x90,%xmm9,%xmm9
+++	vpand	%xmm12,%xmm9,%xmm9
+++	vpaddd	%xmm13,%xmm9,%xmm10
+++	vpaddd	%xmm13,%xmm10,%xmm11
+++	vpaddd	%xmm13,%xmm11,%xmm12
+++
+++	vpxor	%xmm1,%xmm9,%xmm9
+++	vpxor	%xmm1,%xmm10,%xmm10
+++	vpxor	%xmm1,%xmm11,%xmm11
+++	vpxor	%xmm1,%xmm12,%xmm12
+++
+++	vmovdqa	16(%rdx),%xmm1
+++	vaesenc	%xmm1,%xmm9,%xmm9
+++	vaesenc	%xmm1,%xmm10,%xmm10
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++
+++	vmovdqa	32(%rdx),%xmm2
+++	vaesenc	%xmm2,%xmm9,%xmm9
+++	vaesenc	%xmm2,%xmm10,%xmm10
+++	vaesenc	%xmm2,%xmm11,%xmm11
+++	vaesenc	%xmm2,%xmm12,%xmm12
+++
+++	vmovdqa	48(%rdx),%xmm1
+++	vaesenc	%xmm1,%xmm9,%xmm9
+++	vaesenc	%xmm1,%xmm10,%xmm10
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++
+++	vmovdqa	64(%rdx),%xmm2
+++	vaesenc	%xmm2,%xmm9,%xmm9
+++	vaesenc	%xmm2,%xmm10,%xmm10
+++	vaesenc	%xmm2,%xmm11,%xmm11
+++	vaesenc	%xmm2,%xmm12,%xmm12
+++
+++	vmovdqa	80(%rdx),%xmm1
+++	vaesenc	%xmm1,%xmm9,%xmm9
+++	vaesenc	%xmm1,%xmm10,%xmm10
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++
+++	vmovdqa	96(%rdx),%xmm2
+++	vaesenc	%xmm2,%xmm9,%xmm9
+++	vaesenc	%xmm2,%xmm10,%xmm10
+++	vaesenc	%xmm2,%xmm11,%xmm11
+++	vaesenc	%xmm2,%xmm12,%xmm12
+++
+++	vmovdqa	112(%rdx),%xmm1
+++	vaesenc	%xmm1,%xmm9,%xmm9
+++	vaesenc	%xmm1,%xmm10,%xmm10
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++
+++	vmovdqa	128(%rdx),%xmm2
+++	vaesenc	%xmm2,%xmm9,%xmm9
+++	vaesenc	%xmm2,%xmm10,%xmm10
+++	vaesenc	%xmm2,%xmm11,%xmm11
+++	vaesenc	%xmm2,%xmm12,%xmm12
+++
+++	vmovdqa	144(%rdx),%xmm1
+++	vaesenc	%xmm1,%xmm9,%xmm9
+++	vaesenc	%xmm1,%xmm10,%xmm10
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++
+++	vmovdqa	160(%rdx),%xmm2
+++	vaesenclast	%xmm2,%xmm9,%xmm9
+++	vaesenclast	%xmm2,%xmm10,%xmm10
+++	vaesenclast	%xmm2,%xmm11,%xmm11
+++	vaesenclast	%xmm2,%xmm12,%xmm12
+++
+++
+++	vmovdqa	%xmm9,0(%rsi)
+++	vmovdqa	%xmm10,16(%rsi)
+++	vmovdqa	%xmm11,32(%rsi)
+++	vmovdqa	%xmm12,48(%rsi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes128gcmsiv_kdf,.-aes128gcmsiv_kdf
+++.globl	aes128gcmsiv_enc_msg_x4
+++.hidden aes128gcmsiv_enc_msg_x4
+++.type	aes128gcmsiv_enc_msg_x4,@function
+++.align	16
+++aes128gcmsiv_enc_msg_x4:
+++.cfi_startproc	
+++	testq	%r8,%r8
+++	jnz	.L128_enc_msg_x4_start
+++	.byte	0xf3,0xc3
+++
+++.L128_enc_msg_x4_start:
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-16
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-24
+++
+++	shrq	$4,%r8
+++	movq	%r8,%r10
+++	shlq	$62,%r10
+++	shrq	$62,%r10
+++
+++
+++	vmovdqa	(%rdx),%xmm15
+++	vpor	OR_MASK(%rip),%xmm15,%xmm15
+++
+++	vmovdqu	four(%rip),%xmm4
+++	vmovdqa	%xmm15,%xmm0
+++	vpaddd	one(%rip),%xmm15,%xmm1
+++	vpaddd	two(%rip),%xmm15,%xmm2
+++	vpaddd	three(%rip),%xmm15,%xmm3
+++
+++	shrq	$2,%r8
+++	je	.L128_enc_msg_x4_check_remainder
+++
+++	subq	$64,%rsi
+++	subq	$64,%rdi
+++
+++.L128_enc_msg_x4_loop1:
+++	addq	$64,%rsi
+++	addq	$64,%rdi
+++
+++	vmovdqa	%xmm0,%xmm5
+++	vmovdqa	%xmm1,%xmm6
+++	vmovdqa	%xmm2,%xmm7
+++	vmovdqa	%xmm3,%xmm8
+++
+++	vpxor	(%rcx),%xmm5,%xmm5
+++	vpxor	(%rcx),%xmm6,%xmm6
+++	vpxor	(%rcx),%xmm7,%xmm7
+++	vpxor	(%rcx),%xmm8,%xmm8
+++
+++	vmovdqu	16(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vpaddd	%xmm4,%xmm0,%xmm0
+++	vmovdqu	32(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vpaddd	%xmm4,%xmm1,%xmm1
+++	vmovdqu	48(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vpaddd	%xmm4,%xmm2,%xmm2
+++	vmovdqu	64(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vpaddd	%xmm4,%xmm3,%xmm3
+++
+++	vmovdqu	80(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	96(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	112(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	128(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	144(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	160(%rcx),%xmm12
+++	vaesenclast	%xmm12,%xmm5,%xmm5
+++	vaesenclast	%xmm12,%xmm6,%xmm6
+++	vaesenclast	%xmm12,%xmm7,%xmm7
+++	vaesenclast	%xmm12,%xmm8,%xmm8
+++
+++
+++
+++	vpxor	0(%rdi),%xmm5,%xmm5
+++	vpxor	16(%rdi),%xmm6,%xmm6
+++	vpxor	32(%rdi),%xmm7,%xmm7
+++	vpxor	48(%rdi),%xmm8,%xmm8
+++
+++	subq	$1,%r8
+++
+++	vmovdqu	%xmm5,0(%rsi)
+++	vmovdqu	%xmm6,16(%rsi)
+++	vmovdqu	%xmm7,32(%rsi)
+++	vmovdqu	%xmm8,48(%rsi)
+++
+++	jne	.L128_enc_msg_x4_loop1
+++
+++	addq	$64,%rsi
+++	addq	$64,%rdi
+++
+++.L128_enc_msg_x4_check_remainder:
+++	cmpq	$0,%r10
+++	je	.L128_enc_msg_x4_out
+++
+++.L128_enc_msg_x4_loop2:
+++
+++
+++	vmovdqa	%xmm0,%xmm5
+++	vpaddd	one(%rip),%xmm0,%xmm0
+++
+++	vpxor	(%rcx),%xmm5,%xmm5
+++	vaesenc	16(%rcx),%xmm5,%xmm5
+++	vaesenc	32(%rcx),%xmm5,%xmm5
+++	vaesenc	48(%rcx),%xmm5,%xmm5
+++	vaesenc	64(%rcx),%xmm5,%xmm5
+++	vaesenc	80(%rcx),%xmm5,%xmm5
+++	vaesenc	96(%rcx),%xmm5,%xmm5
+++	vaesenc	112(%rcx),%xmm5,%xmm5
+++	vaesenc	128(%rcx),%xmm5,%xmm5
+++	vaesenc	144(%rcx),%xmm5,%xmm5
+++	vaesenclast	160(%rcx),%xmm5,%xmm5
+++
+++
+++	vpxor	(%rdi),%xmm5,%xmm5
+++	vmovdqu	%xmm5,(%rsi)
+++
+++	addq	$16,%rdi
+++	addq	$16,%rsi
+++
+++	subq	$1,%r10
+++	jne	.L128_enc_msg_x4_loop2
+++
+++.L128_enc_msg_x4_out:
+++	popq	%r13
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r13
+++	popq	%r12
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r12
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes128gcmsiv_enc_msg_x4,.-aes128gcmsiv_enc_msg_x4
+++.globl	aes128gcmsiv_enc_msg_x8
+++.hidden aes128gcmsiv_enc_msg_x8
+++.type	aes128gcmsiv_enc_msg_x8,@function
+++.align	16
+++aes128gcmsiv_enc_msg_x8:
+++.cfi_startproc	
+++	testq	%r8,%r8
+++	jnz	.L128_enc_msg_x8_start
+++	.byte	0xf3,0xc3
+++
+++.L128_enc_msg_x8_start:
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-16
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-24
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-32
+++	movq	%rsp,%rbp
+++.cfi_def_cfa_register	rbp
+++
+++
+++	subq	$128,%rsp
+++	andq	$-64,%rsp
+++
+++	shrq	$4,%r8
+++	movq	%r8,%r10
+++	shlq	$61,%r10
+++	shrq	$61,%r10
+++
+++
+++	vmovdqu	(%rdx),%xmm1
+++	vpor	OR_MASK(%rip),%xmm1,%xmm1
+++
+++
+++	vpaddd	seven(%rip),%xmm1,%xmm0
+++	vmovdqu	%xmm0,(%rsp)
+++	vpaddd	one(%rip),%xmm1,%xmm9
+++	vpaddd	two(%rip),%xmm1,%xmm10
+++	vpaddd	three(%rip),%xmm1,%xmm11
+++	vpaddd	four(%rip),%xmm1,%xmm12
+++	vpaddd	five(%rip),%xmm1,%xmm13
+++	vpaddd	six(%rip),%xmm1,%xmm14
+++	vmovdqa	%xmm1,%xmm0
+++
+++	shrq	$3,%r8
+++	je	.L128_enc_msg_x8_check_remainder
+++
+++	subq	$128,%rsi
+++	subq	$128,%rdi
+++
+++.L128_enc_msg_x8_loop1:
+++	addq	$128,%rsi
+++	addq	$128,%rdi
+++
+++	vmovdqa	%xmm0,%xmm1
+++	vmovdqa	%xmm9,%xmm2
+++	vmovdqa	%xmm10,%xmm3
+++	vmovdqa	%xmm11,%xmm4
+++	vmovdqa	%xmm12,%xmm5
+++	vmovdqa	%xmm13,%xmm6
+++	vmovdqa	%xmm14,%xmm7
+++
+++	vmovdqu	(%rsp),%xmm8
+++
+++	vpxor	(%rcx),%xmm1,%xmm1
+++	vpxor	(%rcx),%xmm2,%xmm2
+++	vpxor	(%rcx),%xmm3,%xmm3
+++	vpxor	(%rcx),%xmm4,%xmm4
+++	vpxor	(%rcx),%xmm5,%xmm5
+++	vpxor	(%rcx),%xmm6,%xmm6
+++	vpxor	(%rcx),%xmm7,%xmm7
+++	vpxor	(%rcx),%xmm8,%xmm8
+++
+++	vmovdqu	16(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vmovdqu	(%rsp),%xmm14
+++	vpaddd	eight(%rip),%xmm14,%xmm14
+++	vmovdqu	%xmm14,(%rsp)
+++	vmovdqu	32(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpsubd	one(%rip),%xmm14,%xmm14
+++	vmovdqu	48(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpaddd	eight(%rip),%xmm0,%xmm0
+++	vmovdqu	64(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpaddd	eight(%rip),%xmm9,%xmm9
+++	vmovdqu	80(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpaddd	eight(%rip),%xmm10,%xmm10
+++	vmovdqu	96(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpaddd	eight(%rip),%xmm11,%xmm11
+++	vmovdqu	112(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpaddd	eight(%rip),%xmm12,%xmm12
+++	vmovdqu	128(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpaddd	eight(%rip),%xmm13,%xmm13
+++	vmovdqu	144(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vmovdqu	160(%rcx),%xmm15
+++	vaesenclast	%xmm15,%xmm1,%xmm1
+++	vaesenclast	%xmm15,%xmm2,%xmm2
+++	vaesenclast	%xmm15,%xmm3,%xmm3
+++	vaesenclast	%xmm15,%xmm4,%xmm4
+++	vaesenclast	%xmm15,%xmm5,%xmm5
+++	vaesenclast	%xmm15,%xmm6,%xmm6
+++	vaesenclast	%xmm15,%xmm7,%xmm7
+++	vaesenclast	%xmm15,%xmm8,%xmm8
+++
+++
+++
+++	vpxor	0(%rdi),%xmm1,%xmm1
+++	vpxor	16(%rdi),%xmm2,%xmm2
+++	vpxor	32(%rdi),%xmm3,%xmm3
+++	vpxor	48(%rdi),%xmm4,%xmm4
+++	vpxor	64(%rdi),%xmm5,%xmm5
+++	vpxor	80(%rdi),%xmm6,%xmm6
+++	vpxor	96(%rdi),%xmm7,%xmm7
+++	vpxor	112(%rdi),%xmm8,%xmm8
+++
+++	decq	%r8
+++
+++	vmovdqu	%xmm1,0(%rsi)
+++	vmovdqu	%xmm2,16(%rsi)
+++	vmovdqu	%xmm3,32(%rsi)
+++	vmovdqu	%xmm4,48(%rsi)
+++	vmovdqu	%xmm5,64(%rsi)
+++	vmovdqu	%xmm6,80(%rsi)
+++	vmovdqu	%xmm7,96(%rsi)
+++	vmovdqu	%xmm8,112(%rsi)
+++
+++	jne	.L128_enc_msg_x8_loop1
+++
+++	addq	$128,%rsi
+++	addq	$128,%rdi
+++
+++.L128_enc_msg_x8_check_remainder:
+++	cmpq	$0,%r10
+++	je	.L128_enc_msg_x8_out
+++
+++.L128_enc_msg_x8_loop2:
+++
+++
+++	vmovdqa	%xmm0,%xmm1
+++	vpaddd	one(%rip),%xmm0,%xmm0
+++
+++	vpxor	(%rcx),%xmm1,%xmm1
+++	vaesenc	16(%rcx),%xmm1,%xmm1
+++	vaesenc	32(%rcx),%xmm1,%xmm1
+++	vaesenc	48(%rcx),%xmm1,%xmm1
+++	vaesenc	64(%rcx),%xmm1,%xmm1
+++	vaesenc	80(%rcx),%xmm1,%xmm1
+++	vaesenc	96(%rcx),%xmm1,%xmm1
+++	vaesenc	112(%rcx),%xmm1,%xmm1
+++	vaesenc	128(%rcx),%xmm1,%xmm1
+++	vaesenc	144(%rcx),%xmm1,%xmm1
+++	vaesenclast	160(%rcx),%xmm1,%xmm1
+++
+++
+++	vpxor	(%rdi),%xmm1,%xmm1
+++
+++	vmovdqu	%xmm1,(%rsi)
+++
+++	addq	$16,%rdi
+++	addq	$16,%rsi
+++
+++	decq	%r10
+++	jne	.L128_enc_msg_x8_loop2
+++
+++.L128_enc_msg_x8_out:
+++	movq	%rbp,%rsp
+++.cfi_def_cfa_register	%rsp
+++	popq	%rbp
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%rbp
+++	popq	%r13
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r13
+++	popq	%r12
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r12
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes128gcmsiv_enc_msg_x8,.-aes128gcmsiv_enc_msg_x8
+++.globl	aes128gcmsiv_dec
+++.hidden aes128gcmsiv_dec
+++.type	aes128gcmsiv_dec,@function
+++.align	16
+++aes128gcmsiv_dec:
+++.cfi_startproc	
+++	testq	$~15,%r9
+++	jnz	.L128_dec_start
+++	.byte	0xf3,0xc3
+++
+++.L128_dec_start:
+++	vzeroupper
+++	vmovdqa	(%rdx),%xmm0
+++	movq	%rdx,%rax
+++
+++	leaq	32(%rax),%rax
+++	leaq	32(%rcx),%rcx
+++
+++
+++	vmovdqu	(%rdi,%r9,1),%xmm15
+++	vpor	OR_MASK(%rip),%xmm15,%xmm15
+++	andq	$~15,%r9
+++
+++
+++	cmpq	$96,%r9
+++	jb	.L128_dec_loop2
+++
+++
+++	subq	$96,%r9
+++	vmovdqa	%xmm15,%xmm7
+++	vpaddd	one(%rip),%xmm7,%xmm8
+++	vpaddd	two(%rip),%xmm7,%xmm9
+++	vpaddd	one(%rip),%xmm9,%xmm10
+++	vpaddd	two(%rip),%xmm9,%xmm11
+++	vpaddd	one(%rip),%xmm11,%xmm12
+++	vpaddd	two(%rip),%xmm11,%xmm15
+++
+++	vpxor	(%r8),%xmm7,%xmm7
+++	vpxor	(%r8),%xmm8,%xmm8
+++	vpxor	(%r8),%xmm9,%xmm9
+++	vpxor	(%r8),%xmm10,%xmm10
+++	vpxor	(%r8),%xmm11,%xmm11
+++	vpxor	(%r8),%xmm12,%xmm12
+++
+++	vmovdqu	16(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	32(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	48(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	64(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	80(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	96(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	112(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	128(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	144(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	160(%r8),%xmm4
+++	vaesenclast	%xmm4,%xmm7,%xmm7
+++	vaesenclast	%xmm4,%xmm8,%xmm8
+++	vaesenclast	%xmm4,%xmm9,%xmm9
+++	vaesenclast	%xmm4,%xmm10,%xmm10
+++	vaesenclast	%xmm4,%xmm11,%xmm11
+++	vaesenclast	%xmm4,%xmm12,%xmm12
+++
+++
+++	vpxor	0(%rdi),%xmm7,%xmm7
+++	vpxor	16(%rdi),%xmm8,%xmm8
+++	vpxor	32(%rdi),%xmm9,%xmm9
+++	vpxor	48(%rdi),%xmm10,%xmm10
+++	vpxor	64(%rdi),%xmm11,%xmm11
+++	vpxor	80(%rdi),%xmm12,%xmm12
+++
+++	vmovdqu	%xmm7,0(%rsi)
+++	vmovdqu	%xmm8,16(%rsi)
+++	vmovdqu	%xmm9,32(%rsi)
+++	vmovdqu	%xmm10,48(%rsi)
+++	vmovdqu	%xmm11,64(%rsi)
+++	vmovdqu	%xmm12,80(%rsi)
+++
+++	addq	$96,%rdi
+++	addq	$96,%rsi
+++	jmp	.L128_dec_loop1
+++
+++
+++.align	64
+++.L128_dec_loop1:
+++	cmpq	$96,%r9
+++	jb	.L128_dec_finish_96
+++	subq	$96,%r9
+++
+++	vmovdqa	%xmm12,%xmm6
+++	vmovdqa	%xmm11,16-32(%rax)
+++	vmovdqa	%xmm10,32-32(%rax)
+++	vmovdqa	%xmm9,48-32(%rax)
+++	vmovdqa	%xmm8,64-32(%rax)
+++	vmovdqa	%xmm7,80-32(%rax)
+++
+++	vmovdqa	%xmm15,%xmm7
+++	vpaddd	one(%rip),%xmm7,%xmm8
+++	vpaddd	two(%rip),%xmm7,%xmm9
+++	vpaddd	one(%rip),%xmm9,%xmm10
+++	vpaddd	two(%rip),%xmm9,%xmm11
+++	vpaddd	one(%rip),%xmm11,%xmm12
+++	vpaddd	two(%rip),%xmm11,%xmm15
+++
+++	vmovdqa	(%r8),%xmm4
+++	vpxor	%xmm4,%xmm7,%xmm7
+++	vpxor	%xmm4,%xmm8,%xmm8
+++	vpxor	%xmm4,%xmm9,%xmm9
+++	vpxor	%xmm4,%xmm10,%xmm10
+++	vpxor	%xmm4,%xmm11,%xmm11
+++	vpxor	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	0-32(%rcx),%xmm4
+++	vpclmulqdq	$0x11,%xmm4,%xmm6,%xmm2
+++	vpclmulqdq	$0x00,%xmm4,%xmm6,%xmm3
+++	vpclmulqdq	$0x01,%xmm4,%xmm6,%xmm1
+++	vpclmulqdq	$0x10,%xmm4,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vmovdqu	16(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	-16(%rax),%xmm6
+++	vmovdqu	-16(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++
+++	vmovdqu	32(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	0(%rax),%xmm6
+++	vmovdqu	0(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++
+++	vmovdqu	48(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	16(%rax),%xmm6
+++	vmovdqu	16(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++
+++	vmovdqu	64(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	32(%rax),%xmm6
+++	vmovdqu	32(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++
+++	vmovdqu	80(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	96(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	112(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++
+++	vmovdqa	80-32(%rax),%xmm6
+++	vpxor	%xmm0,%xmm6,%xmm6
+++	vmovdqu	80-32(%rcx),%xmm5
+++
+++	vpclmulqdq	$0x01,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x10,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vmovdqu	128(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++
+++	vpsrldq	$8,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm5
+++	vpslldq	$8,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm0
+++
+++	vmovdqa	poly(%rip),%xmm3
+++
+++	vmovdqu	144(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	160(%r8),%xmm6
+++	vpalignr	$8,%xmm0,%xmm0,%xmm2
+++	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm0
+++	vpxor	%xmm0,%xmm2,%xmm0
+++
+++	vpxor	0(%rdi),%xmm6,%xmm4
+++	vaesenclast	%xmm4,%xmm7,%xmm7
+++	vpxor	16(%rdi),%xmm6,%xmm4
+++	vaesenclast	%xmm4,%xmm8,%xmm8
+++	vpxor	32(%rdi),%xmm6,%xmm4
+++	vaesenclast	%xmm4,%xmm9,%xmm9
+++	vpxor	48(%rdi),%xmm6,%xmm4
+++	vaesenclast	%xmm4,%xmm10,%xmm10
+++	vpxor	64(%rdi),%xmm6,%xmm4
+++	vaesenclast	%xmm4,%xmm11,%xmm11
+++	vpxor	80(%rdi),%xmm6,%xmm4
+++	vaesenclast	%xmm4,%xmm12,%xmm12
+++
+++	vpalignr	$8,%xmm0,%xmm0,%xmm2
+++	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm0
+++	vpxor	%xmm0,%xmm2,%xmm0
+++
+++	vmovdqu	%xmm7,0(%rsi)
+++	vmovdqu	%xmm8,16(%rsi)
+++	vmovdqu	%xmm9,32(%rsi)
+++	vmovdqu	%xmm10,48(%rsi)
+++	vmovdqu	%xmm11,64(%rsi)
+++	vmovdqu	%xmm12,80(%rsi)
+++
+++	vpxor	%xmm5,%xmm0,%xmm0
+++
+++	leaq	96(%rdi),%rdi
+++	leaq	96(%rsi),%rsi
+++	jmp	.L128_dec_loop1
+++
+++.L128_dec_finish_96:
+++	vmovdqa	%xmm12,%xmm6
+++	vmovdqa	%xmm11,16-32(%rax)
+++	vmovdqa	%xmm10,32-32(%rax)
+++	vmovdqa	%xmm9,48-32(%rax)
+++	vmovdqa	%xmm8,64-32(%rax)
+++	vmovdqa	%xmm7,80-32(%rax)
+++
+++	vmovdqu	0-32(%rcx),%xmm4
+++	vpclmulqdq	$0x10,%xmm4,%xmm6,%xmm1
+++	vpclmulqdq	$0x11,%xmm4,%xmm6,%xmm2
+++	vpclmulqdq	$0x00,%xmm4,%xmm6,%xmm3
+++	vpclmulqdq	$0x01,%xmm4,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vmovdqu	-16(%rax),%xmm6
+++	vmovdqu	-16(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vmovdqu	0(%rax),%xmm6
+++	vmovdqu	0(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vmovdqu	16(%rax),%xmm6
+++	vmovdqu	16(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vmovdqu	32(%rax),%xmm6
+++	vmovdqu	32(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++
+++	vmovdqu	80-32(%rax),%xmm6
+++	vpxor	%xmm0,%xmm6,%xmm6
+++	vmovdqu	80-32(%rcx),%xmm5
+++	vpclmulqdq	$0x11,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x10,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x01,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vpsrldq	$8,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm5
+++	vpslldq	$8,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm0
+++
+++	vmovdqa	poly(%rip),%xmm3
+++
+++	vpalignr	$8,%xmm0,%xmm0,%xmm2
+++	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm0
+++	vpxor	%xmm0,%xmm2,%xmm0
+++
+++	vpalignr	$8,%xmm0,%xmm0,%xmm2
+++	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm0
+++	vpxor	%xmm0,%xmm2,%xmm0
+++
+++	vpxor	%xmm5,%xmm0,%xmm0
+++
+++.L128_dec_loop2:
+++
+++
+++
+++	cmpq	$16,%r9
+++	jb	.L128_dec_out
+++	subq	$16,%r9
+++
+++	vmovdqa	%xmm15,%xmm2
+++	vpaddd	one(%rip),%xmm15,%xmm15
+++
+++	vpxor	0(%r8),%xmm2,%xmm2
+++	vaesenc	16(%r8),%xmm2,%xmm2
+++	vaesenc	32(%r8),%xmm2,%xmm2
+++	vaesenc	48(%r8),%xmm2,%xmm2
+++	vaesenc	64(%r8),%xmm2,%xmm2
+++	vaesenc	80(%r8),%xmm2,%xmm2
+++	vaesenc	96(%r8),%xmm2,%xmm2
+++	vaesenc	112(%r8),%xmm2,%xmm2
+++	vaesenc	128(%r8),%xmm2,%xmm2
+++	vaesenc	144(%r8),%xmm2,%xmm2
+++	vaesenclast	160(%r8),%xmm2,%xmm2
+++	vpxor	(%rdi),%xmm2,%xmm2
+++	vmovdqu	%xmm2,(%rsi)
+++	addq	$16,%rdi
+++	addq	$16,%rsi
+++
+++	vpxor	%xmm2,%xmm0,%xmm0
+++	vmovdqa	-32(%rcx),%xmm1
+++	call	GFMUL
+++
+++	jmp	.L128_dec_loop2
+++
+++.L128_dec_out:
+++	vmovdqu	%xmm0,(%rdx)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes128gcmsiv_dec, .-aes128gcmsiv_dec
+++.globl	aes128gcmsiv_ecb_enc_block
+++.hidden aes128gcmsiv_ecb_enc_block
+++.type	aes128gcmsiv_ecb_enc_block,@function
+++.align	16
+++aes128gcmsiv_ecb_enc_block:
+++.cfi_startproc	
+++	vmovdqa	(%rdi),%xmm1
+++
+++	vpxor	(%rdx),%xmm1,%xmm1
+++	vaesenc	16(%rdx),%xmm1,%xmm1
+++	vaesenc	32(%rdx),%xmm1,%xmm1
+++	vaesenc	48(%rdx),%xmm1,%xmm1
+++	vaesenc	64(%rdx),%xmm1,%xmm1
+++	vaesenc	80(%rdx),%xmm1,%xmm1
+++	vaesenc	96(%rdx),%xmm1,%xmm1
+++	vaesenc	112(%rdx),%xmm1,%xmm1
+++	vaesenc	128(%rdx),%xmm1,%xmm1
+++	vaesenc	144(%rdx),%xmm1,%xmm1
+++	vaesenclast	160(%rdx),%xmm1,%xmm1
+++
+++	vmovdqa	%xmm1,(%rsi)
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes128gcmsiv_ecb_enc_block,.-aes128gcmsiv_ecb_enc_block
+++.globl	aes256gcmsiv_aes_ks_enc_x1
+++.hidden aes256gcmsiv_aes_ks_enc_x1
+++.type	aes256gcmsiv_aes_ks_enc_x1,@function
+++.align	16
+++aes256gcmsiv_aes_ks_enc_x1:
+++.cfi_startproc	
+++	vmovdqa	con1(%rip),%xmm0
+++	vmovdqa	mask(%rip),%xmm15
+++	vmovdqa	(%rdi),%xmm8
+++	vmovdqa	(%rcx),%xmm1
+++	vmovdqa	16(%rcx),%xmm3
+++	vpxor	%xmm1,%xmm8,%xmm8
+++	vaesenc	%xmm3,%xmm8,%xmm8
+++	vmovdqu	%xmm1,(%rdx)
+++	vmovdqu	%xmm3,16(%rdx)
+++	vpxor	%xmm14,%xmm14,%xmm14
+++
+++	vpshufb	%xmm15,%xmm3,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpslldq	$4,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	vaesenc	%xmm1,%xmm8,%xmm8
+++	vmovdqu	%xmm1,32(%rdx)
+++
+++	vpshufd	$0xff,%xmm1,%xmm2
+++	vaesenclast	%xmm14,%xmm2,%xmm2
+++	vpslldq	$4,%xmm3,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpxor	%xmm2,%xmm3,%xmm3
+++	vaesenc	%xmm3,%xmm8,%xmm8
+++	vmovdqu	%xmm3,48(%rdx)
+++
+++	vpshufb	%xmm15,%xmm3,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpslldq	$4,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	vaesenc	%xmm1,%xmm8,%xmm8
+++	vmovdqu	%xmm1,64(%rdx)
+++
+++	vpshufd	$0xff,%xmm1,%xmm2
+++	vaesenclast	%xmm14,%xmm2,%xmm2
+++	vpslldq	$4,%xmm3,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpxor	%xmm2,%xmm3,%xmm3
+++	vaesenc	%xmm3,%xmm8,%xmm8
+++	vmovdqu	%xmm3,80(%rdx)
+++
+++	vpshufb	%xmm15,%xmm3,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpslldq	$4,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	vaesenc	%xmm1,%xmm8,%xmm8
+++	vmovdqu	%xmm1,96(%rdx)
+++
+++	vpshufd	$0xff,%xmm1,%xmm2
+++	vaesenclast	%xmm14,%xmm2,%xmm2
+++	vpslldq	$4,%xmm3,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpxor	%xmm2,%xmm3,%xmm3
+++	vaesenc	%xmm3,%xmm8,%xmm8
+++	vmovdqu	%xmm3,112(%rdx)
+++
+++	vpshufb	%xmm15,%xmm3,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpslldq	$4,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	vaesenc	%xmm1,%xmm8,%xmm8
+++	vmovdqu	%xmm1,128(%rdx)
+++
+++	vpshufd	$0xff,%xmm1,%xmm2
+++	vaesenclast	%xmm14,%xmm2,%xmm2
+++	vpslldq	$4,%xmm3,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpxor	%xmm2,%xmm3,%xmm3
+++	vaesenc	%xmm3,%xmm8,%xmm8
+++	vmovdqu	%xmm3,144(%rdx)
+++
+++	vpshufb	%xmm15,%xmm3,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpslldq	$4,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	vaesenc	%xmm1,%xmm8,%xmm8
+++	vmovdqu	%xmm1,160(%rdx)
+++
+++	vpshufd	$0xff,%xmm1,%xmm2
+++	vaesenclast	%xmm14,%xmm2,%xmm2
+++	vpslldq	$4,%xmm3,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpxor	%xmm2,%xmm3,%xmm3
+++	vaesenc	%xmm3,%xmm8,%xmm8
+++	vmovdqu	%xmm3,176(%rdx)
+++
+++	vpshufb	%xmm15,%xmm3,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslld	$1,%xmm0,%xmm0
+++	vpslldq	$4,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	vaesenc	%xmm1,%xmm8,%xmm8
+++	vmovdqu	%xmm1,192(%rdx)
+++
+++	vpshufd	$0xff,%xmm1,%xmm2
+++	vaesenclast	%xmm14,%xmm2,%xmm2
+++	vpslldq	$4,%xmm3,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpxor	%xmm2,%xmm3,%xmm3
+++	vaesenc	%xmm3,%xmm8,%xmm8
+++	vmovdqu	%xmm3,208(%rdx)
+++
+++	vpshufb	%xmm15,%xmm3,%xmm2
+++	vaesenclast	%xmm0,%xmm2,%xmm2
+++	vpslldq	$4,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpslldq	$4,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	vaesenclast	%xmm1,%xmm8,%xmm8
+++	vmovdqu	%xmm1,224(%rdx)
+++
+++	vmovdqa	%xmm8,(%rsi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes256gcmsiv_aes_ks_enc_x1,.-aes256gcmsiv_aes_ks_enc_x1
+++.globl	aes256gcmsiv_ecb_enc_block
+++.hidden aes256gcmsiv_ecb_enc_block
+++.type	aes256gcmsiv_ecb_enc_block,@function
+++.align	16
+++aes256gcmsiv_ecb_enc_block:
+++.cfi_startproc	
+++	vmovdqa	(%rdi),%xmm1
+++	vpxor	(%rdx),%xmm1,%xmm1
+++	vaesenc	16(%rdx),%xmm1,%xmm1
+++	vaesenc	32(%rdx),%xmm1,%xmm1
+++	vaesenc	48(%rdx),%xmm1,%xmm1
+++	vaesenc	64(%rdx),%xmm1,%xmm1
+++	vaesenc	80(%rdx),%xmm1,%xmm1
+++	vaesenc	96(%rdx),%xmm1,%xmm1
+++	vaesenc	112(%rdx),%xmm1,%xmm1
+++	vaesenc	128(%rdx),%xmm1,%xmm1
+++	vaesenc	144(%rdx),%xmm1,%xmm1
+++	vaesenc	160(%rdx),%xmm1,%xmm1
+++	vaesenc	176(%rdx),%xmm1,%xmm1
+++	vaesenc	192(%rdx),%xmm1,%xmm1
+++	vaesenc	208(%rdx),%xmm1,%xmm1
+++	vaesenclast	224(%rdx),%xmm1,%xmm1
+++	vmovdqa	%xmm1,(%rsi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes256gcmsiv_ecb_enc_block,.-aes256gcmsiv_ecb_enc_block
+++.globl	aes256gcmsiv_enc_msg_x4
+++.hidden aes256gcmsiv_enc_msg_x4
+++.type	aes256gcmsiv_enc_msg_x4,@function
+++.align	16
+++aes256gcmsiv_enc_msg_x4:
+++.cfi_startproc	
+++	testq	%r8,%r8
+++	jnz	.L256_enc_msg_x4_start
+++	.byte	0xf3,0xc3
+++
+++.L256_enc_msg_x4_start:
+++	movq	%r8,%r10
+++	shrq	$4,%r8
+++	shlq	$60,%r10
+++	jz	.L256_enc_msg_x4_start2
+++	addq	$1,%r8
+++
+++.L256_enc_msg_x4_start2:
+++	movq	%r8,%r10
+++	shlq	$62,%r10
+++	shrq	$62,%r10
+++
+++
+++	vmovdqa	(%rdx),%xmm15
+++	vpor	OR_MASK(%rip),%xmm15,%xmm15
+++
+++	vmovdqa	four(%rip),%xmm4
+++	vmovdqa	%xmm15,%xmm0
+++	vpaddd	one(%rip),%xmm15,%xmm1
+++	vpaddd	two(%rip),%xmm15,%xmm2
+++	vpaddd	three(%rip),%xmm15,%xmm3
+++
+++	shrq	$2,%r8
+++	je	.L256_enc_msg_x4_check_remainder
+++
+++	subq	$64,%rsi
+++	subq	$64,%rdi
+++
+++.L256_enc_msg_x4_loop1:
+++	addq	$64,%rsi
+++	addq	$64,%rdi
+++
+++	vmovdqa	%xmm0,%xmm5
+++	vmovdqa	%xmm1,%xmm6
+++	vmovdqa	%xmm2,%xmm7
+++	vmovdqa	%xmm3,%xmm8
+++
+++	vpxor	(%rcx),%xmm5,%xmm5
+++	vpxor	(%rcx),%xmm6,%xmm6
+++	vpxor	(%rcx),%xmm7,%xmm7
+++	vpxor	(%rcx),%xmm8,%xmm8
+++
+++	vmovdqu	16(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vpaddd	%xmm4,%xmm0,%xmm0
+++	vmovdqu	32(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vpaddd	%xmm4,%xmm1,%xmm1
+++	vmovdqu	48(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vpaddd	%xmm4,%xmm2,%xmm2
+++	vmovdqu	64(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vpaddd	%xmm4,%xmm3,%xmm3
+++
+++	vmovdqu	80(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	96(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	112(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	128(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	144(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	160(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	176(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	192(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	208(%rcx),%xmm12
+++	vaesenc	%xmm12,%xmm5,%xmm5
+++	vaesenc	%xmm12,%xmm6,%xmm6
+++	vaesenc	%xmm12,%xmm7,%xmm7
+++	vaesenc	%xmm12,%xmm8,%xmm8
+++
+++	vmovdqu	224(%rcx),%xmm12
+++	vaesenclast	%xmm12,%xmm5,%xmm5
+++	vaesenclast	%xmm12,%xmm6,%xmm6
+++	vaesenclast	%xmm12,%xmm7,%xmm7
+++	vaesenclast	%xmm12,%xmm8,%xmm8
+++
+++
+++
+++	vpxor	0(%rdi),%xmm5,%xmm5
+++	vpxor	16(%rdi),%xmm6,%xmm6
+++	vpxor	32(%rdi),%xmm7,%xmm7
+++	vpxor	48(%rdi),%xmm8,%xmm8
+++
+++	subq	$1,%r8
+++
+++	vmovdqu	%xmm5,0(%rsi)
+++	vmovdqu	%xmm6,16(%rsi)
+++	vmovdqu	%xmm7,32(%rsi)
+++	vmovdqu	%xmm8,48(%rsi)
+++
+++	jne	.L256_enc_msg_x4_loop1
+++
+++	addq	$64,%rsi
+++	addq	$64,%rdi
+++
+++.L256_enc_msg_x4_check_remainder:
+++	cmpq	$0,%r10
+++	je	.L256_enc_msg_x4_out
+++
+++.L256_enc_msg_x4_loop2:
+++
+++
+++
+++	vmovdqa	%xmm0,%xmm5
+++	vpaddd	one(%rip),%xmm0,%xmm0
+++	vpxor	(%rcx),%xmm5,%xmm5
+++	vaesenc	16(%rcx),%xmm5,%xmm5
+++	vaesenc	32(%rcx),%xmm5,%xmm5
+++	vaesenc	48(%rcx),%xmm5,%xmm5
+++	vaesenc	64(%rcx),%xmm5,%xmm5
+++	vaesenc	80(%rcx),%xmm5,%xmm5
+++	vaesenc	96(%rcx),%xmm5,%xmm5
+++	vaesenc	112(%rcx),%xmm5,%xmm5
+++	vaesenc	128(%rcx),%xmm5,%xmm5
+++	vaesenc	144(%rcx),%xmm5,%xmm5
+++	vaesenc	160(%rcx),%xmm5,%xmm5
+++	vaesenc	176(%rcx),%xmm5,%xmm5
+++	vaesenc	192(%rcx),%xmm5,%xmm5
+++	vaesenc	208(%rcx),%xmm5,%xmm5
+++	vaesenclast	224(%rcx),%xmm5,%xmm5
+++
+++
+++	vpxor	(%rdi),%xmm5,%xmm5
+++
+++	vmovdqu	%xmm5,(%rsi)
+++
+++	addq	$16,%rdi
+++	addq	$16,%rsi
+++
+++	subq	$1,%r10
+++	jne	.L256_enc_msg_x4_loop2
+++
+++.L256_enc_msg_x4_out:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes256gcmsiv_enc_msg_x4,.-aes256gcmsiv_enc_msg_x4
+++.globl	aes256gcmsiv_enc_msg_x8
+++.hidden aes256gcmsiv_enc_msg_x8
+++.type	aes256gcmsiv_enc_msg_x8,@function
+++.align	16
+++aes256gcmsiv_enc_msg_x8:
+++.cfi_startproc	
+++	testq	%r8,%r8
+++	jnz	.L256_enc_msg_x8_start
+++	.byte	0xf3,0xc3
+++
+++.L256_enc_msg_x8_start:
+++
+++	movq	%rsp,%r11
+++	subq	$16,%r11
+++	andq	$-64,%r11
+++
+++	movq	%r8,%r10
+++	shrq	$4,%r8
+++	shlq	$60,%r10
+++	jz	.L256_enc_msg_x8_start2
+++	addq	$1,%r8
+++
+++.L256_enc_msg_x8_start2:
+++	movq	%r8,%r10
+++	shlq	$61,%r10
+++	shrq	$61,%r10
+++
+++
+++	vmovdqa	(%rdx),%xmm1
+++	vpor	OR_MASK(%rip),%xmm1,%xmm1
+++
+++
+++	vpaddd	seven(%rip),%xmm1,%xmm0
+++	vmovdqa	%xmm0,(%r11)
+++	vpaddd	one(%rip),%xmm1,%xmm9
+++	vpaddd	two(%rip),%xmm1,%xmm10
+++	vpaddd	three(%rip),%xmm1,%xmm11
+++	vpaddd	four(%rip),%xmm1,%xmm12
+++	vpaddd	five(%rip),%xmm1,%xmm13
+++	vpaddd	six(%rip),%xmm1,%xmm14
+++	vmovdqa	%xmm1,%xmm0
+++
+++	shrq	$3,%r8
+++	jz	.L256_enc_msg_x8_check_remainder
+++
+++	subq	$128,%rsi
+++	subq	$128,%rdi
+++
+++.L256_enc_msg_x8_loop1:
+++	addq	$128,%rsi
+++	addq	$128,%rdi
+++
+++	vmovdqa	%xmm0,%xmm1
+++	vmovdqa	%xmm9,%xmm2
+++	vmovdqa	%xmm10,%xmm3
+++	vmovdqa	%xmm11,%xmm4
+++	vmovdqa	%xmm12,%xmm5
+++	vmovdqa	%xmm13,%xmm6
+++	vmovdqa	%xmm14,%xmm7
+++
+++	vmovdqa	(%r11),%xmm8
+++
+++	vpxor	(%rcx),%xmm1,%xmm1
+++	vpxor	(%rcx),%xmm2,%xmm2
+++	vpxor	(%rcx),%xmm3,%xmm3
+++	vpxor	(%rcx),%xmm4,%xmm4
+++	vpxor	(%rcx),%xmm5,%xmm5
+++	vpxor	(%rcx),%xmm6,%xmm6
+++	vpxor	(%rcx),%xmm7,%xmm7
+++	vpxor	(%rcx),%xmm8,%xmm8
+++
+++	vmovdqu	16(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vmovdqa	(%r11),%xmm14
+++	vpaddd	eight(%rip),%xmm14,%xmm14
+++	vmovdqa	%xmm14,(%r11)
+++	vmovdqu	32(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpsubd	one(%rip),%xmm14,%xmm14
+++	vmovdqu	48(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpaddd	eight(%rip),%xmm0,%xmm0
+++	vmovdqu	64(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpaddd	eight(%rip),%xmm9,%xmm9
+++	vmovdqu	80(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpaddd	eight(%rip),%xmm10,%xmm10
+++	vmovdqu	96(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpaddd	eight(%rip),%xmm11,%xmm11
+++	vmovdqu	112(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpaddd	eight(%rip),%xmm12,%xmm12
+++	vmovdqu	128(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vpaddd	eight(%rip),%xmm13,%xmm13
+++	vmovdqu	144(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vmovdqu	160(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vmovdqu	176(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vmovdqu	192(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vmovdqu	208(%rcx),%xmm15
+++	vaesenc	%xmm15,%xmm1,%xmm1
+++	vaesenc	%xmm15,%xmm2,%xmm2
+++	vaesenc	%xmm15,%xmm3,%xmm3
+++	vaesenc	%xmm15,%xmm4,%xmm4
+++	vaesenc	%xmm15,%xmm5,%xmm5
+++	vaesenc	%xmm15,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm8,%xmm8
+++
+++	vmovdqu	224(%rcx),%xmm15
+++	vaesenclast	%xmm15,%xmm1,%xmm1
+++	vaesenclast	%xmm15,%xmm2,%xmm2
+++	vaesenclast	%xmm15,%xmm3,%xmm3
+++	vaesenclast	%xmm15,%xmm4,%xmm4
+++	vaesenclast	%xmm15,%xmm5,%xmm5
+++	vaesenclast	%xmm15,%xmm6,%xmm6
+++	vaesenclast	%xmm15,%xmm7,%xmm7
+++	vaesenclast	%xmm15,%xmm8,%xmm8
+++
+++
+++
+++	vpxor	0(%rdi),%xmm1,%xmm1
+++	vpxor	16(%rdi),%xmm2,%xmm2
+++	vpxor	32(%rdi),%xmm3,%xmm3
+++	vpxor	48(%rdi),%xmm4,%xmm4
+++	vpxor	64(%rdi),%xmm5,%xmm5
+++	vpxor	80(%rdi),%xmm6,%xmm6
+++	vpxor	96(%rdi),%xmm7,%xmm7
+++	vpxor	112(%rdi),%xmm8,%xmm8
+++
+++	subq	$1,%r8
+++
+++	vmovdqu	%xmm1,0(%rsi)
+++	vmovdqu	%xmm2,16(%rsi)
+++	vmovdqu	%xmm3,32(%rsi)
+++	vmovdqu	%xmm4,48(%rsi)
+++	vmovdqu	%xmm5,64(%rsi)
+++	vmovdqu	%xmm6,80(%rsi)
+++	vmovdqu	%xmm7,96(%rsi)
+++	vmovdqu	%xmm8,112(%rsi)
+++
+++	jne	.L256_enc_msg_x8_loop1
+++
+++	addq	$128,%rsi
+++	addq	$128,%rdi
+++
+++.L256_enc_msg_x8_check_remainder:
+++	cmpq	$0,%r10
+++	je	.L256_enc_msg_x8_out
+++
+++.L256_enc_msg_x8_loop2:
+++
+++
+++	vmovdqa	%xmm0,%xmm1
+++	vpaddd	one(%rip),%xmm0,%xmm0
+++
+++	vpxor	(%rcx),%xmm1,%xmm1
+++	vaesenc	16(%rcx),%xmm1,%xmm1
+++	vaesenc	32(%rcx),%xmm1,%xmm1
+++	vaesenc	48(%rcx),%xmm1,%xmm1
+++	vaesenc	64(%rcx),%xmm1,%xmm1
+++	vaesenc	80(%rcx),%xmm1,%xmm1
+++	vaesenc	96(%rcx),%xmm1,%xmm1
+++	vaesenc	112(%rcx),%xmm1,%xmm1
+++	vaesenc	128(%rcx),%xmm1,%xmm1
+++	vaesenc	144(%rcx),%xmm1,%xmm1
+++	vaesenc	160(%rcx),%xmm1,%xmm1
+++	vaesenc	176(%rcx),%xmm1,%xmm1
+++	vaesenc	192(%rcx),%xmm1,%xmm1
+++	vaesenc	208(%rcx),%xmm1,%xmm1
+++	vaesenclast	224(%rcx),%xmm1,%xmm1
+++
+++
+++	vpxor	(%rdi),%xmm1,%xmm1
+++
+++	vmovdqu	%xmm1,(%rsi)
+++
+++	addq	$16,%rdi
+++	addq	$16,%rsi
+++	subq	$1,%r10
+++	jnz	.L256_enc_msg_x8_loop2
+++
+++.L256_enc_msg_x8_out:
+++	.byte	0xf3,0xc3
+++
+++.cfi_endproc	
+++.size	aes256gcmsiv_enc_msg_x8,.-aes256gcmsiv_enc_msg_x8
+++.globl	aes256gcmsiv_dec
+++.hidden aes256gcmsiv_dec
+++.type	aes256gcmsiv_dec,@function
+++.align	16
+++aes256gcmsiv_dec:
+++.cfi_startproc	
+++	testq	$~15,%r9
+++	jnz	.L256_dec_start
+++	.byte	0xf3,0xc3
+++
+++.L256_dec_start:
+++	vzeroupper
+++	vmovdqa	(%rdx),%xmm0
+++	movq	%rdx,%rax
+++
+++	leaq	32(%rax),%rax
+++	leaq	32(%rcx),%rcx
+++
+++
+++	vmovdqu	(%rdi,%r9,1),%xmm15
+++	vpor	OR_MASK(%rip),%xmm15,%xmm15
+++	andq	$~15,%r9
+++
+++
+++	cmpq	$96,%r9
+++	jb	.L256_dec_loop2
+++
+++
+++	subq	$96,%r9
+++	vmovdqa	%xmm15,%xmm7
+++	vpaddd	one(%rip),%xmm7,%xmm8
+++	vpaddd	two(%rip),%xmm7,%xmm9
+++	vpaddd	one(%rip),%xmm9,%xmm10
+++	vpaddd	two(%rip),%xmm9,%xmm11
+++	vpaddd	one(%rip),%xmm11,%xmm12
+++	vpaddd	two(%rip),%xmm11,%xmm15
+++
+++	vpxor	(%r8),%xmm7,%xmm7
+++	vpxor	(%r8),%xmm8,%xmm8
+++	vpxor	(%r8),%xmm9,%xmm9
+++	vpxor	(%r8),%xmm10,%xmm10
+++	vpxor	(%r8),%xmm11,%xmm11
+++	vpxor	(%r8),%xmm12,%xmm12
+++
+++	vmovdqu	16(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	32(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	48(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	64(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	80(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	96(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	112(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	128(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	144(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	160(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	176(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	192(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	208(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	224(%r8),%xmm4
+++	vaesenclast	%xmm4,%xmm7,%xmm7
+++	vaesenclast	%xmm4,%xmm8,%xmm8
+++	vaesenclast	%xmm4,%xmm9,%xmm9
+++	vaesenclast	%xmm4,%xmm10,%xmm10
+++	vaesenclast	%xmm4,%xmm11,%xmm11
+++	vaesenclast	%xmm4,%xmm12,%xmm12
+++
+++
+++	vpxor	0(%rdi),%xmm7,%xmm7
+++	vpxor	16(%rdi),%xmm8,%xmm8
+++	vpxor	32(%rdi),%xmm9,%xmm9
+++	vpxor	48(%rdi),%xmm10,%xmm10
+++	vpxor	64(%rdi),%xmm11,%xmm11
+++	vpxor	80(%rdi),%xmm12,%xmm12
+++
+++	vmovdqu	%xmm7,0(%rsi)
+++	vmovdqu	%xmm8,16(%rsi)
+++	vmovdqu	%xmm9,32(%rsi)
+++	vmovdqu	%xmm10,48(%rsi)
+++	vmovdqu	%xmm11,64(%rsi)
+++	vmovdqu	%xmm12,80(%rsi)
+++
+++	addq	$96,%rdi
+++	addq	$96,%rsi
+++	jmp	.L256_dec_loop1
+++
+++
+++.align	64
+++.L256_dec_loop1:
+++	cmpq	$96,%r9
+++	jb	.L256_dec_finish_96
+++	subq	$96,%r9
+++
+++	vmovdqa	%xmm12,%xmm6
+++	vmovdqa	%xmm11,16-32(%rax)
+++	vmovdqa	%xmm10,32-32(%rax)
+++	vmovdqa	%xmm9,48-32(%rax)
+++	vmovdqa	%xmm8,64-32(%rax)
+++	vmovdqa	%xmm7,80-32(%rax)
+++
+++	vmovdqa	%xmm15,%xmm7
+++	vpaddd	one(%rip),%xmm7,%xmm8
+++	vpaddd	two(%rip),%xmm7,%xmm9
+++	vpaddd	one(%rip),%xmm9,%xmm10
+++	vpaddd	two(%rip),%xmm9,%xmm11
+++	vpaddd	one(%rip),%xmm11,%xmm12
+++	vpaddd	two(%rip),%xmm11,%xmm15
+++
+++	vmovdqa	(%r8),%xmm4
+++	vpxor	%xmm4,%xmm7,%xmm7
+++	vpxor	%xmm4,%xmm8,%xmm8
+++	vpxor	%xmm4,%xmm9,%xmm9
+++	vpxor	%xmm4,%xmm10,%xmm10
+++	vpxor	%xmm4,%xmm11,%xmm11
+++	vpxor	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	0-32(%rcx),%xmm4
+++	vpclmulqdq	$0x11,%xmm4,%xmm6,%xmm2
+++	vpclmulqdq	$0x00,%xmm4,%xmm6,%xmm3
+++	vpclmulqdq	$0x01,%xmm4,%xmm6,%xmm1
+++	vpclmulqdq	$0x10,%xmm4,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vmovdqu	16(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	-16(%rax),%xmm6
+++	vmovdqu	-16(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++
+++	vmovdqu	32(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	0(%rax),%xmm6
+++	vmovdqu	0(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++
+++	vmovdqu	48(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	16(%rax),%xmm6
+++	vmovdqu	16(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++
+++	vmovdqu	64(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	32(%rax),%xmm6
+++	vmovdqu	32(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++
+++	vmovdqu	80(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	96(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	112(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++
+++	vmovdqa	80-32(%rax),%xmm6
+++	vpxor	%xmm0,%xmm6,%xmm6
+++	vmovdqu	80-32(%rcx),%xmm5
+++
+++	vpclmulqdq	$0x01,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x10,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vmovdqu	128(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++
+++	vpsrldq	$8,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm5
+++	vpslldq	$8,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm0
+++
+++	vmovdqa	poly(%rip),%xmm3
+++
+++	vmovdqu	144(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	160(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	176(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	192(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	208(%r8),%xmm4
+++	vaesenc	%xmm4,%xmm7,%xmm7
+++	vaesenc	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm4,%xmm9,%xmm9
+++	vaesenc	%xmm4,%xmm10,%xmm10
+++	vaesenc	%xmm4,%xmm11,%xmm11
+++	vaesenc	%xmm4,%xmm12,%xmm12
+++
+++	vmovdqu	224(%r8),%xmm6
+++	vpalignr	$8,%xmm0,%xmm0,%xmm2
+++	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm0
+++	vpxor	%xmm0,%xmm2,%xmm0
+++
+++	vpxor	0(%rdi),%xmm6,%xmm4
+++	vaesenclast	%xmm4,%xmm7,%xmm7
+++	vpxor	16(%rdi),%xmm6,%xmm4
+++	vaesenclast	%xmm4,%xmm8,%xmm8
+++	vpxor	32(%rdi),%xmm6,%xmm4
+++	vaesenclast	%xmm4,%xmm9,%xmm9
+++	vpxor	48(%rdi),%xmm6,%xmm4
+++	vaesenclast	%xmm4,%xmm10,%xmm10
+++	vpxor	64(%rdi),%xmm6,%xmm4
+++	vaesenclast	%xmm4,%xmm11,%xmm11
+++	vpxor	80(%rdi),%xmm6,%xmm4
+++	vaesenclast	%xmm4,%xmm12,%xmm12
+++
+++	vpalignr	$8,%xmm0,%xmm0,%xmm2
+++	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm0
+++	vpxor	%xmm0,%xmm2,%xmm0
+++
+++	vmovdqu	%xmm7,0(%rsi)
+++	vmovdqu	%xmm8,16(%rsi)
+++	vmovdqu	%xmm9,32(%rsi)
+++	vmovdqu	%xmm10,48(%rsi)
+++	vmovdqu	%xmm11,64(%rsi)
+++	vmovdqu	%xmm12,80(%rsi)
+++
+++	vpxor	%xmm5,%xmm0,%xmm0
+++
+++	leaq	96(%rdi),%rdi
+++	leaq	96(%rsi),%rsi
+++	jmp	.L256_dec_loop1
+++
+++.L256_dec_finish_96:
+++	vmovdqa	%xmm12,%xmm6
+++	vmovdqa	%xmm11,16-32(%rax)
+++	vmovdqa	%xmm10,32-32(%rax)
+++	vmovdqa	%xmm9,48-32(%rax)
+++	vmovdqa	%xmm8,64-32(%rax)
+++	vmovdqa	%xmm7,80-32(%rax)
+++
+++	vmovdqu	0-32(%rcx),%xmm4
+++	vpclmulqdq	$0x10,%xmm4,%xmm6,%xmm1
+++	vpclmulqdq	$0x11,%xmm4,%xmm6,%xmm2
+++	vpclmulqdq	$0x00,%xmm4,%xmm6,%xmm3
+++	vpclmulqdq	$0x01,%xmm4,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vmovdqu	-16(%rax),%xmm6
+++	vmovdqu	-16(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vmovdqu	0(%rax),%xmm6
+++	vmovdqu	0(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vmovdqu	16(%rax),%xmm6
+++	vmovdqu	16(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vmovdqu	32(%rax),%xmm6
+++	vmovdqu	32(%rcx),%xmm13
+++
+++	vpclmulqdq	$0x10,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x11,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x01,%xmm13,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++
+++	vmovdqu	80-32(%rax),%xmm6
+++	vpxor	%xmm0,%xmm6,%xmm6
+++	vmovdqu	80-32(%rcx),%xmm5
+++	vpclmulqdq	$0x11,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	vpclmulqdq	$0x10,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x01,%xmm5,%xmm6,%xmm4
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vpsrldq	$8,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm2,%xmm5
+++	vpslldq	$8,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm0
+++
+++	vmovdqa	poly(%rip),%xmm3
+++
+++	vpalignr	$8,%xmm0,%xmm0,%xmm2
+++	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm0
+++	vpxor	%xmm0,%xmm2,%xmm0
+++
+++	vpalignr	$8,%xmm0,%xmm0,%xmm2
+++	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm0
+++	vpxor	%xmm0,%xmm2,%xmm0
+++
+++	vpxor	%xmm5,%xmm0,%xmm0
+++
+++.L256_dec_loop2:
+++
+++
+++
+++	cmpq	$16,%r9
+++	jb	.L256_dec_out
+++	subq	$16,%r9
+++
+++	vmovdqa	%xmm15,%xmm2
+++	vpaddd	one(%rip),%xmm15,%xmm15
+++
+++	vpxor	0(%r8),%xmm2,%xmm2
+++	vaesenc	16(%r8),%xmm2,%xmm2
+++	vaesenc	32(%r8),%xmm2,%xmm2
+++	vaesenc	48(%r8),%xmm2,%xmm2
+++	vaesenc	64(%r8),%xmm2,%xmm2
+++	vaesenc	80(%r8),%xmm2,%xmm2
+++	vaesenc	96(%r8),%xmm2,%xmm2
+++	vaesenc	112(%r8),%xmm2,%xmm2
+++	vaesenc	128(%r8),%xmm2,%xmm2
+++	vaesenc	144(%r8),%xmm2,%xmm2
+++	vaesenc	160(%r8),%xmm2,%xmm2
+++	vaesenc	176(%r8),%xmm2,%xmm2
+++	vaesenc	192(%r8),%xmm2,%xmm2
+++	vaesenc	208(%r8),%xmm2,%xmm2
+++	vaesenclast	224(%r8),%xmm2,%xmm2
+++	vpxor	(%rdi),%xmm2,%xmm2
+++	vmovdqu	%xmm2,(%rsi)
+++	addq	$16,%rdi
+++	addq	$16,%rsi
+++
+++	vpxor	%xmm2,%xmm0,%xmm0
+++	vmovdqa	-32(%rcx),%xmm1
+++	call	GFMUL
+++
+++	jmp	.L256_dec_loop2
+++
+++.L256_dec_out:
+++	vmovdqu	%xmm0,(%rdx)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes256gcmsiv_dec, .-aes256gcmsiv_dec
+++.globl	aes256gcmsiv_kdf
+++.hidden aes256gcmsiv_kdf
+++.type	aes256gcmsiv_kdf,@function
+++.align	16
+++aes256gcmsiv_kdf:
+++.cfi_startproc	
+++
+++
+++
+++
+++	vmovdqa	(%rdx),%xmm1
+++	vmovdqa	0(%rdi),%xmm4
+++	vmovdqa	and_mask(%rip),%xmm11
+++	vmovdqa	one(%rip),%xmm8
+++	vpshufd	$0x90,%xmm4,%xmm4
+++	vpand	%xmm11,%xmm4,%xmm4
+++	vpaddd	%xmm8,%xmm4,%xmm6
+++	vpaddd	%xmm8,%xmm6,%xmm7
+++	vpaddd	%xmm8,%xmm7,%xmm11
+++	vpaddd	%xmm8,%xmm11,%xmm12
+++	vpaddd	%xmm8,%xmm12,%xmm13
+++
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpxor	%xmm1,%xmm6,%xmm6
+++	vpxor	%xmm1,%xmm7,%xmm7
+++	vpxor	%xmm1,%xmm11,%xmm11
+++	vpxor	%xmm1,%xmm12,%xmm12
+++	vpxor	%xmm1,%xmm13,%xmm13
+++
+++	vmovdqa	16(%rdx),%xmm1
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vaesenc	%xmm1,%xmm6,%xmm6
+++	vaesenc	%xmm1,%xmm7,%xmm7
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++	vaesenc	%xmm1,%xmm13,%xmm13
+++
+++	vmovdqa	32(%rdx),%xmm2
+++	vaesenc	%xmm2,%xmm4,%xmm4
+++	vaesenc	%xmm2,%xmm6,%xmm6
+++	vaesenc	%xmm2,%xmm7,%xmm7
+++	vaesenc	%xmm2,%xmm11,%xmm11
+++	vaesenc	%xmm2,%xmm12,%xmm12
+++	vaesenc	%xmm2,%xmm13,%xmm13
+++
+++	vmovdqa	48(%rdx),%xmm1
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vaesenc	%xmm1,%xmm6,%xmm6
+++	vaesenc	%xmm1,%xmm7,%xmm7
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++	vaesenc	%xmm1,%xmm13,%xmm13
+++
+++	vmovdqa	64(%rdx),%xmm2
+++	vaesenc	%xmm2,%xmm4,%xmm4
+++	vaesenc	%xmm2,%xmm6,%xmm6
+++	vaesenc	%xmm2,%xmm7,%xmm7
+++	vaesenc	%xmm2,%xmm11,%xmm11
+++	vaesenc	%xmm2,%xmm12,%xmm12
+++	vaesenc	%xmm2,%xmm13,%xmm13
+++
+++	vmovdqa	80(%rdx),%xmm1
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vaesenc	%xmm1,%xmm6,%xmm6
+++	vaesenc	%xmm1,%xmm7,%xmm7
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++	vaesenc	%xmm1,%xmm13,%xmm13
+++
+++	vmovdqa	96(%rdx),%xmm2
+++	vaesenc	%xmm2,%xmm4,%xmm4
+++	vaesenc	%xmm2,%xmm6,%xmm6
+++	vaesenc	%xmm2,%xmm7,%xmm7
+++	vaesenc	%xmm2,%xmm11,%xmm11
+++	vaesenc	%xmm2,%xmm12,%xmm12
+++	vaesenc	%xmm2,%xmm13,%xmm13
+++
+++	vmovdqa	112(%rdx),%xmm1
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vaesenc	%xmm1,%xmm6,%xmm6
+++	vaesenc	%xmm1,%xmm7,%xmm7
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++	vaesenc	%xmm1,%xmm13,%xmm13
+++
+++	vmovdqa	128(%rdx),%xmm2
+++	vaesenc	%xmm2,%xmm4,%xmm4
+++	vaesenc	%xmm2,%xmm6,%xmm6
+++	vaesenc	%xmm2,%xmm7,%xmm7
+++	vaesenc	%xmm2,%xmm11,%xmm11
+++	vaesenc	%xmm2,%xmm12,%xmm12
+++	vaesenc	%xmm2,%xmm13,%xmm13
+++
+++	vmovdqa	144(%rdx),%xmm1
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vaesenc	%xmm1,%xmm6,%xmm6
+++	vaesenc	%xmm1,%xmm7,%xmm7
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++	vaesenc	%xmm1,%xmm13,%xmm13
+++
+++	vmovdqa	160(%rdx),%xmm2
+++	vaesenc	%xmm2,%xmm4,%xmm4
+++	vaesenc	%xmm2,%xmm6,%xmm6
+++	vaesenc	%xmm2,%xmm7,%xmm7
+++	vaesenc	%xmm2,%xmm11,%xmm11
+++	vaesenc	%xmm2,%xmm12,%xmm12
+++	vaesenc	%xmm2,%xmm13,%xmm13
+++
+++	vmovdqa	176(%rdx),%xmm1
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vaesenc	%xmm1,%xmm6,%xmm6
+++	vaesenc	%xmm1,%xmm7,%xmm7
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++	vaesenc	%xmm1,%xmm13,%xmm13
+++
+++	vmovdqa	192(%rdx),%xmm2
+++	vaesenc	%xmm2,%xmm4,%xmm4
+++	vaesenc	%xmm2,%xmm6,%xmm6
+++	vaesenc	%xmm2,%xmm7,%xmm7
+++	vaesenc	%xmm2,%xmm11,%xmm11
+++	vaesenc	%xmm2,%xmm12,%xmm12
+++	vaesenc	%xmm2,%xmm13,%xmm13
+++
+++	vmovdqa	208(%rdx),%xmm1
+++	vaesenc	%xmm1,%xmm4,%xmm4
+++	vaesenc	%xmm1,%xmm6,%xmm6
+++	vaesenc	%xmm1,%xmm7,%xmm7
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++	vaesenc	%xmm1,%xmm13,%xmm13
+++
+++	vmovdqa	224(%rdx),%xmm2
+++	vaesenclast	%xmm2,%xmm4,%xmm4
+++	vaesenclast	%xmm2,%xmm6,%xmm6
+++	vaesenclast	%xmm2,%xmm7,%xmm7
+++	vaesenclast	%xmm2,%xmm11,%xmm11
+++	vaesenclast	%xmm2,%xmm12,%xmm12
+++	vaesenclast	%xmm2,%xmm13,%xmm13
+++
+++
+++	vmovdqa	%xmm4,0(%rsi)
+++	vmovdqa	%xmm6,16(%rsi)
+++	vmovdqa	%xmm7,32(%rsi)
+++	vmovdqa	%xmm11,48(%rsi)
+++	vmovdqa	%xmm12,64(%rsi)
+++	vmovdqa	%xmm13,80(%rsi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes256gcmsiv_kdf, .-aes256gcmsiv_kdf
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/cipher_extra/chacha20_poly1305_x86_64.S b/linux-x86_64/ypto/cipher_extra/chacha20_poly1305_x86_64.S
++new file mode 100644
++index 000000000..12368e67e
++--- /dev/null
+++++ b/linux-x86_64/ypto/cipher_extra/chacha20_poly1305_x86_64.S
++@@ -0,0 +1,8922 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++.extern	OPENSSL_ia32cap_P
+++.hidden OPENSSL_ia32cap_P
+++
+++chacha20_poly1305_constants:
+++
+++.align	64
+++.Lchacha20_consts:
+++.byte	'e','x','p','a','n','d',' ','3','2','-','b','y','t','e',' ','k'
+++.byte	'e','x','p','a','n','d',' ','3','2','-','b','y','t','e',' ','k'
+++.Lrol8:
+++.byte	3,0,1,2, 7,4,5,6, 11,8,9,10, 15,12,13,14
+++.byte	3,0,1,2, 7,4,5,6, 11,8,9,10, 15,12,13,14
+++.Lrol16:
+++.byte	2,3,0,1, 6,7,4,5, 10,11,8,9, 14,15,12,13
+++.byte	2,3,0,1, 6,7,4,5, 10,11,8,9, 14,15,12,13
+++.Lavx2_init:
+++.long	0,0,0,0
+++.Lsse_inc:
+++.long	1,0,0,0
+++.Lavx2_inc:
+++.long	2,0,0,0,2,0,0,0
+++.Lclamp:
+++.quad	0x0FFFFFFC0FFFFFFF, 0x0FFFFFFC0FFFFFFC
+++.quad	0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF
+++.align	16
+++.Land_masks:
+++.byte	0xff,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00
+++.byte	0xff,0xff,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00
+++.byte	0xff,0xff,0xff,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00
+++.byte	0xff,0xff,0xff,0xff,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00
+++.byte	0xff,0xff,0xff,0xff,0xff,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00
+++.byte	0xff,0xff,0xff,0xff,0xff,0xff,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00
+++.byte	0xff,0xff,0xff,0xff,0xff,0xff,0xff,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00
+++.byte	0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00
+++.byte	0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0x00,0x00,0x00,0x00,0x00,0x00,0x00
+++.byte	0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0x00,0x00,0x00,0x00,0x00,0x00
+++.byte	0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0x00,0x00,0x00,0x00,0x00
+++.byte	0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0x00,0x00,0x00,0x00
+++.byte	0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0x00,0x00,0x00
+++.byte	0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0x00,0x00
+++.byte	0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0x00
+++.byte	0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff
+++
+++.type	poly_hash_ad_internal,@function
+++.align	64
+++poly_hash_ad_internal:
+++.cfi_startproc	
+++.cfi_def_cfa	rsp, 8
+++	xorq	%r10,%r10
+++	xorq	%r11,%r11
+++	xorq	%r12,%r12
+++	cmpq	$13,%r8
+++	jne	.Lhash_ad_loop
+++.Lpoly_fast_tls_ad:
+++
+++	movq	(%rcx),%r10
+++	movq	5(%rcx),%r11
+++	shrq	$24,%r11
+++	movq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	.byte	0xf3,0xc3
+++.Lhash_ad_loop:
+++
+++	cmpq	$16,%r8
+++	jb	.Lhash_ad_tail
+++	addq	0+0(%rcx),%r10
+++	adcq	8+0(%rcx),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rcx),%rcx
+++	subq	$16,%r8
+++	jmp	.Lhash_ad_loop
+++.Lhash_ad_tail:
+++	cmpq	$0,%r8
+++	je	.Lhash_ad_done
+++
+++	xorq	%r13,%r13
+++	xorq	%r14,%r14
+++	xorq	%r15,%r15
+++	addq	%r8,%rcx
+++.Lhash_ad_tail_loop:
+++	shldq	$8,%r13,%r14
+++	shlq	$8,%r13
+++	movzbq	-1(%rcx),%r15
+++	xorq	%r15,%r13
+++	decq	%rcx
+++	decq	%r8
+++	jne	.Lhash_ad_tail_loop
+++
+++	addq	%r13,%r10
+++	adcq	%r14,%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++
+++.Lhash_ad_done:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	poly_hash_ad_internal, .-poly_hash_ad_internal
+++
+++.globl	chacha20_poly1305_open
+++.hidden chacha20_poly1305_open
+++.type	chacha20_poly1305_open,@function
+++.align	64
+++chacha20_poly1305_open:
+++.cfi_startproc	
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++
+++
+++	pushq	%r9
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r9,-64
+++	subq	$288 + 0 + 32,%rsp
+++.cfi_adjust_cfa_offset	288 + 32
+++
+++	leaq	32(%rsp),%rbp
+++	andq	$-32,%rbp
+++
+++	movq	%rdx,%rbx
+++	movq	%r8,0+0+32(%rbp)
+++	movq	%rbx,8+0+32(%rbp)
+++
+++	movl	OPENSSL_ia32cap_P+8(%rip),%eax
+++	andl	$288,%eax
+++	xorl	$288,%eax
+++	jz	chacha20_poly1305_open_avx2
+++
+++	cmpq	$128,%rbx
+++	jbe	.Lopen_sse_128
+++
+++	movdqa	.Lchacha20_consts(%rip),%xmm0
+++	movdqu	0(%r9),%xmm4
+++	movdqu	16(%r9),%xmm8
+++	movdqu	32(%r9),%xmm12
+++
+++	movdqa	%xmm12,%xmm7
+++
+++	movdqa	%xmm4,0+48(%rbp)
+++	movdqa	%xmm8,0+64(%rbp)
+++	movdqa	%xmm12,0+96(%rbp)
+++	movq	$10,%r10
+++.Lopen_sse_init_rounds:
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,4
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,12
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,12
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,4
+++
+++	decq	%r10
+++	jne	.Lopen_sse_init_rounds
+++
+++	paddd	.Lchacha20_consts(%rip),%xmm0
+++	paddd	0+48(%rbp),%xmm4
+++
+++	pand	.Lclamp(%rip),%xmm0
+++	movdqa	%xmm0,0+0(%rbp)
+++	movdqa	%xmm4,0+16(%rbp)
+++
+++	movq	%r8,%r8
+++	call	poly_hash_ad_internal
+++.Lopen_sse_main_loop:
+++	cmpq	$256,%rbx
+++	jb	.Lopen_sse_tail
+++
+++	movdqa	.Lchacha20_consts(%rip),%xmm0
+++	movdqa	0+48(%rbp),%xmm4
+++	movdqa	0+64(%rbp),%xmm8
+++	movdqa	%xmm0,%xmm1
+++	movdqa	%xmm4,%xmm5
+++	movdqa	%xmm8,%xmm9
+++	movdqa	%xmm0,%xmm2
+++	movdqa	%xmm4,%xmm6
+++	movdqa	%xmm8,%xmm10
+++	movdqa	%xmm0,%xmm3
+++	movdqa	%xmm4,%xmm7
+++	movdqa	%xmm8,%xmm11
+++	movdqa	0+96(%rbp),%xmm15
+++	paddd	.Lsse_inc(%rip),%xmm15
+++	movdqa	%xmm15,%xmm14
+++	paddd	.Lsse_inc(%rip),%xmm14
+++	movdqa	%xmm14,%xmm13
+++	paddd	.Lsse_inc(%rip),%xmm13
+++	movdqa	%xmm13,%xmm12
+++	paddd	.Lsse_inc(%rip),%xmm12
+++	movdqa	%xmm12,0+96(%rbp)
+++	movdqa	%xmm13,0+112(%rbp)
+++	movdqa	%xmm14,0+128(%rbp)
+++	movdqa	%xmm15,0+144(%rbp)
+++
+++
+++
+++	movq	$4,%rcx
+++	movq	%rsi,%r8
+++.Lopen_sse_main_loop_rounds:
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	.Lrol16(%rip),%xmm8
+++	paddd	%xmm7,%xmm3
+++	paddd	%xmm6,%xmm2
+++	paddd	%xmm5,%xmm1
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm15
+++	pxor	%xmm2,%xmm14
+++	pxor	%xmm1,%xmm13
+++	pxor	%xmm0,%xmm12
+++.byte	102,69,15,56,0,248
+++.byte	102,69,15,56,0,240
+++.byte	102,69,15,56,0,232
+++.byte	102,69,15,56,0,224
+++	movdqa	0+80(%rbp),%xmm8
+++	paddd	%xmm15,%xmm11
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm13,%xmm9
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm11,%xmm7
+++	addq	0+0(%r8),%r10
+++	adcq	8+0(%r8),%r11
+++	adcq	$1,%r12
+++
+++	leaq	16(%r8),%r8
+++	pxor	%xmm10,%xmm6
+++	pxor	%xmm9,%xmm5
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	%xmm7,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm7
+++	pxor	%xmm8,%xmm7
+++	movdqa	%xmm6,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm6
+++	pxor	%xmm8,%xmm6
+++	movdqa	%xmm5,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm5
+++	pxor	%xmm8,%xmm5
+++	movdqa	%xmm4,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm4
+++	pxor	%xmm8,%xmm4
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movdqa	.Lrol8(%rip),%xmm8
+++	paddd	%xmm7,%xmm3
+++	paddd	%xmm6,%xmm2
+++	paddd	%xmm5,%xmm1
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm15
+++	pxor	%xmm2,%xmm14
+++	pxor	%xmm1,%xmm13
+++	pxor	%xmm0,%xmm12
+++.byte	102,69,15,56,0,248
+++.byte	102,69,15,56,0,240
+++.byte	102,69,15,56,0,232
+++.byte	102,69,15,56,0,224
+++	movdqa	0+80(%rbp),%xmm8
+++	paddd	%xmm15,%xmm11
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm13,%xmm9
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm11,%xmm7
+++	pxor	%xmm10,%xmm6
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	pxor	%xmm9,%xmm5
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	%xmm7,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm7
+++	pxor	%xmm8,%xmm7
+++	movdqa	%xmm6,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm6
+++	pxor	%xmm8,%xmm6
+++	movdqa	%xmm5,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm5
+++	pxor	%xmm8,%xmm5
+++	movdqa	%xmm4,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm4
+++	pxor	%xmm8,%xmm4
+++	movdqa	0+80(%rbp),%xmm8
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++.byte	102,15,58,15,255,4
+++.byte	102,69,15,58,15,219,8
+++.byte	102,69,15,58,15,255,12
+++.byte	102,15,58,15,246,4
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,12
+++.byte	102,15,58,15,237,4
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,12
+++.byte	102,15,58,15,228,4
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,12
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	.Lrol16(%rip),%xmm8
+++	paddd	%xmm7,%xmm3
+++	paddd	%xmm6,%xmm2
+++	paddd	%xmm5,%xmm1
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm15
+++	pxor	%xmm2,%xmm14
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	pxor	%xmm1,%xmm13
+++	pxor	%xmm0,%xmm12
+++.byte	102,69,15,56,0,248
+++.byte	102,69,15,56,0,240
+++.byte	102,69,15,56,0,232
+++.byte	102,69,15,56,0,224
+++	movdqa	0+80(%rbp),%xmm8
+++	paddd	%xmm15,%xmm11
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm13,%xmm9
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm11,%xmm7
+++	pxor	%xmm10,%xmm6
+++	pxor	%xmm9,%xmm5
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	%xmm7,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm7
+++	pxor	%xmm8,%xmm7
+++	movdqa	%xmm6,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm6
+++	pxor	%xmm8,%xmm6
+++	movdqa	%xmm5,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm5
+++	pxor	%xmm8,%xmm5
+++	movdqa	%xmm4,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm4
+++	pxor	%xmm8,%xmm4
+++	movdqa	.Lrol8(%rip),%xmm8
+++	paddd	%xmm7,%xmm3
+++	paddd	%xmm6,%xmm2
+++	paddd	%xmm5,%xmm1
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm15
+++	pxor	%xmm2,%xmm14
+++	pxor	%xmm1,%xmm13
+++	pxor	%xmm0,%xmm12
+++.byte	102,69,15,56,0,248
+++.byte	102,69,15,56,0,240
+++.byte	102,69,15,56,0,232
+++.byte	102,69,15,56,0,224
+++	movdqa	0+80(%rbp),%xmm8
+++	paddd	%xmm15,%xmm11
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm13,%xmm9
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm11,%xmm7
+++	pxor	%xmm10,%xmm6
+++	pxor	%xmm9,%xmm5
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	%xmm7,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm7
+++	pxor	%xmm8,%xmm7
+++	movdqa	%xmm6,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm6
+++	pxor	%xmm8,%xmm6
+++	movdqa	%xmm5,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm5
+++	pxor	%xmm8,%xmm5
+++	movdqa	%xmm4,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm4
+++	pxor	%xmm8,%xmm4
+++	movdqa	0+80(%rbp),%xmm8
+++.byte	102,15,58,15,255,12
+++.byte	102,69,15,58,15,219,8
+++.byte	102,69,15,58,15,255,4
+++.byte	102,15,58,15,246,12
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,4
+++.byte	102,15,58,15,237,12
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,4
+++.byte	102,15,58,15,228,12
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,4
+++
+++	decq	%rcx
+++	jge	.Lopen_sse_main_loop_rounds
+++	addq	0+0(%r8),%r10
+++	adcq	8+0(%r8),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%r8),%r8
+++	cmpq	$-6,%rcx
+++	jg	.Lopen_sse_main_loop_rounds
+++	paddd	.Lchacha20_consts(%rip),%xmm3
+++	paddd	0+48(%rbp),%xmm7
+++	paddd	0+64(%rbp),%xmm11
+++	paddd	0+144(%rbp),%xmm15
+++	paddd	.Lchacha20_consts(%rip),%xmm2
+++	paddd	0+48(%rbp),%xmm6
+++	paddd	0+64(%rbp),%xmm10
+++	paddd	0+128(%rbp),%xmm14
+++	paddd	.Lchacha20_consts(%rip),%xmm1
+++	paddd	0+48(%rbp),%xmm5
+++	paddd	0+64(%rbp),%xmm9
+++	paddd	0+112(%rbp),%xmm13
+++	paddd	.Lchacha20_consts(%rip),%xmm0
+++	paddd	0+48(%rbp),%xmm4
+++	paddd	0+64(%rbp),%xmm8
+++	paddd	0+96(%rbp),%xmm12
+++	movdqa	%xmm12,0+80(%rbp)
+++	movdqu	0 + 0(%rsi),%xmm12
+++	pxor	%xmm3,%xmm12
+++	movdqu	%xmm12,0 + 0(%rdi)
+++	movdqu	16 + 0(%rsi),%xmm12
+++	pxor	%xmm7,%xmm12
+++	movdqu	%xmm12,16 + 0(%rdi)
+++	movdqu	32 + 0(%rsi),%xmm12
+++	pxor	%xmm11,%xmm12
+++	movdqu	%xmm12,32 + 0(%rdi)
+++	movdqu	48 + 0(%rsi),%xmm12
+++	pxor	%xmm15,%xmm12
+++	movdqu	%xmm12,48 + 0(%rdi)
+++	movdqu	0 + 64(%rsi),%xmm3
+++	movdqu	16 + 64(%rsi),%xmm7
+++	movdqu	32 + 64(%rsi),%xmm11
+++	movdqu	48 + 64(%rsi),%xmm15
+++	pxor	%xmm3,%xmm2
+++	pxor	%xmm7,%xmm6
+++	pxor	%xmm11,%xmm10
+++	pxor	%xmm14,%xmm15
+++	movdqu	%xmm2,0 + 64(%rdi)
+++	movdqu	%xmm6,16 + 64(%rdi)
+++	movdqu	%xmm10,32 + 64(%rdi)
+++	movdqu	%xmm15,48 + 64(%rdi)
+++	movdqu	0 + 128(%rsi),%xmm3
+++	movdqu	16 + 128(%rsi),%xmm7
+++	movdqu	32 + 128(%rsi),%xmm11
+++	movdqu	48 + 128(%rsi),%xmm15
+++	pxor	%xmm3,%xmm1
+++	pxor	%xmm7,%xmm5
+++	pxor	%xmm11,%xmm9
+++	pxor	%xmm13,%xmm15
+++	movdqu	%xmm1,0 + 128(%rdi)
+++	movdqu	%xmm5,16 + 128(%rdi)
+++	movdqu	%xmm9,32 + 128(%rdi)
+++	movdqu	%xmm15,48 + 128(%rdi)
+++	movdqu	0 + 192(%rsi),%xmm3
+++	movdqu	16 + 192(%rsi),%xmm7
+++	movdqu	32 + 192(%rsi),%xmm11
+++	movdqu	48 + 192(%rsi),%xmm15
+++	pxor	%xmm3,%xmm0
+++	pxor	%xmm7,%xmm4
+++	pxor	%xmm11,%xmm8
+++	pxor	0+80(%rbp),%xmm15
+++	movdqu	%xmm0,0 + 192(%rdi)
+++	movdqu	%xmm4,16 + 192(%rdi)
+++	movdqu	%xmm8,32 + 192(%rdi)
+++	movdqu	%xmm15,48 + 192(%rdi)
+++
+++	leaq	256(%rsi),%rsi
+++	leaq	256(%rdi),%rdi
+++	subq	$256,%rbx
+++	jmp	.Lopen_sse_main_loop
+++.Lopen_sse_tail:
+++
+++	testq	%rbx,%rbx
+++	jz	.Lopen_sse_finalize
+++	cmpq	$192,%rbx
+++	ja	.Lopen_sse_tail_256
+++	cmpq	$128,%rbx
+++	ja	.Lopen_sse_tail_192
+++	cmpq	$64,%rbx
+++	ja	.Lopen_sse_tail_128
+++	movdqa	.Lchacha20_consts(%rip),%xmm0
+++	movdqa	0+48(%rbp),%xmm4
+++	movdqa	0+64(%rbp),%xmm8
+++	movdqa	0+96(%rbp),%xmm12
+++	paddd	.Lsse_inc(%rip),%xmm12
+++	movdqa	%xmm12,0+96(%rbp)
+++
+++	xorq	%r8,%r8
+++	movq	%rbx,%rcx
+++	cmpq	$16,%rcx
+++	jb	.Lopen_sse_tail_64_rounds
+++.Lopen_sse_tail_64_rounds_and_x1hash:
+++	addq	0+0(%rsi,%r8,1),%r10
+++	adcq	8+0(%rsi,%r8,1),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	subq	$16,%rcx
+++.Lopen_sse_tail_64_rounds:
+++	addq	$16,%r8
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,4
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,12
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,12
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,4
+++
+++	cmpq	$16,%rcx
+++	jae	.Lopen_sse_tail_64_rounds_and_x1hash
+++	cmpq	$160,%r8
+++	jne	.Lopen_sse_tail_64_rounds
+++	paddd	.Lchacha20_consts(%rip),%xmm0
+++	paddd	0+48(%rbp),%xmm4
+++	paddd	0+64(%rbp),%xmm8
+++	paddd	0+96(%rbp),%xmm12
+++
+++	jmp	.Lopen_sse_tail_64_dec_loop
+++
+++.Lopen_sse_tail_128:
+++	movdqa	.Lchacha20_consts(%rip),%xmm0
+++	movdqa	0+48(%rbp),%xmm4
+++	movdqa	0+64(%rbp),%xmm8
+++	movdqa	%xmm0,%xmm1
+++	movdqa	%xmm4,%xmm5
+++	movdqa	%xmm8,%xmm9
+++	movdqa	0+96(%rbp),%xmm13
+++	paddd	.Lsse_inc(%rip),%xmm13
+++	movdqa	%xmm13,%xmm12
+++	paddd	.Lsse_inc(%rip),%xmm12
+++	movdqa	%xmm12,0+96(%rbp)
+++	movdqa	%xmm13,0+112(%rbp)
+++
+++	movq	%rbx,%rcx
+++	andq	$-16,%rcx
+++	xorq	%r8,%r8
+++.Lopen_sse_tail_128_rounds_and_x1hash:
+++	addq	0+0(%rsi,%r8,1),%r10
+++	adcq	8+0(%rsi,%r8,1),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++.Lopen_sse_tail_128_rounds:
+++	addq	$16,%r8
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,4
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,12
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm5
+++	pxor	%xmm3,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm5
+++	pxor	%xmm3,%xmm5
+++.byte	102,15,58,15,237,4
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,12
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,12
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,4
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm5
+++	pxor	%xmm3,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm5
+++	pxor	%xmm3,%xmm5
+++.byte	102,15,58,15,237,12
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,4
+++
+++	cmpq	%rcx,%r8
+++	jb	.Lopen_sse_tail_128_rounds_and_x1hash
+++	cmpq	$160,%r8
+++	jne	.Lopen_sse_tail_128_rounds
+++	paddd	.Lchacha20_consts(%rip),%xmm1
+++	paddd	0+48(%rbp),%xmm5
+++	paddd	0+64(%rbp),%xmm9
+++	paddd	0+112(%rbp),%xmm13
+++	paddd	.Lchacha20_consts(%rip),%xmm0
+++	paddd	0+48(%rbp),%xmm4
+++	paddd	0+64(%rbp),%xmm8
+++	paddd	0+96(%rbp),%xmm12
+++	movdqu	0 + 0(%rsi),%xmm3
+++	movdqu	16 + 0(%rsi),%xmm7
+++	movdqu	32 + 0(%rsi),%xmm11
+++	movdqu	48 + 0(%rsi),%xmm15
+++	pxor	%xmm3,%xmm1
+++	pxor	%xmm7,%xmm5
+++	pxor	%xmm11,%xmm9
+++	pxor	%xmm13,%xmm15
+++	movdqu	%xmm1,0 + 0(%rdi)
+++	movdqu	%xmm5,16 + 0(%rdi)
+++	movdqu	%xmm9,32 + 0(%rdi)
+++	movdqu	%xmm15,48 + 0(%rdi)
+++
+++	subq	$64,%rbx
+++	leaq	64(%rsi),%rsi
+++	leaq	64(%rdi),%rdi
+++	jmp	.Lopen_sse_tail_64_dec_loop
+++
+++.Lopen_sse_tail_192:
+++	movdqa	.Lchacha20_consts(%rip),%xmm0
+++	movdqa	0+48(%rbp),%xmm4
+++	movdqa	0+64(%rbp),%xmm8
+++	movdqa	%xmm0,%xmm1
+++	movdqa	%xmm4,%xmm5
+++	movdqa	%xmm8,%xmm9
+++	movdqa	%xmm0,%xmm2
+++	movdqa	%xmm4,%xmm6
+++	movdqa	%xmm8,%xmm10
+++	movdqa	0+96(%rbp),%xmm14
+++	paddd	.Lsse_inc(%rip),%xmm14
+++	movdqa	%xmm14,%xmm13
+++	paddd	.Lsse_inc(%rip),%xmm13
+++	movdqa	%xmm13,%xmm12
+++	paddd	.Lsse_inc(%rip),%xmm12
+++	movdqa	%xmm12,0+96(%rbp)
+++	movdqa	%xmm13,0+112(%rbp)
+++	movdqa	%xmm14,0+128(%rbp)
+++
+++	movq	%rbx,%rcx
+++	movq	$160,%r8
+++	cmpq	$160,%rcx
+++	cmovgq	%r8,%rcx
+++	andq	$-16,%rcx
+++	xorq	%r8,%r8
+++.Lopen_sse_tail_192_rounds_and_x1hash:
+++	addq	0+0(%rsi,%r8,1),%r10
+++	adcq	8+0(%rsi,%r8,1),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++.Lopen_sse_tail_192_rounds:
+++	addq	$16,%r8
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,4
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,12
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm5
+++	pxor	%xmm3,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm5
+++	pxor	%xmm3,%xmm5
+++.byte	102,15,58,15,237,4
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,12
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol16(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm6
+++	pxor	%xmm3,%xmm6
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol8(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm6
+++	pxor	%xmm3,%xmm6
+++.byte	102,15,58,15,246,4
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,12
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,12
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,4
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm5
+++	pxor	%xmm3,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm5
+++	pxor	%xmm3,%xmm5
+++.byte	102,15,58,15,237,12
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,4
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol16(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm6
+++	pxor	%xmm3,%xmm6
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol8(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm6
+++	pxor	%xmm3,%xmm6
+++.byte	102,15,58,15,246,12
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,4
+++
+++	cmpq	%rcx,%r8
+++	jb	.Lopen_sse_tail_192_rounds_and_x1hash
+++	cmpq	$160,%r8
+++	jne	.Lopen_sse_tail_192_rounds
+++	cmpq	$176,%rbx
+++	jb	.Lopen_sse_tail_192_finish
+++	addq	0+160(%rsi),%r10
+++	adcq	8+160(%rsi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	cmpq	$192,%rbx
+++	jb	.Lopen_sse_tail_192_finish
+++	addq	0+176(%rsi),%r10
+++	adcq	8+176(%rsi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++.Lopen_sse_tail_192_finish:
+++	paddd	.Lchacha20_consts(%rip),%xmm2
+++	paddd	0+48(%rbp),%xmm6
+++	paddd	0+64(%rbp),%xmm10
+++	paddd	0+128(%rbp),%xmm14
+++	paddd	.Lchacha20_consts(%rip),%xmm1
+++	paddd	0+48(%rbp),%xmm5
+++	paddd	0+64(%rbp),%xmm9
+++	paddd	0+112(%rbp),%xmm13
+++	paddd	.Lchacha20_consts(%rip),%xmm0
+++	paddd	0+48(%rbp),%xmm4
+++	paddd	0+64(%rbp),%xmm8
+++	paddd	0+96(%rbp),%xmm12
+++	movdqu	0 + 0(%rsi),%xmm3
+++	movdqu	16 + 0(%rsi),%xmm7
+++	movdqu	32 + 0(%rsi),%xmm11
+++	movdqu	48 + 0(%rsi),%xmm15
+++	pxor	%xmm3,%xmm2
+++	pxor	%xmm7,%xmm6
+++	pxor	%xmm11,%xmm10
+++	pxor	%xmm14,%xmm15
+++	movdqu	%xmm2,0 + 0(%rdi)
+++	movdqu	%xmm6,16 + 0(%rdi)
+++	movdqu	%xmm10,32 + 0(%rdi)
+++	movdqu	%xmm15,48 + 0(%rdi)
+++	movdqu	0 + 64(%rsi),%xmm3
+++	movdqu	16 + 64(%rsi),%xmm7
+++	movdqu	32 + 64(%rsi),%xmm11
+++	movdqu	48 + 64(%rsi),%xmm15
+++	pxor	%xmm3,%xmm1
+++	pxor	%xmm7,%xmm5
+++	pxor	%xmm11,%xmm9
+++	pxor	%xmm13,%xmm15
+++	movdqu	%xmm1,0 + 64(%rdi)
+++	movdqu	%xmm5,16 + 64(%rdi)
+++	movdqu	%xmm9,32 + 64(%rdi)
+++	movdqu	%xmm15,48 + 64(%rdi)
+++
+++	subq	$128,%rbx
+++	leaq	128(%rsi),%rsi
+++	leaq	128(%rdi),%rdi
+++	jmp	.Lopen_sse_tail_64_dec_loop
+++
+++.Lopen_sse_tail_256:
+++	movdqa	.Lchacha20_consts(%rip),%xmm0
+++	movdqa	0+48(%rbp),%xmm4
+++	movdqa	0+64(%rbp),%xmm8
+++	movdqa	%xmm0,%xmm1
+++	movdqa	%xmm4,%xmm5
+++	movdqa	%xmm8,%xmm9
+++	movdqa	%xmm0,%xmm2
+++	movdqa	%xmm4,%xmm6
+++	movdqa	%xmm8,%xmm10
+++	movdqa	%xmm0,%xmm3
+++	movdqa	%xmm4,%xmm7
+++	movdqa	%xmm8,%xmm11
+++	movdqa	0+96(%rbp),%xmm15
+++	paddd	.Lsse_inc(%rip),%xmm15
+++	movdqa	%xmm15,%xmm14
+++	paddd	.Lsse_inc(%rip),%xmm14
+++	movdqa	%xmm14,%xmm13
+++	paddd	.Lsse_inc(%rip),%xmm13
+++	movdqa	%xmm13,%xmm12
+++	paddd	.Lsse_inc(%rip),%xmm12
+++	movdqa	%xmm12,0+96(%rbp)
+++	movdqa	%xmm13,0+112(%rbp)
+++	movdqa	%xmm14,0+128(%rbp)
+++	movdqa	%xmm15,0+144(%rbp)
+++
+++	xorq	%r8,%r8
+++.Lopen_sse_tail_256_rounds_and_x1hash:
+++	addq	0+0(%rsi,%r8,1),%r10
+++	adcq	8+0(%rsi,%r8,1),%r11
+++	adcq	$1,%r12
+++	movdqa	%xmm11,0+80(%rbp)
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm11
+++	pslld	$12,%xmm11
+++	psrld	$20,%xmm4
+++	pxor	%xmm11,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm11
+++	pslld	$7,%xmm11
+++	psrld	$25,%xmm4
+++	pxor	%xmm11,%xmm4
+++.byte	102,15,58,15,228,4
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,12
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm11
+++	pslld	$12,%xmm11
+++	psrld	$20,%xmm5
+++	pxor	%xmm11,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm11
+++	pslld	$7,%xmm11
+++	psrld	$25,%xmm5
+++	pxor	%xmm11,%xmm5
+++.byte	102,15,58,15,237,4
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,12
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol16(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm11
+++	pslld	$12,%xmm11
+++	psrld	$20,%xmm6
+++	pxor	%xmm11,%xmm6
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol8(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm11
+++	pslld	$7,%xmm11
+++	psrld	$25,%xmm6
+++	pxor	%xmm11,%xmm6
+++.byte	102,15,58,15,246,4
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,12
+++	movdqa	0+80(%rbp),%xmm11
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movdqa	%xmm9,0+80(%rbp)
+++	paddd	%xmm7,%xmm3
+++	pxor	%xmm3,%xmm15
+++	pshufb	.Lrol16(%rip),%xmm15
+++	paddd	%xmm15,%xmm11
+++	pxor	%xmm11,%xmm7
+++	movdqa	%xmm7,%xmm9
+++	pslld	$12,%xmm9
+++	psrld	$20,%xmm7
+++	pxor	%xmm9,%xmm7
+++	paddd	%xmm7,%xmm3
+++	pxor	%xmm3,%xmm15
+++	pshufb	.Lrol8(%rip),%xmm15
+++	paddd	%xmm15,%xmm11
+++	pxor	%xmm11,%xmm7
+++	movdqa	%xmm7,%xmm9
+++	pslld	$7,%xmm9
+++	psrld	$25,%xmm7
+++	pxor	%xmm9,%xmm7
+++.byte	102,15,58,15,255,4
+++.byte	102,69,15,58,15,219,8
+++.byte	102,69,15,58,15,255,12
+++	movdqa	0+80(%rbp),%xmm9
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	movdqa	%xmm11,0+80(%rbp)
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm11
+++	pslld	$12,%xmm11
+++	psrld	$20,%xmm4
+++	pxor	%xmm11,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm11
+++	pslld	$7,%xmm11
+++	psrld	$25,%xmm4
+++	pxor	%xmm11,%xmm4
+++.byte	102,15,58,15,228,12
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,4
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm11
+++	pslld	$12,%xmm11
+++	psrld	$20,%xmm5
+++	pxor	%xmm11,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm11
+++	pslld	$7,%xmm11
+++	psrld	$25,%xmm5
+++	pxor	%xmm11,%xmm5
+++.byte	102,15,58,15,237,12
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,4
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol16(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm11
+++	pslld	$12,%xmm11
+++	psrld	$20,%xmm6
+++	pxor	%xmm11,%xmm6
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol8(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm11
+++	pslld	$7,%xmm11
+++	psrld	$25,%xmm6
+++	pxor	%xmm11,%xmm6
+++.byte	102,15,58,15,246,12
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,4
+++	movdqa	0+80(%rbp),%xmm11
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	movdqa	%xmm9,0+80(%rbp)
+++	paddd	%xmm7,%xmm3
+++	pxor	%xmm3,%xmm15
+++	pshufb	.Lrol16(%rip),%xmm15
+++	paddd	%xmm15,%xmm11
+++	pxor	%xmm11,%xmm7
+++	movdqa	%xmm7,%xmm9
+++	pslld	$12,%xmm9
+++	psrld	$20,%xmm7
+++	pxor	%xmm9,%xmm7
+++	paddd	%xmm7,%xmm3
+++	pxor	%xmm3,%xmm15
+++	pshufb	.Lrol8(%rip),%xmm15
+++	paddd	%xmm15,%xmm11
+++	pxor	%xmm11,%xmm7
+++	movdqa	%xmm7,%xmm9
+++	pslld	$7,%xmm9
+++	psrld	$25,%xmm7
+++	pxor	%xmm9,%xmm7
+++.byte	102,15,58,15,255,12
+++.byte	102,69,15,58,15,219,8
+++.byte	102,69,15,58,15,255,4
+++	movdqa	0+80(%rbp),%xmm9
+++
+++	addq	$16,%r8
+++	cmpq	$160,%r8
+++	jb	.Lopen_sse_tail_256_rounds_and_x1hash
+++
+++	movq	%rbx,%rcx
+++	andq	$-16,%rcx
+++.Lopen_sse_tail_256_hash:
+++	addq	0+0(%rsi,%r8,1),%r10
+++	adcq	8+0(%rsi,%r8,1),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	addq	$16,%r8
+++	cmpq	%rcx,%r8
+++	jb	.Lopen_sse_tail_256_hash
+++	paddd	.Lchacha20_consts(%rip),%xmm3
+++	paddd	0+48(%rbp),%xmm7
+++	paddd	0+64(%rbp),%xmm11
+++	paddd	0+144(%rbp),%xmm15
+++	paddd	.Lchacha20_consts(%rip),%xmm2
+++	paddd	0+48(%rbp),%xmm6
+++	paddd	0+64(%rbp),%xmm10
+++	paddd	0+128(%rbp),%xmm14
+++	paddd	.Lchacha20_consts(%rip),%xmm1
+++	paddd	0+48(%rbp),%xmm5
+++	paddd	0+64(%rbp),%xmm9
+++	paddd	0+112(%rbp),%xmm13
+++	paddd	.Lchacha20_consts(%rip),%xmm0
+++	paddd	0+48(%rbp),%xmm4
+++	paddd	0+64(%rbp),%xmm8
+++	paddd	0+96(%rbp),%xmm12
+++	movdqa	%xmm12,0+80(%rbp)
+++	movdqu	0 + 0(%rsi),%xmm12
+++	pxor	%xmm3,%xmm12
+++	movdqu	%xmm12,0 + 0(%rdi)
+++	movdqu	16 + 0(%rsi),%xmm12
+++	pxor	%xmm7,%xmm12
+++	movdqu	%xmm12,16 + 0(%rdi)
+++	movdqu	32 + 0(%rsi),%xmm12
+++	pxor	%xmm11,%xmm12
+++	movdqu	%xmm12,32 + 0(%rdi)
+++	movdqu	48 + 0(%rsi),%xmm12
+++	pxor	%xmm15,%xmm12
+++	movdqu	%xmm12,48 + 0(%rdi)
+++	movdqu	0 + 64(%rsi),%xmm3
+++	movdqu	16 + 64(%rsi),%xmm7
+++	movdqu	32 + 64(%rsi),%xmm11
+++	movdqu	48 + 64(%rsi),%xmm15
+++	pxor	%xmm3,%xmm2
+++	pxor	%xmm7,%xmm6
+++	pxor	%xmm11,%xmm10
+++	pxor	%xmm14,%xmm15
+++	movdqu	%xmm2,0 + 64(%rdi)
+++	movdqu	%xmm6,16 + 64(%rdi)
+++	movdqu	%xmm10,32 + 64(%rdi)
+++	movdqu	%xmm15,48 + 64(%rdi)
+++	movdqu	0 + 128(%rsi),%xmm3
+++	movdqu	16 + 128(%rsi),%xmm7
+++	movdqu	32 + 128(%rsi),%xmm11
+++	movdqu	48 + 128(%rsi),%xmm15
+++	pxor	%xmm3,%xmm1
+++	pxor	%xmm7,%xmm5
+++	pxor	%xmm11,%xmm9
+++	pxor	%xmm13,%xmm15
+++	movdqu	%xmm1,0 + 128(%rdi)
+++	movdqu	%xmm5,16 + 128(%rdi)
+++	movdqu	%xmm9,32 + 128(%rdi)
+++	movdqu	%xmm15,48 + 128(%rdi)
+++
+++	movdqa	0+80(%rbp),%xmm12
+++	subq	$192,%rbx
+++	leaq	192(%rsi),%rsi
+++	leaq	192(%rdi),%rdi
+++
+++
+++.Lopen_sse_tail_64_dec_loop:
+++	cmpq	$16,%rbx
+++	jb	.Lopen_sse_tail_16_init
+++	subq	$16,%rbx
+++	movdqu	(%rsi),%xmm3
+++	pxor	%xmm3,%xmm0
+++	movdqu	%xmm0,(%rdi)
+++	leaq	16(%rsi),%rsi
+++	leaq	16(%rdi),%rdi
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm8,%xmm4
+++	movdqa	%xmm12,%xmm8
+++	jmp	.Lopen_sse_tail_64_dec_loop
+++.Lopen_sse_tail_16_init:
+++	movdqa	%xmm0,%xmm1
+++
+++
+++.Lopen_sse_tail_16:
+++	testq	%rbx,%rbx
+++	jz	.Lopen_sse_finalize
+++
+++
+++
+++	pxor	%xmm3,%xmm3
+++	leaq	-1(%rsi,%rbx,1),%rsi
+++	movq	%rbx,%r8
+++.Lopen_sse_tail_16_compose:
+++	pslldq	$1,%xmm3
+++	pinsrb	$0,(%rsi),%xmm3
+++	subq	$1,%rsi
+++	subq	$1,%r8
+++	jnz	.Lopen_sse_tail_16_compose
+++
+++.byte	102,73,15,126,221
+++	pextrq	$1,%xmm3,%r14
+++
+++	pxor	%xmm1,%xmm3
+++
+++
+++.Lopen_sse_tail_16_extract:
+++	pextrb	$0,%xmm3,(%rdi)
+++	psrldq	$1,%xmm3
+++	addq	$1,%rdi
+++	subq	$1,%rbx
+++	jne	.Lopen_sse_tail_16_extract
+++
+++	addq	%r13,%r10
+++	adcq	%r14,%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++
+++.Lopen_sse_finalize:
+++	addq	0+0+32(%rbp),%r10
+++	adcq	8+0+32(%rbp),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++
+++	movq	%r10,%r13
+++	movq	%r11,%r14
+++	movq	%r12,%r15
+++	subq	$-5,%r10
+++	sbbq	$-1,%r11
+++	sbbq	$3,%r12
+++	cmovcq	%r13,%r10
+++	cmovcq	%r14,%r11
+++	cmovcq	%r15,%r12
+++
+++	addq	0+0+16(%rbp),%r10
+++	adcq	8+0+16(%rbp),%r11
+++
+++.cfi_remember_state	
+++	addq	$288 + 0 + 32,%rsp
+++.cfi_adjust_cfa_offset	-(288 + 32)
+++
+++	popq	%r9
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r9
+++	movq	%r10,(%r9)
+++	movq	%r11,8(%r9)
+++	popq	%r15
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r15
+++	popq	%r14
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r14
+++	popq	%r13
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r13
+++	popq	%r12
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r12
+++	popq	%rbx
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%rbx
+++	popq	%rbp
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%rbp
+++	.byte	0xf3,0xc3
+++
+++.Lopen_sse_128:
+++.cfi_restore_state	
+++	movdqu	.Lchacha20_consts(%rip),%xmm0
+++	movdqa	%xmm0,%xmm1
+++	movdqa	%xmm0,%xmm2
+++	movdqu	0(%r9),%xmm4
+++	movdqa	%xmm4,%xmm5
+++	movdqa	%xmm4,%xmm6
+++	movdqu	16(%r9),%xmm8
+++	movdqa	%xmm8,%xmm9
+++	movdqa	%xmm8,%xmm10
+++	movdqu	32(%r9),%xmm12
+++	movdqa	%xmm12,%xmm13
+++	paddd	.Lsse_inc(%rip),%xmm13
+++	movdqa	%xmm13,%xmm14
+++	paddd	.Lsse_inc(%rip),%xmm14
+++	movdqa	%xmm4,%xmm7
+++	movdqa	%xmm8,%xmm11
+++	movdqa	%xmm13,%xmm15
+++	movq	$10,%r10
+++
+++.Lopen_sse_128_rounds:
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,4
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,12
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm5
+++	pxor	%xmm3,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm5
+++	pxor	%xmm3,%xmm5
+++.byte	102,15,58,15,237,4
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,12
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol16(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm6
+++	pxor	%xmm3,%xmm6
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol8(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm6
+++	pxor	%xmm3,%xmm6
+++.byte	102,15,58,15,246,4
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,12
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,12
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,4
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm5
+++	pxor	%xmm3,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm5
+++	pxor	%xmm3,%xmm5
+++.byte	102,15,58,15,237,12
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,4
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol16(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm6
+++	pxor	%xmm3,%xmm6
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol8(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm6
+++	pxor	%xmm3,%xmm6
+++.byte	102,15,58,15,246,12
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,4
+++
+++	decq	%r10
+++	jnz	.Lopen_sse_128_rounds
+++	paddd	.Lchacha20_consts(%rip),%xmm0
+++	paddd	.Lchacha20_consts(%rip),%xmm1
+++	paddd	.Lchacha20_consts(%rip),%xmm2
+++	paddd	%xmm7,%xmm4
+++	paddd	%xmm7,%xmm5
+++	paddd	%xmm7,%xmm6
+++	paddd	%xmm11,%xmm9
+++	paddd	%xmm11,%xmm10
+++	paddd	%xmm15,%xmm13
+++	paddd	.Lsse_inc(%rip),%xmm15
+++	paddd	%xmm15,%xmm14
+++
+++	pand	.Lclamp(%rip),%xmm0
+++	movdqa	%xmm0,0+0(%rbp)
+++	movdqa	%xmm4,0+16(%rbp)
+++
+++	movq	%r8,%r8
+++	call	poly_hash_ad_internal
+++.Lopen_sse_128_xor_hash:
+++	cmpq	$16,%rbx
+++	jb	.Lopen_sse_tail_16
+++	subq	$16,%rbx
+++	addq	0+0(%rsi),%r10
+++	adcq	8+0(%rsi),%r11
+++	adcq	$1,%r12
+++
+++
+++	movdqu	0(%rsi),%xmm3
+++	pxor	%xmm3,%xmm1
+++	movdqu	%xmm1,0(%rdi)
+++	leaq	16(%rsi),%rsi
+++	leaq	16(%rdi),%rdi
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++
+++	movdqa	%xmm5,%xmm1
+++	movdqa	%xmm9,%xmm5
+++	movdqa	%xmm13,%xmm9
+++	movdqa	%xmm2,%xmm13
+++	movdqa	%xmm6,%xmm2
+++	movdqa	%xmm10,%xmm6
+++	movdqa	%xmm14,%xmm10
+++	jmp	.Lopen_sse_128_xor_hash
+++.size	chacha20_poly1305_open, .-chacha20_poly1305_open
+++.cfi_endproc	
+++
+++
+++
+++
+++
+++
+++
+++.globl	chacha20_poly1305_seal
+++.hidden chacha20_poly1305_seal
+++.type	chacha20_poly1305_seal,@function
+++.align	64
+++chacha20_poly1305_seal:
+++.cfi_startproc	
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++
+++
+++	pushq	%r9
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r9,-64
+++	subq	$288 + 0 + 32,%rsp
+++.cfi_adjust_cfa_offset	288 + 32
+++	leaq	32(%rsp),%rbp
+++	andq	$-32,%rbp
+++
+++	movq	56(%r9),%rbx
+++	addq	%rdx,%rbx
+++	movq	%r8,0+0+32(%rbp)
+++	movq	%rbx,8+0+32(%rbp)
+++	movq	%rdx,%rbx
+++
+++	movl	OPENSSL_ia32cap_P+8(%rip),%eax
+++	andl	$288,%eax
+++	xorl	$288,%eax
+++	jz	chacha20_poly1305_seal_avx2
+++
+++	cmpq	$128,%rbx
+++	jbe	.Lseal_sse_128
+++
+++	movdqa	.Lchacha20_consts(%rip),%xmm0
+++	movdqu	0(%r9),%xmm4
+++	movdqu	16(%r9),%xmm8
+++	movdqu	32(%r9),%xmm12
+++
+++	movdqa	%xmm0,%xmm1
+++	movdqa	%xmm0,%xmm2
+++	movdqa	%xmm0,%xmm3
+++	movdqa	%xmm4,%xmm5
+++	movdqa	%xmm4,%xmm6
+++	movdqa	%xmm4,%xmm7
+++	movdqa	%xmm8,%xmm9
+++	movdqa	%xmm8,%xmm10
+++	movdqa	%xmm8,%xmm11
+++	movdqa	%xmm12,%xmm15
+++	paddd	.Lsse_inc(%rip),%xmm12
+++	movdqa	%xmm12,%xmm14
+++	paddd	.Lsse_inc(%rip),%xmm12
+++	movdqa	%xmm12,%xmm13
+++	paddd	.Lsse_inc(%rip),%xmm12
+++
+++	movdqa	%xmm4,0+48(%rbp)
+++	movdqa	%xmm8,0+64(%rbp)
+++	movdqa	%xmm12,0+96(%rbp)
+++	movdqa	%xmm13,0+112(%rbp)
+++	movdqa	%xmm14,0+128(%rbp)
+++	movdqa	%xmm15,0+144(%rbp)
+++	movq	$10,%r10
+++.Lseal_sse_init_rounds:
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	.Lrol16(%rip),%xmm8
+++	paddd	%xmm7,%xmm3
+++	paddd	%xmm6,%xmm2
+++	paddd	%xmm5,%xmm1
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm15
+++	pxor	%xmm2,%xmm14
+++	pxor	%xmm1,%xmm13
+++	pxor	%xmm0,%xmm12
+++.byte	102,69,15,56,0,248
+++.byte	102,69,15,56,0,240
+++.byte	102,69,15,56,0,232
+++.byte	102,69,15,56,0,224
+++	movdqa	0+80(%rbp),%xmm8
+++	paddd	%xmm15,%xmm11
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm13,%xmm9
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm11,%xmm7
+++	pxor	%xmm10,%xmm6
+++	pxor	%xmm9,%xmm5
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	%xmm7,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm7
+++	pxor	%xmm8,%xmm7
+++	movdqa	%xmm6,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm6
+++	pxor	%xmm8,%xmm6
+++	movdqa	%xmm5,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm5
+++	pxor	%xmm8,%xmm5
+++	movdqa	%xmm4,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm4
+++	pxor	%xmm8,%xmm4
+++	movdqa	.Lrol8(%rip),%xmm8
+++	paddd	%xmm7,%xmm3
+++	paddd	%xmm6,%xmm2
+++	paddd	%xmm5,%xmm1
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm15
+++	pxor	%xmm2,%xmm14
+++	pxor	%xmm1,%xmm13
+++	pxor	%xmm0,%xmm12
+++.byte	102,69,15,56,0,248
+++.byte	102,69,15,56,0,240
+++.byte	102,69,15,56,0,232
+++.byte	102,69,15,56,0,224
+++	movdqa	0+80(%rbp),%xmm8
+++	paddd	%xmm15,%xmm11
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm13,%xmm9
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm11,%xmm7
+++	pxor	%xmm10,%xmm6
+++	pxor	%xmm9,%xmm5
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	%xmm7,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm7
+++	pxor	%xmm8,%xmm7
+++	movdqa	%xmm6,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm6
+++	pxor	%xmm8,%xmm6
+++	movdqa	%xmm5,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm5
+++	pxor	%xmm8,%xmm5
+++	movdqa	%xmm4,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm4
+++	pxor	%xmm8,%xmm4
+++	movdqa	0+80(%rbp),%xmm8
+++.byte	102,15,58,15,255,4
+++.byte	102,69,15,58,15,219,8
+++.byte	102,69,15,58,15,255,12
+++.byte	102,15,58,15,246,4
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,12
+++.byte	102,15,58,15,237,4
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,12
+++.byte	102,15,58,15,228,4
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,12
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	.Lrol16(%rip),%xmm8
+++	paddd	%xmm7,%xmm3
+++	paddd	%xmm6,%xmm2
+++	paddd	%xmm5,%xmm1
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm15
+++	pxor	%xmm2,%xmm14
+++	pxor	%xmm1,%xmm13
+++	pxor	%xmm0,%xmm12
+++.byte	102,69,15,56,0,248
+++.byte	102,69,15,56,0,240
+++.byte	102,69,15,56,0,232
+++.byte	102,69,15,56,0,224
+++	movdqa	0+80(%rbp),%xmm8
+++	paddd	%xmm15,%xmm11
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm13,%xmm9
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm11,%xmm7
+++	pxor	%xmm10,%xmm6
+++	pxor	%xmm9,%xmm5
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	%xmm7,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm7
+++	pxor	%xmm8,%xmm7
+++	movdqa	%xmm6,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm6
+++	pxor	%xmm8,%xmm6
+++	movdqa	%xmm5,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm5
+++	pxor	%xmm8,%xmm5
+++	movdqa	%xmm4,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm4
+++	pxor	%xmm8,%xmm4
+++	movdqa	.Lrol8(%rip),%xmm8
+++	paddd	%xmm7,%xmm3
+++	paddd	%xmm6,%xmm2
+++	paddd	%xmm5,%xmm1
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm15
+++	pxor	%xmm2,%xmm14
+++	pxor	%xmm1,%xmm13
+++	pxor	%xmm0,%xmm12
+++.byte	102,69,15,56,0,248
+++.byte	102,69,15,56,0,240
+++.byte	102,69,15,56,0,232
+++.byte	102,69,15,56,0,224
+++	movdqa	0+80(%rbp),%xmm8
+++	paddd	%xmm15,%xmm11
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm13,%xmm9
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm11,%xmm7
+++	pxor	%xmm10,%xmm6
+++	pxor	%xmm9,%xmm5
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	%xmm7,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm7
+++	pxor	%xmm8,%xmm7
+++	movdqa	%xmm6,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm6
+++	pxor	%xmm8,%xmm6
+++	movdqa	%xmm5,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm5
+++	pxor	%xmm8,%xmm5
+++	movdqa	%xmm4,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm4
+++	pxor	%xmm8,%xmm4
+++	movdqa	0+80(%rbp),%xmm8
+++.byte	102,15,58,15,255,12
+++.byte	102,69,15,58,15,219,8
+++.byte	102,69,15,58,15,255,4
+++.byte	102,15,58,15,246,12
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,4
+++.byte	102,15,58,15,237,12
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,4
+++.byte	102,15,58,15,228,12
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,4
+++
+++	decq	%r10
+++	jnz	.Lseal_sse_init_rounds
+++	paddd	.Lchacha20_consts(%rip),%xmm3
+++	paddd	0+48(%rbp),%xmm7
+++	paddd	0+64(%rbp),%xmm11
+++	paddd	0+144(%rbp),%xmm15
+++	paddd	.Lchacha20_consts(%rip),%xmm2
+++	paddd	0+48(%rbp),%xmm6
+++	paddd	0+64(%rbp),%xmm10
+++	paddd	0+128(%rbp),%xmm14
+++	paddd	.Lchacha20_consts(%rip),%xmm1
+++	paddd	0+48(%rbp),%xmm5
+++	paddd	0+64(%rbp),%xmm9
+++	paddd	0+112(%rbp),%xmm13
+++	paddd	.Lchacha20_consts(%rip),%xmm0
+++	paddd	0+48(%rbp),%xmm4
+++	paddd	0+64(%rbp),%xmm8
+++	paddd	0+96(%rbp),%xmm12
+++
+++
+++	pand	.Lclamp(%rip),%xmm3
+++	movdqa	%xmm3,0+0(%rbp)
+++	movdqa	%xmm7,0+16(%rbp)
+++
+++	movq	%r8,%r8
+++	call	poly_hash_ad_internal
+++	movdqu	0 + 0(%rsi),%xmm3
+++	movdqu	16 + 0(%rsi),%xmm7
+++	movdqu	32 + 0(%rsi),%xmm11
+++	movdqu	48 + 0(%rsi),%xmm15
+++	pxor	%xmm3,%xmm2
+++	pxor	%xmm7,%xmm6
+++	pxor	%xmm11,%xmm10
+++	pxor	%xmm14,%xmm15
+++	movdqu	%xmm2,0 + 0(%rdi)
+++	movdqu	%xmm6,16 + 0(%rdi)
+++	movdqu	%xmm10,32 + 0(%rdi)
+++	movdqu	%xmm15,48 + 0(%rdi)
+++	movdqu	0 + 64(%rsi),%xmm3
+++	movdqu	16 + 64(%rsi),%xmm7
+++	movdqu	32 + 64(%rsi),%xmm11
+++	movdqu	48 + 64(%rsi),%xmm15
+++	pxor	%xmm3,%xmm1
+++	pxor	%xmm7,%xmm5
+++	pxor	%xmm11,%xmm9
+++	pxor	%xmm13,%xmm15
+++	movdqu	%xmm1,0 + 64(%rdi)
+++	movdqu	%xmm5,16 + 64(%rdi)
+++	movdqu	%xmm9,32 + 64(%rdi)
+++	movdqu	%xmm15,48 + 64(%rdi)
+++
+++	cmpq	$192,%rbx
+++	ja	.Lseal_sse_main_init
+++	movq	$128,%rcx
+++	subq	$128,%rbx
+++	leaq	128(%rsi),%rsi
+++	jmp	.Lseal_sse_128_tail_hash
+++.Lseal_sse_main_init:
+++	movdqu	0 + 128(%rsi),%xmm3
+++	movdqu	16 + 128(%rsi),%xmm7
+++	movdqu	32 + 128(%rsi),%xmm11
+++	movdqu	48 + 128(%rsi),%xmm15
+++	pxor	%xmm3,%xmm0
+++	pxor	%xmm7,%xmm4
+++	pxor	%xmm11,%xmm8
+++	pxor	%xmm12,%xmm15
+++	movdqu	%xmm0,0 + 128(%rdi)
+++	movdqu	%xmm4,16 + 128(%rdi)
+++	movdqu	%xmm8,32 + 128(%rdi)
+++	movdqu	%xmm15,48 + 128(%rdi)
+++
+++	movq	$192,%rcx
+++	subq	$192,%rbx
+++	leaq	192(%rsi),%rsi
+++	movq	$2,%rcx
+++	movq	$8,%r8
+++	cmpq	$64,%rbx
+++	jbe	.Lseal_sse_tail_64
+++	cmpq	$128,%rbx
+++	jbe	.Lseal_sse_tail_128
+++	cmpq	$192,%rbx
+++	jbe	.Lseal_sse_tail_192
+++
+++.Lseal_sse_main_loop:
+++	movdqa	.Lchacha20_consts(%rip),%xmm0
+++	movdqa	0+48(%rbp),%xmm4
+++	movdqa	0+64(%rbp),%xmm8
+++	movdqa	%xmm0,%xmm1
+++	movdqa	%xmm4,%xmm5
+++	movdqa	%xmm8,%xmm9
+++	movdqa	%xmm0,%xmm2
+++	movdqa	%xmm4,%xmm6
+++	movdqa	%xmm8,%xmm10
+++	movdqa	%xmm0,%xmm3
+++	movdqa	%xmm4,%xmm7
+++	movdqa	%xmm8,%xmm11
+++	movdqa	0+96(%rbp),%xmm15
+++	paddd	.Lsse_inc(%rip),%xmm15
+++	movdqa	%xmm15,%xmm14
+++	paddd	.Lsse_inc(%rip),%xmm14
+++	movdqa	%xmm14,%xmm13
+++	paddd	.Lsse_inc(%rip),%xmm13
+++	movdqa	%xmm13,%xmm12
+++	paddd	.Lsse_inc(%rip),%xmm12
+++	movdqa	%xmm12,0+96(%rbp)
+++	movdqa	%xmm13,0+112(%rbp)
+++	movdqa	%xmm14,0+128(%rbp)
+++	movdqa	%xmm15,0+144(%rbp)
+++
+++.align	32
+++.Lseal_sse_main_rounds:
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	.Lrol16(%rip),%xmm8
+++	paddd	%xmm7,%xmm3
+++	paddd	%xmm6,%xmm2
+++	paddd	%xmm5,%xmm1
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm15
+++	pxor	%xmm2,%xmm14
+++	pxor	%xmm1,%xmm13
+++	pxor	%xmm0,%xmm12
+++.byte	102,69,15,56,0,248
+++.byte	102,69,15,56,0,240
+++.byte	102,69,15,56,0,232
+++.byte	102,69,15,56,0,224
+++	movdqa	0+80(%rbp),%xmm8
+++	paddd	%xmm15,%xmm11
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm13,%xmm9
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm11,%xmm7
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	pxor	%xmm10,%xmm6
+++	pxor	%xmm9,%xmm5
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	%xmm7,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm7
+++	pxor	%xmm8,%xmm7
+++	movdqa	%xmm6,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm6
+++	pxor	%xmm8,%xmm6
+++	movdqa	%xmm5,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm5
+++	pxor	%xmm8,%xmm5
+++	movdqa	%xmm4,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm4
+++	pxor	%xmm8,%xmm4
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movdqa	.Lrol8(%rip),%xmm8
+++	paddd	%xmm7,%xmm3
+++	paddd	%xmm6,%xmm2
+++	paddd	%xmm5,%xmm1
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm15
+++	pxor	%xmm2,%xmm14
+++	pxor	%xmm1,%xmm13
+++	pxor	%xmm0,%xmm12
+++.byte	102,69,15,56,0,248
+++.byte	102,69,15,56,0,240
+++.byte	102,69,15,56,0,232
+++.byte	102,69,15,56,0,224
+++	movdqa	0+80(%rbp),%xmm8
+++	paddd	%xmm15,%xmm11
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm13,%xmm9
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm11,%xmm7
+++	pxor	%xmm10,%xmm6
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	pxor	%xmm9,%xmm5
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	%xmm7,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm7
+++	pxor	%xmm8,%xmm7
+++	movdqa	%xmm6,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm6
+++	pxor	%xmm8,%xmm6
+++	movdqa	%xmm5,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm5
+++	pxor	%xmm8,%xmm5
+++	movdqa	%xmm4,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm4
+++	pxor	%xmm8,%xmm4
+++	movdqa	0+80(%rbp),%xmm8
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++.byte	102,15,58,15,255,4
+++.byte	102,69,15,58,15,219,8
+++.byte	102,69,15,58,15,255,12
+++.byte	102,15,58,15,246,4
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,12
+++.byte	102,15,58,15,237,4
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,12
+++.byte	102,15,58,15,228,4
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,12
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	.Lrol16(%rip),%xmm8
+++	paddd	%xmm7,%xmm3
+++	paddd	%xmm6,%xmm2
+++	paddd	%xmm5,%xmm1
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm15
+++	pxor	%xmm2,%xmm14
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	pxor	%xmm1,%xmm13
+++	pxor	%xmm0,%xmm12
+++.byte	102,69,15,56,0,248
+++.byte	102,69,15,56,0,240
+++.byte	102,69,15,56,0,232
+++.byte	102,69,15,56,0,224
+++	movdqa	0+80(%rbp),%xmm8
+++	paddd	%xmm15,%xmm11
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm13,%xmm9
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm11,%xmm7
+++	pxor	%xmm10,%xmm6
+++	pxor	%xmm9,%xmm5
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	%xmm7,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm7
+++	pxor	%xmm8,%xmm7
+++	movdqa	%xmm6,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm6
+++	pxor	%xmm8,%xmm6
+++	movdqa	%xmm5,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm5
+++	pxor	%xmm8,%xmm5
+++	movdqa	%xmm4,%xmm8
+++	psrld	$20,%xmm8
+++	pslld	$32-20,%xmm4
+++	pxor	%xmm8,%xmm4
+++	movdqa	.Lrol8(%rip),%xmm8
+++	paddd	%xmm7,%xmm3
+++	paddd	%xmm6,%xmm2
+++	paddd	%xmm5,%xmm1
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm15
+++	pxor	%xmm2,%xmm14
+++	pxor	%xmm1,%xmm13
+++	pxor	%xmm0,%xmm12
+++.byte	102,69,15,56,0,248
+++.byte	102,69,15,56,0,240
+++.byte	102,69,15,56,0,232
+++.byte	102,69,15,56,0,224
+++	movdqa	0+80(%rbp),%xmm8
+++	paddd	%xmm15,%xmm11
+++	paddd	%xmm14,%xmm10
+++	paddd	%xmm13,%xmm9
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm11,%xmm7
+++	pxor	%xmm10,%xmm6
+++	pxor	%xmm9,%xmm5
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm8,0+80(%rbp)
+++	movdqa	%xmm7,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm7
+++	pxor	%xmm8,%xmm7
+++	movdqa	%xmm6,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm6
+++	pxor	%xmm8,%xmm6
+++	movdqa	%xmm5,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm5
+++	pxor	%xmm8,%xmm5
+++	movdqa	%xmm4,%xmm8
+++	psrld	$25,%xmm8
+++	pslld	$32-25,%xmm4
+++	pxor	%xmm8,%xmm4
+++	movdqa	0+80(%rbp),%xmm8
+++.byte	102,15,58,15,255,12
+++.byte	102,69,15,58,15,219,8
+++.byte	102,69,15,58,15,255,4
+++.byte	102,15,58,15,246,12
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,4
+++.byte	102,15,58,15,237,12
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,4
+++.byte	102,15,58,15,228,12
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,4
+++
+++	leaq	16(%rdi),%rdi
+++	decq	%r8
+++	jge	.Lseal_sse_main_rounds
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rdi),%rdi
+++	decq	%rcx
+++	jg	.Lseal_sse_main_rounds
+++	paddd	.Lchacha20_consts(%rip),%xmm3
+++	paddd	0+48(%rbp),%xmm7
+++	paddd	0+64(%rbp),%xmm11
+++	paddd	0+144(%rbp),%xmm15
+++	paddd	.Lchacha20_consts(%rip),%xmm2
+++	paddd	0+48(%rbp),%xmm6
+++	paddd	0+64(%rbp),%xmm10
+++	paddd	0+128(%rbp),%xmm14
+++	paddd	.Lchacha20_consts(%rip),%xmm1
+++	paddd	0+48(%rbp),%xmm5
+++	paddd	0+64(%rbp),%xmm9
+++	paddd	0+112(%rbp),%xmm13
+++	paddd	.Lchacha20_consts(%rip),%xmm0
+++	paddd	0+48(%rbp),%xmm4
+++	paddd	0+64(%rbp),%xmm8
+++	paddd	0+96(%rbp),%xmm12
+++
+++	movdqa	%xmm14,0+80(%rbp)
+++	movdqa	%xmm14,0+80(%rbp)
+++	movdqu	0 + 0(%rsi),%xmm14
+++	pxor	%xmm3,%xmm14
+++	movdqu	%xmm14,0 + 0(%rdi)
+++	movdqu	16 + 0(%rsi),%xmm14
+++	pxor	%xmm7,%xmm14
+++	movdqu	%xmm14,16 + 0(%rdi)
+++	movdqu	32 + 0(%rsi),%xmm14
+++	pxor	%xmm11,%xmm14
+++	movdqu	%xmm14,32 + 0(%rdi)
+++	movdqu	48 + 0(%rsi),%xmm14
+++	pxor	%xmm15,%xmm14
+++	movdqu	%xmm14,48 + 0(%rdi)
+++
+++	movdqa	0+80(%rbp),%xmm14
+++	movdqu	0 + 64(%rsi),%xmm3
+++	movdqu	16 + 64(%rsi),%xmm7
+++	movdqu	32 + 64(%rsi),%xmm11
+++	movdqu	48 + 64(%rsi),%xmm15
+++	pxor	%xmm3,%xmm2
+++	pxor	%xmm7,%xmm6
+++	pxor	%xmm11,%xmm10
+++	pxor	%xmm14,%xmm15
+++	movdqu	%xmm2,0 + 64(%rdi)
+++	movdqu	%xmm6,16 + 64(%rdi)
+++	movdqu	%xmm10,32 + 64(%rdi)
+++	movdqu	%xmm15,48 + 64(%rdi)
+++	movdqu	0 + 128(%rsi),%xmm3
+++	movdqu	16 + 128(%rsi),%xmm7
+++	movdqu	32 + 128(%rsi),%xmm11
+++	movdqu	48 + 128(%rsi),%xmm15
+++	pxor	%xmm3,%xmm1
+++	pxor	%xmm7,%xmm5
+++	pxor	%xmm11,%xmm9
+++	pxor	%xmm13,%xmm15
+++	movdqu	%xmm1,0 + 128(%rdi)
+++	movdqu	%xmm5,16 + 128(%rdi)
+++	movdqu	%xmm9,32 + 128(%rdi)
+++	movdqu	%xmm15,48 + 128(%rdi)
+++
+++	cmpq	$256,%rbx
+++	ja	.Lseal_sse_main_loop_xor
+++
+++	movq	$192,%rcx
+++	subq	$192,%rbx
+++	leaq	192(%rsi),%rsi
+++	jmp	.Lseal_sse_128_tail_hash
+++.Lseal_sse_main_loop_xor:
+++	movdqu	0 + 192(%rsi),%xmm3
+++	movdqu	16 + 192(%rsi),%xmm7
+++	movdqu	32 + 192(%rsi),%xmm11
+++	movdqu	48 + 192(%rsi),%xmm15
+++	pxor	%xmm3,%xmm0
+++	pxor	%xmm7,%xmm4
+++	pxor	%xmm11,%xmm8
+++	pxor	%xmm12,%xmm15
+++	movdqu	%xmm0,0 + 192(%rdi)
+++	movdqu	%xmm4,16 + 192(%rdi)
+++	movdqu	%xmm8,32 + 192(%rdi)
+++	movdqu	%xmm15,48 + 192(%rdi)
+++
+++	leaq	256(%rsi),%rsi
+++	subq	$256,%rbx
+++	movq	$6,%rcx
+++	movq	$4,%r8
+++	cmpq	$192,%rbx
+++	jg	.Lseal_sse_main_loop
+++	movq	%rbx,%rcx
+++	testq	%rbx,%rbx
+++	je	.Lseal_sse_128_tail_hash
+++	movq	$6,%rcx
+++	cmpq	$128,%rbx
+++	ja	.Lseal_sse_tail_192
+++	cmpq	$64,%rbx
+++	ja	.Lseal_sse_tail_128
+++
+++.Lseal_sse_tail_64:
+++	movdqa	.Lchacha20_consts(%rip),%xmm0
+++	movdqa	0+48(%rbp),%xmm4
+++	movdqa	0+64(%rbp),%xmm8
+++	movdqa	0+96(%rbp),%xmm12
+++	paddd	.Lsse_inc(%rip),%xmm12
+++	movdqa	%xmm12,0+96(%rbp)
+++
+++.Lseal_sse_tail_64_rounds_and_x2hash:
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rdi),%rdi
+++.Lseal_sse_tail_64_rounds_and_x1hash:
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,4
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,12
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,12
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,4
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rdi),%rdi
+++	decq	%rcx
+++	jg	.Lseal_sse_tail_64_rounds_and_x2hash
+++	decq	%r8
+++	jge	.Lseal_sse_tail_64_rounds_and_x1hash
+++	paddd	.Lchacha20_consts(%rip),%xmm0
+++	paddd	0+48(%rbp),%xmm4
+++	paddd	0+64(%rbp),%xmm8
+++	paddd	0+96(%rbp),%xmm12
+++
+++	jmp	.Lseal_sse_128_tail_xor
+++
+++.Lseal_sse_tail_128:
+++	movdqa	.Lchacha20_consts(%rip),%xmm0
+++	movdqa	0+48(%rbp),%xmm4
+++	movdqa	0+64(%rbp),%xmm8
+++	movdqa	%xmm0,%xmm1
+++	movdqa	%xmm4,%xmm5
+++	movdqa	%xmm8,%xmm9
+++	movdqa	0+96(%rbp),%xmm13
+++	paddd	.Lsse_inc(%rip),%xmm13
+++	movdqa	%xmm13,%xmm12
+++	paddd	.Lsse_inc(%rip),%xmm12
+++	movdqa	%xmm12,0+96(%rbp)
+++	movdqa	%xmm13,0+112(%rbp)
+++
+++.Lseal_sse_tail_128_rounds_and_x2hash:
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rdi),%rdi
+++.Lseal_sse_tail_128_rounds_and_x1hash:
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,4
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,12
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm5
+++	pxor	%xmm3,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm5
+++	pxor	%xmm3,%xmm5
+++.byte	102,15,58,15,237,4
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,12
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,12
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,4
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm5
+++	pxor	%xmm3,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm5
+++	pxor	%xmm3,%xmm5
+++.byte	102,15,58,15,237,12
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,4
+++
+++	leaq	16(%rdi),%rdi
+++	decq	%rcx
+++	jg	.Lseal_sse_tail_128_rounds_and_x2hash
+++	decq	%r8
+++	jge	.Lseal_sse_tail_128_rounds_and_x1hash
+++	paddd	.Lchacha20_consts(%rip),%xmm1
+++	paddd	0+48(%rbp),%xmm5
+++	paddd	0+64(%rbp),%xmm9
+++	paddd	0+112(%rbp),%xmm13
+++	paddd	.Lchacha20_consts(%rip),%xmm0
+++	paddd	0+48(%rbp),%xmm4
+++	paddd	0+64(%rbp),%xmm8
+++	paddd	0+96(%rbp),%xmm12
+++	movdqu	0 + 0(%rsi),%xmm3
+++	movdqu	16 + 0(%rsi),%xmm7
+++	movdqu	32 + 0(%rsi),%xmm11
+++	movdqu	48 + 0(%rsi),%xmm15
+++	pxor	%xmm3,%xmm1
+++	pxor	%xmm7,%xmm5
+++	pxor	%xmm11,%xmm9
+++	pxor	%xmm13,%xmm15
+++	movdqu	%xmm1,0 + 0(%rdi)
+++	movdqu	%xmm5,16 + 0(%rdi)
+++	movdqu	%xmm9,32 + 0(%rdi)
+++	movdqu	%xmm15,48 + 0(%rdi)
+++
+++	movq	$64,%rcx
+++	subq	$64,%rbx
+++	leaq	64(%rsi),%rsi
+++	jmp	.Lseal_sse_128_tail_hash
+++
+++.Lseal_sse_tail_192:
+++	movdqa	.Lchacha20_consts(%rip),%xmm0
+++	movdqa	0+48(%rbp),%xmm4
+++	movdqa	0+64(%rbp),%xmm8
+++	movdqa	%xmm0,%xmm1
+++	movdqa	%xmm4,%xmm5
+++	movdqa	%xmm8,%xmm9
+++	movdqa	%xmm0,%xmm2
+++	movdqa	%xmm4,%xmm6
+++	movdqa	%xmm8,%xmm10
+++	movdqa	0+96(%rbp),%xmm14
+++	paddd	.Lsse_inc(%rip),%xmm14
+++	movdqa	%xmm14,%xmm13
+++	paddd	.Lsse_inc(%rip),%xmm13
+++	movdqa	%xmm13,%xmm12
+++	paddd	.Lsse_inc(%rip),%xmm12
+++	movdqa	%xmm12,0+96(%rbp)
+++	movdqa	%xmm13,0+112(%rbp)
+++	movdqa	%xmm14,0+128(%rbp)
+++
+++.Lseal_sse_tail_192_rounds_and_x2hash:
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rdi),%rdi
+++.Lseal_sse_tail_192_rounds_and_x1hash:
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,4
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,12
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm5
+++	pxor	%xmm3,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm5
+++	pxor	%xmm3,%xmm5
+++.byte	102,15,58,15,237,4
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,12
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol16(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm6
+++	pxor	%xmm3,%xmm6
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol8(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm6
+++	pxor	%xmm3,%xmm6
+++.byte	102,15,58,15,246,4
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,12
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,12
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,4
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm5
+++	pxor	%xmm3,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm5
+++	pxor	%xmm3,%xmm5
+++.byte	102,15,58,15,237,12
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,4
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol16(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm6
+++	pxor	%xmm3,%xmm6
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol8(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm6
+++	pxor	%xmm3,%xmm6
+++.byte	102,15,58,15,246,12
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,4
+++
+++	leaq	16(%rdi),%rdi
+++	decq	%rcx
+++	jg	.Lseal_sse_tail_192_rounds_and_x2hash
+++	decq	%r8
+++	jge	.Lseal_sse_tail_192_rounds_and_x1hash
+++	paddd	.Lchacha20_consts(%rip),%xmm2
+++	paddd	0+48(%rbp),%xmm6
+++	paddd	0+64(%rbp),%xmm10
+++	paddd	0+128(%rbp),%xmm14
+++	paddd	.Lchacha20_consts(%rip),%xmm1
+++	paddd	0+48(%rbp),%xmm5
+++	paddd	0+64(%rbp),%xmm9
+++	paddd	0+112(%rbp),%xmm13
+++	paddd	.Lchacha20_consts(%rip),%xmm0
+++	paddd	0+48(%rbp),%xmm4
+++	paddd	0+64(%rbp),%xmm8
+++	paddd	0+96(%rbp),%xmm12
+++	movdqu	0 + 0(%rsi),%xmm3
+++	movdqu	16 + 0(%rsi),%xmm7
+++	movdqu	32 + 0(%rsi),%xmm11
+++	movdqu	48 + 0(%rsi),%xmm15
+++	pxor	%xmm3,%xmm2
+++	pxor	%xmm7,%xmm6
+++	pxor	%xmm11,%xmm10
+++	pxor	%xmm14,%xmm15
+++	movdqu	%xmm2,0 + 0(%rdi)
+++	movdqu	%xmm6,16 + 0(%rdi)
+++	movdqu	%xmm10,32 + 0(%rdi)
+++	movdqu	%xmm15,48 + 0(%rdi)
+++	movdqu	0 + 64(%rsi),%xmm3
+++	movdqu	16 + 64(%rsi),%xmm7
+++	movdqu	32 + 64(%rsi),%xmm11
+++	movdqu	48 + 64(%rsi),%xmm15
+++	pxor	%xmm3,%xmm1
+++	pxor	%xmm7,%xmm5
+++	pxor	%xmm11,%xmm9
+++	pxor	%xmm13,%xmm15
+++	movdqu	%xmm1,0 + 64(%rdi)
+++	movdqu	%xmm5,16 + 64(%rdi)
+++	movdqu	%xmm9,32 + 64(%rdi)
+++	movdqu	%xmm15,48 + 64(%rdi)
+++
+++	movq	$128,%rcx
+++	subq	$128,%rbx
+++	leaq	128(%rsi),%rsi
+++
+++.Lseal_sse_128_tail_hash:
+++	cmpq	$16,%rcx
+++	jb	.Lseal_sse_128_tail_xor
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	subq	$16,%rcx
+++	leaq	16(%rdi),%rdi
+++	jmp	.Lseal_sse_128_tail_hash
+++
+++.Lseal_sse_128_tail_xor:
+++	cmpq	$16,%rbx
+++	jb	.Lseal_sse_tail_16
+++	subq	$16,%rbx
+++
+++	movdqu	0(%rsi),%xmm3
+++	pxor	%xmm3,%xmm0
+++	movdqu	%xmm0,0(%rdi)
+++
+++	addq	0(%rdi),%r10
+++	adcq	8(%rdi),%r11
+++	adcq	$1,%r12
+++	leaq	16(%rsi),%rsi
+++	leaq	16(%rdi),%rdi
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm8,%xmm4
+++	movdqa	%xmm12,%xmm8
+++	movdqa	%xmm1,%xmm12
+++	movdqa	%xmm5,%xmm1
+++	movdqa	%xmm9,%xmm5
+++	movdqa	%xmm13,%xmm9
+++	jmp	.Lseal_sse_128_tail_xor
+++
+++.Lseal_sse_tail_16:
+++	testq	%rbx,%rbx
+++	jz	.Lprocess_blocks_of_extra_in
+++
+++	movq	%rbx,%r8
+++	movq	%rbx,%rcx
+++	leaq	-1(%rsi,%rbx,1),%rsi
+++	pxor	%xmm15,%xmm15
+++.Lseal_sse_tail_16_compose:
+++	pslldq	$1,%xmm15
+++	pinsrb	$0,(%rsi),%xmm15
+++	leaq	-1(%rsi),%rsi
+++	decq	%rcx
+++	jne	.Lseal_sse_tail_16_compose
+++
+++
+++	pxor	%xmm0,%xmm15
+++
+++
+++	movq	%rbx,%rcx
+++	movdqu	%xmm15,%xmm0
+++.Lseal_sse_tail_16_extract:
+++	pextrb	$0,%xmm0,(%rdi)
+++	psrldq	$1,%xmm0
+++	addq	$1,%rdi
+++	subq	$1,%rcx
+++	jnz	.Lseal_sse_tail_16_extract
+++
+++
+++
+++
+++
+++
+++
+++
+++	movq	288 + 0 + 32(%rsp),%r9
+++	movq	56(%r9),%r14
+++	movq	48(%r9),%r13
+++	testq	%r14,%r14
+++	jz	.Lprocess_partial_block
+++
+++	movq	$16,%r15
+++	subq	%rbx,%r15
+++	cmpq	%r15,%r14
+++
+++	jge	.Lload_extra_in
+++	movq	%r14,%r15
+++
+++.Lload_extra_in:
+++
+++
+++	leaq	-1(%r13,%r15,1),%rsi
+++
+++
+++	addq	%r15,%r13
+++	subq	%r15,%r14
+++	movq	%r13,48(%r9)
+++	movq	%r14,56(%r9)
+++
+++
+++
+++	addq	%r15,%r8
+++
+++
+++	pxor	%xmm11,%xmm11
+++.Lload_extra_load_loop:
+++	pslldq	$1,%xmm11
+++	pinsrb	$0,(%rsi),%xmm11
+++	leaq	-1(%rsi),%rsi
+++	subq	$1,%r15
+++	jnz	.Lload_extra_load_loop
+++
+++
+++
+++
+++	movq	%rbx,%r15
+++
+++.Lload_extra_shift_loop:
+++	pslldq	$1,%xmm11
+++	subq	$1,%r15
+++	jnz	.Lload_extra_shift_loop
+++
+++
+++
+++
+++	leaq	.Land_masks(%rip),%r15
+++	shlq	$4,%rbx
+++	pand	-16(%r15,%rbx,1),%xmm15
+++
+++
+++	por	%xmm11,%xmm15
+++
+++
+++
+++.byte	102,77,15,126,253
+++	pextrq	$1,%xmm15,%r14
+++	addq	%r13,%r10
+++	adcq	%r14,%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++
+++.Lprocess_blocks_of_extra_in:
+++
+++	movq	288+32+0 (%rsp),%r9
+++	movq	48(%r9),%rsi
+++	movq	56(%r9),%r8
+++	movq	%r8,%rcx
+++	shrq	$4,%r8
+++
+++.Lprocess_extra_hash_loop:
+++	jz	process_extra_in_trailer
+++	addq	0+0(%rsi),%r10
+++	adcq	8+0(%rsi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rsi),%rsi
+++	subq	$1,%r8
+++	jmp	.Lprocess_extra_hash_loop
+++process_extra_in_trailer:
+++	andq	$15,%rcx
+++	movq	%rcx,%rbx
+++	jz	.Ldo_length_block
+++	leaq	-1(%rsi,%rcx,1),%rsi
+++
+++.Lprocess_extra_in_trailer_load:
+++	pslldq	$1,%xmm15
+++	pinsrb	$0,(%rsi),%xmm15
+++	leaq	-1(%rsi),%rsi
+++	subq	$1,%rcx
+++	jnz	.Lprocess_extra_in_trailer_load
+++
+++.Lprocess_partial_block:
+++
+++	leaq	.Land_masks(%rip),%r15
+++	shlq	$4,%rbx
+++	pand	-16(%r15,%rbx,1),%xmm15
+++.byte	102,77,15,126,253
+++	pextrq	$1,%xmm15,%r14
+++	addq	%r13,%r10
+++	adcq	%r14,%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++
+++.Ldo_length_block:
+++	addq	0+0+32(%rbp),%r10
+++	adcq	8+0+32(%rbp),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++
+++	movq	%r10,%r13
+++	movq	%r11,%r14
+++	movq	%r12,%r15
+++	subq	$-5,%r10
+++	sbbq	$-1,%r11
+++	sbbq	$3,%r12
+++	cmovcq	%r13,%r10
+++	cmovcq	%r14,%r11
+++	cmovcq	%r15,%r12
+++
+++	addq	0+0+16(%rbp),%r10
+++	adcq	8+0+16(%rbp),%r11
+++
+++.cfi_remember_state	
+++	addq	$288 + 0 + 32,%rsp
+++.cfi_adjust_cfa_offset	-(288 + 32)
+++
+++	popq	%r9
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r9
+++	movq	%r10,(%r9)
+++	movq	%r11,8(%r9)
+++	popq	%r15
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r15
+++	popq	%r14
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r14
+++	popq	%r13
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r13
+++	popq	%r12
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r12
+++	popq	%rbx
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%rbx
+++	popq	%rbp
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%rbp
+++	.byte	0xf3,0xc3
+++
+++.Lseal_sse_128:
+++.cfi_restore_state	
+++	movdqu	.Lchacha20_consts(%rip),%xmm0
+++	movdqa	%xmm0,%xmm1
+++	movdqa	%xmm0,%xmm2
+++	movdqu	0(%r9),%xmm4
+++	movdqa	%xmm4,%xmm5
+++	movdqa	%xmm4,%xmm6
+++	movdqu	16(%r9),%xmm8
+++	movdqa	%xmm8,%xmm9
+++	movdqa	%xmm8,%xmm10
+++	movdqu	32(%r9),%xmm14
+++	movdqa	%xmm14,%xmm12
+++	paddd	.Lsse_inc(%rip),%xmm12
+++	movdqa	%xmm12,%xmm13
+++	paddd	.Lsse_inc(%rip),%xmm13
+++	movdqa	%xmm4,%xmm7
+++	movdqa	%xmm8,%xmm11
+++	movdqa	%xmm12,%xmm15
+++	movq	$10,%r10
+++
+++.Lseal_sse_128_rounds:
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,4
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,12
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm5
+++	pxor	%xmm3,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm5
+++	pxor	%xmm3,%xmm5
+++.byte	102,15,58,15,237,4
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,12
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol16(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm6
+++	pxor	%xmm3,%xmm6
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol8(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm6
+++	pxor	%xmm3,%xmm6
+++.byte	102,15,58,15,246,4
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,12
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol16(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm4
+++	pxor	%xmm3,%xmm4
+++	paddd	%xmm4,%xmm0
+++	pxor	%xmm0,%xmm12
+++	pshufb	.Lrol8(%rip),%xmm12
+++	paddd	%xmm12,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,15,228,12
+++.byte	102,69,15,58,15,192,8
+++.byte	102,69,15,58,15,228,4
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol16(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm5
+++	pxor	%xmm3,%xmm5
+++	paddd	%xmm5,%xmm1
+++	pxor	%xmm1,%xmm13
+++	pshufb	.Lrol8(%rip),%xmm13
+++	paddd	%xmm13,%xmm9
+++	pxor	%xmm9,%xmm5
+++	movdqa	%xmm5,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm5
+++	pxor	%xmm3,%xmm5
+++.byte	102,15,58,15,237,12
+++.byte	102,69,15,58,15,201,8
+++.byte	102,69,15,58,15,237,4
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol16(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$12,%xmm3
+++	psrld	$20,%xmm6
+++	pxor	%xmm3,%xmm6
+++	paddd	%xmm6,%xmm2
+++	pxor	%xmm2,%xmm14
+++	pshufb	.Lrol8(%rip),%xmm14
+++	paddd	%xmm14,%xmm10
+++	pxor	%xmm10,%xmm6
+++	movdqa	%xmm6,%xmm3
+++	pslld	$7,%xmm3
+++	psrld	$25,%xmm6
+++	pxor	%xmm3,%xmm6
+++.byte	102,15,58,15,246,12
+++.byte	102,69,15,58,15,210,8
+++.byte	102,69,15,58,15,246,4
+++
+++	decq	%r10
+++	jnz	.Lseal_sse_128_rounds
+++	paddd	.Lchacha20_consts(%rip),%xmm0
+++	paddd	.Lchacha20_consts(%rip),%xmm1
+++	paddd	.Lchacha20_consts(%rip),%xmm2
+++	paddd	%xmm7,%xmm4
+++	paddd	%xmm7,%xmm5
+++	paddd	%xmm7,%xmm6
+++	paddd	%xmm11,%xmm8
+++	paddd	%xmm11,%xmm9
+++	paddd	%xmm15,%xmm12
+++	paddd	.Lsse_inc(%rip),%xmm15
+++	paddd	%xmm15,%xmm13
+++
+++	pand	.Lclamp(%rip),%xmm2
+++	movdqa	%xmm2,0+0(%rbp)
+++	movdqa	%xmm6,0+16(%rbp)
+++
+++	movq	%r8,%r8
+++	call	poly_hash_ad_internal
+++	jmp	.Lseal_sse_128_tail_xor
+++.size	chacha20_poly1305_seal, .-chacha20_poly1305_seal
+++.cfi_endproc	
+++
+++
+++.type	chacha20_poly1305_open_avx2,@function
+++.align	64
+++chacha20_poly1305_open_avx2:
+++.cfi_startproc	
+++
+++
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r9,-64
+++.cfi_adjust_cfa_offset	288 + 32
+++
+++	vzeroupper
+++	vmovdqa	.Lchacha20_consts(%rip),%ymm0
+++	vbroadcasti128	0(%r9),%ymm4
+++	vbroadcasti128	16(%r9),%ymm8
+++	vbroadcasti128	32(%r9),%ymm12
+++	vpaddd	.Lavx2_init(%rip),%ymm12,%ymm12
+++	cmpq	$192,%rbx
+++	jbe	.Lopen_avx2_192
+++	cmpq	$320,%rbx
+++	jbe	.Lopen_avx2_320
+++
+++	vmovdqa	%ymm4,0+64(%rbp)
+++	vmovdqa	%ymm8,0+96(%rbp)
+++	vmovdqa	%ymm12,0+160(%rbp)
+++	movq	$10,%r10
+++.Lopen_avx2_init_rounds:
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++
+++	decq	%r10
+++	jne	.Lopen_avx2_init_rounds
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	0+64(%rbp),%ymm4,%ymm4
+++	vpaddd	0+96(%rbp),%ymm8,%ymm8
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm3
+++
+++	vpand	.Lclamp(%rip),%ymm3,%ymm3
+++	vmovdqa	%ymm3,0+0(%rbp)
+++
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm4
+++
+++	movq	%r8,%r8
+++	call	poly_hash_ad_internal
+++
+++	xorq	%rcx,%rcx
+++.Lopen_avx2_init_hash:
+++	addq	0+0(%rsi,%rcx,1),%r10
+++	adcq	8+0(%rsi,%rcx,1),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	addq	$16,%rcx
+++	cmpq	$64,%rcx
+++	jne	.Lopen_avx2_init_hash
+++
+++	vpxor	0(%rsi),%ymm0,%ymm0
+++	vpxor	32(%rsi),%ymm4,%ymm4
+++
+++	vmovdqu	%ymm0,0(%rdi)
+++	vmovdqu	%ymm4,32(%rdi)
+++	leaq	64(%rsi),%rsi
+++	leaq	64(%rdi),%rdi
+++	subq	$64,%rbx
+++.Lopen_avx2_main_loop:
+++
+++	cmpq	$512,%rbx
+++	jb	.Lopen_avx2_main_loop_done
+++	vmovdqa	.Lchacha20_consts(%rip),%ymm0
+++	vmovdqa	0+64(%rbp),%ymm4
+++	vmovdqa	0+96(%rbp),%ymm8
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	%ymm0,%ymm2
+++	vmovdqa	%ymm4,%ymm6
+++	vmovdqa	%ymm8,%ymm10
+++	vmovdqa	%ymm0,%ymm3
+++	vmovdqa	%ymm4,%ymm7
+++	vmovdqa	%ymm8,%ymm11
+++	vmovdqa	.Lavx2_inc(%rip),%ymm12
+++	vpaddd	0+160(%rbp),%ymm12,%ymm15
+++	vpaddd	%ymm15,%ymm12,%ymm14
+++	vpaddd	%ymm14,%ymm12,%ymm13
+++	vpaddd	%ymm13,%ymm12,%ymm12
+++	vmovdqa	%ymm15,0+256(%rbp)
+++	vmovdqa	%ymm14,0+224(%rbp)
+++	vmovdqa	%ymm13,0+192(%rbp)
+++	vmovdqa	%ymm12,0+160(%rbp)
+++
+++	xorq	%rcx,%rcx
+++.Lopen_avx2_main_loop_rounds:
+++	addq	0+0(%rsi,%rcx,1),%r10
+++	adcq	8+0(%rsi,%rcx,1),%r11
+++	adcq	$1,%r12
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vmovdqa	.Lrol16(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$20,%ymm7,%ymm8
+++	vpslld	$32-20,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$20,%ymm6,%ymm8
+++	vpslld	$32-20,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$20,%ymm5,%ymm8
+++	vpslld	$32-20,%ymm5,%ymm5
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$20,%ymm4,%ymm8
+++	vpslld	$32-20,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	.Lrol8(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	addq	0+16(%rsi,%rcx,1),%r10
+++	adcq	8+16(%rsi,%rcx,1),%r11
+++	adcq	$1,%r12
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$25,%ymm7,%ymm8
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	vpslld	$32-25,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$25,%ymm6,%ymm8
+++	vpslld	$32-25,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$25,%ymm5,%ymm8
+++	vpslld	$32-25,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$25,%ymm4,%ymm8
+++	vpslld	$32-25,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	0+128(%rbp),%ymm8
+++	vpalignr	$4,%ymm7,%ymm7,%ymm7
+++	vpalignr	$8,%ymm11,%ymm11,%ymm11
+++	vpalignr	$12,%ymm15,%ymm15,%ymm15
+++	vpalignr	$4,%ymm6,%ymm6,%ymm6
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$12,%ymm14,%ymm14,%ymm14
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vmovdqa	.Lrol16(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$20,%ymm7,%ymm8
+++	vpslld	$32-20,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$20,%ymm6,%ymm8
+++	vpslld	$32-20,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	addq	0+32(%rsi,%rcx,1),%r10
+++	adcq	8+32(%rsi,%rcx,1),%r11
+++	adcq	$1,%r12
+++
+++	leaq	48(%rcx),%rcx
+++	vpsrld	$20,%ymm5,%ymm8
+++	vpslld	$32-20,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$20,%ymm4,%ymm8
+++	vpslld	$32-20,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	.Lrol8(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$25,%ymm7,%ymm8
+++	vpslld	$32-25,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$25,%ymm6,%ymm8
+++	vpslld	$32-25,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	vpsrld	$25,%ymm5,%ymm8
+++	vpslld	$32-25,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$25,%ymm4,%ymm8
+++	vpslld	$32-25,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	0+128(%rbp),%ymm8
+++	vpalignr	$12,%ymm7,%ymm7,%ymm7
+++	vpalignr	$8,%ymm11,%ymm11,%ymm11
+++	vpalignr	$4,%ymm15,%ymm15,%ymm15
+++	vpalignr	$12,%ymm6,%ymm6,%ymm6
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$4,%ymm14,%ymm14,%ymm14
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++
+++	cmpq	$60*8,%rcx
+++	jne	.Lopen_avx2_main_loop_rounds
+++	vpaddd	.Lchacha20_consts(%rip),%ymm3,%ymm3
+++	vpaddd	0+64(%rbp),%ymm7,%ymm7
+++	vpaddd	0+96(%rbp),%ymm11,%ymm11
+++	vpaddd	0+256(%rbp),%ymm15,%ymm15
+++	vpaddd	.Lchacha20_consts(%rip),%ymm2,%ymm2
+++	vpaddd	0+64(%rbp),%ymm6,%ymm6
+++	vpaddd	0+96(%rbp),%ymm10,%ymm10
+++	vpaddd	0+224(%rbp),%ymm14,%ymm14
+++	vpaddd	.Lchacha20_consts(%rip),%ymm1,%ymm1
+++	vpaddd	0+64(%rbp),%ymm5,%ymm5
+++	vpaddd	0+96(%rbp),%ymm9,%ymm9
+++	vpaddd	0+192(%rbp),%ymm13,%ymm13
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	0+64(%rbp),%ymm4,%ymm4
+++	vpaddd	0+96(%rbp),%ymm8,%ymm8
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++
+++	vmovdqa	%ymm0,0+128(%rbp)
+++	addq	0+60*8(%rsi),%r10
+++	adcq	8+60*8(%rsi),%r11
+++	adcq	$1,%r12
+++	vperm2i128	$0x02,%ymm3,%ymm7,%ymm0
+++	vperm2i128	$0x13,%ymm3,%ymm7,%ymm7
+++	vperm2i128	$0x02,%ymm11,%ymm15,%ymm3
+++	vperm2i128	$0x13,%ymm11,%ymm15,%ymm11
+++	vpxor	0+0(%rsi),%ymm0,%ymm0
+++	vpxor	32+0(%rsi),%ymm3,%ymm3
+++	vpxor	64+0(%rsi),%ymm7,%ymm7
+++	vpxor	96+0(%rsi),%ymm11,%ymm11
+++	vmovdqu	%ymm0,0+0(%rdi)
+++	vmovdqu	%ymm3,32+0(%rdi)
+++	vmovdqu	%ymm7,64+0(%rdi)
+++	vmovdqu	%ymm11,96+0(%rdi)
+++
+++	vmovdqa	0+128(%rbp),%ymm0
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	vperm2i128	$0x02,%ymm2,%ymm6,%ymm3
+++	vperm2i128	$0x13,%ymm2,%ymm6,%ymm6
+++	vperm2i128	$0x02,%ymm10,%ymm14,%ymm2
+++	vperm2i128	$0x13,%ymm10,%ymm14,%ymm10
+++	vpxor	0+128(%rsi),%ymm3,%ymm3
+++	vpxor	32+128(%rsi),%ymm2,%ymm2
+++	vpxor	64+128(%rsi),%ymm6,%ymm6
+++	vpxor	96+128(%rsi),%ymm10,%ymm10
+++	vmovdqu	%ymm3,0+128(%rdi)
+++	vmovdqu	%ymm2,32+128(%rdi)
+++	vmovdqu	%ymm6,64+128(%rdi)
+++	vmovdqu	%ymm10,96+128(%rdi)
+++	addq	0+60*8+16(%rsi),%r10
+++	adcq	8+60*8+16(%rsi),%r11
+++	adcq	$1,%r12
+++	vperm2i128	$0x02,%ymm1,%ymm5,%ymm3
+++	vperm2i128	$0x13,%ymm1,%ymm5,%ymm5
+++	vperm2i128	$0x02,%ymm9,%ymm13,%ymm1
+++	vperm2i128	$0x13,%ymm9,%ymm13,%ymm9
+++	vpxor	0+256(%rsi),%ymm3,%ymm3
+++	vpxor	32+256(%rsi),%ymm1,%ymm1
+++	vpxor	64+256(%rsi),%ymm5,%ymm5
+++	vpxor	96+256(%rsi),%ymm9,%ymm9
+++	vmovdqu	%ymm3,0+256(%rdi)
+++	vmovdqu	%ymm1,32+256(%rdi)
+++	vmovdqu	%ymm5,64+256(%rdi)
+++	vmovdqu	%ymm9,96+256(%rdi)
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm3
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm4
+++	vperm2i128	$0x02,%ymm8,%ymm12,%ymm0
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm8
+++	vpxor	0+384(%rsi),%ymm3,%ymm3
+++	vpxor	32+384(%rsi),%ymm0,%ymm0
+++	vpxor	64+384(%rsi),%ymm4,%ymm4
+++	vpxor	96+384(%rsi),%ymm8,%ymm8
+++	vmovdqu	%ymm3,0+384(%rdi)
+++	vmovdqu	%ymm0,32+384(%rdi)
+++	vmovdqu	%ymm4,64+384(%rdi)
+++	vmovdqu	%ymm8,96+384(%rdi)
+++
+++	leaq	512(%rsi),%rsi
+++	leaq	512(%rdi),%rdi
+++	subq	$512,%rbx
+++	jmp	.Lopen_avx2_main_loop
+++.Lopen_avx2_main_loop_done:
+++	testq	%rbx,%rbx
+++	vzeroupper
+++	je	.Lopen_sse_finalize
+++
+++	cmpq	$384,%rbx
+++	ja	.Lopen_avx2_tail_512
+++	cmpq	$256,%rbx
+++	ja	.Lopen_avx2_tail_384
+++	cmpq	$128,%rbx
+++	ja	.Lopen_avx2_tail_256
+++	vmovdqa	.Lchacha20_consts(%rip),%ymm0
+++	vmovdqa	0+64(%rbp),%ymm4
+++	vmovdqa	0+96(%rbp),%ymm8
+++	vmovdqa	.Lavx2_inc(%rip),%ymm12
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++	vmovdqa	%ymm12,0+160(%rbp)
+++
+++	xorq	%r8,%r8
+++	movq	%rbx,%rcx
+++	andq	$-16,%rcx
+++	testq	%rcx,%rcx
+++	je	.Lopen_avx2_tail_128_rounds
+++.Lopen_avx2_tail_128_rounds_and_x1hash:
+++	addq	0+0(%rsi,%r8,1),%r10
+++	adcq	8+0(%rsi,%r8,1),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++.Lopen_avx2_tail_128_rounds:
+++	addq	$16,%r8
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++
+++	cmpq	%rcx,%r8
+++	jb	.Lopen_avx2_tail_128_rounds_and_x1hash
+++	cmpq	$160,%r8
+++	jne	.Lopen_avx2_tail_128_rounds
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	0+64(%rbp),%ymm4,%ymm4
+++	vpaddd	0+96(%rbp),%ymm8,%ymm8
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm3
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x02,%ymm8,%ymm12,%ymm4
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm12
+++	vmovdqa	%ymm3,%ymm8
+++
+++	jmp	.Lopen_avx2_tail_128_xor
+++
+++.Lopen_avx2_tail_256:
+++	vmovdqa	.Lchacha20_consts(%rip),%ymm0
+++	vmovdqa	0+64(%rbp),%ymm4
+++	vmovdqa	0+96(%rbp),%ymm8
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	.Lavx2_inc(%rip),%ymm12
+++	vpaddd	0+160(%rbp),%ymm12,%ymm13
+++	vpaddd	%ymm13,%ymm12,%ymm12
+++	vmovdqa	%ymm12,0+160(%rbp)
+++	vmovdqa	%ymm13,0+192(%rbp)
+++
+++	movq	%rbx,0+128(%rbp)
+++	movq	%rbx,%rcx
+++	subq	$128,%rcx
+++	shrq	$4,%rcx
+++	movq	$10,%r8
+++	cmpq	$10,%rcx
+++	cmovgq	%r8,%rcx
+++	movq	%rsi,%rbx
+++	xorq	%r8,%r8
+++.Lopen_avx2_tail_256_rounds_and_x1hash:
+++	addq	0+0(%rbx),%r10
+++	adcq	8+0(%rbx),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rbx),%rbx
+++.Lopen_avx2_tail_256_rounds:
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++
+++	incq	%r8
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol16(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpsrld	$20,%ymm6,%ymm3
+++	vpslld	$12,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol8(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpslld	$7,%ymm6,%ymm3
+++	vpsrld	$25,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpalignr	$4,%ymm14,%ymm14,%ymm14
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$12,%ymm6,%ymm6,%ymm6
+++
+++	cmpq	%rcx,%r8
+++	jb	.Lopen_avx2_tail_256_rounds_and_x1hash
+++	cmpq	$10,%r8
+++	jne	.Lopen_avx2_tail_256_rounds
+++	movq	%rbx,%r8
+++	subq	%rsi,%rbx
+++	movq	%rbx,%rcx
+++	movq	0+128(%rbp),%rbx
+++.Lopen_avx2_tail_256_hash:
+++	addq	$16,%rcx
+++	cmpq	%rbx,%rcx
+++	jg	.Lopen_avx2_tail_256_done
+++	addq	0+0(%r8),%r10
+++	adcq	8+0(%r8),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%r8),%r8
+++	jmp	.Lopen_avx2_tail_256_hash
+++.Lopen_avx2_tail_256_done:
+++	vpaddd	.Lchacha20_consts(%rip),%ymm1,%ymm1
+++	vpaddd	0+64(%rbp),%ymm5,%ymm5
+++	vpaddd	0+96(%rbp),%ymm9,%ymm9
+++	vpaddd	0+192(%rbp),%ymm13,%ymm13
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	0+64(%rbp),%ymm4,%ymm4
+++	vpaddd	0+96(%rbp),%ymm8,%ymm8
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++	vperm2i128	$0x02,%ymm1,%ymm5,%ymm3
+++	vperm2i128	$0x13,%ymm1,%ymm5,%ymm5
+++	vperm2i128	$0x02,%ymm9,%ymm13,%ymm1
+++	vperm2i128	$0x13,%ymm9,%ymm13,%ymm9
+++	vpxor	0+0(%rsi),%ymm3,%ymm3
+++	vpxor	32+0(%rsi),%ymm1,%ymm1
+++	vpxor	64+0(%rsi),%ymm5,%ymm5
+++	vpxor	96+0(%rsi),%ymm9,%ymm9
+++	vmovdqu	%ymm3,0+0(%rdi)
+++	vmovdqu	%ymm1,32+0(%rdi)
+++	vmovdqu	%ymm5,64+0(%rdi)
+++	vmovdqu	%ymm9,96+0(%rdi)
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm3
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x02,%ymm8,%ymm12,%ymm4
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm12
+++	vmovdqa	%ymm3,%ymm8
+++
+++	leaq	128(%rsi),%rsi
+++	leaq	128(%rdi),%rdi
+++	subq	$128,%rbx
+++	jmp	.Lopen_avx2_tail_128_xor
+++
+++.Lopen_avx2_tail_384:
+++	vmovdqa	.Lchacha20_consts(%rip),%ymm0
+++	vmovdqa	0+64(%rbp),%ymm4
+++	vmovdqa	0+96(%rbp),%ymm8
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	%ymm0,%ymm2
+++	vmovdqa	%ymm4,%ymm6
+++	vmovdqa	%ymm8,%ymm10
+++	vmovdqa	.Lavx2_inc(%rip),%ymm12
+++	vpaddd	0+160(%rbp),%ymm12,%ymm14
+++	vpaddd	%ymm14,%ymm12,%ymm13
+++	vpaddd	%ymm13,%ymm12,%ymm12
+++	vmovdqa	%ymm12,0+160(%rbp)
+++	vmovdqa	%ymm13,0+192(%rbp)
+++	vmovdqa	%ymm14,0+224(%rbp)
+++
+++	movq	%rbx,0+128(%rbp)
+++	movq	%rbx,%rcx
+++	subq	$256,%rcx
+++	shrq	$4,%rcx
+++	addq	$6,%rcx
+++	movq	$10,%r8
+++	cmpq	$10,%rcx
+++	cmovgq	%r8,%rcx
+++	movq	%rsi,%rbx
+++	xorq	%r8,%r8
+++.Lopen_avx2_tail_384_rounds_and_x2hash:
+++	addq	0+0(%rbx),%r10
+++	adcq	8+0(%rbx),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rbx),%rbx
+++.Lopen_avx2_tail_384_rounds_and_x1hash:
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol16(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpsrld	$20,%ymm6,%ymm3
+++	vpslld	$12,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol8(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpslld	$7,%ymm6,%ymm3
+++	vpsrld	$25,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpalignr	$12,%ymm14,%ymm14,%ymm14
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$4,%ymm6,%ymm6,%ymm6
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	addq	0+0(%rbx),%r10
+++	adcq	8+0(%rbx),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rbx),%rbx
+++	incq	%r8
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol16(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpsrld	$20,%ymm6,%ymm3
+++	vpslld	$12,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol8(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpslld	$7,%ymm6,%ymm3
+++	vpsrld	$25,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpalignr	$4,%ymm14,%ymm14,%ymm14
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$12,%ymm6,%ymm6,%ymm6
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++
+++	cmpq	%rcx,%r8
+++	jb	.Lopen_avx2_tail_384_rounds_and_x2hash
+++	cmpq	$10,%r8
+++	jne	.Lopen_avx2_tail_384_rounds_and_x1hash
+++	movq	%rbx,%r8
+++	subq	%rsi,%rbx
+++	movq	%rbx,%rcx
+++	movq	0+128(%rbp),%rbx
+++.Lopen_avx2_384_tail_hash:
+++	addq	$16,%rcx
+++	cmpq	%rbx,%rcx
+++	jg	.Lopen_avx2_384_tail_done
+++	addq	0+0(%r8),%r10
+++	adcq	8+0(%r8),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%r8),%r8
+++	jmp	.Lopen_avx2_384_tail_hash
+++.Lopen_avx2_384_tail_done:
+++	vpaddd	.Lchacha20_consts(%rip),%ymm2,%ymm2
+++	vpaddd	0+64(%rbp),%ymm6,%ymm6
+++	vpaddd	0+96(%rbp),%ymm10,%ymm10
+++	vpaddd	0+224(%rbp),%ymm14,%ymm14
+++	vpaddd	.Lchacha20_consts(%rip),%ymm1,%ymm1
+++	vpaddd	0+64(%rbp),%ymm5,%ymm5
+++	vpaddd	0+96(%rbp),%ymm9,%ymm9
+++	vpaddd	0+192(%rbp),%ymm13,%ymm13
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	0+64(%rbp),%ymm4,%ymm4
+++	vpaddd	0+96(%rbp),%ymm8,%ymm8
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++	vperm2i128	$0x02,%ymm2,%ymm6,%ymm3
+++	vperm2i128	$0x13,%ymm2,%ymm6,%ymm6
+++	vperm2i128	$0x02,%ymm10,%ymm14,%ymm2
+++	vperm2i128	$0x13,%ymm10,%ymm14,%ymm10
+++	vpxor	0+0(%rsi),%ymm3,%ymm3
+++	vpxor	32+0(%rsi),%ymm2,%ymm2
+++	vpxor	64+0(%rsi),%ymm6,%ymm6
+++	vpxor	96+0(%rsi),%ymm10,%ymm10
+++	vmovdqu	%ymm3,0+0(%rdi)
+++	vmovdqu	%ymm2,32+0(%rdi)
+++	vmovdqu	%ymm6,64+0(%rdi)
+++	vmovdqu	%ymm10,96+0(%rdi)
+++	vperm2i128	$0x02,%ymm1,%ymm5,%ymm3
+++	vperm2i128	$0x13,%ymm1,%ymm5,%ymm5
+++	vperm2i128	$0x02,%ymm9,%ymm13,%ymm1
+++	vperm2i128	$0x13,%ymm9,%ymm13,%ymm9
+++	vpxor	0+128(%rsi),%ymm3,%ymm3
+++	vpxor	32+128(%rsi),%ymm1,%ymm1
+++	vpxor	64+128(%rsi),%ymm5,%ymm5
+++	vpxor	96+128(%rsi),%ymm9,%ymm9
+++	vmovdqu	%ymm3,0+128(%rdi)
+++	vmovdqu	%ymm1,32+128(%rdi)
+++	vmovdqu	%ymm5,64+128(%rdi)
+++	vmovdqu	%ymm9,96+128(%rdi)
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm3
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x02,%ymm8,%ymm12,%ymm4
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm12
+++	vmovdqa	%ymm3,%ymm8
+++
+++	leaq	256(%rsi),%rsi
+++	leaq	256(%rdi),%rdi
+++	subq	$256,%rbx
+++	jmp	.Lopen_avx2_tail_128_xor
+++
+++.Lopen_avx2_tail_512:
+++	vmovdqa	.Lchacha20_consts(%rip),%ymm0
+++	vmovdqa	0+64(%rbp),%ymm4
+++	vmovdqa	0+96(%rbp),%ymm8
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	%ymm0,%ymm2
+++	vmovdqa	%ymm4,%ymm6
+++	vmovdqa	%ymm8,%ymm10
+++	vmovdqa	%ymm0,%ymm3
+++	vmovdqa	%ymm4,%ymm7
+++	vmovdqa	%ymm8,%ymm11
+++	vmovdqa	.Lavx2_inc(%rip),%ymm12
+++	vpaddd	0+160(%rbp),%ymm12,%ymm15
+++	vpaddd	%ymm15,%ymm12,%ymm14
+++	vpaddd	%ymm14,%ymm12,%ymm13
+++	vpaddd	%ymm13,%ymm12,%ymm12
+++	vmovdqa	%ymm15,0+256(%rbp)
+++	vmovdqa	%ymm14,0+224(%rbp)
+++	vmovdqa	%ymm13,0+192(%rbp)
+++	vmovdqa	%ymm12,0+160(%rbp)
+++
+++	xorq	%rcx,%rcx
+++	movq	%rsi,%r8
+++.Lopen_avx2_tail_512_rounds_and_x2hash:
+++	addq	0+0(%r8),%r10
+++	adcq	8+0(%r8),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%r8),%r8
+++.Lopen_avx2_tail_512_rounds_and_x1hash:
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vmovdqa	.Lrol16(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$20,%ymm7,%ymm8
+++	vpslld	$32-20,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$20,%ymm6,%ymm8
+++	vpslld	$32-20,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$20,%ymm5,%ymm8
+++	vpslld	$32-20,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$20,%ymm4,%ymm8
+++	vpslld	$32-20,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	.Lrol8(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	addq	0+0(%r8),%r10
+++	adcq	8+0(%r8),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$25,%ymm7,%ymm8
+++	vpslld	$32-25,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$25,%ymm6,%ymm8
+++	vpslld	$32-25,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$25,%ymm5,%ymm8
+++	vpslld	$32-25,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$25,%ymm4,%ymm8
+++	vpslld	$32-25,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	0+128(%rbp),%ymm8
+++	vpalignr	$4,%ymm7,%ymm7,%ymm7
+++	vpalignr	$8,%ymm11,%ymm11,%ymm11
+++	vpalignr	$12,%ymm15,%ymm15,%ymm15
+++	vpalignr	$4,%ymm6,%ymm6,%ymm6
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$12,%ymm14,%ymm14,%ymm14
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vmovdqa	.Lrol16(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	addq	0+16(%r8),%r10
+++	adcq	8+16(%r8),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	32(%r8),%r8
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$20,%ymm7,%ymm8
+++	vpslld	$32-20,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$20,%ymm6,%ymm8
+++	vpslld	$32-20,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$20,%ymm5,%ymm8
+++	vpslld	$32-20,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$20,%ymm4,%ymm8
+++	vpslld	$32-20,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	.Lrol8(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$25,%ymm7,%ymm8
+++	vpslld	$32-25,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$25,%ymm6,%ymm8
+++	vpslld	$32-25,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$25,%ymm5,%ymm8
+++	vpslld	$32-25,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$25,%ymm4,%ymm8
+++	vpslld	$32-25,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	0+128(%rbp),%ymm8
+++	vpalignr	$12,%ymm7,%ymm7,%ymm7
+++	vpalignr	$8,%ymm11,%ymm11,%ymm11
+++	vpalignr	$4,%ymm15,%ymm15,%ymm15
+++	vpalignr	$12,%ymm6,%ymm6,%ymm6
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$4,%ymm14,%ymm14,%ymm14
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++
+++	incq	%rcx
+++	cmpq	$4,%rcx
+++	jl	.Lopen_avx2_tail_512_rounds_and_x2hash
+++	cmpq	$10,%rcx
+++	jne	.Lopen_avx2_tail_512_rounds_and_x1hash
+++	movq	%rbx,%rcx
+++	subq	$384,%rcx
+++	andq	$-16,%rcx
+++.Lopen_avx2_tail_512_hash:
+++	testq	%rcx,%rcx
+++	je	.Lopen_avx2_tail_512_done
+++	addq	0+0(%r8),%r10
+++	adcq	8+0(%r8),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%r8),%r8
+++	subq	$16,%rcx
+++	jmp	.Lopen_avx2_tail_512_hash
+++.Lopen_avx2_tail_512_done:
+++	vpaddd	.Lchacha20_consts(%rip),%ymm3,%ymm3
+++	vpaddd	0+64(%rbp),%ymm7,%ymm7
+++	vpaddd	0+96(%rbp),%ymm11,%ymm11
+++	vpaddd	0+256(%rbp),%ymm15,%ymm15
+++	vpaddd	.Lchacha20_consts(%rip),%ymm2,%ymm2
+++	vpaddd	0+64(%rbp),%ymm6,%ymm6
+++	vpaddd	0+96(%rbp),%ymm10,%ymm10
+++	vpaddd	0+224(%rbp),%ymm14,%ymm14
+++	vpaddd	.Lchacha20_consts(%rip),%ymm1,%ymm1
+++	vpaddd	0+64(%rbp),%ymm5,%ymm5
+++	vpaddd	0+96(%rbp),%ymm9,%ymm9
+++	vpaddd	0+192(%rbp),%ymm13,%ymm13
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	0+64(%rbp),%ymm4,%ymm4
+++	vpaddd	0+96(%rbp),%ymm8,%ymm8
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++
+++	vmovdqa	%ymm0,0+128(%rbp)
+++	vperm2i128	$0x02,%ymm3,%ymm7,%ymm0
+++	vperm2i128	$0x13,%ymm3,%ymm7,%ymm7
+++	vperm2i128	$0x02,%ymm11,%ymm15,%ymm3
+++	vperm2i128	$0x13,%ymm11,%ymm15,%ymm11
+++	vpxor	0+0(%rsi),%ymm0,%ymm0
+++	vpxor	32+0(%rsi),%ymm3,%ymm3
+++	vpxor	64+0(%rsi),%ymm7,%ymm7
+++	vpxor	96+0(%rsi),%ymm11,%ymm11
+++	vmovdqu	%ymm0,0+0(%rdi)
+++	vmovdqu	%ymm3,32+0(%rdi)
+++	vmovdqu	%ymm7,64+0(%rdi)
+++	vmovdqu	%ymm11,96+0(%rdi)
+++
+++	vmovdqa	0+128(%rbp),%ymm0
+++	vperm2i128	$0x02,%ymm2,%ymm6,%ymm3
+++	vperm2i128	$0x13,%ymm2,%ymm6,%ymm6
+++	vperm2i128	$0x02,%ymm10,%ymm14,%ymm2
+++	vperm2i128	$0x13,%ymm10,%ymm14,%ymm10
+++	vpxor	0+128(%rsi),%ymm3,%ymm3
+++	vpxor	32+128(%rsi),%ymm2,%ymm2
+++	vpxor	64+128(%rsi),%ymm6,%ymm6
+++	vpxor	96+128(%rsi),%ymm10,%ymm10
+++	vmovdqu	%ymm3,0+128(%rdi)
+++	vmovdqu	%ymm2,32+128(%rdi)
+++	vmovdqu	%ymm6,64+128(%rdi)
+++	vmovdqu	%ymm10,96+128(%rdi)
+++	vperm2i128	$0x02,%ymm1,%ymm5,%ymm3
+++	vperm2i128	$0x13,%ymm1,%ymm5,%ymm5
+++	vperm2i128	$0x02,%ymm9,%ymm13,%ymm1
+++	vperm2i128	$0x13,%ymm9,%ymm13,%ymm9
+++	vpxor	0+256(%rsi),%ymm3,%ymm3
+++	vpxor	32+256(%rsi),%ymm1,%ymm1
+++	vpxor	64+256(%rsi),%ymm5,%ymm5
+++	vpxor	96+256(%rsi),%ymm9,%ymm9
+++	vmovdqu	%ymm3,0+256(%rdi)
+++	vmovdqu	%ymm1,32+256(%rdi)
+++	vmovdqu	%ymm5,64+256(%rdi)
+++	vmovdqu	%ymm9,96+256(%rdi)
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm3
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x02,%ymm8,%ymm12,%ymm4
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm12
+++	vmovdqa	%ymm3,%ymm8
+++
+++	leaq	384(%rsi),%rsi
+++	leaq	384(%rdi),%rdi
+++	subq	$384,%rbx
+++.Lopen_avx2_tail_128_xor:
+++	cmpq	$32,%rbx
+++	jb	.Lopen_avx2_tail_32_xor
+++	subq	$32,%rbx
+++	vpxor	(%rsi),%ymm0,%ymm0
+++	vmovdqu	%ymm0,(%rdi)
+++	leaq	32(%rsi),%rsi
+++	leaq	32(%rdi),%rdi
+++	vmovdqa	%ymm4,%ymm0
+++	vmovdqa	%ymm8,%ymm4
+++	vmovdqa	%ymm12,%ymm8
+++	jmp	.Lopen_avx2_tail_128_xor
+++.Lopen_avx2_tail_32_xor:
+++	cmpq	$16,%rbx
+++	vmovdqa	%xmm0,%xmm1
+++	jb	.Lopen_avx2_exit
+++	subq	$16,%rbx
+++
+++	vpxor	(%rsi),%xmm0,%xmm1
+++	vmovdqu	%xmm1,(%rdi)
+++	leaq	16(%rsi),%rsi
+++	leaq	16(%rdi),%rdi
+++	vperm2i128	$0x11,%ymm0,%ymm0,%ymm0
+++	vmovdqa	%xmm0,%xmm1
+++.Lopen_avx2_exit:
+++	vzeroupper
+++	jmp	.Lopen_sse_tail_16
+++
+++.Lopen_avx2_192:
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm0,%ymm2
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm4,%ymm6
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	%ymm8,%ymm10
+++	vpaddd	.Lavx2_inc(%rip),%ymm12,%ymm13
+++	vmovdqa	%ymm12,%ymm11
+++	vmovdqa	%ymm13,%ymm15
+++	movq	$10,%r10
+++.Lopen_avx2_192_rounds:
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++
+++	decq	%r10
+++	jne	.Lopen_avx2_192_rounds
+++	vpaddd	%ymm2,%ymm0,%ymm0
+++	vpaddd	%ymm2,%ymm1,%ymm1
+++	vpaddd	%ymm6,%ymm4,%ymm4
+++	vpaddd	%ymm6,%ymm5,%ymm5
+++	vpaddd	%ymm10,%ymm8,%ymm8
+++	vpaddd	%ymm10,%ymm9,%ymm9
+++	vpaddd	%ymm11,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm13,%ymm13
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm3
+++
+++	vpand	.Lclamp(%rip),%ymm3,%ymm3
+++	vmovdqa	%ymm3,0+0(%rbp)
+++
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm4
+++	vperm2i128	$0x02,%ymm1,%ymm5,%ymm8
+++	vperm2i128	$0x02,%ymm9,%ymm13,%ymm12
+++	vperm2i128	$0x13,%ymm1,%ymm5,%ymm1
+++	vperm2i128	$0x13,%ymm9,%ymm13,%ymm5
+++.Lopen_avx2_short:
+++	movq	%r8,%r8
+++	call	poly_hash_ad_internal
+++.Lopen_avx2_short_hash_and_xor_loop:
+++	cmpq	$32,%rbx
+++	jb	.Lopen_avx2_short_tail_32
+++	subq	$32,%rbx
+++	addq	0+0(%rsi),%r10
+++	adcq	8+0(%rsi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	addq	0+16(%rsi),%r10
+++	adcq	8+16(%rsi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++
+++	vpxor	(%rsi),%ymm0,%ymm0
+++	vmovdqu	%ymm0,(%rdi)
+++	leaq	32(%rsi),%rsi
+++	leaq	32(%rdi),%rdi
+++
+++	vmovdqa	%ymm4,%ymm0
+++	vmovdqa	%ymm8,%ymm4
+++	vmovdqa	%ymm12,%ymm8
+++	vmovdqa	%ymm1,%ymm12
+++	vmovdqa	%ymm5,%ymm1
+++	vmovdqa	%ymm9,%ymm5
+++	vmovdqa	%ymm13,%ymm9
+++	vmovdqa	%ymm2,%ymm13
+++	vmovdqa	%ymm6,%ymm2
+++	jmp	.Lopen_avx2_short_hash_and_xor_loop
+++.Lopen_avx2_short_tail_32:
+++	cmpq	$16,%rbx
+++	vmovdqa	%xmm0,%xmm1
+++	jb	.Lopen_avx2_short_tail_32_exit
+++	subq	$16,%rbx
+++	addq	0+0(%rsi),%r10
+++	adcq	8+0(%rsi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	vpxor	(%rsi),%xmm0,%xmm3
+++	vmovdqu	%xmm3,(%rdi)
+++	leaq	16(%rsi),%rsi
+++	leaq	16(%rdi),%rdi
+++	vextracti128	$1,%ymm0,%xmm1
+++.Lopen_avx2_short_tail_32_exit:
+++	vzeroupper
+++	jmp	.Lopen_sse_tail_16
+++
+++.Lopen_avx2_320:
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm0,%ymm2
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm4,%ymm6
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	%ymm8,%ymm10
+++	vpaddd	.Lavx2_inc(%rip),%ymm12,%ymm13
+++	vpaddd	.Lavx2_inc(%rip),%ymm13,%ymm14
+++	vmovdqa	%ymm4,%ymm7
+++	vmovdqa	%ymm8,%ymm11
+++	vmovdqa	%ymm12,0+160(%rbp)
+++	vmovdqa	%ymm13,0+192(%rbp)
+++	vmovdqa	%ymm14,0+224(%rbp)
+++	movq	$10,%r10
+++.Lopen_avx2_320_rounds:
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol16(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpsrld	$20,%ymm6,%ymm3
+++	vpslld	$12,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol8(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpslld	$7,%ymm6,%ymm3
+++	vpsrld	$25,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpalignr	$12,%ymm14,%ymm14,%ymm14
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$4,%ymm6,%ymm6,%ymm6
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol16(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpsrld	$20,%ymm6,%ymm3
+++	vpslld	$12,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol8(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpslld	$7,%ymm6,%ymm3
+++	vpsrld	$25,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpalignr	$4,%ymm14,%ymm14,%ymm14
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$12,%ymm6,%ymm6,%ymm6
+++
+++	decq	%r10
+++	jne	.Lopen_avx2_320_rounds
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	.Lchacha20_consts(%rip),%ymm1,%ymm1
+++	vpaddd	.Lchacha20_consts(%rip),%ymm2,%ymm2
+++	vpaddd	%ymm7,%ymm4,%ymm4
+++	vpaddd	%ymm7,%ymm5,%ymm5
+++	vpaddd	%ymm7,%ymm6,%ymm6
+++	vpaddd	%ymm11,%ymm8,%ymm8
+++	vpaddd	%ymm11,%ymm9,%ymm9
+++	vpaddd	%ymm11,%ymm10,%ymm10
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++	vpaddd	0+192(%rbp),%ymm13,%ymm13
+++	vpaddd	0+224(%rbp),%ymm14,%ymm14
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm3
+++
+++	vpand	.Lclamp(%rip),%ymm3,%ymm3
+++	vmovdqa	%ymm3,0+0(%rbp)
+++
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm4
+++	vperm2i128	$0x02,%ymm1,%ymm5,%ymm8
+++	vperm2i128	$0x02,%ymm9,%ymm13,%ymm12
+++	vperm2i128	$0x13,%ymm1,%ymm5,%ymm1
+++	vperm2i128	$0x13,%ymm9,%ymm13,%ymm5
+++	vperm2i128	$0x02,%ymm2,%ymm6,%ymm9
+++	vperm2i128	$0x02,%ymm10,%ymm14,%ymm13
+++	vperm2i128	$0x13,%ymm2,%ymm6,%ymm2
+++	vperm2i128	$0x13,%ymm10,%ymm14,%ymm6
+++	jmp	.Lopen_avx2_short
+++.size	chacha20_poly1305_open_avx2, .-chacha20_poly1305_open_avx2
+++.cfi_endproc	
+++
+++
+++.type	chacha20_poly1305_seal_avx2,@function
+++.align	64
+++chacha20_poly1305_seal_avx2:
+++.cfi_startproc	
+++
+++
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r9,-64
+++.cfi_adjust_cfa_offset	288 + 32
+++
+++	vzeroupper
+++	vmovdqa	.Lchacha20_consts(%rip),%ymm0
+++	vbroadcasti128	0(%r9),%ymm4
+++	vbroadcasti128	16(%r9),%ymm8
+++	vbroadcasti128	32(%r9),%ymm12
+++	vpaddd	.Lavx2_init(%rip),%ymm12,%ymm12
+++	cmpq	$192,%rbx
+++	jbe	.Lseal_avx2_192
+++	cmpq	$320,%rbx
+++	jbe	.Lseal_avx2_320
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm0,%ymm2
+++	vmovdqa	%ymm0,%ymm3
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm4,%ymm6
+++	vmovdqa	%ymm4,%ymm7
+++	vmovdqa	%ymm4,0+64(%rbp)
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	%ymm8,%ymm10
+++	vmovdqa	%ymm8,%ymm11
+++	vmovdqa	%ymm8,0+96(%rbp)
+++	vmovdqa	%ymm12,%ymm15
+++	vpaddd	.Lavx2_inc(%rip),%ymm15,%ymm14
+++	vpaddd	.Lavx2_inc(%rip),%ymm14,%ymm13
+++	vpaddd	.Lavx2_inc(%rip),%ymm13,%ymm12
+++	vmovdqa	%ymm12,0+160(%rbp)
+++	vmovdqa	%ymm13,0+192(%rbp)
+++	vmovdqa	%ymm14,0+224(%rbp)
+++	vmovdqa	%ymm15,0+256(%rbp)
+++	movq	$10,%r10
+++.Lseal_avx2_init_rounds:
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vmovdqa	.Lrol16(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$20,%ymm7,%ymm8
+++	vpslld	$32-20,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$20,%ymm6,%ymm8
+++	vpslld	$32-20,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$20,%ymm5,%ymm8
+++	vpslld	$32-20,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$20,%ymm4,%ymm8
+++	vpslld	$32-20,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	.Lrol8(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$25,%ymm7,%ymm8
+++	vpslld	$32-25,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$25,%ymm6,%ymm8
+++	vpslld	$32-25,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$25,%ymm5,%ymm8
+++	vpslld	$32-25,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$25,%ymm4,%ymm8
+++	vpslld	$32-25,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	0+128(%rbp),%ymm8
+++	vpalignr	$4,%ymm7,%ymm7,%ymm7
+++	vpalignr	$8,%ymm11,%ymm11,%ymm11
+++	vpalignr	$12,%ymm15,%ymm15,%ymm15
+++	vpalignr	$4,%ymm6,%ymm6,%ymm6
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$12,%ymm14,%ymm14,%ymm14
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vmovdqa	.Lrol16(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$20,%ymm7,%ymm8
+++	vpslld	$32-20,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$20,%ymm6,%ymm8
+++	vpslld	$32-20,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$20,%ymm5,%ymm8
+++	vpslld	$32-20,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$20,%ymm4,%ymm8
+++	vpslld	$32-20,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	.Lrol8(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$25,%ymm7,%ymm8
+++	vpslld	$32-25,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$25,%ymm6,%ymm8
+++	vpslld	$32-25,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$25,%ymm5,%ymm8
+++	vpslld	$32-25,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$25,%ymm4,%ymm8
+++	vpslld	$32-25,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	0+128(%rbp),%ymm8
+++	vpalignr	$12,%ymm7,%ymm7,%ymm7
+++	vpalignr	$8,%ymm11,%ymm11,%ymm11
+++	vpalignr	$4,%ymm15,%ymm15,%ymm15
+++	vpalignr	$12,%ymm6,%ymm6,%ymm6
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$4,%ymm14,%ymm14,%ymm14
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++
+++	decq	%r10
+++	jnz	.Lseal_avx2_init_rounds
+++	vpaddd	.Lchacha20_consts(%rip),%ymm3,%ymm3
+++	vpaddd	0+64(%rbp),%ymm7,%ymm7
+++	vpaddd	0+96(%rbp),%ymm11,%ymm11
+++	vpaddd	0+256(%rbp),%ymm15,%ymm15
+++	vpaddd	.Lchacha20_consts(%rip),%ymm2,%ymm2
+++	vpaddd	0+64(%rbp),%ymm6,%ymm6
+++	vpaddd	0+96(%rbp),%ymm10,%ymm10
+++	vpaddd	0+224(%rbp),%ymm14,%ymm14
+++	vpaddd	.Lchacha20_consts(%rip),%ymm1,%ymm1
+++	vpaddd	0+64(%rbp),%ymm5,%ymm5
+++	vpaddd	0+96(%rbp),%ymm9,%ymm9
+++	vpaddd	0+192(%rbp),%ymm13,%ymm13
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	0+64(%rbp),%ymm4,%ymm4
+++	vpaddd	0+96(%rbp),%ymm8,%ymm8
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++
+++	vperm2i128	$0x13,%ymm11,%ymm15,%ymm11
+++	vperm2i128	$0x02,%ymm3,%ymm7,%ymm15
+++	vperm2i128	$0x13,%ymm3,%ymm7,%ymm3
+++	vpand	.Lclamp(%rip),%ymm15,%ymm15
+++	vmovdqa	%ymm15,0+0(%rbp)
+++	movq	%r8,%r8
+++	call	poly_hash_ad_internal
+++
+++	vpxor	0(%rsi),%ymm3,%ymm3
+++	vpxor	32(%rsi),%ymm11,%ymm11
+++	vmovdqu	%ymm3,0(%rdi)
+++	vmovdqu	%ymm11,32(%rdi)
+++	vperm2i128	$0x02,%ymm2,%ymm6,%ymm15
+++	vperm2i128	$0x13,%ymm2,%ymm6,%ymm6
+++	vperm2i128	$0x02,%ymm10,%ymm14,%ymm2
+++	vperm2i128	$0x13,%ymm10,%ymm14,%ymm10
+++	vpxor	0+64(%rsi),%ymm15,%ymm15
+++	vpxor	32+64(%rsi),%ymm2,%ymm2
+++	vpxor	64+64(%rsi),%ymm6,%ymm6
+++	vpxor	96+64(%rsi),%ymm10,%ymm10
+++	vmovdqu	%ymm15,0+64(%rdi)
+++	vmovdqu	%ymm2,32+64(%rdi)
+++	vmovdqu	%ymm6,64+64(%rdi)
+++	vmovdqu	%ymm10,96+64(%rdi)
+++	vperm2i128	$0x02,%ymm1,%ymm5,%ymm15
+++	vperm2i128	$0x13,%ymm1,%ymm5,%ymm5
+++	vperm2i128	$0x02,%ymm9,%ymm13,%ymm1
+++	vperm2i128	$0x13,%ymm9,%ymm13,%ymm9
+++	vpxor	0+192(%rsi),%ymm15,%ymm15
+++	vpxor	32+192(%rsi),%ymm1,%ymm1
+++	vpxor	64+192(%rsi),%ymm5,%ymm5
+++	vpxor	96+192(%rsi),%ymm9,%ymm9
+++	vmovdqu	%ymm15,0+192(%rdi)
+++	vmovdqu	%ymm1,32+192(%rdi)
+++	vmovdqu	%ymm5,64+192(%rdi)
+++	vmovdqu	%ymm9,96+192(%rdi)
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm15
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x02,%ymm8,%ymm12,%ymm4
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm12
+++	vmovdqa	%ymm15,%ymm8
+++
+++	leaq	320(%rsi),%rsi
+++	subq	$320,%rbx
+++	movq	$320,%rcx
+++	cmpq	$128,%rbx
+++	jbe	.Lseal_avx2_short_hash_remainder
+++	vpxor	0(%rsi),%ymm0,%ymm0
+++	vpxor	32(%rsi),%ymm4,%ymm4
+++	vpxor	64(%rsi),%ymm8,%ymm8
+++	vpxor	96(%rsi),%ymm12,%ymm12
+++	vmovdqu	%ymm0,320(%rdi)
+++	vmovdqu	%ymm4,352(%rdi)
+++	vmovdqu	%ymm8,384(%rdi)
+++	vmovdqu	%ymm12,416(%rdi)
+++	leaq	128(%rsi),%rsi
+++	subq	$128,%rbx
+++	movq	$8,%rcx
+++	movq	$2,%r8
+++	cmpq	$128,%rbx
+++	jbe	.Lseal_avx2_tail_128
+++	cmpq	$256,%rbx
+++	jbe	.Lseal_avx2_tail_256
+++	cmpq	$384,%rbx
+++	jbe	.Lseal_avx2_tail_384
+++	cmpq	$512,%rbx
+++	jbe	.Lseal_avx2_tail_512
+++	vmovdqa	.Lchacha20_consts(%rip),%ymm0
+++	vmovdqa	0+64(%rbp),%ymm4
+++	vmovdqa	0+96(%rbp),%ymm8
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	%ymm0,%ymm2
+++	vmovdqa	%ymm4,%ymm6
+++	vmovdqa	%ymm8,%ymm10
+++	vmovdqa	%ymm0,%ymm3
+++	vmovdqa	%ymm4,%ymm7
+++	vmovdqa	%ymm8,%ymm11
+++	vmovdqa	.Lavx2_inc(%rip),%ymm12
+++	vpaddd	0+160(%rbp),%ymm12,%ymm15
+++	vpaddd	%ymm15,%ymm12,%ymm14
+++	vpaddd	%ymm14,%ymm12,%ymm13
+++	vpaddd	%ymm13,%ymm12,%ymm12
+++	vmovdqa	%ymm15,0+256(%rbp)
+++	vmovdqa	%ymm14,0+224(%rbp)
+++	vmovdqa	%ymm13,0+192(%rbp)
+++	vmovdqa	%ymm12,0+160(%rbp)
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vmovdqa	.Lrol16(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$20,%ymm7,%ymm8
+++	vpslld	$32-20,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$20,%ymm6,%ymm8
+++	vpslld	$32-20,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$20,%ymm5,%ymm8
+++	vpslld	$32-20,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$20,%ymm4,%ymm8
+++	vpslld	$32-20,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	.Lrol8(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$25,%ymm7,%ymm8
+++	vpslld	$32-25,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$25,%ymm6,%ymm8
+++	vpslld	$32-25,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$25,%ymm5,%ymm8
+++	vpslld	$32-25,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$25,%ymm4,%ymm8
+++	vpslld	$32-25,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	0+128(%rbp),%ymm8
+++	vpalignr	$4,%ymm7,%ymm7,%ymm7
+++	vpalignr	$8,%ymm11,%ymm11,%ymm11
+++	vpalignr	$12,%ymm15,%ymm15,%ymm15
+++	vpalignr	$4,%ymm6,%ymm6,%ymm6
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$12,%ymm14,%ymm14,%ymm14
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vmovdqa	.Lrol16(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$20,%ymm7,%ymm8
+++	vpslld	$32-20,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$20,%ymm6,%ymm8
+++	vpslld	$32-20,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$20,%ymm5,%ymm8
+++	vpslld	$32-20,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$20,%ymm4,%ymm8
+++	vpslld	$32-20,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	.Lrol8(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$25,%ymm7,%ymm8
+++	vpslld	$32-25,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$25,%ymm6,%ymm8
+++	vpslld	$32-25,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$25,%ymm5,%ymm8
+++	vpslld	$32-25,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$25,%ymm4,%ymm8
+++	vpslld	$32-25,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	0+128(%rbp),%ymm8
+++	vpalignr	$12,%ymm7,%ymm7,%ymm7
+++	vpalignr	$8,%ymm11,%ymm11,%ymm11
+++	vpalignr	$4,%ymm15,%ymm15,%ymm15
+++	vpalignr	$12,%ymm6,%ymm6,%ymm6
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$4,%ymm14,%ymm14,%ymm14
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vmovdqa	.Lrol16(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$20,%ymm7,%ymm8
+++	vpslld	$32-20,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$20,%ymm6,%ymm8
+++	vpslld	$32-20,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$20,%ymm5,%ymm8
+++	vpslld	$32-20,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$20,%ymm4,%ymm8
+++	vpslld	$32-20,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	.Lrol8(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++
+++	subq	$16,%rdi
+++	movq	$9,%rcx
+++	jmp	.Lseal_avx2_main_loop_rounds_entry
+++.align	32
+++.Lseal_avx2_main_loop:
+++	vmovdqa	.Lchacha20_consts(%rip),%ymm0
+++	vmovdqa	0+64(%rbp),%ymm4
+++	vmovdqa	0+96(%rbp),%ymm8
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	%ymm0,%ymm2
+++	vmovdqa	%ymm4,%ymm6
+++	vmovdqa	%ymm8,%ymm10
+++	vmovdqa	%ymm0,%ymm3
+++	vmovdqa	%ymm4,%ymm7
+++	vmovdqa	%ymm8,%ymm11
+++	vmovdqa	.Lavx2_inc(%rip),%ymm12
+++	vpaddd	0+160(%rbp),%ymm12,%ymm15
+++	vpaddd	%ymm15,%ymm12,%ymm14
+++	vpaddd	%ymm14,%ymm12,%ymm13
+++	vpaddd	%ymm13,%ymm12,%ymm12
+++	vmovdqa	%ymm15,0+256(%rbp)
+++	vmovdqa	%ymm14,0+224(%rbp)
+++	vmovdqa	%ymm13,0+192(%rbp)
+++	vmovdqa	%ymm12,0+160(%rbp)
+++
+++	movq	$10,%rcx
+++.align	32
+++.Lseal_avx2_main_loop_rounds:
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vmovdqa	.Lrol16(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$20,%ymm7,%ymm8
+++	vpslld	$32-20,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$20,%ymm6,%ymm8
+++	vpslld	$32-20,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$20,%ymm5,%ymm8
+++	vpslld	$32-20,%ymm5,%ymm5
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$20,%ymm4,%ymm8
+++	vpslld	$32-20,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	.Lrol8(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++.Lseal_avx2_main_loop_rounds_entry:
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	addq	0+16(%rdi),%r10
+++	adcq	8+16(%rdi),%r11
+++	adcq	$1,%r12
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$25,%ymm7,%ymm8
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	vpslld	$32-25,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$25,%ymm6,%ymm8
+++	vpslld	$32-25,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$25,%ymm5,%ymm8
+++	vpslld	$32-25,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$25,%ymm4,%ymm8
+++	vpslld	$32-25,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	0+128(%rbp),%ymm8
+++	vpalignr	$4,%ymm7,%ymm7,%ymm7
+++	vpalignr	$8,%ymm11,%ymm11,%ymm11
+++	vpalignr	$12,%ymm15,%ymm15,%ymm15
+++	vpalignr	$4,%ymm6,%ymm6,%ymm6
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$12,%ymm14,%ymm14,%ymm14
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vmovdqa	.Lrol16(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$20,%ymm7,%ymm8
+++	vpslld	$32-20,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$20,%ymm6,%ymm8
+++	vpslld	$32-20,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	addq	0+32(%rdi),%r10
+++	adcq	8+32(%rdi),%r11
+++	adcq	$1,%r12
+++
+++	leaq	48(%rdi),%rdi
+++	vpsrld	$20,%ymm5,%ymm8
+++	vpslld	$32-20,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$20,%ymm4,%ymm8
+++	vpslld	$32-20,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	.Lrol8(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$25,%ymm7,%ymm8
+++	vpslld	$32-25,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$25,%ymm6,%ymm8
+++	vpslld	$32-25,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	vpsrld	$25,%ymm5,%ymm8
+++	vpslld	$32-25,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$25,%ymm4,%ymm8
+++	vpslld	$32-25,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	0+128(%rbp),%ymm8
+++	vpalignr	$12,%ymm7,%ymm7,%ymm7
+++	vpalignr	$8,%ymm11,%ymm11,%ymm11
+++	vpalignr	$4,%ymm15,%ymm15,%ymm15
+++	vpalignr	$12,%ymm6,%ymm6,%ymm6
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$4,%ymm14,%ymm14,%ymm14
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++
+++	decq	%rcx
+++	jne	.Lseal_avx2_main_loop_rounds
+++	vpaddd	.Lchacha20_consts(%rip),%ymm3,%ymm3
+++	vpaddd	0+64(%rbp),%ymm7,%ymm7
+++	vpaddd	0+96(%rbp),%ymm11,%ymm11
+++	vpaddd	0+256(%rbp),%ymm15,%ymm15
+++	vpaddd	.Lchacha20_consts(%rip),%ymm2,%ymm2
+++	vpaddd	0+64(%rbp),%ymm6,%ymm6
+++	vpaddd	0+96(%rbp),%ymm10,%ymm10
+++	vpaddd	0+224(%rbp),%ymm14,%ymm14
+++	vpaddd	.Lchacha20_consts(%rip),%ymm1,%ymm1
+++	vpaddd	0+64(%rbp),%ymm5,%ymm5
+++	vpaddd	0+96(%rbp),%ymm9,%ymm9
+++	vpaddd	0+192(%rbp),%ymm13,%ymm13
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	0+64(%rbp),%ymm4,%ymm4
+++	vpaddd	0+96(%rbp),%ymm8,%ymm8
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++
+++	vmovdqa	%ymm0,0+128(%rbp)
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	addq	0+16(%rdi),%r10
+++	adcq	8+16(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	32(%rdi),%rdi
+++	vperm2i128	$0x02,%ymm3,%ymm7,%ymm0
+++	vperm2i128	$0x13,%ymm3,%ymm7,%ymm7
+++	vperm2i128	$0x02,%ymm11,%ymm15,%ymm3
+++	vperm2i128	$0x13,%ymm11,%ymm15,%ymm11
+++	vpxor	0+0(%rsi),%ymm0,%ymm0
+++	vpxor	32+0(%rsi),%ymm3,%ymm3
+++	vpxor	64+0(%rsi),%ymm7,%ymm7
+++	vpxor	96+0(%rsi),%ymm11,%ymm11
+++	vmovdqu	%ymm0,0+0(%rdi)
+++	vmovdqu	%ymm3,32+0(%rdi)
+++	vmovdqu	%ymm7,64+0(%rdi)
+++	vmovdqu	%ymm11,96+0(%rdi)
+++
+++	vmovdqa	0+128(%rbp),%ymm0
+++	vperm2i128	$0x02,%ymm2,%ymm6,%ymm3
+++	vperm2i128	$0x13,%ymm2,%ymm6,%ymm6
+++	vperm2i128	$0x02,%ymm10,%ymm14,%ymm2
+++	vperm2i128	$0x13,%ymm10,%ymm14,%ymm10
+++	vpxor	0+128(%rsi),%ymm3,%ymm3
+++	vpxor	32+128(%rsi),%ymm2,%ymm2
+++	vpxor	64+128(%rsi),%ymm6,%ymm6
+++	vpxor	96+128(%rsi),%ymm10,%ymm10
+++	vmovdqu	%ymm3,0+128(%rdi)
+++	vmovdqu	%ymm2,32+128(%rdi)
+++	vmovdqu	%ymm6,64+128(%rdi)
+++	vmovdqu	%ymm10,96+128(%rdi)
+++	vperm2i128	$0x02,%ymm1,%ymm5,%ymm3
+++	vperm2i128	$0x13,%ymm1,%ymm5,%ymm5
+++	vperm2i128	$0x02,%ymm9,%ymm13,%ymm1
+++	vperm2i128	$0x13,%ymm9,%ymm13,%ymm9
+++	vpxor	0+256(%rsi),%ymm3,%ymm3
+++	vpxor	32+256(%rsi),%ymm1,%ymm1
+++	vpxor	64+256(%rsi),%ymm5,%ymm5
+++	vpxor	96+256(%rsi),%ymm9,%ymm9
+++	vmovdqu	%ymm3,0+256(%rdi)
+++	vmovdqu	%ymm1,32+256(%rdi)
+++	vmovdqu	%ymm5,64+256(%rdi)
+++	vmovdqu	%ymm9,96+256(%rdi)
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm3
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm4
+++	vperm2i128	$0x02,%ymm8,%ymm12,%ymm0
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm8
+++	vpxor	0+384(%rsi),%ymm3,%ymm3
+++	vpxor	32+384(%rsi),%ymm0,%ymm0
+++	vpxor	64+384(%rsi),%ymm4,%ymm4
+++	vpxor	96+384(%rsi),%ymm8,%ymm8
+++	vmovdqu	%ymm3,0+384(%rdi)
+++	vmovdqu	%ymm0,32+384(%rdi)
+++	vmovdqu	%ymm4,64+384(%rdi)
+++	vmovdqu	%ymm8,96+384(%rdi)
+++
+++	leaq	512(%rsi),%rsi
+++	subq	$512,%rbx
+++	cmpq	$512,%rbx
+++	jg	.Lseal_avx2_main_loop
+++
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	addq	0+16(%rdi),%r10
+++	adcq	8+16(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	32(%rdi),%rdi
+++	movq	$10,%rcx
+++	xorq	%r8,%r8
+++
+++	cmpq	$384,%rbx
+++	ja	.Lseal_avx2_tail_512
+++	cmpq	$256,%rbx
+++	ja	.Lseal_avx2_tail_384
+++	cmpq	$128,%rbx
+++	ja	.Lseal_avx2_tail_256
+++
+++.Lseal_avx2_tail_128:
+++	vmovdqa	.Lchacha20_consts(%rip),%ymm0
+++	vmovdqa	0+64(%rbp),%ymm4
+++	vmovdqa	0+96(%rbp),%ymm8
+++	vmovdqa	.Lavx2_inc(%rip),%ymm12
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++	vmovdqa	%ymm12,0+160(%rbp)
+++
+++.Lseal_avx2_tail_128_rounds_and_3xhash:
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rdi),%rdi
+++.Lseal_avx2_tail_128_rounds_and_2xhash:
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	addq	0+16(%rdi),%r10
+++	adcq	8+16(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	32(%rdi),%rdi
+++	decq	%rcx
+++	jg	.Lseal_avx2_tail_128_rounds_and_3xhash
+++	decq	%r8
+++	jge	.Lseal_avx2_tail_128_rounds_and_2xhash
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	0+64(%rbp),%ymm4,%ymm4
+++	vpaddd	0+96(%rbp),%ymm8,%ymm8
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm3
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x02,%ymm8,%ymm12,%ymm4
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm12
+++	vmovdqa	%ymm3,%ymm8
+++
+++	jmp	.Lseal_avx2_short_loop
+++
+++.Lseal_avx2_tail_256:
+++	vmovdqa	.Lchacha20_consts(%rip),%ymm0
+++	vmovdqa	0+64(%rbp),%ymm4
+++	vmovdqa	0+96(%rbp),%ymm8
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	.Lavx2_inc(%rip),%ymm12
+++	vpaddd	0+160(%rbp),%ymm12,%ymm13
+++	vpaddd	%ymm13,%ymm12,%ymm12
+++	vmovdqa	%ymm12,0+160(%rbp)
+++	vmovdqa	%ymm13,0+192(%rbp)
+++
+++.Lseal_avx2_tail_256_rounds_and_3xhash:
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rdi),%rdi
+++.Lseal_avx2_tail_256_rounds_and_2xhash:
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++	addq	0+16(%rdi),%r10
+++	adcq	8+16(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	32(%rdi),%rdi
+++	decq	%rcx
+++	jg	.Lseal_avx2_tail_256_rounds_and_3xhash
+++	decq	%r8
+++	jge	.Lseal_avx2_tail_256_rounds_and_2xhash
+++	vpaddd	.Lchacha20_consts(%rip),%ymm1,%ymm1
+++	vpaddd	0+64(%rbp),%ymm5,%ymm5
+++	vpaddd	0+96(%rbp),%ymm9,%ymm9
+++	vpaddd	0+192(%rbp),%ymm13,%ymm13
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	0+64(%rbp),%ymm4,%ymm4
+++	vpaddd	0+96(%rbp),%ymm8,%ymm8
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++	vperm2i128	$0x02,%ymm1,%ymm5,%ymm3
+++	vperm2i128	$0x13,%ymm1,%ymm5,%ymm5
+++	vperm2i128	$0x02,%ymm9,%ymm13,%ymm1
+++	vperm2i128	$0x13,%ymm9,%ymm13,%ymm9
+++	vpxor	0+0(%rsi),%ymm3,%ymm3
+++	vpxor	32+0(%rsi),%ymm1,%ymm1
+++	vpxor	64+0(%rsi),%ymm5,%ymm5
+++	vpxor	96+0(%rsi),%ymm9,%ymm9
+++	vmovdqu	%ymm3,0+0(%rdi)
+++	vmovdqu	%ymm1,32+0(%rdi)
+++	vmovdqu	%ymm5,64+0(%rdi)
+++	vmovdqu	%ymm9,96+0(%rdi)
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm3
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x02,%ymm8,%ymm12,%ymm4
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm12
+++	vmovdqa	%ymm3,%ymm8
+++
+++	movq	$128,%rcx
+++	leaq	128(%rsi),%rsi
+++	subq	$128,%rbx
+++	jmp	.Lseal_avx2_short_hash_remainder
+++
+++.Lseal_avx2_tail_384:
+++	vmovdqa	.Lchacha20_consts(%rip),%ymm0
+++	vmovdqa	0+64(%rbp),%ymm4
+++	vmovdqa	0+96(%rbp),%ymm8
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	%ymm0,%ymm2
+++	vmovdqa	%ymm4,%ymm6
+++	vmovdqa	%ymm8,%ymm10
+++	vmovdqa	.Lavx2_inc(%rip),%ymm12
+++	vpaddd	0+160(%rbp),%ymm12,%ymm14
+++	vpaddd	%ymm14,%ymm12,%ymm13
+++	vpaddd	%ymm13,%ymm12,%ymm12
+++	vmovdqa	%ymm12,0+160(%rbp)
+++	vmovdqa	%ymm13,0+192(%rbp)
+++	vmovdqa	%ymm14,0+224(%rbp)
+++
+++.Lseal_avx2_tail_384_rounds_and_3xhash:
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rdi),%rdi
+++.Lseal_avx2_tail_384_rounds_and_2xhash:
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol16(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpsrld	$20,%ymm6,%ymm3
+++	vpslld	$12,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol8(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpslld	$7,%ymm6,%ymm3
+++	vpsrld	$25,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpalignr	$12,%ymm14,%ymm14,%ymm14
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$4,%ymm6,%ymm6,%ymm6
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	addq	0+16(%rdi),%r10
+++	adcq	8+16(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol16(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpsrld	$20,%ymm6,%ymm3
+++	vpslld	$12,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol8(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpslld	$7,%ymm6,%ymm3
+++	vpsrld	$25,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpalignr	$4,%ymm14,%ymm14,%ymm14
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$12,%ymm6,%ymm6,%ymm6
+++
+++	leaq	32(%rdi),%rdi
+++	decq	%rcx
+++	jg	.Lseal_avx2_tail_384_rounds_and_3xhash
+++	decq	%r8
+++	jge	.Lseal_avx2_tail_384_rounds_and_2xhash
+++	vpaddd	.Lchacha20_consts(%rip),%ymm2,%ymm2
+++	vpaddd	0+64(%rbp),%ymm6,%ymm6
+++	vpaddd	0+96(%rbp),%ymm10,%ymm10
+++	vpaddd	0+224(%rbp),%ymm14,%ymm14
+++	vpaddd	.Lchacha20_consts(%rip),%ymm1,%ymm1
+++	vpaddd	0+64(%rbp),%ymm5,%ymm5
+++	vpaddd	0+96(%rbp),%ymm9,%ymm9
+++	vpaddd	0+192(%rbp),%ymm13,%ymm13
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	0+64(%rbp),%ymm4,%ymm4
+++	vpaddd	0+96(%rbp),%ymm8,%ymm8
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++	vperm2i128	$0x02,%ymm2,%ymm6,%ymm3
+++	vperm2i128	$0x13,%ymm2,%ymm6,%ymm6
+++	vperm2i128	$0x02,%ymm10,%ymm14,%ymm2
+++	vperm2i128	$0x13,%ymm10,%ymm14,%ymm10
+++	vpxor	0+0(%rsi),%ymm3,%ymm3
+++	vpxor	32+0(%rsi),%ymm2,%ymm2
+++	vpxor	64+0(%rsi),%ymm6,%ymm6
+++	vpxor	96+0(%rsi),%ymm10,%ymm10
+++	vmovdqu	%ymm3,0+0(%rdi)
+++	vmovdqu	%ymm2,32+0(%rdi)
+++	vmovdqu	%ymm6,64+0(%rdi)
+++	vmovdqu	%ymm10,96+0(%rdi)
+++	vperm2i128	$0x02,%ymm1,%ymm5,%ymm3
+++	vperm2i128	$0x13,%ymm1,%ymm5,%ymm5
+++	vperm2i128	$0x02,%ymm9,%ymm13,%ymm1
+++	vperm2i128	$0x13,%ymm9,%ymm13,%ymm9
+++	vpxor	0+128(%rsi),%ymm3,%ymm3
+++	vpxor	32+128(%rsi),%ymm1,%ymm1
+++	vpxor	64+128(%rsi),%ymm5,%ymm5
+++	vpxor	96+128(%rsi),%ymm9,%ymm9
+++	vmovdqu	%ymm3,0+128(%rdi)
+++	vmovdqu	%ymm1,32+128(%rdi)
+++	vmovdqu	%ymm5,64+128(%rdi)
+++	vmovdqu	%ymm9,96+128(%rdi)
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm3
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x02,%ymm8,%ymm12,%ymm4
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm12
+++	vmovdqa	%ymm3,%ymm8
+++
+++	movq	$256,%rcx
+++	leaq	256(%rsi),%rsi
+++	subq	$256,%rbx
+++	jmp	.Lseal_avx2_short_hash_remainder
+++
+++.Lseal_avx2_tail_512:
+++	vmovdqa	.Lchacha20_consts(%rip),%ymm0
+++	vmovdqa	0+64(%rbp),%ymm4
+++	vmovdqa	0+96(%rbp),%ymm8
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	%ymm0,%ymm2
+++	vmovdqa	%ymm4,%ymm6
+++	vmovdqa	%ymm8,%ymm10
+++	vmovdqa	%ymm0,%ymm3
+++	vmovdqa	%ymm4,%ymm7
+++	vmovdqa	%ymm8,%ymm11
+++	vmovdqa	.Lavx2_inc(%rip),%ymm12
+++	vpaddd	0+160(%rbp),%ymm12,%ymm15
+++	vpaddd	%ymm15,%ymm12,%ymm14
+++	vpaddd	%ymm14,%ymm12,%ymm13
+++	vpaddd	%ymm13,%ymm12,%ymm12
+++	vmovdqa	%ymm15,0+256(%rbp)
+++	vmovdqa	%ymm14,0+224(%rbp)
+++	vmovdqa	%ymm13,0+192(%rbp)
+++	vmovdqa	%ymm12,0+160(%rbp)
+++
+++.Lseal_avx2_tail_512_rounds_and_3xhash:
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rdi),%rdi
+++.Lseal_avx2_tail_512_rounds_and_2xhash:
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vmovdqa	.Lrol16(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$20,%ymm7,%ymm8
+++	vpslld	$32-20,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$20,%ymm6,%ymm8
+++	vpslld	$32-20,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$20,%ymm5,%ymm8
+++	vpslld	$32-20,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$20,%ymm4,%ymm8
+++	vpslld	$32-20,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	.Lrol8(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$25,%ymm7,%ymm8
+++	vpslld	$32-25,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	vpsrld	$25,%ymm6,%ymm8
+++	vpslld	$32-25,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$25,%ymm5,%ymm8
+++	vpslld	$32-25,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$25,%ymm4,%ymm8
+++	vpslld	$32-25,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	0+128(%rbp),%ymm8
+++	vpalignr	$4,%ymm7,%ymm7,%ymm7
+++	vpalignr	$8,%ymm11,%ymm11,%ymm11
+++	vpalignr	$12,%ymm15,%ymm15,%ymm15
+++	vpalignr	$4,%ymm6,%ymm6,%ymm6
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$12,%ymm14,%ymm14,%ymm14
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vmovdqa	.Lrol16(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$20,%ymm7,%ymm8
+++	vpslld	$32-20,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$20,%ymm6,%ymm8
+++	vpslld	$32-20,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$20,%ymm5,%ymm8
+++	vpslld	$32-20,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$20,%ymm4,%ymm8
+++	vpslld	$32-20,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	.Lrol8(%rip),%ymm8
+++	vpaddd	%ymm7,%ymm3,%ymm3
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	addq	0+16(%rdi),%r10
+++	adcq	8+16(%rdi),%r11
+++	adcq	$1,%r12
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm3,%ymm15,%ymm15
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	%ymm8,%ymm15,%ymm15
+++	vpshufb	%ymm8,%ymm14,%ymm14
+++	vpshufb	%ymm8,%ymm13,%ymm13
+++	vpshufb	%ymm8,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm11,%ymm11
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpaddd	0+128(%rbp),%ymm12,%ymm8
+++	vpxor	%ymm11,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	%ymm8,0+128(%rbp)
+++	vpsrld	$25,%ymm7,%ymm8
+++	movq	0+0+0(%rbp),%rdx
+++	movq	%rdx,%r15
+++	mulxq	%r10,%r13,%r14
+++	mulxq	%r11,%rax,%rdx
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	vpslld	$32-25,%ymm7,%ymm7
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$25,%ymm6,%ymm8
+++	vpslld	$32-25,%ymm6,%ymm6
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$25,%ymm5,%ymm8
+++	vpslld	$32-25,%ymm5,%ymm5
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$25,%ymm4,%ymm8
+++	vpslld	$32-25,%ymm4,%ymm4
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vmovdqa	0+128(%rbp),%ymm8
+++	vpalignr	$12,%ymm7,%ymm7,%ymm7
+++	vpalignr	$8,%ymm11,%ymm11,%ymm11
+++	vpalignr	$4,%ymm15,%ymm15,%ymm15
+++	vpalignr	$12,%ymm6,%ymm6,%ymm6
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$4,%ymm14,%ymm14,%ymm14
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	movq	8+0+0(%rbp),%rdx
+++	mulxq	%r10,%r10,%rax
+++	addq	%r10,%r14
+++	mulxq	%r11,%r11,%r9
+++	adcq	%r11,%r15
+++	adcq	$0,%r9
+++	imulq	%r12,%rdx
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	addq	%rax,%r15
+++	adcq	%rdx,%r9
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	32(%rdi),%rdi
+++	decq	%rcx
+++	jg	.Lseal_avx2_tail_512_rounds_and_3xhash
+++	decq	%r8
+++	jge	.Lseal_avx2_tail_512_rounds_and_2xhash
+++	vpaddd	.Lchacha20_consts(%rip),%ymm3,%ymm3
+++	vpaddd	0+64(%rbp),%ymm7,%ymm7
+++	vpaddd	0+96(%rbp),%ymm11,%ymm11
+++	vpaddd	0+256(%rbp),%ymm15,%ymm15
+++	vpaddd	.Lchacha20_consts(%rip),%ymm2,%ymm2
+++	vpaddd	0+64(%rbp),%ymm6,%ymm6
+++	vpaddd	0+96(%rbp),%ymm10,%ymm10
+++	vpaddd	0+224(%rbp),%ymm14,%ymm14
+++	vpaddd	.Lchacha20_consts(%rip),%ymm1,%ymm1
+++	vpaddd	0+64(%rbp),%ymm5,%ymm5
+++	vpaddd	0+96(%rbp),%ymm9,%ymm9
+++	vpaddd	0+192(%rbp),%ymm13,%ymm13
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	0+64(%rbp),%ymm4,%ymm4
+++	vpaddd	0+96(%rbp),%ymm8,%ymm8
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++
+++	vmovdqa	%ymm0,0+128(%rbp)
+++	vperm2i128	$0x02,%ymm3,%ymm7,%ymm0
+++	vperm2i128	$0x13,%ymm3,%ymm7,%ymm7
+++	vperm2i128	$0x02,%ymm11,%ymm15,%ymm3
+++	vperm2i128	$0x13,%ymm11,%ymm15,%ymm11
+++	vpxor	0+0(%rsi),%ymm0,%ymm0
+++	vpxor	32+0(%rsi),%ymm3,%ymm3
+++	vpxor	64+0(%rsi),%ymm7,%ymm7
+++	vpxor	96+0(%rsi),%ymm11,%ymm11
+++	vmovdqu	%ymm0,0+0(%rdi)
+++	vmovdqu	%ymm3,32+0(%rdi)
+++	vmovdqu	%ymm7,64+0(%rdi)
+++	vmovdqu	%ymm11,96+0(%rdi)
+++
+++	vmovdqa	0+128(%rbp),%ymm0
+++	vperm2i128	$0x02,%ymm2,%ymm6,%ymm3
+++	vperm2i128	$0x13,%ymm2,%ymm6,%ymm6
+++	vperm2i128	$0x02,%ymm10,%ymm14,%ymm2
+++	vperm2i128	$0x13,%ymm10,%ymm14,%ymm10
+++	vpxor	0+128(%rsi),%ymm3,%ymm3
+++	vpxor	32+128(%rsi),%ymm2,%ymm2
+++	vpxor	64+128(%rsi),%ymm6,%ymm6
+++	vpxor	96+128(%rsi),%ymm10,%ymm10
+++	vmovdqu	%ymm3,0+128(%rdi)
+++	vmovdqu	%ymm2,32+128(%rdi)
+++	vmovdqu	%ymm6,64+128(%rdi)
+++	vmovdqu	%ymm10,96+128(%rdi)
+++	vperm2i128	$0x02,%ymm1,%ymm5,%ymm3
+++	vperm2i128	$0x13,%ymm1,%ymm5,%ymm5
+++	vperm2i128	$0x02,%ymm9,%ymm13,%ymm1
+++	vperm2i128	$0x13,%ymm9,%ymm13,%ymm9
+++	vpxor	0+256(%rsi),%ymm3,%ymm3
+++	vpxor	32+256(%rsi),%ymm1,%ymm1
+++	vpxor	64+256(%rsi),%ymm5,%ymm5
+++	vpxor	96+256(%rsi),%ymm9,%ymm9
+++	vmovdqu	%ymm3,0+256(%rdi)
+++	vmovdqu	%ymm1,32+256(%rdi)
+++	vmovdqu	%ymm5,64+256(%rdi)
+++	vmovdqu	%ymm9,96+256(%rdi)
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm3
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x02,%ymm8,%ymm12,%ymm4
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm12
+++	vmovdqa	%ymm3,%ymm8
+++
+++	movq	$384,%rcx
+++	leaq	384(%rsi),%rsi
+++	subq	$384,%rbx
+++	jmp	.Lseal_avx2_short_hash_remainder
+++
+++.Lseal_avx2_320:
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm0,%ymm2
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm4,%ymm6
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	%ymm8,%ymm10
+++	vpaddd	.Lavx2_inc(%rip),%ymm12,%ymm13
+++	vpaddd	.Lavx2_inc(%rip),%ymm13,%ymm14
+++	vmovdqa	%ymm4,%ymm7
+++	vmovdqa	%ymm8,%ymm11
+++	vmovdqa	%ymm12,0+160(%rbp)
+++	vmovdqa	%ymm13,0+192(%rbp)
+++	vmovdqa	%ymm14,0+224(%rbp)
+++	movq	$10,%r10
+++.Lseal_avx2_320_rounds:
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol16(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpsrld	$20,%ymm6,%ymm3
+++	vpslld	$12,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol8(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpslld	$7,%ymm6,%ymm3
+++	vpsrld	$25,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpalignr	$12,%ymm14,%ymm14,%ymm14
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$4,%ymm6,%ymm6,%ymm6
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol16(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpsrld	$20,%ymm6,%ymm3
+++	vpslld	$12,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpaddd	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm2,%ymm14,%ymm14
+++	vpshufb	.Lrol8(%rip),%ymm14,%ymm14
+++	vpaddd	%ymm14,%ymm10,%ymm10
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpslld	$7,%ymm6,%ymm3
+++	vpsrld	$25,%ymm6,%ymm6
+++	vpxor	%ymm3,%ymm6,%ymm6
+++	vpalignr	$4,%ymm14,%ymm14,%ymm14
+++	vpalignr	$8,%ymm10,%ymm10,%ymm10
+++	vpalignr	$12,%ymm6,%ymm6,%ymm6
+++
+++	decq	%r10
+++	jne	.Lseal_avx2_320_rounds
+++	vpaddd	.Lchacha20_consts(%rip),%ymm0,%ymm0
+++	vpaddd	.Lchacha20_consts(%rip),%ymm1,%ymm1
+++	vpaddd	.Lchacha20_consts(%rip),%ymm2,%ymm2
+++	vpaddd	%ymm7,%ymm4,%ymm4
+++	vpaddd	%ymm7,%ymm5,%ymm5
+++	vpaddd	%ymm7,%ymm6,%ymm6
+++	vpaddd	%ymm11,%ymm8,%ymm8
+++	vpaddd	%ymm11,%ymm9,%ymm9
+++	vpaddd	%ymm11,%ymm10,%ymm10
+++	vpaddd	0+160(%rbp),%ymm12,%ymm12
+++	vpaddd	0+192(%rbp),%ymm13,%ymm13
+++	vpaddd	0+224(%rbp),%ymm14,%ymm14
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm3
+++
+++	vpand	.Lclamp(%rip),%ymm3,%ymm3
+++	vmovdqa	%ymm3,0+0(%rbp)
+++
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm4
+++	vperm2i128	$0x02,%ymm1,%ymm5,%ymm8
+++	vperm2i128	$0x02,%ymm9,%ymm13,%ymm12
+++	vperm2i128	$0x13,%ymm1,%ymm5,%ymm1
+++	vperm2i128	$0x13,%ymm9,%ymm13,%ymm5
+++	vperm2i128	$0x02,%ymm2,%ymm6,%ymm9
+++	vperm2i128	$0x02,%ymm10,%ymm14,%ymm13
+++	vperm2i128	$0x13,%ymm2,%ymm6,%ymm2
+++	vperm2i128	$0x13,%ymm10,%ymm14,%ymm6
+++	jmp	.Lseal_avx2_short
+++
+++.Lseal_avx2_192:
+++	vmovdqa	%ymm0,%ymm1
+++	vmovdqa	%ymm0,%ymm2
+++	vmovdqa	%ymm4,%ymm5
+++	vmovdqa	%ymm4,%ymm6
+++	vmovdqa	%ymm8,%ymm9
+++	vmovdqa	%ymm8,%ymm10
+++	vpaddd	.Lavx2_inc(%rip),%ymm12,%ymm13
+++	vmovdqa	%ymm12,%ymm11
+++	vmovdqa	%ymm13,%ymm15
+++	movq	$10,%r10
+++.Lseal_avx2_192_rounds:
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$12,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$4,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$12,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$4,%ymm5,%ymm5,%ymm5
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol16(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$20,%ymm4,%ymm3
+++	vpslld	$12,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpaddd	%ymm4,%ymm0,%ymm0
+++	vpxor	%ymm0,%ymm12,%ymm12
+++	vpshufb	.Lrol8(%rip),%ymm12,%ymm12
+++	vpaddd	%ymm12,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpslld	$7,%ymm4,%ymm3
+++	vpsrld	$25,%ymm4,%ymm4
+++	vpxor	%ymm3,%ymm4,%ymm4
+++	vpalignr	$4,%ymm12,%ymm12,%ymm12
+++	vpalignr	$8,%ymm8,%ymm8,%ymm8
+++	vpalignr	$12,%ymm4,%ymm4,%ymm4
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol16(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpsrld	$20,%ymm5,%ymm3
+++	vpslld	$12,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpaddd	%ymm5,%ymm1,%ymm1
+++	vpxor	%ymm1,%ymm13,%ymm13
+++	vpshufb	.Lrol8(%rip),%ymm13,%ymm13
+++	vpaddd	%ymm13,%ymm9,%ymm9
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpslld	$7,%ymm5,%ymm3
+++	vpsrld	$25,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm5,%ymm5
+++	vpalignr	$4,%ymm13,%ymm13,%ymm13
+++	vpalignr	$8,%ymm9,%ymm9,%ymm9
+++	vpalignr	$12,%ymm5,%ymm5,%ymm5
+++
+++	decq	%r10
+++	jne	.Lseal_avx2_192_rounds
+++	vpaddd	%ymm2,%ymm0,%ymm0
+++	vpaddd	%ymm2,%ymm1,%ymm1
+++	vpaddd	%ymm6,%ymm4,%ymm4
+++	vpaddd	%ymm6,%ymm5,%ymm5
+++	vpaddd	%ymm10,%ymm8,%ymm8
+++	vpaddd	%ymm10,%ymm9,%ymm9
+++	vpaddd	%ymm11,%ymm12,%ymm12
+++	vpaddd	%ymm15,%ymm13,%ymm13
+++	vperm2i128	$0x02,%ymm0,%ymm4,%ymm3
+++
+++	vpand	.Lclamp(%rip),%ymm3,%ymm3
+++	vmovdqa	%ymm3,0+0(%rbp)
+++
+++	vperm2i128	$0x13,%ymm0,%ymm4,%ymm0
+++	vperm2i128	$0x13,%ymm8,%ymm12,%ymm4
+++	vperm2i128	$0x02,%ymm1,%ymm5,%ymm8
+++	vperm2i128	$0x02,%ymm9,%ymm13,%ymm12
+++	vperm2i128	$0x13,%ymm1,%ymm5,%ymm1
+++	vperm2i128	$0x13,%ymm9,%ymm13,%ymm5
+++.Lseal_avx2_short:
+++	movq	%r8,%r8
+++	call	poly_hash_ad_internal
+++	xorq	%rcx,%rcx
+++.Lseal_avx2_short_hash_remainder:
+++	cmpq	$16,%rcx
+++	jb	.Lseal_avx2_short_loop
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	subq	$16,%rcx
+++	addq	$16,%rdi
+++	jmp	.Lseal_avx2_short_hash_remainder
+++.Lseal_avx2_short_loop:
+++	cmpq	$32,%rbx
+++	jb	.Lseal_avx2_short_tail
+++	subq	$32,%rbx
+++
+++	vpxor	(%rsi),%ymm0,%ymm0
+++	vmovdqu	%ymm0,(%rdi)
+++	leaq	32(%rsi),%rsi
+++
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++	addq	0+16(%rdi),%r10
+++	adcq	8+16(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	32(%rdi),%rdi
+++
+++	vmovdqa	%ymm4,%ymm0
+++	vmovdqa	%ymm8,%ymm4
+++	vmovdqa	%ymm12,%ymm8
+++	vmovdqa	%ymm1,%ymm12
+++	vmovdqa	%ymm5,%ymm1
+++	vmovdqa	%ymm9,%ymm5
+++	vmovdqa	%ymm13,%ymm9
+++	vmovdqa	%ymm2,%ymm13
+++	vmovdqa	%ymm6,%ymm2
+++	jmp	.Lseal_avx2_short_loop
+++.Lseal_avx2_short_tail:
+++	cmpq	$16,%rbx
+++	jb	.Lseal_avx2_exit
+++	subq	$16,%rbx
+++	vpxor	(%rsi),%xmm0,%xmm3
+++	vmovdqu	%xmm3,(%rdi)
+++	leaq	16(%rsi),%rsi
+++	addq	0+0(%rdi),%r10
+++	adcq	8+0(%rdi),%r11
+++	adcq	$1,%r12
+++	movq	0+0+0(%rbp),%rax
+++	movq	%rax,%r15
+++	mulq	%r10
+++	movq	%rax,%r13
+++	movq	%rdx,%r14
+++	movq	0+0+0(%rbp),%rax
+++	mulq	%r11
+++	imulq	%r12,%r15
+++	addq	%rax,%r14
+++	adcq	%rdx,%r15
+++	movq	8+0+0(%rbp),%rax
+++	movq	%rax,%r9
+++	mulq	%r10
+++	addq	%rax,%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++	movq	8+0+0(%rbp),%rax
+++	mulq	%r11
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	imulq	%r12,%r9
+++	addq	%r10,%r15
+++	adcq	%rdx,%r9
+++	movq	%r13,%r10
+++	movq	%r14,%r11
+++	movq	%r15,%r12
+++	andq	$3,%r12
+++	movq	%r15,%r13
+++	andq	$-4,%r13
+++	movq	%r9,%r14
+++	shrdq	$2,%r9,%r15
+++	shrq	$2,%r9
+++	addq	%r13,%r15
+++	adcq	%r14,%r9
+++	addq	%r15,%r10
+++	adcq	%r9,%r11
+++	adcq	$0,%r12
+++
+++	leaq	16(%rdi),%rdi
+++	vextracti128	$1,%ymm0,%xmm0
+++.Lseal_avx2_exit:
+++	vzeroupper
+++	jmp	.Lseal_sse_tail_16
+++.cfi_endproc	
+++.size	chacha20_poly1305_seal_avx2, .-chacha20_poly1305_seal_avx2
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/aesni-gcm-x86_64.S b/linux-x86_64/ypto/fipsmodule/aesni-gcm-x86_64.S
++new file mode 100644
++index 000000000..65ab5c78f
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/aesni-gcm-x86_64.S
++@@ -0,0 +1,852 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++
+++.type	_aesni_ctr32_ghash_6x,@function
+++.align	32
+++_aesni_ctr32_ghash_6x:
+++.cfi_startproc	
+++	vmovdqu	32(%r11),%xmm2
+++	subq	$6,%rdx
+++	vpxor	%xmm4,%xmm4,%xmm4
+++	vmovdqu	0-128(%rcx),%xmm15
+++	vpaddb	%xmm2,%xmm1,%xmm10
+++	vpaddb	%xmm2,%xmm10,%xmm11
+++	vpaddb	%xmm2,%xmm11,%xmm12
+++	vpaddb	%xmm2,%xmm12,%xmm13
+++	vpaddb	%xmm2,%xmm13,%xmm14
+++	vpxor	%xmm15,%xmm1,%xmm9
+++	vmovdqu	%xmm4,16+8(%rsp)
+++	jmp	.Loop6x
+++
+++.align	32
+++.Loop6x:
+++	addl	$100663296,%ebx
+++	jc	.Lhandle_ctr32
+++	vmovdqu	0-32(%r9),%xmm3
+++	vpaddb	%xmm2,%xmm14,%xmm1
+++	vpxor	%xmm15,%xmm10,%xmm10
+++	vpxor	%xmm15,%xmm11,%xmm11
+++
+++.Lresume_ctr32:
+++	vmovdqu	%xmm1,(%r8)
+++	vpclmulqdq	$0x10,%xmm3,%xmm7,%xmm5
+++	vpxor	%xmm15,%xmm12,%xmm12
+++	vmovups	16-128(%rcx),%xmm2
+++	vpclmulqdq	$0x01,%xmm3,%xmm7,%xmm6
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	xorq	%r12,%r12
+++	cmpq	%r14,%r15
+++
+++	vaesenc	%xmm2,%xmm9,%xmm9
+++	vmovdqu	48+8(%rsp),%xmm0
+++	vpxor	%xmm15,%xmm13,%xmm13
+++	vpclmulqdq	$0x00,%xmm3,%xmm7,%xmm1
+++	vaesenc	%xmm2,%xmm10,%xmm10
+++	vpxor	%xmm15,%xmm14,%xmm14
+++	setnc	%r12b
+++	vpclmulqdq	$0x11,%xmm3,%xmm7,%xmm7
+++	vaesenc	%xmm2,%xmm11,%xmm11
+++	vmovdqu	16-32(%r9),%xmm3
+++	negq	%r12
+++	vaesenc	%xmm2,%xmm12,%xmm12
+++	vpxor	%xmm5,%xmm6,%xmm6
+++	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm5
+++	vpxor	%xmm4,%xmm8,%xmm8
+++	vaesenc	%xmm2,%xmm13,%xmm13
+++	vpxor	%xmm5,%xmm1,%xmm4
+++	andq	$0x60,%r12
+++	vmovups	32-128(%rcx),%xmm15
+++	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm1
+++	vaesenc	%xmm2,%xmm14,%xmm14
+++
+++	vpclmulqdq	$0x01,%xmm3,%xmm0,%xmm2
+++	leaq	(%r14,%r12,1),%r14
+++	vaesenc	%xmm15,%xmm9,%xmm9
+++	vpxor	16+8(%rsp),%xmm8,%xmm8
+++	vpclmulqdq	$0x11,%xmm3,%xmm0,%xmm3
+++	vmovdqu	64+8(%rsp),%xmm0
+++	vaesenc	%xmm15,%xmm10,%xmm10
+++	movbeq	88(%r14),%r13
+++	vaesenc	%xmm15,%xmm11,%xmm11
+++	movbeq	80(%r14),%r12
+++	vaesenc	%xmm15,%xmm12,%xmm12
+++	movq	%r13,32+8(%rsp)
+++	vaesenc	%xmm15,%xmm13,%xmm13
+++	movq	%r12,40+8(%rsp)
+++	vmovdqu	48-32(%r9),%xmm5
+++	vaesenc	%xmm15,%xmm14,%xmm14
+++
+++	vmovups	48-128(%rcx),%xmm15
+++	vpxor	%xmm1,%xmm6,%xmm6
+++	vpclmulqdq	$0x00,%xmm5,%xmm0,%xmm1
+++	vaesenc	%xmm15,%xmm9,%xmm9
+++	vpxor	%xmm2,%xmm6,%xmm6
+++	vpclmulqdq	$0x10,%xmm5,%xmm0,%xmm2
+++	vaesenc	%xmm15,%xmm10,%xmm10
+++	vpxor	%xmm3,%xmm7,%xmm7
+++	vpclmulqdq	$0x01,%xmm5,%xmm0,%xmm3
+++	vaesenc	%xmm15,%xmm11,%xmm11
+++	vpclmulqdq	$0x11,%xmm5,%xmm0,%xmm5
+++	vmovdqu	80+8(%rsp),%xmm0
+++	vaesenc	%xmm15,%xmm12,%xmm12
+++	vaesenc	%xmm15,%xmm13,%xmm13
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vmovdqu	64-32(%r9),%xmm1
+++	vaesenc	%xmm15,%xmm14,%xmm14
+++
+++	vmovups	64-128(%rcx),%xmm15
+++	vpxor	%xmm2,%xmm6,%xmm6
+++	vpclmulqdq	$0x00,%xmm1,%xmm0,%xmm2
+++	vaesenc	%xmm15,%xmm9,%xmm9
+++	vpxor	%xmm3,%xmm6,%xmm6
+++	vpclmulqdq	$0x10,%xmm1,%xmm0,%xmm3
+++	vaesenc	%xmm15,%xmm10,%xmm10
+++	movbeq	72(%r14),%r13
+++	vpxor	%xmm5,%xmm7,%xmm7
+++	vpclmulqdq	$0x01,%xmm1,%xmm0,%xmm5
+++	vaesenc	%xmm15,%xmm11,%xmm11
+++	movbeq	64(%r14),%r12
+++	vpclmulqdq	$0x11,%xmm1,%xmm0,%xmm1
+++	vmovdqu	96+8(%rsp),%xmm0
+++	vaesenc	%xmm15,%xmm12,%xmm12
+++	movq	%r13,48+8(%rsp)
+++	vaesenc	%xmm15,%xmm13,%xmm13
+++	movq	%r12,56+8(%rsp)
+++	vpxor	%xmm2,%xmm4,%xmm4
+++	vmovdqu	96-32(%r9),%xmm2
+++	vaesenc	%xmm15,%xmm14,%xmm14
+++
+++	vmovups	80-128(%rcx),%xmm15
+++	vpxor	%xmm3,%xmm6,%xmm6
+++	vpclmulqdq	$0x00,%xmm2,%xmm0,%xmm3
+++	vaesenc	%xmm15,%xmm9,%xmm9
+++	vpxor	%xmm5,%xmm6,%xmm6
+++	vpclmulqdq	$0x10,%xmm2,%xmm0,%xmm5
+++	vaesenc	%xmm15,%xmm10,%xmm10
+++	movbeq	56(%r14),%r13
+++	vpxor	%xmm1,%xmm7,%xmm7
+++	vpclmulqdq	$0x01,%xmm2,%xmm0,%xmm1
+++	vpxor	112+8(%rsp),%xmm8,%xmm8
+++	vaesenc	%xmm15,%xmm11,%xmm11
+++	movbeq	48(%r14),%r12
+++	vpclmulqdq	$0x11,%xmm2,%xmm0,%xmm2
+++	vaesenc	%xmm15,%xmm12,%xmm12
+++	movq	%r13,64+8(%rsp)
+++	vaesenc	%xmm15,%xmm13,%xmm13
+++	movq	%r12,72+8(%rsp)
+++	vpxor	%xmm3,%xmm4,%xmm4
+++	vmovdqu	112-32(%r9),%xmm3
+++	vaesenc	%xmm15,%xmm14,%xmm14
+++
+++	vmovups	96-128(%rcx),%xmm15
+++	vpxor	%xmm5,%xmm6,%xmm6
+++	vpclmulqdq	$0x10,%xmm3,%xmm8,%xmm5
+++	vaesenc	%xmm15,%xmm9,%xmm9
+++	vpxor	%xmm1,%xmm6,%xmm6
+++	vpclmulqdq	$0x01,%xmm3,%xmm8,%xmm1
+++	vaesenc	%xmm15,%xmm10,%xmm10
+++	movbeq	40(%r14),%r13
+++	vpxor	%xmm2,%xmm7,%xmm7
+++	vpclmulqdq	$0x00,%xmm3,%xmm8,%xmm2
+++	vaesenc	%xmm15,%xmm11,%xmm11
+++	movbeq	32(%r14),%r12
+++	vpclmulqdq	$0x11,%xmm3,%xmm8,%xmm8
+++	vaesenc	%xmm15,%xmm12,%xmm12
+++	movq	%r13,80+8(%rsp)
+++	vaesenc	%xmm15,%xmm13,%xmm13
+++	movq	%r12,88+8(%rsp)
+++	vpxor	%xmm5,%xmm6,%xmm6
+++	vaesenc	%xmm15,%xmm14,%xmm14
+++	vpxor	%xmm1,%xmm6,%xmm6
+++
+++	vmovups	112-128(%rcx),%xmm15
+++	vpslldq	$8,%xmm6,%xmm5
+++	vpxor	%xmm2,%xmm4,%xmm4
+++	vmovdqu	16(%r11),%xmm3
+++
+++	vaesenc	%xmm15,%xmm9,%xmm9
+++	vpxor	%xmm8,%xmm7,%xmm7
+++	vaesenc	%xmm15,%xmm10,%xmm10
+++	vpxor	%xmm5,%xmm4,%xmm4
+++	movbeq	24(%r14),%r13
+++	vaesenc	%xmm15,%xmm11,%xmm11
+++	movbeq	16(%r14),%r12
+++	vpalignr	$8,%xmm4,%xmm4,%xmm0
+++	vpclmulqdq	$0x10,%xmm3,%xmm4,%xmm4
+++	movq	%r13,96+8(%rsp)
+++	vaesenc	%xmm15,%xmm12,%xmm12
+++	movq	%r12,104+8(%rsp)
+++	vaesenc	%xmm15,%xmm13,%xmm13
+++	vmovups	128-128(%rcx),%xmm1
+++	vaesenc	%xmm15,%xmm14,%xmm14
+++
+++	vaesenc	%xmm1,%xmm9,%xmm9
+++	vmovups	144-128(%rcx),%xmm15
+++	vaesenc	%xmm1,%xmm10,%xmm10
+++	vpsrldq	$8,%xmm6,%xmm6
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vpxor	%xmm6,%xmm7,%xmm7
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++	vpxor	%xmm0,%xmm4,%xmm4
+++	movbeq	8(%r14),%r13
+++	vaesenc	%xmm1,%xmm13,%xmm13
+++	movbeq	0(%r14),%r12
+++	vaesenc	%xmm1,%xmm14,%xmm14
+++	vmovups	160-128(%rcx),%xmm1
+++	cmpl	$11,%ebp
+++	jb	.Lenc_tail
+++
+++	vaesenc	%xmm15,%xmm9,%xmm9
+++	vaesenc	%xmm15,%xmm10,%xmm10
+++	vaesenc	%xmm15,%xmm11,%xmm11
+++	vaesenc	%xmm15,%xmm12,%xmm12
+++	vaesenc	%xmm15,%xmm13,%xmm13
+++	vaesenc	%xmm15,%xmm14,%xmm14
+++
+++	vaesenc	%xmm1,%xmm9,%xmm9
+++	vaesenc	%xmm1,%xmm10,%xmm10
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++	vaesenc	%xmm1,%xmm13,%xmm13
+++	vmovups	176-128(%rcx),%xmm15
+++	vaesenc	%xmm1,%xmm14,%xmm14
+++	vmovups	192-128(%rcx),%xmm1
+++	je	.Lenc_tail
+++
+++	vaesenc	%xmm15,%xmm9,%xmm9
+++	vaesenc	%xmm15,%xmm10,%xmm10
+++	vaesenc	%xmm15,%xmm11,%xmm11
+++	vaesenc	%xmm15,%xmm12,%xmm12
+++	vaesenc	%xmm15,%xmm13,%xmm13
+++	vaesenc	%xmm15,%xmm14,%xmm14
+++
+++	vaesenc	%xmm1,%xmm9,%xmm9
+++	vaesenc	%xmm1,%xmm10,%xmm10
+++	vaesenc	%xmm1,%xmm11,%xmm11
+++	vaesenc	%xmm1,%xmm12,%xmm12
+++	vaesenc	%xmm1,%xmm13,%xmm13
+++	vmovups	208-128(%rcx),%xmm15
+++	vaesenc	%xmm1,%xmm14,%xmm14
+++	vmovups	224-128(%rcx),%xmm1
+++	jmp	.Lenc_tail
+++
+++.align	32
+++.Lhandle_ctr32:
+++	vmovdqu	(%r11),%xmm0
+++	vpshufb	%xmm0,%xmm1,%xmm6
+++	vmovdqu	48(%r11),%xmm5
+++	vpaddd	64(%r11),%xmm6,%xmm10
+++	vpaddd	%xmm5,%xmm6,%xmm11
+++	vmovdqu	0-32(%r9),%xmm3
+++	vpaddd	%xmm5,%xmm10,%xmm12
+++	vpshufb	%xmm0,%xmm10,%xmm10
+++	vpaddd	%xmm5,%xmm11,%xmm13
+++	vpshufb	%xmm0,%xmm11,%xmm11
+++	vpxor	%xmm15,%xmm10,%xmm10
+++	vpaddd	%xmm5,%xmm12,%xmm14
+++	vpshufb	%xmm0,%xmm12,%xmm12
+++	vpxor	%xmm15,%xmm11,%xmm11
+++	vpaddd	%xmm5,%xmm13,%xmm1
+++	vpshufb	%xmm0,%xmm13,%xmm13
+++	vpshufb	%xmm0,%xmm14,%xmm14
+++	vpshufb	%xmm0,%xmm1,%xmm1
+++	jmp	.Lresume_ctr32
+++
+++.align	32
+++.Lenc_tail:
+++	vaesenc	%xmm15,%xmm9,%xmm9
+++	vmovdqu	%xmm7,16+8(%rsp)
+++	vpalignr	$8,%xmm4,%xmm4,%xmm8
+++	vaesenc	%xmm15,%xmm10,%xmm10
+++	vpclmulqdq	$0x10,%xmm3,%xmm4,%xmm4
+++	vpxor	0(%rdi),%xmm1,%xmm2
+++	vaesenc	%xmm15,%xmm11,%xmm11
+++	vpxor	16(%rdi),%xmm1,%xmm0
+++	vaesenc	%xmm15,%xmm12,%xmm12
+++	vpxor	32(%rdi),%xmm1,%xmm5
+++	vaesenc	%xmm15,%xmm13,%xmm13
+++	vpxor	48(%rdi),%xmm1,%xmm6
+++	vaesenc	%xmm15,%xmm14,%xmm14
+++	vpxor	64(%rdi),%xmm1,%xmm7
+++	vpxor	80(%rdi),%xmm1,%xmm3
+++	vmovdqu	(%r8),%xmm1
+++
+++	vaesenclast	%xmm2,%xmm9,%xmm9
+++	vmovdqu	32(%r11),%xmm2
+++	vaesenclast	%xmm0,%xmm10,%xmm10
+++	vpaddb	%xmm2,%xmm1,%xmm0
+++	movq	%r13,112+8(%rsp)
+++	leaq	96(%rdi),%rdi
+++	vaesenclast	%xmm5,%xmm11,%xmm11
+++	vpaddb	%xmm2,%xmm0,%xmm5
+++	movq	%r12,120+8(%rsp)
+++	leaq	96(%rsi),%rsi
+++	vmovdqu	0-128(%rcx),%xmm15
+++	vaesenclast	%xmm6,%xmm12,%xmm12
+++	vpaddb	%xmm2,%xmm5,%xmm6
+++	vaesenclast	%xmm7,%xmm13,%xmm13
+++	vpaddb	%xmm2,%xmm6,%xmm7
+++	vaesenclast	%xmm3,%xmm14,%xmm14
+++	vpaddb	%xmm2,%xmm7,%xmm3
+++
+++	addq	$0x60,%r10
+++	subq	$0x6,%rdx
+++	jc	.L6x_done
+++
+++	vmovups	%xmm9,-96(%rsi)
+++	vpxor	%xmm15,%xmm1,%xmm9
+++	vmovups	%xmm10,-80(%rsi)
+++	vmovdqa	%xmm0,%xmm10
+++	vmovups	%xmm11,-64(%rsi)
+++	vmovdqa	%xmm5,%xmm11
+++	vmovups	%xmm12,-48(%rsi)
+++	vmovdqa	%xmm6,%xmm12
+++	vmovups	%xmm13,-32(%rsi)
+++	vmovdqa	%xmm7,%xmm13
+++	vmovups	%xmm14,-16(%rsi)
+++	vmovdqa	%xmm3,%xmm14
+++	vmovdqu	32+8(%rsp),%xmm7
+++	jmp	.Loop6x
+++
+++.L6x_done:
+++	vpxor	16+8(%rsp),%xmm8,%xmm8
+++	vpxor	%xmm4,%xmm8,%xmm8
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_aesni_ctr32_ghash_6x,.-_aesni_ctr32_ghash_6x
+++.globl	aesni_gcm_decrypt
+++.hidden aesni_gcm_decrypt
+++.type	aesni_gcm_decrypt,@function
+++.align	32
+++aesni_gcm_decrypt:
+++.cfi_startproc	
+++	xorq	%r10,%r10
+++
+++
+++
+++	cmpq	$0x60,%rdx
+++	jb	.Lgcm_dec_abort
+++
+++	leaq	(%rsp),%rax
+++.cfi_def_cfa_register	%rax
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++	vzeroupper
+++
+++	vmovdqu	(%r8),%xmm1
+++	addq	$-128,%rsp
+++	movl	12(%r8),%ebx
+++	leaq	.Lbswap_mask(%rip),%r11
+++	leaq	-128(%rcx),%r14
+++	movq	$0xf80,%r15
+++	vmovdqu	(%r9),%xmm8
+++	andq	$-128,%rsp
+++	vmovdqu	(%r11),%xmm0
+++	leaq	128(%rcx),%rcx
+++	leaq	32+32(%r9),%r9
+++	movl	240-128(%rcx),%ebp
+++	vpshufb	%xmm0,%xmm8,%xmm8
+++
+++	andq	%r15,%r14
+++	andq	%rsp,%r15
+++	subq	%r14,%r15
+++	jc	.Ldec_no_key_aliasing
+++	cmpq	$768,%r15
+++	jnc	.Ldec_no_key_aliasing
+++	subq	%r15,%rsp
+++.Ldec_no_key_aliasing:
+++
+++	vmovdqu	80(%rdi),%xmm7
+++	leaq	(%rdi),%r14
+++	vmovdqu	64(%rdi),%xmm4
+++
+++
+++
+++
+++
+++
+++
+++	leaq	-192(%rdi,%rdx,1),%r15
+++
+++	vmovdqu	48(%rdi),%xmm5
+++	shrq	$4,%rdx
+++	xorq	%r10,%r10
+++	vmovdqu	32(%rdi),%xmm6
+++	vpshufb	%xmm0,%xmm7,%xmm7
+++	vmovdqu	16(%rdi),%xmm2
+++	vpshufb	%xmm0,%xmm4,%xmm4
+++	vmovdqu	(%rdi),%xmm3
+++	vpshufb	%xmm0,%xmm5,%xmm5
+++	vmovdqu	%xmm4,48(%rsp)
+++	vpshufb	%xmm0,%xmm6,%xmm6
+++	vmovdqu	%xmm5,64(%rsp)
+++	vpshufb	%xmm0,%xmm2,%xmm2
+++	vmovdqu	%xmm6,80(%rsp)
+++	vpshufb	%xmm0,%xmm3,%xmm3
+++	vmovdqu	%xmm2,96(%rsp)
+++	vmovdqu	%xmm3,112(%rsp)
+++
+++	call	_aesni_ctr32_ghash_6x
+++
+++	vmovups	%xmm9,-96(%rsi)
+++	vmovups	%xmm10,-80(%rsi)
+++	vmovups	%xmm11,-64(%rsi)
+++	vmovups	%xmm12,-48(%rsi)
+++	vmovups	%xmm13,-32(%rsi)
+++	vmovups	%xmm14,-16(%rsi)
+++
+++	vpshufb	(%r11),%xmm8,%xmm8
+++	vmovdqu	%xmm8,-64(%r9)
+++
+++	vzeroupper
+++	movq	-48(%rax),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rax),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rax),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rax),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rax),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rax),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rax),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lgcm_dec_abort:
+++	movq	%r10,%rax
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aesni_gcm_decrypt,.-aesni_gcm_decrypt
+++.type	_aesni_ctr32_6x,@function
+++.align	32
+++_aesni_ctr32_6x:
+++.cfi_startproc	
+++	vmovdqu	0-128(%rcx),%xmm4
+++	vmovdqu	32(%r11),%xmm2
+++	leaq	-1(%rbp),%r13
+++	vmovups	16-128(%rcx),%xmm15
+++	leaq	32-128(%rcx),%r12
+++	vpxor	%xmm4,%xmm1,%xmm9
+++	addl	$100663296,%ebx
+++	jc	.Lhandle_ctr32_2
+++	vpaddb	%xmm2,%xmm1,%xmm10
+++	vpaddb	%xmm2,%xmm10,%xmm11
+++	vpxor	%xmm4,%xmm10,%xmm10
+++	vpaddb	%xmm2,%xmm11,%xmm12
+++	vpxor	%xmm4,%xmm11,%xmm11
+++	vpaddb	%xmm2,%xmm12,%xmm13
+++	vpxor	%xmm4,%xmm12,%xmm12
+++	vpaddb	%xmm2,%xmm13,%xmm14
+++	vpxor	%xmm4,%xmm13,%xmm13
+++	vpaddb	%xmm2,%xmm14,%xmm1
+++	vpxor	%xmm4,%xmm14,%xmm14
+++	jmp	.Loop_ctr32
+++
+++.align	16
+++.Loop_ctr32:
+++	vaesenc	%xmm15,%xmm9,%xmm9
+++	vaesenc	%xmm15,%xmm10,%xmm10
+++	vaesenc	%xmm15,%xmm11,%xmm11
+++	vaesenc	%xmm15,%xmm12,%xmm12
+++	vaesenc	%xmm15,%xmm13,%xmm13
+++	vaesenc	%xmm15,%xmm14,%xmm14
+++	vmovups	(%r12),%xmm15
+++	leaq	16(%r12),%r12
+++	decl	%r13d
+++	jnz	.Loop_ctr32
+++
+++	vmovdqu	(%r12),%xmm3
+++	vaesenc	%xmm15,%xmm9,%xmm9
+++	vpxor	0(%rdi),%xmm3,%xmm4
+++	vaesenc	%xmm15,%xmm10,%xmm10
+++	vpxor	16(%rdi),%xmm3,%xmm5
+++	vaesenc	%xmm15,%xmm11,%xmm11
+++	vpxor	32(%rdi),%xmm3,%xmm6
+++	vaesenc	%xmm15,%xmm12,%xmm12
+++	vpxor	48(%rdi),%xmm3,%xmm8
+++	vaesenc	%xmm15,%xmm13,%xmm13
+++	vpxor	64(%rdi),%xmm3,%xmm2
+++	vaesenc	%xmm15,%xmm14,%xmm14
+++	vpxor	80(%rdi),%xmm3,%xmm3
+++	leaq	96(%rdi),%rdi
+++
+++	vaesenclast	%xmm4,%xmm9,%xmm9
+++	vaesenclast	%xmm5,%xmm10,%xmm10
+++	vaesenclast	%xmm6,%xmm11,%xmm11
+++	vaesenclast	%xmm8,%xmm12,%xmm12
+++	vaesenclast	%xmm2,%xmm13,%xmm13
+++	vaesenclast	%xmm3,%xmm14,%xmm14
+++	vmovups	%xmm9,0(%rsi)
+++	vmovups	%xmm10,16(%rsi)
+++	vmovups	%xmm11,32(%rsi)
+++	vmovups	%xmm12,48(%rsi)
+++	vmovups	%xmm13,64(%rsi)
+++	vmovups	%xmm14,80(%rsi)
+++	leaq	96(%rsi),%rsi
+++
+++	.byte	0xf3,0xc3
+++.align	32
+++.Lhandle_ctr32_2:
+++	vpshufb	%xmm0,%xmm1,%xmm6
+++	vmovdqu	48(%r11),%xmm5
+++	vpaddd	64(%r11),%xmm6,%xmm10
+++	vpaddd	%xmm5,%xmm6,%xmm11
+++	vpaddd	%xmm5,%xmm10,%xmm12
+++	vpshufb	%xmm0,%xmm10,%xmm10
+++	vpaddd	%xmm5,%xmm11,%xmm13
+++	vpshufb	%xmm0,%xmm11,%xmm11
+++	vpxor	%xmm4,%xmm10,%xmm10
+++	vpaddd	%xmm5,%xmm12,%xmm14
+++	vpshufb	%xmm0,%xmm12,%xmm12
+++	vpxor	%xmm4,%xmm11,%xmm11
+++	vpaddd	%xmm5,%xmm13,%xmm1
+++	vpshufb	%xmm0,%xmm13,%xmm13
+++	vpxor	%xmm4,%xmm12,%xmm12
+++	vpshufb	%xmm0,%xmm14,%xmm14
+++	vpxor	%xmm4,%xmm13,%xmm13
+++	vpshufb	%xmm0,%xmm1,%xmm1
+++	vpxor	%xmm4,%xmm14,%xmm14
+++	jmp	.Loop_ctr32
+++.cfi_endproc	
+++.size	_aesni_ctr32_6x,.-_aesni_ctr32_6x
+++
+++.globl	aesni_gcm_encrypt
+++.hidden aesni_gcm_encrypt
+++.type	aesni_gcm_encrypt,@function
+++.align	32
+++aesni_gcm_encrypt:
+++.cfi_startproc	
+++#ifdef BORINGSSL_DISPATCH_TEST
+++.extern	BORINGSSL_function_hit
+++.hidden BORINGSSL_function_hit
+++	movb	$1,BORINGSSL_function_hit+2(%rip)
+++#endif
+++	xorq	%r10,%r10
+++
+++
+++
+++
+++	cmpq	$288,%rdx
+++	jb	.Lgcm_enc_abort
+++
+++	leaq	(%rsp),%rax
+++.cfi_def_cfa_register	%rax
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++	vzeroupper
+++
+++	vmovdqu	(%r8),%xmm1
+++	addq	$-128,%rsp
+++	movl	12(%r8),%ebx
+++	leaq	.Lbswap_mask(%rip),%r11
+++	leaq	-128(%rcx),%r14
+++	movq	$0xf80,%r15
+++	leaq	128(%rcx),%rcx
+++	vmovdqu	(%r11),%xmm0
+++	andq	$-128,%rsp
+++	movl	240-128(%rcx),%ebp
+++
+++	andq	%r15,%r14
+++	andq	%rsp,%r15
+++	subq	%r14,%r15
+++	jc	.Lenc_no_key_aliasing
+++	cmpq	$768,%r15
+++	jnc	.Lenc_no_key_aliasing
+++	subq	%r15,%rsp
+++.Lenc_no_key_aliasing:
+++
+++	leaq	(%rsi),%r14
+++
+++
+++
+++
+++
+++
+++
+++
+++	leaq	-192(%rsi,%rdx,1),%r15
+++
+++	shrq	$4,%rdx
+++
+++	call	_aesni_ctr32_6x
+++	vpshufb	%xmm0,%xmm9,%xmm8
+++	vpshufb	%xmm0,%xmm10,%xmm2
+++	vmovdqu	%xmm8,112(%rsp)
+++	vpshufb	%xmm0,%xmm11,%xmm4
+++	vmovdqu	%xmm2,96(%rsp)
+++	vpshufb	%xmm0,%xmm12,%xmm5
+++	vmovdqu	%xmm4,80(%rsp)
+++	vpshufb	%xmm0,%xmm13,%xmm6
+++	vmovdqu	%xmm5,64(%rsp)
+++	vpshufb	%xmm0,%xmm14,%xmm7
+++	vmovdqu	%xmm6,48(%rsp)
+++
+++	call	_aesni_ctr32_6x
+++
+++	vmovdqu	(%r9),%xmm8
+++	leaq	32+32(%r9),%r9
+++	subq	$12,%rdx
+++	movq	$192,%r10
+++	vpshufb	%xmm0,%xmm8,%xmm8
+++
+++	call	_aesni_ctr32_ghash_6x
+++	vmovdqu	32(%rsp),%xmm7
+++	vmovdqu	(%r11),%xmm0
+++	vmovdqu	0-32(%r9),%xmm3
+++	vpunpckhqdq	%xmm7,%xmm7,%xmm1
+++	vmovdqu	32-32(%r9),%xmm15
+++	vmovups	%xmm9,-96(%rsi)
+++	vpshufb	%xmm0,%xmm9,%xmm9
+++	vpxor	%xmm7,%xmm1,%xmm1
+++	vmovups	%xmm10,-80(%rsi)
+++	vpshufb	%xmm0,%xmm10,%xmm10
+++	vmovups	%xmm11,-64(%rsi)
+++	vpshufb	%xmm0,%xmm11,%xmm11
+++	vmovups	%xmm12,-48(%rsi)
+++	vpshufb	%xmm0,%xmm12,%xmm12
+++	vmovups	%xmm13,-32(%rsi)
+++	vpshufb	%xmm0,%xmm13,%xmm13
+++	vmovups	%xmm14,-16(%rsi)
+++	vpshufb	%xmm0,%xmm14,%xmm14
+++	vmovdqu	%xmm9,16(%rsp)
+++	vmovdqu	48(%rsp),%xmm6
+++	vmovdqu	16-32(%r9),%xmm0
+++	vpunpckhqdq	%xmm6,%xmm6,%xmm2
+++	vpclmulqdq	$0x00,%xmm3,%xmm7,%xmm5
+++	vpxor	%xmm6,%xmm2,%xmm2
+++	vpclmulqdq	$0x11,%xmm3,%xmm7,%xmm7
+++	vpclmulqdq	$0x00,%xmm15,%xmm1,%xmm1
+++
+++	vmovdqu	64(%rsp),%xmm9
+++	vpclmulqdq	$0x00,%xmm0,%xmm6,%xmm4
+++	vmovdqu	48-32(%r9),%xmm3
+++	vpxor	%xmm5,%xmm4,%xmm4
+++	vpunpckhqdq	%xmm9,%xmm9,%xmm5
+++	vpclmulqdq	$0x11,%xmm0,%xmm6,%xmm6
+++	vpxor	%xmm9,%xmm5,%xmm5
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	vpclmulqdq	$0x10,%xmm15,%xmm2,%xmm2
+++	vmovdqu	80-32(%r9),%xmm15
+++	vpxor	%xmm1,%xmm2,%xmm2
+++
+++	vmovdqu	80(%rsp),%xmm1
+++	vpclmulqdq	$0x00,%xmm3,%xmm9,%xmm7
+++	vmovdqu	64-32(%r9),%xmm0
+++	vpxor	%xmm4,%xmm7,%xmm7
+++	vpunpckhqdq	%xmm1,%xmm1,%xmm4
+++	vpclmulqdq	$0x11,%xmm3,%xmm9,%xmm9
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpxor	%xmm6,%xmm9,%xmm9
+++	vpclmulqdq	$0x00,%xmm15,%xmm5,%xmm5
+++	vpxor	%xmm2,%xmm5,%xmm5
+++
+++	vmovdqu	96(%rsp),%xmm2
+++	vpclmulqdq	$0x00,%xmm0,%xmm1,%xmm6
+++	vmovdqu	96-32(%r9),%xmm3
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	vpunpckhqdq	%xmm2,%xmm2,%xmm7
+++	vpclmulqdq	$0x11,%xmm0,%xmm1,%xmm1
+++	vpxor	%xmm2,%xmm7,%xmm7
+++	vpxor	%xmm9,%xmm1,%xmm1
+++	vpclmulqdq	$0x10,%xmm15,%xmm4,%xmm4
+++	vmovdqu	128-32(%r9),%xmm15
+++	vpxor	%xmm5,%xmm4,%xmm4
+++
+++	vpxor	112(%rsp),%xmm8,%xmm8
+++	vpclmulqdq	$0x00,%xmm3,%xmm2,%xmm5
+++	vmovdqu	112-32(%r9),%xmm0
+++	vpunpckhqdq	%xmm8,%xmm8,%xmm9
+++	vpxor	%xmm6,%xmm5,%xmm5
+++	vpclmulqdq	$0x11,%xmm3,%xmm2,%xmm2
+++	vpxor	%xmm8,%xmm9,%xmm9
+++	vpxor	%xmm1,%xmm2,%xmm2
+++	vpclmulqdq	$0x00,%xmm15,%xmm7,%xmm7
+++	vpxor	%xmm4,%xmm7,%xmm4
+++
+++	vpclmulqdq	$0x00,%xmm0,%xmm8,%xmm6
+++	vmovdqu	0-32(%r9),%xmm3
+++	vpunpckhqdq	%xmm14,%xmm14,%xmm1
+++	vpclmulqdq	$0x11,%xmm0,%xmm8,%xmm8
+++	vpxor	%xmm14,%xmm1,%xmm1
+++	vpxor	%xmm5,%xmm6,%xmm5
+++	vpclmulqdq	$0x10,%xmm15,%xmm9,%xmm9
+++	vmovdqu	32-32(%r9),%xmm15
+++	vpxor	%xmm2,%xmm8,%xmm7
+++	vpxor	%xmm4,%xmm9,%xmm6
+++
+++	vmovdqu	16-32(%r9),%xmm0
+++	vpxor	%xmm5,%xmm7,%xmm9
+++	vpclmulqdq	$0x00,%xmm3,%xmm14,%xmm4
+++	vpxor	%xmm9,%xmm6,%xmm6
+++	vpunpckhqdq	%xmm13,%xmm13,%xmm2
+++	vpclmulqdq	$0x11,%xmm3,%xmm14,%xmm14
+++	vpxor	%xmm13,%xmm2,%xmm2
+++	vpslldq	$8,%xmm6,%xmm9
+++	vpclmulqdq	$0x00,%xmm15,%xmm1,%xmm1
+++	vpxor	%xmm9,%xmm5,%xmm8
+++	vpsrldq	$8,%xmm6,%xmm6
+++	vpxor	%xmm6,%xmm7,%xmm7
+++
+++	vpclmulqdq	$0x00,%xmm0,%xmm13,%xmm5
+++	vmovdqu	48-32(%r9),%xmm3
+++	vpxor	%xmm4,%xmm5,%xmm5
+++	vpunpckhqdq	%xmm12,%xmm12,%xmm9
+++	vpclmulqdq	$0x11,%xmm0,%xmm13,%xmm13
+++	vpxor	%xmm12,%xmm9,%xmm9
+++	vpxor	%xmm14,%xmm13,%xmm13
+++	vpalignr	$8,%xmm8,%xmm8,%xmm14
+++	vpclmulqdq	$0x10,%xmm15,%xmm2,%xmm2
+++	vmovdqu	80-32(%r9),%xmm15
+++	vpxor	%xmm1,%xmm2,%xmm2
+++
+++	vpclmulqdq	$0x00,%xmm3,%xmm12,%xmm4
+++	vmovdqu	64-32(%r9),%xmm0
+++	vpxor	%xmm5,%xmm4,%xmm4
+++	vpunpckhqdq	%xmm11,%xmm11,%xmm1
+++	vpclmulqdq	$0x11,%xmm3,%xmm12,%xmm12
+++	vpxor	%xmm11,%xmm1,%xmm1
+++	vpxor	%xmm13,%xmm12,%xmm12
+++	vxorps	16(%rsp),%xmm7,%xmm7
+++	vpclmulqdq	$0x00,%xmm15,%xmm9,%xmm9
+++	vpxor	%xmm2,%xmm9,%xmm9
+++
+++	vpclmulqdq	$0x10,16(%r11),%xmm8,%xmm8
+++	vxorps	%xmm14,%xmm8,%xmm8
+++
+++	vpclmulqdq	$0x00,%xmm0,%xmm11,%xmm5
+++	vmovdqu	96-32(%r9),%xmm3
+++	vpxor	%xmm4,%xmm5,%xmm5
+++	vpunpckhqdq	%xmm10,%xmm10,%xmm2
+++	vpclmulqdq	$0x11,%xmm0,%xmm11,%xmm11
+++	vpxor	%xmm10,%xmm2,%xmm2
+++	vpalignr	$8,%xmm8,%xmm8,%xmm14
+++	vpxor	%xmm12,%xmm11,%xmm11
+++	vpclmulqdq	$0x10,%xmm15,%xmm1,%xmm1
+++	vmovdqu	128-32(%r9),%xmm15
+++	vpxor	%xmm9,%xmm1,%xmm1
+++
+++	vxorps	%xmm7,%xmm14,%xmm14
+++	vpclmulqdq	$0x10,16(%r11),%xmm8,%xmm8
+++	vxorps	%xmm14,%xmm8,%xmm8
+++
+++	vpclmulqdq	$0x00,%xmm3,%xmm10,%xmm4
+++	vmovdqu	112-32(%r9),%xmm0
+++	vpxor	%xmm5,%xmm4,%xmm4
+++	vpunpckhqdq	%xmm8,%xmm8,%xmm9
+++	vpclmulqdq	$0x11,%xmm3,%xmm10,%xmm10
+++	vpxor	%xmm8,%xmm9,%xmm9
+++	vpxor	%xmm11,%xmm10,%xmm10
+++	vpclmulqdq	$0x00,%xmm15,%xmm2,%xmm2
+++	vpxor	%xmm1,%xmm2,%xmm2
+++
+++	vpclmulqdq	$0x00,%xmm0,%xmm8,%xmm5
+++	vpclmulqdq	$0x11,%xmm0,%xmm8,%xmm7
+++	vpxor	%xmm4,%xmm5,%xmm5
+++	vpclmulqdq	$0x10,%xmm15,%xmm9,%xmm6
+++	vpxor	%xmm10,%xmm7,%xmm7
+++	vpxor	%xmm2,%xmm6,%xmm6
+++
+++	vpxor	%xmm5,%xmm7,%xmm4
+++	vpxor	%xmm4,%xmm6,%xmm6
+++	vpslldq	$8,%xmm6,%xmm1
+++	vmovdqu	16(%r11),%xmm3
+++	vpsrldq	$8,%xmm6,%xmm6
+++	vpxor	%xmm1,%xmm5,%xmm8
+++	vpxor	%xmm6,%xmm7,%xmm7
+++
+++	vpalignr	$8,%xmm8,%xmm8,%xmm2
+++	vpclmulqdq	$0x10,%xmm3,%xmm8,%xmm8
+++	vpxor	%xmm2,%xmm8,%xmm8
+++
+++	vpalignr	$8,%xmm8,%xmm8,%xmm2
+++	vpclmulqdq	$0x10,%xmm3,%xmm8,%xmm8
+++	vpxor	%xmm7,%xmm2,%xmm2
+++	vpxor	%xmm2,%xmm8,%xmm8
+++	vpshufb	(%r11),%xmm8,%xmm8
+++	vmovdqu	%xmm8,-64(%r9)
+++
+++	vzeroupper
+++	movq	-48(%rax),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rax),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rax),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rax),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rax),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rax),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rax),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lgcm_enc_abort:
+++	movq	%r10,%rax
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aesni_gcm_encrypt,.-aesni_gcm_encrypt
+++.align	64
+++.Lbswap_mask:
+++.byte	15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0
+++.Lpoly:
+++.byte	0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0xc2
+++.Lone_msb:
+++.byte	0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
+++.Ltwo_lsb:
+++.byte	2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
+++.Lone_lsb:
+++.byte	1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
+++.byte	65,69,83,45,78,73,32,71,67,77,32,109,111,100,117,108,101,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
+++.align	64
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/aesni-x86_64.S b/linux-x86_64/ypto/fipsmodule/aesni-x86_64.S
++new file mode 100644
++index 000000000..b98107f36
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/aesni-x86_64.S
++@@ -0,0 +1,2506 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++.extern	OPENSSL_ia32cap_P
+++.hidden OPENSSL_ia32cap_P
+++.globl	aes_hw_encrypt
+++.hidden aes_hw_encrypt
+++.type	aes_hw_encrypt,@function
+++.align	16
+++aes_hw_encrypt:
+++.cfi_startproc	
+++#ifdef BORINGSSL_DISPATCH_TEST
+++.extern	BORINGSSL_function_hit
+++.hidden BORINGSSL_function_hit
+++	movb	$1,BORINGSSL_function_hit+1(%rip)
+++#endif
+++	movups	(%rdi),%xmm2
+++	movl	240(%rdx),%eax
+++	movups	(%rdx),%xmm0
+++	movups	16(%rdx),%xmm1
+++	leaq	32(%rdx),%rdx
+++	xorps	%xmm0,%xmm2
+++.Loop_enc1_1:
+++.byte	102,15,56,220,209
+++	decl	%eax
+++	movups	(%rdx),%xmm1
+++	leaq	16(%rdx),%rdx
+++	jnz	.Loop_enc1_1
+++.byte	102,15,56,221,209
+++	pxor	%xmm0,%xmm0
+++	pxor	%xmm1,%xmm1
+++	movups	%xmm2,(%rsi)
+++	pxor	%xmm2,%xmm2
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes_hw_encrypt,.-aes_hw_encrypt
+++
+++.globl	aes_hw_decrypt
+++.hidden aes_hw_decrypt
+++.type	aes_hw_decrypt,@function
+++.align	16
+++aes_hw_decrypt:
+++.cfi_startproc	
+++	movups	(%rdi),%xmm2
+++	movl	240(%rdx),%eax
+++	movups	(%rdx),%xmm0
+++	movups	16(%rdx),%xmm1
+++	leaq	32(%rdx),%rdx
+++	xorps	%xmm0,%xmm2
+++.Loop_dec1_2:
+++.byte	102,15,56,222,209
+++	decl	%eax
+++	movups	(%rdx),%xmm1
+++	leaq	16(%rdx),%rdx
+++	jnz	.Loop_dec1_2
+++.byte	102,15,56,223,209
+++	pxor	%xmm0,%xmm0
+++	pxor	%xmm1,%xmm1
+++	movups	%xmm2,(%rsi)
+++	pxor	%xmm2,%xmm2
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes_hw_decrypt, .-aes_hw_decrypt
+++.type	_aesni_encrypt2,@function
+++.align	16
+++_aesni_encrypt2:
+++.cfi_startproc	
+++	movups	(%rcx),%xmm0
+++	shll	$4,%eax
+++	movups	16(%rcx),%xmm1
+++	xorps	%xmm0,%xmm2
+++	xorps	%xmm0,%xmm3
+++	movups	32(%rcx),%xmm0
+++	leaq	32(%rcx,%rax,1),%rcx
+++	negq	%rax
+++	addq	$16,%rax
+++
+++.Lenc_loop2:
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++	movups	(%rcx,%rax,1),%xmm1
+++	addq	$32,%rax
+++.byte	102,15,56,220,208
+++.byte	102,15,56,220,216
+++	movups	-16(%rcx,%rax,1),%xmm0
+++	jnz	.Lenc_loop2
+++
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++.byte	102,15,56,221,208
+++.byte	102,15,56,221,216
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_aesni_encrypt2,.-_aesni_encrypt2
+++.type	_aesni_decrypt2,@function
+++.align	16
+++_aesni_decrypt2:
+++.cfi_startproc	
+++	movups	(%rcx),%xmm0
+++	shll	$4,%eax
+++	movups	16(%rcx),%xmm1
+++	xorps	%xmm0,%xmm2
+++	xorps	%xmm0,%xmm3
+++	movups	32(%rcx),%xmm0
+++	leaq	32(%rcx,%rax,1),%rcx
+++	negq	%rax
+++	addq	$16,%rax
+++
+++.Ldec_loop2:
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++	movups	(%rcx,%rax,1),%xmm1
+++	addq	$32,%rax
+++.byte	102,15,56,222,208
+++.byte	102,15,56,222,216
+++	movups	-16(%rcx,%rax,1),%xmm0
+++	jnz	.Ldec_loop2
+++
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.byte	102,15,56,223,208
+++.byte	102,15,56,223,216
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_aesni_decrypt2,.-_aesni_decrypt2
+++.type	_aesni_encrypt3,@function
+++.align	16
+++_aesni_encrypt3:
+++.cfi_startproc	
+++	movups	(%rcx),%xmm0
+++	shll	$4,%eax
+++	movups	16(%rcx),%xmm1
+++	xorps	%xmm0,%xmm2
+++	xorps	%xmm0,%xmm3
+++	xorps	%xmm0,%xmm4
+++	movups	32(%rcx),%xmm0
+++	leaq	32(%rcx,%rax,1),%rcx
+++	negq	%rax
+++	addq	$16,%rax
+++
+++.Lenc_loop3:
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++.byte	102,15,56,220,225
+++	movups	(%rcx,%rax,1),%xmm1
+++	addq	$32,%rax
+++.byte	102,15,56,220,208
+++.byte	102,15,56,220,216
+++.byte	102,15,56,220,224
+++	movups	-16(%rcx,%rax,1),%xmm0
+++	jnz	.Lenc_loop3
+++
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++.byte	102,15,56,220,225
+++.byte	102,15,56,221,208
+++.byte	102,15,56,221,216
+++.byte	102,15,56,221,224
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_aesni_encrypt3,.-_aesni_encrypt3
+++.type	_aesni_decrypt3,@function
+++.align	16
+++_aesni_decrypt3:
+++.cfi_startproc	
+++	movups	(%rcx),%xmm0
+++	shll	$4,%eax
+++	movups	16(%rcx),%xmm1
+++	xorps	%xmm0,%xmm2
+++	xorps	%xmm0,%xmm3
+++	xorps	%xmm0,%xmm4
+++	movups	32(%rcx),%xmm0
+++	leaq	32(%rcx,%rax,1),%rcx
+++	negq	%rax
+++	addq	$16,%rax
+++
+++.Ldec_loop3:
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.byte	102,15,56,222,225
+++	movups	(%rcx,%rax,1),%xmm1
+++	addq	$32,%rax
+++.byte	102,15,56,222,208
+++.byte	102,15,56,222,216
+++.byte	102,15,56,222,224
+++	movups	-16(%rcx,%rax,1),%xmm0
+++	jnz	.Ldec_loop3
+++
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.byte	102,15,56,222,225
+++.byte	102,15,56,223,208
+++.byte	102,15,56,223,216
+++.byte	102,15,56,223,224
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_aesni_decrypt3,.-_aesni_decrypt3
+++.type	_aesni_encrypt4,@function
+++.align	16
+++_aesni_encrypt4:
+++.cfi_startproc	
+++	movups	(%rcx),%xmm0
+++	shll	$4,%eax
+++	movups	16(%rcx),%xmm1
+++	xorps	%xmm0,%xmm2
+++	xorps	%xmm0,%xmm3
+++	xorps	%xmm0,%xmm4
+++	xorps	%xmm0,%xmm5
+++	movups	32(%rcx),%xmm0
+++	leaq	32(%rcx,%rax,1),%rcx
+++	negq	%rax
+++.byte	0x0f,0x1f,0x00
+++	addq	$16,%rax
+++
+++.Lenc_loop4:
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++.byte	102,15,56,220,225
+++.byte	102,15,56,220,233
+++	movups	(%rcx,%rax,1),%xmm1
+++	addq	$32,%rax
+++.byte	102,15,56,220,208
+++.byte	102,15,56,220,216
+++.byte	102,15,56,220,224
+++.byte	102,15,56,220,232
+++	movups	-16(%rcx,%rax,1),%xmm0
+++	jnz	.Lenc_loop4
+++
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++.byte	102,15,56,220,225
+++.byte	102,15,56,220,233
+++.byte	102,15,56,221,208
+++.byte	102,15,56,221,216
+++.byte	102,15,56,221,224
+++.byte	102,15,56,221,232
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_aesni_encrypt4,.-_aesni_encrypt4
+++.type	_aesni_decrypt4,@function
+++.align	16
+++_aesni_decrypt4:
+++.cfi_startproc	
+++	movups	(%rcx),%xmm0
+++	shll	$4,%eax
+++	movups	16(%rcx),%xmm1
+++	xorps	%xmm0,%xmm2
+++	xorps	%xmm0,%xmm3
+++	xorps	%xmm0,%xmm4
+++	xorps	%xmm0,%xmm5
+++	movups	32(%rcx),%xmm0
+++	leaq	32(%rcx,%rax,1),%rcx
+++	negq	%rax
+++.byte	0x0f,0x1f,0x00
+++	addq	$16,%rax
+++
+++.Ldec_loop4:
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.byte	102,15,56,222,225
+++.byte	102,15,56,222,233
+++	movups	(%rcx,%rax,1),%xmm1
+++	addq	$32,%rax
+++.byte	102,15,56,222,208
+++.byte	102,15,56,222,216
+++.byte	102,15,56,222,224
+++.byte	102,15,56,222,232
+++	movups	-16(%rcx,%rax,1),%xmm0
+++	jnz	.Ldec_loop4
+++
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.byte	102,15,56,222,225
+++.byte	102,15,56,222,233
+++.byte	102,15,56,223,208
+++.byte	102,15,56,223,216
+++.byte	102,15,56,223,224
+++.byte	102,15,56,223,232
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_aesni_decrypt4,.-_aesni_decrypt4
+++.type	_aesni_encrypt6,@function
+++.align	16
+++_aesni_encrypt6:
+++.cfi_startproc	
+++	movups	(%rcx),%xmm0
+++	shll	$4,%eax
+++	movups	16(%rcx),%xmm1
+++	xorps	%xmm0,%xmm2
+++	pxor	%xmm0,%xmm3
+++	pxor	%xmm0,%xmm4
+++.byte	102,15,56,220,209
+++	leaq	32(%rcx,%rax,1),%rcx
+++	negq	%rax
+++.byte	102,15,56,220,217
+++	pxor	%xmm0,%xmm5
+++	pxor	%xmm0,%xmm6
+++.byte	102,15,56,220,225
+++	pxor	%xmm0,%xmm7
+++	movups	(%rcx,%rax,1),%xmm0
+++	addq	$16,%rax
+++	jmp	.Lenc_loop6_enter
+++.align	16
+++.Lenc_loop6:
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++.byte	102,15,56,220,225
+++.Lenc_loop6_enter:
+++.byte	102,15,56,220,233
+++.byte	102,15,56,220,241
+++.byte	102,15,56,220,249
+++	movups	(%rcx,%rax,1),%xmm1
+++	addq	$32,%rax
+++.byte	102,15,56,220,208
+++.byte	102,15,56,220,216
+++.byte	102,15,56,220,224
+++.byte	102,15,56,220,232
+++.byte	102,15,56,220,240
+++.byte	102,15,56,220,248
+++	movups	-16(%rcx,%rax,1),%xmm0
+++	jnz	.Lenc_loop6
+++
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++.byte	102,15,56,220,225
+++.byte	102,15,56,220,233
+++.byte	102,15,56,220,241
+++.byte	102,15,56,220,249
+++.byte	102,15,56,221,208
+++.byte	102,15,56,221,216
+++.byte	102,15,56,221,224
+++.byte	102,15,56,221,232
+++.byte	102,15,56,221,240
+++.byte	102,15,56,221,248
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_aesni_encrypt6,.-_aesni_encrypt6
+++.type	_aesni_decrypt6,@function
+++.align	16
+++_aesni_decrypt6:
+++.cfi_startproc	
+++	movups	(%rcx),%xmm0
+++	shll	$4,%eax
+++	movups	16(%rcx),%xmm1
+++	xorps	%xmm0,%xmm2
+++	pxor	%xmm0,%xmm3
+++	pxor	%xmm0,%xmm4
+++.byte	102,15,56,222,209
+++	leaq	32(%rcx,%rax,1),%rcx
+++	negq	%rax
+++.byte	102,15,56,222,217
+++	pxor	%xmm0,%xmm5
+++	pxor	%xmm0,%xmm6
+++.byte	102,15,56,222,225
+++	pxor	%xmm0,%xmm7
+++	movups	(%rcx,%rax,1),%xmm0
+++	addq	$16,%rax
+++	jmp	.Ldec_loop6_enter
+++.align	16
+++.Ldec_loop6:
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.byte	102,15,56,222,225
+++.Ldec_loop6_enter:
+++.byte	102,15,56,222,233
+++.byte	102,15,56,222,241
+++.byte	102,15,56,222,249
+++	movups	(%rcx,%rax,1),%xmm1
+++	addq	$32,%rax
+++.byte	102,15,56,222,208
+++.byte	102,15,56,222,216
+++.byte	102,15,56,222,224
+++.byte	102,15,56,222,232
+++.byte	102,15,56,222,240
+++.byte	102,15,56,222,248
+++	movups	-16(%rcx,%rax,1),%xmm0
+++	jnz	.Ldec_loop6
+++
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.byte	102,15,56,222,225
+++.byte	102,15,56,222,233
+++.byte	102,15,56,222,241
+++.byte	102,15,56,222,249
+++.byte	102,15,56,223,208
+++.byte	102,15,56,223,216
+++.byte	102,15,56,223,224
+++.byte	102,15,56,223,232
+++.byte	102,15,56,223,240
+++.byte	102,15,56,223,248
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_aesni_decrypt6,.-_aesni_decrypt6
+++.type	_aesni_encrypt8,@function
+++.align	16
+++_aesni_encrypt8:
+++.cfi_startproc	
+++	movups	(%rcx),%xmm0
+++	shll	$4,%eax
+++	movups	16(%rcx),%xmm1
+++	xorps	%xmm0,%xmm2
+++	xorps	%xmm0,%xmm3
+++	pxor	%xmm0,%xmm4
+++	pxor	%xmm0,%xmm5
+++	pxor	%xmm0,%xmm6
+++	leaq	32(%rcx,%rax,1),%rcx
+++	negq	%rax
+++.byte	102,15,56,220,209
+++	pxor	%xmm0,%xmm7
+++	pxor	%xmm0,%xmm8
+++.byte	102,15,56,220,217
+++	pxor	%xmm0,%xmm9
+++	movups	(%rcx,%rax,1),%xmm0
+++	addq	$16,%rax
+++	jmp	.Lenc_loop8_inner
+++.align	16
+++.Lenc_loop8:
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++.Lenc_loop8_inner:
+++.byte	102,15,56,220,225
+++.byte	102,15,56,220,233
+++.byte	102,15,56,220,241
+++.byte	102,15,56,220,249
+++.byte	102,68,15,56,220,193
+++.byte	102,68,15,56,220,201
+++.Lenc_loop8_enter:
+++	movups	(%rcx,%rax,1),%xmm1
+++	addq	$32,%rax
+++.byte	102,15,56,220,208
+++.byte	102,15,56,220,216
+++.byte	102,15,56,220,224
+++.byte	102,15,56,220,232
+++.byte	102,15,56,220,240
+++.byte	102,15,56,220,248
+++.byte	102,68,15,56,220,192
+++.byte	102,68,15,56,220,200
+++	movups	-16(%rcx,%rax,1),%xmm0
+++	jnz	.Lenc_loop8
+++
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++.byte	102,15,56,220,225
+++.byte	102,15,56,220,233
+++.byte	102,15,56,220,241
+++.byte	102,15,56,220,249
+++.byte	102,68,15,56,220,193
+++.byte	102,68,15,56,220,201
+++.byte	102,15,56,221,208
+++.byte	102,15,56,221,216
+++.byte	102,15,56,221,224
+++.byte	102,15,56,221,232
+++.byte	102,15,56,221,240
+++.byte	102,15,56,221,248
+++.byte	102,68,15,56,221,192
+++.byte	102,68,15,56,221,200
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_aesni_encrypt8,.-_aesni_encrypt8
+++.type	_aesni_decrypt8,@function
+++.align	16
+++_aesni_decrypt8:
+++.cfi_startproc	
+++	movups	(%rcx),%xmm0
+++	shll	$4,%eax
+++	movups	16(%rcx),%xmm1
+++	xorps	%xmm0,%xmm2
+++	xorps	%xmm0,%xmm3
+++	pxor	%xmm0,%xmm4
+++	pxor	%xmm0,%xmm5
+++	pxor	%xmm0,%xmm6
+++	leaq	32(%rcx,%rax,1),%rcx
+++	negq	%rax
+++.byte	102,15,56,222,209
+++	pxor	%xmm0,%xmm7
+++	pxor	%xmm0,%xmm8
+++.byte	102,15,56,222,217
+++	pxor	%xmm0,%xmm9
+++	movups	(%rcx,%rax,1),%xmm0
+++	addq	$16,%rax
+++	jmp	.Ldec_loop8_inner
+++.align	16
+++.Ldec_loop8:
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.Ldec_loop8_inner:
+++.byte	102,15,56,222,225
+++.byte	102,15,56,222,233
+++.byte	102,15,56,222,241
+++.byte	102,15,56,222,249
+++.byte	102,68,15,56,222,193
+++.byte	102,68,15,56,222,201
+++.Ldec_loop8_enter:
+++	movups	(%rcx,%rax,1),%xmm1
+++	addq	$32,%rax
+++.byte	102,15,56,222,208
+++.byte	102,15,56,222,216
+++.byte	102,15,56,222,224
+++.byte	102,15,56,222,232
+++.byte	102,15,56,222,240
+++.byte	102,15,56,222,248
+++.byte	102,68,15,56,222,192
+++.byte	102,68,15,56,222,200
+++	movups	-16(%rcx,%rax,1),%xmm0
+++	jnz	.Ldec_loop8
+++
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.byte	102,15,56,222,225
+++.byte	102,15,56,222,233
+++.byte	102,15,56,222,241
+++.byte	102,15,56,222,249
+++.byte	102,68,15,56,222,193
+++.byte	102,68,15,56,222,201
+++.byte	102,15,56,223,208
+++.byte	102,15,56,223,216
+++.byte	102,15,56,223,224
+++.byte	102,15,56,223,232
+++.byte	102,15,56,223,240
+++.byte	102,15,56,223,248
+++.byte	102,68,15,56,223,192
+++.byte	102,68,15,56,223,200
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_aesni_decrypt8,.-_aesni_decrypt8
+++.globl	aes_hw_ecb_encrypt
+++.hidden aes_hw_ecb_encrypt
+++.type	aes_hw_ecb_encrypt,@function
+++.align	16
+++aes_hw_ecb_encrypt:
+++.cfi_startproc	
+++	andq	$-16,%rdx
+++	jz	.Lecb_ret
+++
+++	movl	240(%rcx),%eax
+++	movups	(%rcx),%xmm0
+++	movq	%rcx,%r11
+++	movl	%eax,%r10d
+++	testl	%r8d,%r8d
+++	jz	.Lecb_decrypt
+++
+++	cmpq	$0x80,%rdx
+++	jb	.Lecb_enc_tail
+++
+++	movdqu	(%rdi),%xmm2
+++	movdqu	16(%rdi),%xmm3
+++	movdqu	32(%rdi),%xmm4
+++	movdqu	48(%rdi),%xmm5
+++	movdqu	64(%rdi),%xmm6
+++	movdqu	80(%rdi),%xmm7
+++	movdqu	96(%rdi),%xmm8
+++	movdqu	112(%rdi),%xmm9
+++	leaq	128(%rdi),%rdi
+++	subq	$0x80,%rdx
+++	jmp	.Lecb_enc_loop8_enter
+++.align	16
+++.Lecb_enc_loop8:
+++	movups	%xmm2,(%rsi)
+++	movq	%r11,%rcx
+++	movdqu	(%rdi),%xmm2
+++	movl	%r10d,%eax
+++	movups	%xmm3,16(%rsi)
+++	movdqu	16(%rdi),%xmm3
+++	movups	%xmm4,32(%rsi)
+++	movdqu	32(%rdi),%xmm4
+++	movups	%xmm5,48(%rsi)
+++	movdqu	48(%rdi),%xmm5
+++	movups	%xmm6,64(%rsi)
+++	movdqu	64(%rdi),%xmm6
+++	movups	%xmm7,80(%rsi)
+++	movdqu	80(%rdi),%xmm7
+++	movups	%xmm8,96(%rsi)
+++	movdqu	96(%rdi),%xmm8
+++	movups	%xmm9,112(%rsi)
+++	leaq	128(%rsi),%rsi
+++	movdqu	112(%rdi),%xmm9
+++	leaq	128(%rdi),%rdi
+++.Lecb_enc_loop8_enter:
+++
+++	call	_aesni_encrypt8
+++
+++	subq	$0x80,%rdx
+++	jnc	.Lecb_enc_loop8
+++
+++	movups	%xmm2,(%rsi)
+++	movq	%r11,%rcx
+++	movups	%xmm3,16(%rsi)
+++	movl	%r10d,%eax
+++	movups	%xmm4,32(%rsi)
+++	movups	%xmm5,48(%rsi)
+++	movups	%xmm6,64(%rsi)
+++	movups	%xmm7,80(%rsi)
+++	movups	%xmm8,96(%rsi)
+++	movups	%xmm9,112(%rsi)
+++	leaq	128(%rsi),%rsi
+++	addq	$0x80,%rdx
+++	jz	.Lecb_ret
+++
+++.Lecb_enc_tail:
+++	movups	(%rdi),%xmm2
+++	cmpq	$0x20,%rdx
+++	jb	.Lecb_enc_one
+++	movups	16(%rdi),%xmm3
+++	je	.Lecb_enc_two
+++	movups	32(%rdi),%xmm4
+++	cmpq	$0x40,%rdx
+++	jb	.Lecb_enc_three
+++	movups	48(%rdi),%xmm5
+++	je	.Lecb_enc_four
+++	movups	64(%rdi),%xmm6
+++	cmpq	$0x60,%rdx
+++	jb	.Lecb_enc_five
+++	movups	80(%rdi),%xmm7
+++	je	.Lecb_enc_six
+++	movdqu	96(%rdi),%xmm8
+++	xorps	%xmm9,%xmm9
+++	call	_aesni_encrypt8
+++	movups	%xmm2,(%rsi)
+++	movups	%xmm3,16(%rsi)
+++	movups	%xmm4,32(%rsi)
+++	movups	%xmm5,48(%rsi)
+++	movups	%xmm6,64(%rsi)
+++	movups	%xmm7,80(%rsi)
+++	movups	%xmm8,96(%rsi)
+++	jmp	.Lecb_ret
+++.align	16
+++.Lecb_enc_one:
+++	movups	(%rcx),%xmm0
+++	movups	16(%rcx),%xmm1
+++	leaq	32(%rcx),%rcx
+++	xorps	%xmm0,%xmm2
+++.Loop_enc1_3:
+++.byte	102,15,56,220,209
+++	decl	%eax
+++	movups	(%rcx),%xmm1
+++	leaq	16(%rcx),%rcx
+++	jnz	.Loop_enc1_3
+++.byte	102,15,56,221,209
+++	movups	%xmm2,(%rsi)
+++	jmp	.Lecb_ret
+++.align	16
+++.Lecb_enc_two:
+++	call	_aesni_encrypt2
+++	movups	%xmm2,(%rsi)
+++	movups	%xmm3,16(%rsi)
+++	jmp	.Lecb_ret
+++.align	16
+++.Lecb_enc_three:
+++	call	_aesni_encrypt3
+++	movups	%xmm2,(%rsi)
+++	movups	%xmm3,16(%rsi)
+++	movups	%xmm4,32(%rsi)
+++	jmp	.Lecb_ret
+++.align	16
+++.Lecb_enc_four:
+++	call	_aesni_encrypt4
+++	movups	%xmm2,(%rsi)
+++	movups	%xmm3,16(%rsi)
+++	movups	%xmm4,32(%rsi)
+++	movups	%xmm5,48(%rsi)
+++	jmp	.Lecb_ret
+++.align	16
+++.Lecb_enc_five:
+++	xorps	%xmm7,%xmm7
+++	call	_aesni_encrypt6
+++	movups	%xmm2,(%rsi)
+++	movups	%xmm3,16(%rsi)
+++	movups	%xmm4,32(%rsi)
+++	movups	%xmm5,48(%rsi)
+++	movups	%xmm6,64(%rsi)
+++	jmp	.Lecb_ret
+++.align	16
+++.Lecb_enc_six:
+++	call	_aesni_encrypt6
+++	movups	%xmm2,(%rsi)
+++	movups	%xmm3,16(%rsi)
+++	movups	%xmm4,32(%rsi)
+++	movups	%xmm5,48(%rsi)
+++	movups	%xmm6,64(%rsi)
+++	movups	%xmm7,80(%rsi)
+++	jmp	.Lecb_ret
+++
+++.align	16
+++.Lecb_decrypt:
+++	cmpq	$0x80,%rdx
+++	jb	.Lecb_dec_tail
+++
+++	movdqu	(%rdi),%xmm2
+++	movdqu	16(%rdi),%xmm3
+++	movdqu	32(%rdi),%xmm4
+++	movdqu	48(%rdi),%xmm5
+++	movdqu	64(%rdi),%xmm6
+++	movdqu	80(%rdi),%xmm7
+++	movdqu	96(%rdi),%xmm8
+++	movdqu	112(%rdi),%xmm9
+++	leaq	128(%rdi),%rdi
+++	subq	$0x80,%rdx
+++	jmp	.Lecb_dec_loop8_enter
+++.align	16
+++.Lecb_dec_loop8:
+++	movups	%xmm2,(%rsi)
+++	movq	%r11,%rcx
+++	movdqu	(%rdi),%xmm2
+++	movl	%r10d,%eax
+++	movups	%xmm3,16(%rsi)
+++	movdqu	16(%rdi),%xmm3
+++	movups	%xmm4,32(%rsi)
+++	movdqu	32(%rdi),%xmm4
+++	movups	%xmm5,48(%rsi)
+++	movdqu	48(%rdi),%xmm5
+++	movups	%xmm6,64(%rsi)
+++	movdqu	64(%rdi),%xmm6
+++	movups	%xmm7,80(%rsi)
+++	movdqu	80(%rdi),%xmm7
+++	movups	%xmm8,96(%rsi)
+++	movdqu	96(%rdi),%xmm8
+++	movups	%xmm9,112(%rsi)
+++	leaq	128(%rsi),%rsi
+++	movdqu	112(%rdi),%xmm9
+++	leaq	128(%rdi),%rdi
+++.Lecb_dec_loop8_enter:
+++
+++	call	_aesni_decrypt8
+++
+++	movups	(%r11),%xmm0
+++	subq	$0x80,%rdx
+++	jnc	.Lecb_dec_loop8
+++
+++	movups	%xmm2,(%rsi)
+++	pxor	%xmm2,%xmm2
+++	movq	%r11,%rcx
+++	movups	%xmm3,16(%rsi)
+++	pxor	%xmm3,%xmm3
+++	movl	%r10d,%eax
+++	movups	%xmm4,32(%rsi)
+++	pxor	%xmm4,%xmm4
+++	movups	%xmm5,48(%rsi)
+++	pxor	%xmm5,%xmm5
+++	movups	%xmm6,64(%rsi)
+++	pxor	%xmm6,%xmm6
+++	movups	%xmm7,80(%rsi)
+++	pxor	%xmm7,%xmm7
+++	movups	%xmm8,96(%rsi)
+++	pxor	%xmm8,%xmm8
+++	movups	%xmm9,112(%rsi)
+++	pxor	%xmm9,%xmm9
+++	leaq	128(%rsi),%rsi
+++	addq	$0x80,%rdx
+++	jz	.Lecb_ret
+++
+++.Lecb_dec_tail:
+++	movups	(%rdi),%xmm2
+++	cmpq	$0x20,%rdx
+++	jb	.Lecb_dec_one
+++	movups	16(%rdi),%xmm3
+++	je	.Lecb_dec_two
+++	movups	32(%rdi),%xmm4
+++	cmpq	$0x40,%rdx
+++	jb	.Lecb_dec_three
+++	movups	48(%rdi),%xmm5
+++	je	.Lecb_dec_four
+++	movups	64(%rdi),%xmm6
+++	cmpq	$0x60,%rdx
+++	jb	.Lecb_dec_five
+++	movups	80(%rdi),%xmm7
+++	je	.Lecb_dec_six
+++	movups	96(%rdi),%xmm8
+++	movups	(%rcx),%xmm0
+++	xorps	%xmm9,%xmm9
+++	call	_aesni_decrypt8
+++	movups	%xmm2,(%rsi)
+++	pxor	%xmm2,%xmm2
+++	movups	%xmm3,16(%rsi)
+++	pxor	%xmm3,%xmm3
+++	movups	%xmm4,32(%rsi)
+++	pxor	%xmm4,%xmm4
+++	movups	%xmm5,48(%rsi)
+++	pxor	%xmm5,%xmm5
+++	movups	%xmm6,64(%rsi)
+++	pxor	%xmm6,%xmm6
+++	movups	%xmm7,80(%rsi)
+++	pxor	%xmm7,%xmm7
+++	movups	%xmm8,96(%rsi)
+++	pxor	%xmm8,%xmm8
+++	pxor	%xmm9,%xmm9
+++	jmp	.Lecb_ret
+++.align	16
+++.Lecb_dec_one:
+++	movups	(%rcx),%xmm0
+++	movups	16(%rcx),%xmm1
+++	leaq	32(%rcx),%rcx
+++	xorps	%xmm0,%xmm2
+++.Loop_dec1_4:
+++.byte	102,15,56,222,209
+++	decl	%eax
+++	movups	(%rcx),%xmm1
+++	leaq	16(%rcx),%rcx
+++	jnz	.Loop_dec1_4
+++.byte	102,15,56,223,209
+++	movups	%xmm2,(%rsi)
+++	pxor	%xmm2,%xmm2
+++	jmp	.Lecb_ret
+++.align	16
+++.Lecb_dec_two:
+++	call	_aesni_decrypt2
+++	movups	%xmm2,(%rsi)
+++	pxor	%xmm2,%xmm2
+++	movups	%xmm3,16(%rsi)
+++	pxor	%xmm3,%xmm3
+++	jmp	.Lecb_ret
+++.align	16
+++.Lecb_dec_three:
+++	call	_aesni_decrypt3
+++	movups	%xmm2,(%rsi)
+++	pxor	%xmm2,%xmm2
+++	movups	%xmm3,16(%rsi)
+++	pxor	%xmm3,%xmm3
+++	movups	%xmm4,32(%rsi)
+++	pxor	%xmm4,%xmm4
+++	jmp	.Lecb_ret
+++.align	16
+++.Lecb_dec_four:
+++	call	_aesni_decrypt4
+++	movups	%xmm2,(%rsi)
+++	pxor	%xmm2,%xmm2
+++	movups	%xmm3,16(%rsi)
+++	pxor	%xmm3,%xmm3
+++	movups	%xmm4,32(%rsi)
+++	pxor	%xmm4,%xmm4
+++	movups	%xmm5,48(%rsi)
+++	pxor	%xmm5,%xmm5
+++	jmp	.Lecb_ret
+++.align	16
+++.Lecb_dec_five:
+++	xorps	%xmm7,%xmm7
+++	call	_aesni_decrypt6
+++	movups	%xmm2,(%rsi)
+++	pxor	%xmm2,%xmm2
+++	movups	%xmm3,16(%rsi)
+++	pxor	%xmm3,%xmm3
+++	movups	%xmm4,32(%rsi)
+++	pxor	%xmm4,%xmm4
+++	movups	%xmm5,48(%rsi)
+++	pxor	%xmm5,%xmm5
+++	movups	%xmm6,64(%rsi)
+++	pxor	%xmm6,%xmm6
+++	pxor	%xmm7,%xmm7
+++	jmp	.Lecb_ret
+++.align	16
+++.Lecb_dec_six:
+++	call	_aesni_decrypt6
+++	movups	%xmm2,(%rsi)
+++	pxor	%xmm2,%xmm2
+++	movups	%xmm3,16(%rsi)
+++	pxor	%xmm3,%xmm3
+++	movups	%xmm4,32(%rsi)
+++	pxor	%xmm4,%xmm4
+++	movups	%xmm5,48(%rsi)
+++	pxor	%xmm5,%xmm5
+++	movups	%xmm6,64(%rsi)
+++	pxor	%xmm6,%xmm6
+++	movups	%xmm7,80(%rsi)
+++	pxor	%xmm7,%xmm7
+++
+++.Lecb_ret:
+++	xorps	%xmm0,%xmm0
+++	pxor	%xmm1,%xmm1
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes_hw_ecb_encrypt,.-aes_hw_ecb_encrypt
+++.globl	aes_hw_ctr32_encrypt_blocks
+++.hidden aes_hw_ctr32_encrypt_blocks
+++.type	aes_hw_ctr32_encrypt_blocks,@function
+++.align	16
+++aes_hw_ctr32_encrypt_blocks:
+++.cfi_startproc	
+++#ifdef BORINGSSL_DISPATCH_TEST
+++	movb	$1,BORINGSSL_function_hit(%rip)
+++#endif
+++	cmpq	$1,%rdx
+++	jne	.Lctr32_bulk
+++
+++
+++
+++	movups	(%r8),%xmm2
+++	movups	(%rdi),%xmm3
+++	movl	240(%rcx),%edx
+++	movups	(%rcx),%xmm0
+++	movups	16(%rcx),%xmm1
+++	leaq	32(%rcx),%rcx
+++	xorps	%xmm0,%xmm2
+++.Loop_enc1_5:
+++.byte	102,15,56,220,209
+++	decl	%edx
+++	movups	(%rcx),%xmm1
+++	leaq	16(%rcx),%rcx
+++	jnz	.Loop_enc1_5
+++.byte	102,15,56,221,209
+++	pxor	%xmm0,%xmm0
+++	pxor	%xmm1,%xmm1
+++	xorps	%xmm3,%xmm2
+++	pxor	%xmm3,%xmm3
+++	movups	%xmm2,(%rsi)
+++	xorps	%xmm2,%xmm2
+++	jmp	.Lctr32_epilogue
+++
+++.align	16
+++.Lctr32_bulk:
+++	leaq	(%rsp),%r11
+++.cfi_def_cfa_register	%r11
+++	pushq	%rbp
+++.cfi_offset	%rbp,-16
+++	subq	$128,%rsp
+++	andq	$-16,%rsp
+++
+++
+++
+++
+++	movdqu	(%r8),%xmm2
+++	movdqu	(%rcx),%xmm0
+++	movl	12(%r8),%r8d
+++	pxor	%xmm0,%xmm2
+++	movl	12(%rcx),%ebp
+++	movdqa	%xmm2,0(%rsp)
+++	bswapl	%r8d
+++	movdqa	%xmm2,%xmm3
+++	movdqa	%xmm2,%xmm4
+++	movdqa	%xmm2,%xmm5
+++	movdqa	%xmm2,64(%rsp)
+++	movdqa	%xmm2,80(%rsp)
+++	movdqa	%xmm2,96(%rsp)
+++	movq	%rdx,%r10
+++	movdqa	%xmm2,112(%rsp)
+++
+++	leaq	1(%r8),%rax
+++	leaq	2(%r8),%rdx
+++	bswapl	%eax
+++	bswapl	%edx
+++	xorl	%ebp,%eax
+++	xorl	%ebp,%edx
+++.byte	102,15,58,34,216,3
+++	leaq	3(%r8),%rax
+++	movdqa	%xmm3,16(%rsp)
+++.byte	102,15,58,34,226,3
+++	bswapl	%eax
+++	movq	%r10,%rdx
+++	leaq	4(%r8),%r10
+++	movdqa	%xmm4,32(%rsp)
+++	xorl	%ebp,%eax
+++	bswapl	%r10d
+++.byte	102,15,58,34,232,3
+++	xorl	%ebp,%r10d
+++	movdqa	%xmm5,48(%rsp)
+++	leaq	5(%r8),%r9
+++	movl	%r10d,64+12(%rsp)
+++	bswapl	%r9d
+++	leaq	6(%r8),%r10
+++	movl	240(%rcx),%eax
+++	xorl	%ebp,%r9d
+++	bswapl	%r10d
+++	movl	%r9d,80+12(%rsp)
+++	xorl	%ebp,%r10d
+++	leaq	7(%r8),%r9
+++	movl	%r10d,96+12(%rsp)
+++	bswapl	%r9d
+++	leaq	OPENSSL_ia32cap_P(%rip),%r10
+++	movl	4(%r10),%r10d
+++	xorl	%ebp,%r9d
+++	andl	$71303168,%r10d
+++	movl	%r9d,112+12(%rsp)
+++
+++	movups	16(%rcx),%xmm1
+++
+++	movdqa	64(%rsp),%xmm6
+++	movdqa	80(%rsp),%xmm7
+++
+++	cmpq	$8,%rdx
+++	jb	.Lctr32_tail
+++
+++	subq	$6,%rdx
+++	cmpl	$4194304,%r10d
+++	je	.Lctr32_6x
+++
+++	leaq	128(%rcx),%rcx
+++	subq	$2,%rdx
+++	jmp	.Lctr32_loop8
+++
+++.align	16
+++.Lctr32_6x:
+++	shll	$4,%eax
+++	movl	$48,%r10d
+++	bswapl	%ebp
+++	leaq	32(%rcx,%rax,1),%rcx
+++	subq	%rax,%r10
+++	jmp	.Lctr32_loop6
+++
+++.align	16
+++.Lctr32_loop6:
+++	addl	$6,%r8d
+++	movups	-48(%rcx,%r10,1),%xmm0
+++.byte	102,15,56,220,209
+++	movl	%r8d,%eax
+++	xorl	%ebp,%eax
+++.byte	102,15,56,220,217
+++.byte	0x0f,0x38,0xf1,0x44,0x24,12
+++	leal	1(%r8),%eax
+++.byte	102,15,56,220,225
+++	xorl	%ebp,%eax
+++.byte	0x0f,0x38,0xf1,0x44,0x24,28
+++.byte	102,15,56,220,233
+++	leal	2(%r8),%eax
+++	xorl	%ebp,%eax
+++.byte	102,15,56,220,241
+++.byte	0x0f,0x38,0xf1,0x44,0x24,44
+++	leal	3(%r8),%eax
+++.byte	102,15,56,220,249
+++	movups	-32(%rcx,%r10,1),%xmm1
+++	xorl	%ebp,%eax
+++
+++.byte	102,15,56,220,208
+++.byte	0x0f,0x38,0xf1,0x44,0x24,60
+++	leal	4(%r8),%eax
+++.byte	102,15,56,220,216
+++	xorl	%ebp,%eax
+++.byte	0x0f,0x38,0xf1,0x44,0x24,76
+++.byte	102,15,56,220,224
+++	leal	5(%r8),%eax
+++	xorl	%ebp,%eax
+++.byte	102,15,56,220,232
+++.byte	0x0f,0x38,0xf1,0x44,0x24,92
+++	movq	%r10,%rax
+++.byte	102,15,56,220,240
+++.byte	102,15,56,220,248
+++	movups	-16(%rcx,%r10,1),%xmm0
+++
+++	call	.Lenc_loop6
+++
+++	movdqu	(%rdi),%xmm8
+++	movdqu	16(%rdi),%xmm9
+++	movdqu	32(%rdi),%xmm10
+++	movdqu	48(%rdi),%xmm11
+++	movdqu	64(%rdi),%xmm12
+++	movdqu	80(%rdi),%xmm13
+++	leaq	96(%rdi),%rdi
+++	movups	-64(%rcx,%r10,1),%xmm1
+++	pxor	%xmm2,%xmm8
+++	movaps	0(%rsp),%xmm2
+++	pxor	%xmm3,%xmm9
+++	movaps	16(%rsp),%xmm3
+++	pxor	%xmm4,%xmm10
+++	movaps	32(%rsp),%xmm4
+++	pxor	%xmm5,%xmm11
+++	movaps	48(%rsp),%xmm5
+++	pxor	%xmm6,%xmm12
+++	movaps	64(%rsp),%xmm6
+++	pxor	%xmm7,%xmm13
+++	movaps	80(%rsp),%xmm7
+++	movdqu	%xmm8,(%rsi)
+++	movdqu	%xmm9,16(%rsi)
+++	movdqu	%xmm10,32(%rsi)
+++	movdqu	%xmm11,48(%rsi)
+++	movdqu	%xmm12,64(%rsi)
+++	movdqu	%xmm13,80(%rsi)
+++	leaq	96(%rsi),%rsi
+++
+++	subq	$6,%rdx
+++	jnc	.Lctr32_loop6
+++
+++	addq	$6,%rdx
+++	jz	.Lctr32_done
+++
+++	leal	-48(%r10),%eax
+++	leaq	-80(%rcx,%r10,1),%rcx
+++	negl	%eax
+++	shrl	$4,%eax
+++	jmp	.Lctr32_tail
+++
+++.align	32
+++.Lctr32_loop8:
+++	addl	$8,%r8d
+++	movdqa	96(%rsp),%xmm8
+++.byte	102,15,56,220,209
+++	movl	%r8d,%r9d
+++	movdqa	112(%rsp),%xmm9
+++.byte	102,15,56,220,217
+++	bswapl	%r9d
+++	movups	32-128(%rcx),%xmm0
+++.byte	102,15,56,220,225
+++	xorl	%ebp,%r9d
+++	nop
+++.byte	102,15,56,220,233
+++	movl	%r9d,0+12(%rsp)
+++	leaq	1(%r8),%r9
+++.byte	102,15,56,220,241
+++.byte	102,15,56,220,249
+++.byte	102,68,15,56,220,193
+++.byte	102,68,15,56,220,201
+++	movups	48-128(%rcx),%xmm1
+++	bswapl	%r9d
+++.byte	102,15,56,220,208
+++.byte	102,15,56,220,216
+++	xorl	%ebp,%r9d
+++.byte	0x66,0x90
+++.byte	102,15,56,220,224
+++.byte	102,15,56,220,232
+++	movl	%r9d,16+12(%rsp)
+++	leaq	2(%r8),%r9
+++.byte	102,15,56,220,240
+++.byte	102,15,56,220,248
+++.byte	102,68,15,56,220,192
+++.byte	102,68,15,56,220,200
+++	movups	64-128(%rcx),%xmm0
+++	bswapl	%r9d
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++	xorl	%ebp,%r9d
+++.byte	0x66,0x90
+++.byte	102,15,56,220,225
+++.byte	102,15,56,220,233
+++	movl	%r9d,32+12(%rsp)
+++	leaq	3(%r8),%r9
+++.byte	102,15,56,220,241
+++.byte	102,15,56,220,249
+++.byte	102,68,15,56,220,193
+++.byte	102,68,15,56,220,201
+++	movups	80-128(%rcx),%xmm1
+++	bswapl	%r9d
+++.byte	102,15,56,220,208
+++.byte	102,15,56,220,216
+++	xorl	%ebp,%r9d
+++.byte	0x66,0x90
+++.byte	102,15,56,220,224
+++.byte	102,15,56,220,232
+++	movl	%r9d,48+12(%rsp)
+++	leaq	4(%r8),%r9
+++.byte	102,15,56,220,240
+++.byte	102,15,56,220,248
+++.byte	102,68,15,56,220,192
+++.byte	102,68,15,56,220,200
+++	movups	96-128(%rcx),%xmm0
+++	bswapl	%r9d
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++	xorl	%ebp,%r9d
+++.byte	0x66,0x90
+++.byte	102,15,56,220,225
+++.byte	102,15,56,220,233
+++	movl	%r9d,64+12(%rsp)
+++	leaq	5(%r8),%r9
+++.byte	102,15,56,220,241
+++.byte	102,15,56,220,249
+++.byte	102,68,15,56,220,193
+++.byte	102,68,15,56,220,201
+++	movups	112-128(%rcx),%xmm1
+++	bswapl	%r9d
+++.byte	102,15,56,220,208
+++.byte	102,15,56,220,216
+++	xorl	%ebp,%r9d
+++.byte	0x66,0x90
+++.byte	102,15,56,220,224
+++.byte	102,15,56,220,232
+++	movl	%r9d,80+12(%rsp)
+++	leaq	6(%r8),%r9
+++.byte	102,15,56,220,240
+++.byte	102,15,56,220,248
+++.byte	102,68,15,56,220,192
+++.byte	102,68,15,56,220,200
+++	movups	128-128(%rcx),%xmm0
+++	bswapl	%r9d
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++	xorl	%ebp,%r9d
+++.byte	0x66,0x90
+++.byte	102,15,56,220,225
+++.byte	102,15,56,220,233
+++	movl	%r9d,96+12(%rsp)
+++	leaq	7(%r8),%r9
+++.byte	102,15,56,220,241
+++.byte	102,15,56,220,249
+++.byte	102,68,15,56,220,193
+++.byte	102,68,15,56,220,201
+++	movups	144-128(%rcx),%xmm1
+++	bswapl	%r9d
+++.byte	102,15,56,220,208
+++.byte	102,15,56,220,216
+++.byte	102,15,56,220,224
+++	xorl	%ebp,%r9d
+++	movdqu	0(%rdi),%xmm10
+++.byte	102,15,56,220,232
+++	movl	%r9d,112+12(%rsp)
+++	cmpl	$11,%eax
+++.byte	102,15,56,220,240
+++.byte	102,15,56,220,248
+++.byte	102,68,15,56,220,192
+++.byte	102,68,15,56,220,200
+++	movups	160-128(%rcx),%xmm0
+++
+++	jb	.Lctr32_enc_done
+++
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++.byte	102,15,56,220,225
+++.byte	102,15,56,220,233
+++.byte	102,15,56,220,241
+++.byte	102,15,56,220,249
+++.byte	102,68,15,56,220,193
+++.byte	102,68,15,56,220,201
+++	movups	176-128(%rcx),%xmm1
+++
+++.byte	102,15,56,220,208
+++.byte	102,15,56,220,216
+++.byte	102,15,56,220,224
+++.byte	102,15,56,220,232
+++.byte	102,15,56,220,240
+++.byte	102,15,56,220,248
+++.byte	102,68,15,56,220,192
+++.byte	102,68,15,56,220,200
+++	movups	192-128(%rcx),%xmm0
+++	je	.Lctr32_enc_done
+++
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++.byte	102,15,56,220,225
+++.byte	102,15,56,220,233
+++.byte	102,15,56,220,241
+++.byte	102,15,56,220,249
+++.byte	102,68,15,56,220,193
+++.byte	102,68,15,56,220,201
+++	movups	208-128(%rcx),%xmm1
+++
+++.byte	102,15,56,220,208
+++.byte	102,15,56,220,216
+++.byte	102,15,56,220,224
+++.byte	102,15,56,220,232
+++.byte	102,15,56,220,240
+++.byte	102,15,56,220,248
+++.byte	102,68,15,56,220,192
+++.byte	102,68,15,56,220,200
+++	movups	224-128(%rcx),%xmm0
+++	jmp	.Lctr32_enc_done
+++
+++.align	16
+++.Lctr32_enc_done:
+++	movdqu	16(%rdi),%xmm11
+++	pxor	%xmm0,%xmm10
+++	movdqu	32(%rdi),%xmm12
+++	pxor	%xmm0,%xmm11
+++	movdqu	48(%rdi),%xmm13
+++	pxor	%xmm0,%xmm12
+++	movdqu	64(%rdi),%xmm14
+++	pxor	%xmm0,%xmm13
+++	movdqu	80(%rdi),%xmm15
+++	pxor	%xmm0,%xmm14
+++	pxor	%xmm0,%xmm15
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++.byte	102,15,56,220,225
+++.byte	102,15,56,220,233
+++.byte	102,15,56,220,241
+++.byte	102,15,56,220,249
+++.byte	102,68,15,56,220,193
+++.byte	102,68,15,56,220,201
+++	movdqu	96(%rdi),%xmm1
+++	leaq	128(%rdi),%rdi
+++
+++.byte	102,65,15,56,221,210
+++	pxor	%xmm0,%xmm1
+++	movdqu	112-128(%rdi),%xmm10
+++.byte	102,65,15,56,221,219
+++	pxor	%xmm0,%xmm10
+++	movdqa	0(%rsp),%xmm11
+++.byte	102,65,15,56,221,228
+++.byte	102,65,15,56,221,237
+++	movdqa	16(%rsp),%xmm12
+++	movdqa	32(%rsp),%xmm13
+++.byte	102,65,15,56,221,246
+++.byte	102,65,15,56,221,255
+++	movdqa	48(%rsp),%xmm14
+++	movdqa	64(%rsp),%xmm15
+++.byte	102,68,15,56,221,193
+++	movdqa	80(%rsp),%xmm0
+++	movups	16-128(%rcx),%xmm1
+++.byte	102,69,15,56,221,202
+++
+++	movups	%xmm2,(%rsi)
+++	movdqa	%xmm11,%xmm2
+++	movups	%xmm3,16(%rsi)
+++	movdqa	%xmm12,%xmm3
+++	movups	%xmm4,32(%rsi)
+++	movdqa	%xmm13,%xmm4
+++	movups	%xmm5,48(%rsi)
+++	movdqa	%xmm14,%xmm5
+++	movups	%xmm6,64(%rsi)
+++	movdqa	%xmm15,%xmm6
+++	movups	%xmm7,80(%rsi)
+++	movdqa	%xmm0,%xmm7
+++	movups	%xmm8,96(%rsi)
+++	movups	%xmm9,112(%rsi)
+++	leaq	128(%rsi),%rsi
+++
+++	subq	$8,%rdx
+++	jnc	.Lctr32_loop8
+++
+++	addq	$8,%rdx
+++	jz	.Lctr32_done
+++	leaq	-128(%rcx),%rcx
+++
+++.Lctr32_tail:
+++
+++
+++	leaq	16(%rcx),%rcx
+++	cmpq	$4,%rdx
+++	jb	.Lctr32_loop3
+++	je	.Lctr32_loop4
+++
+++
+++	shll	$4,%eax
+++	movdqa	96(%rsp),%xmm8
+++	pxor	%xmm9,%xmm9
+++
+++	movups	16(%rcx),%xmm0
+++.byte	102,15,56,220,209
+++.byte	102,15,56,220,217
+++	leaq	32-16(%rcx,%rax,1),%rcx
+++	negq	%rax
+++.byte	102,15,56,220,225
+++	addq	$16,%rax
+++	movups	(%rdi),%xmm10
+++.byte	102,15,56,220,233
+++.byte	102,15,56,220,241
+++	movups	16(%rdi),%xmm11
+++	movups	32(%rdi),%xmm12
+++.byte	102,15,56,220,249
+++.byte	102,68,15,56,220,193
+++
+++	call	.Lenc_loop8_enter
+++
+++	movdqu	48(%rdi),%xmm13
+++	pxor	%xmm10,%xmm2
+++	movdqu	64(%rdi),%xmm10
+++	pxor	%xmm11,%xmm3
+++	movdqu	%xmm2,(%rsi)
+++	pxor	%xmm12,%xmm4
+++	movdqu	%xmm3,16(%rsi)
+++	pxor	%xmm13,%xmm5
+++	movdqu	%xmm4,32(%rsi)
+++	pxor	%xmm10,%xmm6
+++	movdqu	%xmm5,48(%rsi)
+++	movdqu	%xmm6,64(%rsi)
+++	cmpq	$6,%rdx
+++	jb	.Lctr32_done
+++
+++	movups	80(%rdi),%xmm11
+++	xorps	%xmm11,%xmm7
+++	movups	%xmm7,80(%rsi)
+++	je	.Lctr32_done
+++
+++	movups	96(%rdi),%xmm12
+++	xorps	%xmm12,%xmm8
+++	movups	%xmm8,96(%rsi)
+++	jmp	.Lctr32_done
+++
+++.align	32
+++.Lctr32_loop4:
+++.byte	102,15,56,220,209
+++	leaq	16(%rcx),%rcx
+++	decl	%eax
+++.byte	102,15,56,220,217
+++.byte	102,15,56,220,225
+++.byte	102,15,56,220,233
+++	movups	(%rcx),%xmm1
+++	jnz	.Lctr32_loop4
+++.byte	102,15,56,221,209
+++.byte	102,15,56,221,217
+++	movups	(%rdi),%xmm10
+++	movups	16(%rdi),%xmm11
+++.byte	102,15,56,221,225
+++.byte	102,15,56,221,233
+++	movups	32(%rdi),%xmm12
+++	movups	48(%rdi),%xmm13
+++
+++	xorps	%xmm10,%xmm2
+++	movups	%xmm2,(%rsi)
+++	xorps	%xmm11,%xmm3
+++	movups	%xmm3,16(%rsi)
+++	pxor	%xmm12,%xmm4
+++	movdqu	%xmm4,32(%rsi)
+++	pxor	%xmm13,%xmm5
+++	movdqu	%xmm5,48(%rsi)
+++	jmp	.Lctr32_done
+++
+++.align	32
+++.Lctr32_loop3:
+++.byte	102,15,56,220,209
+++	leaq	16(%rcx),%rcx
+++	decl	%eax
+++.byte	102,15,56,220,217
+++.byte	102,15,56,220,225
+++	movups	(%rcx),%xmm1
+++	jnz	.Lctr32_loop3
+++.byte	102,15,56,221,209
+++.byte	102,15,56,221,217
+++.byte	102,15,56,221,225
+++
+++	movups	(%rdi),%xmm10
+++	xorps	%xmm10,%xmm2
+++	movups	%xmm2,(%rsi)
+++	cmpq	$2,%rdx
+++	jb	.Lctr32_done
+++
+++	movups	16(%rdi),%xmm11
+++	xorps	%xmm11,%xmm3
+++	movups	%xmm3,16(%rsi)
+++	je	.Lctr32_done
+++
+++	movups	32(%rdi),%xmm12
+++	xorps	%xmm12,%xmm4
+++	movups	%xmm4,32(%rsi)
+++
+++.Lctr32_done:
+++	xorps	%xmm0,%xmm0
+++	xorl	%ebp,%ebp
+++	pxor	%xmm1,%xmm1
+++	pxor	%xmm2,%xmm2
+++	pxor	%xmm3,%xmm3
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm5,%xmm5
+++	pxor	%xmm6,%xmm6
+++	pxor	%xmm7,%xmm7
+++	movaps	%xmm0,0(%rsp)
+++	pxor	%xmm8,%xmm8
+++	movaps	%xmm0,16(%rsp)
+++	pxor	%xmm9,%xmm9
+++	movaps	%xmm0,32(%rsp)
+++	pxor	%xmm10,%xmm10
+++	movaps	%xmm0,48(%rsp)
+++	pxor	%xmm11,%xmm11
+++	movaps	%xmm0,64(%rsp)
+++	pxor	%xmm12,%xmm12
+++	movaps	%xmm0,80(%rsp)
+++	pxor	%xmm13,%xmm13
+++	movaps	%xmm0,96(%rsp)
+++	pxor	%xmm14,%xmm14
+++	movaps	%xmm0,112(%rsp)
+++	pxor	%xmm15,%xmm15
+++	movq	-8(%r11),%rbp
+++.cfi_restore	%rbp
+++	leaq	(%r11),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lctr32_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes_hw_ctr32_encrypt_blocks,.-aes_hw_ctr32_encrypt_blocks
+++.globl	aes_hw_cbc_encrypt
+++.hidden aes_hw_cbc_encrypt
+++.type	aes_hw_cbc_encrypt,@function
+++.align	16
+++aes_hw_cbc_encrypt:
+++.cfi_startproc	
+++	testq	%rdx,%rdx
+++	jz	.Lcbc_ret
+++
+++	movl	240(%rcx),%r10d
+++	movq	%rcx,%r11
+++	testl	%r9d,%r9d
+++	jz	.Lcbc_decrypt
+++
+++	movups	(%r8),%xmm2
+++	movl	%r10d,%eax
+++	cmpq	$16,%rdx
+++	jb	.Lcbc_enc_tail
+++	subq	$16,%rdx
+++	jmp	.Lcbc_enc_loop
+++.align	16
+++.Lcbc_enc_loop:
+++	movups	(%rdi),%xmm3
+++	leaq	16(%rdi),%rdi
+++
+++	movups	(%rcx),%xmm0
+++	movups	16(%rcx),%xmm1
+++	xorps	%xmm0,%xmm3
+++	leaq	32(%rcx),%rcx
+++	xorps	%xmm3,%xmm2
+++.Loop_enc1_6:
+++.byte	102,15,56,220,209
+++	decl	%eax
+++	movups	(%rcx),%xmm1
+++	leaq	16(%rcx),%rcx
+++	jnz	.Loop_enc1_6
+++.byte	102,15,56,221,209
+++	movl	%r10d,%eax
+++	movq	%r11,%rcx
+++	movups	%xmm2,0(%rsi)
+++	leaq	16(%rsi),%rsi
+++	subq	$16,%rdx
+++	jnc	.Lcbc_enc_loop
+++	addq	$16,%rdx
+++	jnz	.Lcbc_enc_tail
+++	pxor	%xmm0,%xmm0
+++	pxor	%xmm1,%xmm1
+++	movups	%xmm2,(%r8)
+++	pxor	%xmm2,%xmm2
+++	pxor	%xmm3,%xmm3
+++	jmp	.Lcbc_ret
+++
+++.Lcbc_enc_tail:
+++	movq	%rdx,%rcx
+++	xchgq	%rdi,%rsi
+++.long	0x9066A4F3
+++	movl	$16,%ecx
+++	subq	%rdx,%rcx
+++	xorl	%eax,%eax
+++.long	0x9066AAF3
+++	leaq	-16(%rdi),%rdi
+++	movl	%r10d,%eax
+++	movq	%rdi,%rsi
+++	movq	%r11,%rcx
+++	xorq	%rdx,%rdx
+++	jmp	.Lcbc_enc_loop
+++
+++.align	16
+++.Lcbc_decrypt:
+++	cmpq	$16,%rdx
+++	jne	.Lcbc_decrypt_bulk
+++
+++
+++
+++	movdqu	(%rdi),%xmm2
+++	movdqu	(%r8),%xmm3
+++	movdqa	%xmm2,%xmm4
+++	movups	(%rcx),%xmm0
+++	movups	16(%rcx),%xmm1
+++	leaq	32(%rcx),%rcx
+++	xorps	%xmm0,%xmm2
+++.Loop_dec1_7:
+++.byte	102,15,56,222,209
+++	decl	%r10d
+++	movups	(%rcx),%xmm1
+++	leaq	16(%rcx),%rcx
+++	jnz	.Loop_dec1_7
+++.byte	102,15,56,223,209
+++	pxor	%xmm0,%xmm0
+++	pxor	%xmm1,%xmm1
+++	movdqu	%xmm4,(%r8)
+++	xorps	%xmm3,%xmm2
+++	pxor	%xmm3,%xmm3
+++	movups	%xmm2,(%rsi)
+++	pxor	%xmm2,%xmm2
+++	jmp	.Lcbc_ret
+++.align	16
+++.Lcbc_decrypt_bulk:
+++	leaq	(%rsp),%r11
+++.cfi_def_cfa_register	%r11
+++	pushq	%rbp
+++.cfi_offset	%rbp,-16
+++	subq	$16,%rsp
+++	andq	$-16,%rsp
+++	movq	%rcx,%rbp
+++	movups	(%r8),%xmm10
+++	movl	%r10d,%eax
+++	cmpq	$0x50,%rdx
+++	jbe	.Lcbc_dec_tail
+++
+++	movups	(%rcx),%xmm0
+++	movdqu	0(%rdi),%xmm2
+++	movdqu	16(%rdi),%xmm3
+++	movdqa	%xmm2,%xmm11
+++	movdqu	32(%rdi),%xmm4
+++	movdqa	%xmm3,%xmm12
+++	movdqu	48(%rdi),%xmm5
+++	movdqa	%xmm4,%xmm13
+++	movdqu	64(%rdi),%xmm6
+++	movdqa	%xmm5,%xmm14
+++	movdqu	80(%rdi),%xmm7
+++	movdqa	%xmm6,%xmm15
+++	leaq	OPENSSL_ia32cap_P(%rip),%r9
+++	movl	4(%r9),%r9d
+++	cmpq	$0x70,%rdx
+++	jbe	.Lcbc_dec_six_or_seven
+++
+++	andl	$71303168,%r9d
+++	subq	$0x50,%rdx
+++	cmpl	$4194304,%r9d
+++	je	.Lcbc_dec_loop6_enter
+++	subq	$0x20,%rdx
+++	leaq	112(%rcx),%rcx
+++	jmp	.Lcbc_dec_loop8_enter
+++.align	16
+++.Lcbc_dec_loop8:
+++	movups	%xmm9,(%rsi)
+++	leaq	16(%rsi),%rsi
+++.Lcbc_dec_loop8_enter:
+++	movdqu	96(%rdi),%xmm8
+++	pxor	%xmm0,%xmm2
+++	movdqu	112(%rdi),%xmm9
+++	pxor	%xmm0,%xmm3
+++	movups	16-112(%rcx),%xmm1
+++	pxor	%xmm0,%xmm4
+++	movq	$-1,%rbp
+++	cmpq	$0x70,%rdx
+++	pxor	%xmm0,%xmm5
+++	pxor	%xmm0,%xmm6
+++	pxor	%xmm0,%xmm7
+++	pxor	%xmm0,%xmm8
+++
+++.byte	102,15,56,222,209
+++	pxor	%xmm0,%xmm9
+++	movups	32-112(%rcx),%xmm0
+++.byte	102,15,56,222,217
+++.byte	102,15,56,222,225
+++.byte	102,15,56,222,233
+++.byte	102,15,56,222,241
+++.byte	102,15,56,222,249
+++.byte	102,68,15,56,222,193
+++	adcq	$0,%rbp
+++	andq	$128,%rbp
+++.byte	102,68,15,56,222,201
+++	addq	%rdi,%rbp
+++	movups	48-112(%rcx),%xmm1
+++.byte	102,15,56,222,208
+++.byte	102,15,56,222,216
+++.byte	102,15,56,222,224
+++.byte	102,15,56,222,232
+++.byte	102,15,56,222,240
+++.byte	102,15,56,222,248
+++.byte	102,68,15,56,222,192
+++.byte	102,68,15,56,222,200
+++	movups	64-112(%rcx),%xmm0
+++	nop
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.byte	102,15,56,222,225
+++.byte	102,15,56,222,233
+++.byte	102,15,56,222,241
+++.byte	102,15,56,222,249
+++.byte	102,68,15,56,222,193
+++.byte	102,68,15,56,222,201
+++	movups	80-112(%rcx),%xmm1
+++	nop
+++.byte	102,15,56,222,208
+++.byte	102,15,56,222,216
+++.byte	102,15,56,222,224
+++.byte	102,15,56,222,232
+++.byte	102,15,56,222,240
+++.byte	102,15,56,222,248
+++.byte	102,68,15,56,222,192
+++.byte	102,68,15,56,222,200
+++	movups	96-112(%rcx),%xmm0
+++	nop
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.byte	102,15,56,222,225
+++.byte	102,15,56,222,233
+++.byte	102,15,56,222,241
+++.byte	102,15,56,222,249
+++.byte	102,68,15,56,222,193
+++.byte	102,68,15,56,222,201
+++	movups	112-112(%rcx),%xmm1
+++	nop
+++.byte	102,15,56,222,208
+++.byte	102,15,56,222,216
+++.byte	102,15,56,222,224
+++.byte	102,15,56,222,232
+++.byte	102,15,56,222,240
+++.byte	102,15,56,222,248
+++.byte	102,68,15,56,222,192
+++.byte	102,68,15,56,222,200
+++	movups	128-112(%rcx),%xmm0
+++	nop
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.byte	102,15,56,222,225
+++.byte	102,15,56,222,233
+++.byte	102,15,56,222,241
+++.byte	102,15,56,222,249
+++.byte	102,68,15,56,222,193
+++.byte	102,68,15,56,222,201
+++	movups	144-112(%rcx),%xmm1
+++	cmpl	$11,%eax
+++.byte	102,15,56,222,208
+++.byte	102,15,56,222,216
+++.byte	102,15,56,222,224
+++.byte	102,15,56,222,232
+++.byte	102,15,56,222,240
+++.byte	102,15,56,222,248
+++.byte	102,68,15,56,222,192
+++.byte	102,68,15,56,222,200
+++	movups	160-112(%rcx),%xmm0
+++	jb	.Lcbc_dec_done
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.byte	102,15,56,222,225
+++.byte	102,15,56,222,233
+++.byte	102,15,56,222,241
+++.byte	102,15,56,222,249
+++.byte	102,68,15,56,222,193
+++.byte	102,68,15,56,222,201
+++	movups	176-112(%rcx),%xmm1
+++	nop
+++.byte	102,15,56,222,208
+++.byte	102,15,56,222,216
+++.byte	102,15,56,222,224
+++.byte	102,15,56,222,232
+++.byte	102,15,56,222,240
+++.byte	102,15,56,222,248
+++.byte	102,68,15,56,222,192
+++.byte	102,68,15,56,222,200
+++	movups	192-112(%rcx),%xmm0
+++	je	.Lcbc_dec_done
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++.byte	102,15,56,222,225
+++.byte	102,15,56,222,233
+++.byte	102,15,56,222,241
+++.byte	102,15,56,222,249
+++.byte	102,68,15,56,222,193
+++.byte	102,68,15,56,222,201
+++	movups	208-112(%rcx),%xmm1
+++	nop
+++.byte	102,15,56,222,208
+++.byte	102,15,56,222,216
+++.byte	102,15,56,222,224
+++.byte	102,15,56,222,232
+++.byte	102,15,56,222,240
+++.byte	102,15,56,222,248
+++.byte	102,68,15,56,222,192
+++.byte	102,68,15,56,222,200
+++	movups	224-112(%rcx),%xmm0
+++	jmp	.Lcbc_dec_done
+++.align	16
+++.Lcbc_dec_done:
+++.byte	102,15,56,222,209
+++.byte	102,15,56,222,217
+++	pxor	%xmm0,%xmm10
+++	pxor	%xmm0,%xmm11
+++.byte	102,15,56,222,225
+++.byte	102,15,56,222,233
+++	pxor	%xmm0,%xmm12
+++	pxor	%xmm0,%xmm13
+++.byte	102,15,56,222,241
+++.byte	102,15,56,222,249
+++	pxor	%xmm0,%xmm14
+++	pxor	%xmm0,%xmm15
+++.byte	102,68,15,56,222,193
+++.byte	102,68,15,56,222,201
+++	movdqu	80(%rdi),%xmm1
+++
+++.byte	102,65,15,56,223,210
+++	movdqu	96(%rdi),%xmm10
+++	pxor	%xmm0,%xmm1
+++.byte	102,65,15,56,223,219
+++	pxor	%xmm0,%xmm10
+++	movdqu	112(%rdi),%xmm0
+++.byte	102,65,15,56,223,228
+++	leaq	128(%rdi),%rdi
+++	movdqu	0(%rbp),%xmm11
+++.byte	102,65,15,56,223,237
+++.byte	102,65,15,56,223,246
+++	movdqu	16(%rbp),%xmm12
+++	movdqu	32(%rbp),%xmm13
+++.byte	102,65,15,56,223,255
+++.byte	102,68,15,56,223,193
+++	movdqu	48(%rbp),%xmm14
+++	movdqu	64(%rbp),%xmm15
+++.byte	102,69,15,56,223,202
+++	movdqa	%xmm0,%xmm10
+++	movdqu	80(%rbp),%xmm1
+++	movups	-112(%rcx),%xmm0
+++
+++	movups	%xmm2,(%rsi)
+++	movdqa	%xmm11,%xmm2
+++	movups	%xmm3,16(%rsi)
+++	movdqa	%xmm12,%xmm3
+++	movups	%xmm4,32(%rsi)
+++	movdqa	%xmm13,%xmm4
+++	movups	%xmm5,48(%rsi)
+++	movdqa	%xmm14,%xmm5
+++	movups	%xmm6,64(%rsi)
+++	movdqa	%xmm15,%xmm6
+++	movups	%xmm7,80(%rsi)
+++	movdqa	%xmm1,%xmm7
+++	movups	%xmm8,96(%rsi)
+++	leaq	112(%rsi),%rsi
+++
+++	subq	$0x80,%rdx
+++	ja	.Lcbc_dec_loop8
+++
+++	movaps	%xmm9,%xmm2
+++	leaq	-112(%rcx),%rcx
+++	addq	$0x70,%rdx
+++	jle	.Lcbc_dec_clear_tail_collected
+++	movups	%xmm9,(%rsi)
+++	leaq	16(%rsi),%rsi
+++	cmpq	$0x50,%rdx
+++	jbe	.Lcbc_dec_tail
+++
+++	movaps	%xmm11,%xmm2
+++.Lcbc_dec_six_or_seven:
+++	cmpq	$0x60,%rdx
+++	ja	.Lcbc_dec_seven
+++
+++	movaps	%xmm7,%xmm8
+++	call	_aesni_decrypt6
+++	pxor	%xmm10,%xmm2
+++	movaps	%xmm8,%xmm10
+++	pxor	%xmm11,%xmm3
+++	movdqu	%xmm2,(%rsi)
+++	pxor	%xmm12,%xmm4
+++	movdqu	%xmm3,16(%rsi)
+++	pxor	%xmm3,%xmm3
+++	pxor	%xmm13,%xmm5
+++	movdqu	%xmm4,32(%rsi)
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm14,%xmm6
+++	movdqu	%xmm5,48(%rsi)
+++	pxor	%xmm5,%xmm5
+++	pxor	%xmm15,%xmm7
+++	movdqu	%xmm6,64(%rsi)
+++	pxor	%xmm6,%xmm6
+++	leaq	80(%rsi),%rsi
+++	movdqa	%xmm7,%xmm2
+++	pxor	%xmm7,%xmm7
+++	jmp	.Lcbc_dec_tail_collected
+++
+++.align	16
+++.Lcbc_dec_seven:
+++	movups	96(%rdi),%xmm8
+++	xorps	%xmm9,%xmm9
+++	call	_aesni_decrypt8
+++	movups	80(%rdi),%xmm9
+++	pxor	%xmm10,%xmm2
+++	movups	96(%rdi),%xmm10
+++	pxor	%xmm11,%xmm3
+++	movdqu	%xmm2,(%rsi)
+++	pxor	%xmm12,%xmm4
+++	movdqu	%xmm3,16(%rsi)
+++	pxor	%xmm3,%xmm3
+++	pxor	%xmm13,%xmm5
+++	movdqu	%xmm4,32(%rsi)
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm14,%xmm6
+++	movdqu	%xmm5,48(%rsi)
+++	pxor	%xmm5,%xmm5
+++	pxor	%xmm15,%xmm7
+++	movdqu	%xmm6,64(%rsi)
+++	pxor	%xmm6,%xmm6
+++	pxor	%xmm9,%xmm8
+++	movdqu	%xmm7,80(%rsi)
+++	pxor	%xmm7,%xmm7
+++	leaq	96(%rsi),%rsi
+++	movdqa	%xmm8,%xmm2
+++	pxor	%xmm8,%xmm8
+++	pxor	%xmm9,%xmm9
+++	jmp	.Lcbc_dec_tail_collected
+++
+++.align	16
+++.Lcbc_dec_loop6:
+++	movups	%xmm7,(%rsi)
+++	leaq	16(%rsi),%rsi
+++	movdqu	0(%rdi),%xmm2
+++	movdqu	16(%rdi),%xmm3
+++	movdqa	%xmm2,%xmm11
+++	movdqu	32(%rdi),%xmm4
+++	movdqa	%xmm3,%xmm12
+++	movdqu	48(%rdi),%xmm5
+++	movdqa	%xmm4,%xmm13
+++	movdqu	64(%rdi),%xmm6
+++	movdqa	%xmm5,%xmm14
+++	movdqu	80(%rdi),%xmm7
+++	movdqa	%xmm6,%xmm15
+++.Lcbc_dec_loop6_enter:
+++	leaq	96(%rdi),%rdi
+++	movdqa	%xmm7,%xmm8
+++
+++	call	_aesni_decrypt6
+++
+++	pxor	%xmm10,%xmm2
+++	movdqa	%xmm8,%xmm10
+++	pxor	%xmm11,%xmm3
+++	movdqu	%xmm2,(%rsi)
+++	pxor	%xmm12,%xmm4
+++	movdqu	%xmm3,16(%rsi)
+++	pxor	%xmm13,%xmm5
+++	movdqu	%xmm4,32(%rsi)
+++	pxor	%xmm14,%xmm6
+++	movq	%rbp,%rcx
+++	movdqu	%xmm5,48(%rsi)
+++	pxor	%xmm15,%xmm7
+++	movl	%r10d,%eax
+++	movdqu	%xmm6,64(%rsi)
+++	leaq	80(%rsi),%rsi
+++	subq	$0x60,%rdx
+++	ja	.Lcbc_dec_loop6
+++
+++	movdqa	%xmm7,%xmm2
+++	addq	$0x50,%rdx
+++	jle	.Lcbc_dec_clear_tail_collected
+++	movups	%xmm7,(%rsi)
+++	leaq	16(%rsi),%rsi
+++
+++.Lcbc_dec_tail:
+++	movups	(%rdi),%xmm2
+++	subq	$0x10,%rdx
+++	jbe	.Lcbc_dec_one
+++
+++	movups	16(%rdi),%xmm3
+++	movaps	%xmm2,%xmm11
+++	subq	$0x10,%rdx
+++	jbe	.Lcbc_dec_two
+++
+++	movups	32(%rdi),%xmm4
+++	movaps	%xmm3,%xmm12
+++	subq	$0x10,%rdx
+++	jbe	.Lcbc_dec_three
+++
+++	movups	48(%rdi),%xmm5
+++	movaps	%xmm4,%xmm13
+++	subq	$0x10,%rdx
+++	jbe	.Lcbc_dec_four
+++
+++	movups	64(%rdi),%xmm6
+++	movaps	%xmm5,%xmm14
+++	movaps	%xmm6,%xmm15
+++	xorps	%xmm7,%xmm7
+++	call	_aesni_decrypt6
+++	pxor	%xmm10,%xmm2
+++	movaps	%xmm15,%xmm10
+++	pxor	%xmm11,%xmm3
+++	movdqu	%xmm2,(%rsi)
+++	pxor	%xmm12,%xmm4
+++	movdqu	%xmm3,16(%rsi)
+++	pxor	%xmm3,%xmm3
+++	pxor	%xmm13,%xmm5
+++	movdqu	%xmm4,32(%rsi)
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm14,%xmm6
+++	movdqu	%xmm5,48(%rsi)
+++	pxor	%xmm5,%xmm5
+++	leaq	64(%rsi),%rsi
+++	movdqa	%xmm6,%xmm2
+++	pxor	%xmm6,%xmm6
+++	pxor	%xmm7,%xmm7
+++	subq	$0x10,%rdx
+++	jmp	.Lcbc_dec_tail_collected
+++
+++.align	16
+++.Lcbc_dec_one:
+++	movaps	%xmm2,%xmm11
+++	movups	(%rcx),%xmm0
+++	movups	16(%rcx),%xmm1
+++	leaq	32(%rcx),%rcx
+++	xorps	%xmm0,%xmm2
+++.Loop_dec1_8:
+++.byte	102,15,56,222,209
+++	decl	%eax
+++	movups	(%rcx),%xmm1
+++	leaq	16(%rcx),%rcx
+++	jnz	.Loop_dec1_8
+++.byte	102,15,56,223,209
+++	xorps	%xmm10,%xmm2
+++	movaps	%xmm11,%xmm10
+++	jmp	.Lcbc_dec_tail_collected
+++.align	16
+++.Lcbc_dec_two:
+++	movaps	%xmm3,%xmm12
+++	call	_aesni_decrypt2
+++	pxor	%xmm10,%xmm2
+++	movaps	%xmm12,%xmm10
+++	pxor	%xmm11,%xmm3
+++	movdqu	%xmm2,(%rsi)
+++	movdqa	%xmm3,%xmm2
+++	pxor	%xmm3,%xmm3
+++	leaq	16(%rsi),%rsi
+++	jmp	.Lcbc_dec_tail_collected
+++.align	16
+++.Lcbc_dec_three:
+++	movaps	%xmm4,%xmm13
+++	call	_aesni_decrypt3
+++	pxor	%xmm10,%xmm2
+++	movaps	%xmm13,%xmm10
+++	pxor	%xmm11,%xmm3
+++	movdqu	%xmm2,(%rsi)
+++	pxor	%xmm12,%xmm4
+++	movdqu	%xmm3,16(%rsi)
+++	pxor	%xmm3,%xmm3
+++	movdqa	%xmm4,%xmm2
+++	pxor	%xmm4,%xmm4
+++	leaq	32(%rsi),%rsi
+++	jmp	.Lcbc_dec_tail_collected
+++.align	16
+++.Lcbc_dec_four:
+++	movaps	%xmm5,%xmm14
+++	call	_aesni_decrypt4
+++	pxor	%xmm10,%xmm2
+++	movaps	%xmm14,%xmm10
+++	pxor	%xmm11,%xmm3
+++	movdqu	%xmm2,(%rsi)
+++	pxor	%xmm12,%xmm4
+++	movdqu	%xmm3,16(%rsi)
+++	pxor	%xmm3,%xmm3
+++	pxor	%xmm13,%xmm5
+++	movdqu	%xmm4,32(%rsi)
+++	pxor	%xmm4,%xmm4
+++	movdqa	%xmm5,%xmm2
+++	pxor	%xmm5,%xmm5
+++	leaq	48(%rsi),%rsi
+++	jmp	.Lcbc_dec_tail_collected
+++
+++.align	16
+++.Lcbc_dec_clear_tail_collected:
+++	pxor	%xmm3,%xmm3
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm5,%xmm5
+++	pxor	%xmm6,%xmm6
+++	pxor	%xmm7,%xmm7
+++	pxor	%xmm8,%xmm8
+++	pxor	%xmm9,%xmm9
+++.Lcbc_dec_tail_collected:
+++	movups	%xmm10,(%r8)
+++	andq	$15,%rdx
+++	jnz	.Lcbc_dec_tail_partial
+++	movups	%xmm2,(%rsi)
+++	pxor	%xmm2,%xmm2
+++	jmp	.Lcbc_dec_ret
+++.align	16
+++.Lcbc_dec_tail_partial:
+++	movaps	%xmm2,(%rsp)
+++	pxor	%xmm2,%xmm2
+++	movq	$16,%rcx
+++	movq	%rsi,%rdi
+++	subq	%rdx,%rcx
+++	leaq	(%rsp),%rsi
+++.long	0x9066A4F3
+++	movdqa	%xmm2,(%rsp)
+++
+++.Lcbc_dec_ret:
+++	xorps	%xmm0,%xmm0
+++	pxor	%xmm1,%xmm1
+++	movq	-8(%r11),%rbp
+++.cfi_restore	%rbp
+++	leaq	(%r11),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lcbc_ret:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	aes_hw_cbc_encrypt,.-aes_hw_cbc_encrypt
+++.globl	aes_hw_set_decrypt_key
+++.hidden aes_hw_set_decrypt_key
+++.type	aes_hw_set_decrypt_key,@function
+++.align	16
+++aes_hw_set_decrypt_key:
+++.cfi_startproc	
+++.byte	0x48,0x83,0xEC,0x08
+++.cfi_adjust_cfa_offset	8
+++	call	__aesni_set_encrypt_key
+++	shll	$4,%esi
+++	testl	%eax,%eax
+++	jnz	.Ldec_key_ret
+++	leaq	16(%rdx,%rsi,1),%rdi
+++
+++	movups	(%rdx),%xmm0
+++	movups	(%rdi),%xmm1
+++	movups	%xmm0,(%rdi)
+++	movups	%xmm1,(%rdx)
+++	leaq	16(%rdx),%rdx
+++	leaq	-16(%rdi),%rdi
+++
+++.Ldec_key_inverse:
+++	movups	(%rdx),%xmm0
+++	movups	(%rdi),%xmm1
+++.byte	102,15,56,219,192
+++.byte	102,15,56,219,201
+++	leaq	16(%rdx),%rdx
+++	leaq	-16(%rdi),%rdi
+++	movups	%xmm0,16(%rdi)
+++	movups	%xmm1,-16(%rdx)
+++	cmpq	%rdx,%rdi
+++	ja	.Ldec_key_inverse
+++
+++	movups	(%rdx),%xmm0
+++.byte	102,15,56,219,192
+++	pxor	%xmm1,%xmm1
+++	movups	%xmm0,(%rdi)
+++	pxor	%xmm0,%xmm0
+++.Ldec_key_ret:
+++	addq	$8,%rsp
+++.cfi_adjust_cfa_offset	-8
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.LSEH_end_set_decrypt_key:
+++.size	aes_hw_set_decrypt_key,.-aes_hw_set_decrypt_key
+++.globl	aes_hw_set_encrypt_key
+++.hidden aes_hw_set_encrypt_key
+++.type	aes_hw_set_encrypt_key,@function
+++.align	16
+++aes_hw_set_encrypt_key:
+++__aesni_set_encrypt_key:
+++.cfi_startproc	
+++#ifdef BORINGSSL_DISPATCH_TEST
+++	movb	$1,BORINGSSL_function_hit+3(%rip)
+++#endif
+++.byte	0x48,0x83,0xEC,0x08
+++.cfi_adjust_cfa_offset	8
+++	movq	$-1,%rax
+++	testq	%rdi,%rdi
+++	jz	.Lenc_key_ret
+++	testq	%rdx,%rdx
+++	jz	.Lenc_key_ret
+++
+++	movups	(%rdi),%xmm0
+++	xorps	%xmm4,%xmm4
+++	leaq	OPENSSL_ia32cap_P(%rip),%r10
+++	movl	4(%r10),%r10d
+++	andl	$268437504,%r10d
+++	leaq	16(%rdx),%rax
+++	cmpl	$256,%esi
+++	je	.L14rounds
+++	cmpl	$192,%esi
+++	je	.L12rounds
+++	cmpl	$128,%esi
+++	jne	.Lbad_keybits
+++
+++.L10rounds:
+++	movl	$9,%esi
+++	cmpl	$268435456,%r10d
+++	je	.L10rounds_alt
+++
+++	movups	%xmm0,(%rdx)
+++.byte	102,15,58,223,200,1
+++	call	.Lkey_expansion_128_cold
+++.byte	102,15,58,223,200,2
+++	call	.Lkey_expansion_128
+++.byte	102,15,58,223,200,4
+++	call	.Lkey_expansion_128
+++.byte	102,15,58,223,200,8
+++	call	.Lkey_expansion_128
+++.byte	102,15,58,223,200,16
+++	call	.Lkey_expansion_128
+++.byte	102,15,58,223,200,32
+++	call	.Lkey_expansion_128
+++.byte	102,15,58,223,200,64
+++	call	.Lkey_expansion_128
+++.byte	102,15,58,223,200,128
+++	call	.Lkey_expansion_128
+++.byte	102,15,58,223,200,27
+++	call	.Lkey_expansion_128
+++.byte	102,15,58,223,200,54
+++	call	.Lkey_expansion_128
+++	movups	%xmm0,(%rax)
+++	movl	%esi,80(%rax)
+++	xorl	%eax,%eax
+++	jmp	.Lenc_key_ret
+++
+++.align	16
+++.L10rounds_alt:
+++	movdqa	.Lkey_rotate(%rip),%xmm5
+++	movl	$8,%r10d
+++	movdqa	.Lkey_rcon1(%rip),%xmm4
+++	movdqa	%xmm0,%xmm2
+++	movdqu	%xmm0,(%rdx)
+++	jmp	.Loop_key128
+++
+++.align	16
+++.Loop_key128:
+++.byte	102,15,56,0,197
+++.byte	102,15,56,221,196
+++	pslld	$1,%xmm4
+++	leaq	16(%rax),%rax
+++
+++	movdqa	%xmm2,%xmm3
+++	pslldq	$4,%xmm2
+++	pxor	%xmm2,%xmm3
+++	pslldq	$4,%xmm2
+++	pxor	%xmm2,%xmm3
+++	pslldq	$4,%xmm2
+++	pxor	%xmm3,%xmm2
+++
+++	pxor	%xmm2,%xmm0
+++	movdqu	%xmm0,-16(%rax)
+++	movdqa	%xmm0,%xmm2
+++
+++	decl	%r10d
+++	jnz	.Loop_key128
+++
+++	movdqa	.Lkey_rcon1b(%rip),%xmm4
+++
+++.byte	102,15,56,0,197
+++.byte	102,15,56,221,196
+++	pslld	$1,%xmm4
+++
+++	movdqa	%xmm2,%xmm3
+++	pslldq	$4,%xmm2
+++	pxor	%xmm2,%xmm3
+++	pslldq	$4,%xmm2
+++	pxor	%xmm2,%xmm3
+++	pslldq	$4,%xmm2
+++	pxor	%xmm3,%xmm2
+++
+++	pxor	%xmm2,%xmm0
+++	movdqu	%xmm0,(%rax)
+++
+++	movdqa	%xmm0,%xmm2
+++.byte	102,15,56,0,197
+++.byte	102,15,56,221,196
+++
+++	movdqa	%xmm2,%xmm3
+++	pslldq	$4,%xmm2
+++	pxor	%xmm2,%xmm3
+++	pslldq	$4,%xmm2
+++	pxor	%xmm2,%xmm3
+++	pslldq	$4,%xmm2
+++	pxor	%xmm3,%xmm2
+++
+++	pxor	%xmm2,%xmm0
+++	movdqu	%xmm0,16(%rax)
+++
+++	movl	%esi,96(%rax)
+++	xorl	%eax,%eax
+++	jmp	.Lenc_key_ret
+++
+++.align	16
+++.L12rounds:
+++	movq	16(%rdi),%xmm2
+++	movl	$11,%esi
+++	cmpl	$268435456,%r10d
+++	je	.L12rounds_alt
+++
+++	movups	%xmm0,(%rdx)
+++.byte	102,15,58,223,202,1
+++	call	.Lkey_expansion_192a_cold
+++.byte	102,15,58,223,202,2
+++	call	.Lkey_expansion_192b
+++.byte	102,15,58,223,202,4
+++	call	.Lkey_expansion_192a
+++.byte	102,15,58,223,202,8
+++	call	.Lkey_expansion_192b
+++.byte	102,15,58,223,202,16
+++	call	.Lkey_expansion_192a
+++.byte	102,15,58,223,202,32
+++	call	.Lkey_expansion_192b
+++.byte	102,15,58,223,202,64
+++	call	.Lkey_expansion_192a
+++.byte	102,15,58,223,202,128
+++	call	.Lkey_expansion_192b
+++	movups	%xmm0,(%rax)
+++	movl	%esi,48(%rax)
+++	xorq	%rax,%rax
+++	jmp	.Lenc_key_ret
+++
+++.align	16
+++.L12rounds_alt:
+++	movdqa	.Lkey_rotate192(%rip),%xmm5
+++	movdqa	.Lkey_rcon1(%rip),%xmm4
+++	movl	$8,%r10d
+++	movdqu	%xmm0,(%rdx)
+++	jmp	.Loop_key192
+++
+++.align	16
+++.Loop_key192:
+++	movq	%xmm2,0(%rax)
+++	movdqa	%xmm2,%xmm1
+++.byte	102,15,56,0,213
+++.byte	102,15,56,221,212
+++	pslld	$1,%xmm4
+++	leaq	24(%rax),%rax
+++
+++	movdqa	%xmm0,%xmm3
+++	pslldq	$4,%xmm0
+++	pxor	%xmm0,%xmm3
+++	pslldq	$4,%xmm0
+++	pxor	%xmm0,%xmm3
+++	pslldq	$4,%xmm0
+++	pxor	%xmm3,%xmm0
+++
+++	pshufd	$0xff,%xmm0,%xmm3
+++	pxor	%xmm1,%xmm3
+++	pslldq	$4,%xmm1
+++	pxor	%xmm1,%xmm3
+++
+++	pxor	%xmm2,%xmm0
+++	pxor	%xmm3,%xmm2
+++	movdqu	%xmm0,-16(%rax)
+++
+++	decl	%r10d
+++	jnz	.Loop_key192
+++
+++	movl	%esi,32(%rax)
+++	xorl	%eax,%eax
+++	jmp	.Lenc_key_ret
+++
+++.align	16
+++.L14rounds:
+++	movups	16(%rdi),%xmm2
+++	movl	$13,%esi
+++	leaq	16(%rax),%rax
+++	cmpl	$268435456,%r10d
+++	je	.L14rounds_alt
+++
+++	movups	%xmm0,(%rdx)
+++	movups	%xmm2,16(%rdx)
+++.byte	102,15,58,223,202,1
+++	call	.Lkey_expansion_256a_cold
+++.byte	102,15,58,223,200,1
+++	call	.Lkey_expansion_256b
+++.byte	102,15,58,223,202,2
+++	call	.Lkey_expansion_256a
+++.byte	102,15,58,223,200,2
+++	call	.Lkey_expansion_256b
+++.byte	102,15,58,223,202,4
+++	call	.Lkey_expansion_256a
+++.byte	102,15,58,223,200,4
+++	call	.Lkey_expansion_256b
+++.byte	102,15,58,223,202,8
+++	call	.Lkey_expansion_256a
+++.byte	102,15,58,223,200,8
+++	call	.Lkey_expansion_256b
+++.byte	102,15,58,223,202,16
+++	call	.Lkey_expansion_256a
+++.byte	102,15,58,223,200,16
+++	call	.Lkey_expansion_256b
+++.byte	102,15,58,223,202,32
+++	call	.Lkey_expansion_256a
+++.byte	102,15,58,223,200,32
+++	call	.Lkey_expansion_256b
+++.byte	102,15,58,223,202,64
+++	call	.Lkey_expansion_256a
+++	movups	%xmm0,(%rax)
+++	movl	%esi,16(%rax)
+++	xorq	%rax,%rax
+++	jmp	.Lenc_key_ret
+++
+++.align	16
+++.L14rounds_alt:
+++	movdqa	.Lkey_rotate(%rip),%xmm5
+++	movdqa	.Lkey_rcon1(%rip),%xmm4
+++	movl	$7,%r10d
+++	movdqu	%xmm0,0(%rdx)
+++	movdqa	%xmm2,%xmm1
+++	movdqu	%xmm2,16(%rdx)
+++	jmp	.Loop_key256
+++
+++.align	16
+++.Loop_key256:
+++.byte	102,15,56,0,213
+++.byte	102,15,56,221,212
+++
+++	movdqa	%xmm0,%xmm3
+++	pslldq	$4,%xmm0
+++	pxor	%xmm0,%xmm3
+++	pslldq	$4,%xmm0
+++	pxor	%xmm0,%xmm3
+++	pslldq	$4,%xmm0
+++	pxor	%xmm3,%xmm0
+++	pslld	$1,%xmm4
+++
+++	pxor	%xmm2,%xmm0
+++	movdqu	%xmm0,(%rax)
+++
+++	decl	%r10d
+++	jz	.Ldone_key256
+++
+++	pshufd	$0xff,%xmm0,%xmm2
+++	pxor	%xmm3,%xmm3
+++.byte	102,15,56,221,211
+++
+++	movdqa	%xmm1,%xmm3
+++	pslldq	$4,%xmm1
+++	pxor	%xmm1,%xmm3
+++	pslldq	$4,%xmm1
+++	pxor	%xmm1,%xmm3
+++	pslldq	$4,%xmm1
+++	pxor	%xmm3,%xmm1
+++
+++	pxor	%xmm1,%xmm2
+++	movdqu	%xmm2,16(%rax)
+++	leaq	32(%rax),%rax
+++	movdqa	%xmm2,%xmm1
+++
+++	jmp	.Loop_key256
+++
+++.Ldone_key256:
+++	movl	%esi,16(%rax)
+++	xorl	%eax,%eax
+++	jmp	.Lenc_key_ret
+++
+++.align	16
+++.Lbad_keybits:
+++	movq	$-2,%rax
+++.Lenc_key_ret:
+++	pxor	%xmm0,%xmm0
+++	pxor	%xmm1,%xmm1
+++	pxor	%xmm2,%xmm2
+++	pxor	%xmm3,%xmm3
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm5,%xmm5
+++	addq	$8,%rsp
+++.cfi_adjust_cfa_offset	-8
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.LSEH_end_set_encrypt_key:
+++
+++.align	16
+++.Lkey_expansion_128:
+++	movups	%xmm0,(%rax)
+++	leaq	16(%rax),%rax
+++.Lkey_expansion_128_cold:
+++	shufps	$16,%xmm0,%xmm4
+++	xorps	%xmm4,%xmm0
+++	shufps	$140,%xmm0,%xmm4
+++	xorps	%xmm4,%xmm0
+++	shufps	$255,%xmm1,%xmm1
+++	xorps	%xmm1,%xmm0
+++	.byte	0xf3,0xc3
+++
+++.align	16
+++.Lkey_expansion_192a:
+++	movups	%xmm0,(%rax)
+++	leaq	16(%rax),%rax
+++.Lkey_expansion_192a_cold:
+++	movaps	%xmm2,%xmm5
+++.Lkey_expansion_192b_warm:
+++	shufps	$16,%xmm0,%xmm4
+++	movdqa	%xmm2,%xmm3
+++	xorps	%xmm4,%xmm0
+++	shufps	$140,%xmm0,%xmm4
+++	pslldq	$4,%xmm3
+++	xorps	%xmm4,%xmm0
+++	pshufd	$85,%xmm1,%xmm1
+++	pxor	%xmm3,%xmm2
+++	pxor	%xmm1,%xmm0
+++	pshufd	$255,%xmm0,%xmm3
+++	pxor	%xmm3,%xmm2
+++	.byte	0xf3,0xc3
+++
+++.align	16
+++.Lkey_expansion_192b:
+++	movaps	%xmm0,%xmm3
+++	shufps	$68,%xmm0,%xmm5
+++	movups	%xmm5,(%rax)
+++	shufps	$78,%xmm2,%xmm3
+++	movups	%xmm3,16(%rax)
+++	leaq	32(%rax),%rax
+++	jmp	.Lkey_expansion_192b_warm
+++
+++.align	16
+++.Lkey_expansion_256a:
+++	movups	%xmm2,(%rax)
+++	leaq	16(%rax),%rax
+++.Lkey_expansion_256a_cold:
+++	shufps	$16,%xmm0,%xmm4
+++	xorps	%xmm4,%xmm0
+++	shufps	$140,%xmm0,%xmm4
+++	xorps	%xmm4,%xmm0
+++	shufps	$255,%xmm1,%xmm1
+++	xorps	%xmm1,%xmm0
+++	.byte	0xf3,0xc3
+++
+++.align	16
+++.Lkey_expansion_256b:
+++	movups	%xmm0,(%rax)
+++	leaq	16(%rax),%rax
+++
+++	shufps	$16,%xmm2,%xmm4
+++	xorps	%xmm4,%xmm2
+++	shufps	$140,%xmm2,%xmm4
+++	xorps	%xmm4,%xmm2
+++	shufps	$170,%xmm1,%xmm1
+++	xorps	%xmm1,%xmm2
+++	.byte	0xf3,0xc3
+++.size	aes_hw_set_encrypt_key,.-aes_hw_set_encrypt_key
+++.size	__aesni_set_encrypt_key,.-__aesni_set_encrypt_key
+++.align	64
+++.Lbswap_mask:
+++.byte	15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0
+++.Lincrement32:
+++.long	6,6,6,0
+++.Lincrement64:
+++.long	1,0,0,0
+++.Lxts_magic:
+++.long	0x87,0,1,0
+++.Lincrement1:
+++.byte	0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
+++.Lkey_rotate:
+++.long	0x0c0f0e0d,0x0c0f0e0d,0x0c0f0e0d,0x0c0f0e0d
+++.Lkey_rotate192:
+++.long	0x04070605,0x04070605,0x04070605,0x04070605
+++.Lkey_rcon1:
+++.long	1,1,1,1
+++.Lkey_rcon1b:
+++.long	0x1b,0x1b,0x1b,0x1b
+++
+++.byte	65,69,83,32,102,111,114,32,73,110,116,101,108,32,65,69,83,45,78,73,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
+++.align	64
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/ghash-ssse3-x86_64.S b/linux-x86_64/ypto/fipsmodule/ghash-ssse3-x86_64.S
++new file mode 100644
++index 000000000..a44790b16
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/ghash-ssse3-x86_64.S
++@@ -0,0 +1,427 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++
+++
+++
+++
+++
+++.type	gcm_gmult_ssse3, @function
+++.globl	gcm_gmult_ssse3
+++.hidden gcm_gmult_ssse3
+++.align	16
+++gcm_gmult_ssse3:
+++.cfi_startproc	
+++.Lgmult_seh_begin:
+++	movdqu	(%rdi),%xmm0
+++	movdqa	.Lreverse_bytes(%rip),%xmm10
+++	movdqa	.Llow4_mask(%rip),%xmm2
+++
+++
+++.byte	102,65,15,56,0,194
+++
+++
+++	movdqa	%xmm2,%xmm1
+++	pandn	%xmm0,%xmm1
+++	psrld	$4,%xmm1
+++	pand	%xmm2,%xmm0
+++
+++
+++
+++
+++	pxor	%xmm2,%xmm2
+++	pxor	%xmm3,%xmm3
+++	movq	$5,%rax
+++.Loop_row_1:
+++	movdqa	(%rsi),%xmm4
+++	leaq	16(%rsi),%rsi
+++
+++
+++	movdqa	%xmm2,%xmm6
+++.byte	102,15,58,15,243,1
+++	movdqa	%xmm6,%xmm3
+++	psrldq	$1,%xmm2
+++
+++
+++
+++
+++	movdqa	%xmm4,%xmm5
+++.byte	102,15,56,0,224
+++.byte	102,15,56,0,233
+++
+++
+++	pxor	%xmm5,%xmm2
+++
+++
+++
+++	movdqa	%xmm4,%xmm5
+++	psllq	$60,%xmm5
+++	movdqa	%xmm5,%xmm6
+++	pslldq	$8,%xmm6
+++	pxor	%xmm6,%xmm3
+++
+++
+++	psrldq	$8,%xmm5
+++	pxor	%xmm5,%xmm2
+++	psrlq	$4,%xmm4
+++	pxor	%xmm4,%xmm2
+++
+++	subq	$1,%rax
+++	jnz	.Loop_row_1
+++
+++
+++
+++	pxor	%xmm3,%xmm2
+++	psrlq	$1,%xmm3
+++	pxor	%xmm3,%xmm2
+++	psrlq	$1,%xmm3
+++	pxor	%xmm3,%xmm2
+++	psrlq	$5,%xmm3
+++	pxor	%xmm3,%xmm2
+++	pxor	%xmm3,%xmm3
+++	movq	$5,%rax
+++.Loop_row_2:
+++	movdqa	(%rsi),%xmm4
+++	leaq	16(%rsi),%rsi
+++
+++
+++	movdqa	%xmm2,%xmm6
+++.byte	102,15,58,15,243,1
+++	movdqa	%xmm6,%xmm3
+++	psrldq	$1,%xmm2
+++
+++
+++
+++
+++	movdqa	%xmm4,%xmm5
+++.byte	102,15,56,0,224
+++.byte	102,15,56,0,233
+++
+++
+++	pxor	%xmm5,%xmm2
+++
+++
+++
+++	movdqa	%xmm4,%xmm5
+++	psllq	$60,%xmm5
+++	movdqa	%xmm5,%xmm6
+++	pslldq	$8,%xmm6
+++	pxor	%xmm6,%xmm3
+++
+++
+++	psrldq	$8,%xmm5
+++	pxor	%xmm5,%xmm2
+++	psrlq	$4,%xmm4
+++	pxor	%xmm4,%xmm2
+++
+++	subq	$1,%rax
+++	jnz	.Loop_row_2
+++
+++
+++
+++	pxor	%xmm3,%xmm2
+++	psrlq	$1,%xmm3
+++	pxor	%xmm3,%xmm2
+++	psrlq	$1,%xmm3
+++	pxor	%xmm3,%xmm2
+++	psrlq	$5,%xmm3
+++	pxor	%xmm3,%xmm2
+++	pxor	%xmm3,%xmm3
+++	movq	$6,%rax
+++.Loop_row_3:
+++	movdqa	(%rsi),%xmm4
+++	leaq	16(%rsi),%rsi
+++
+++
+++	movdqa	%xmm2,%xmm6
+++.byte	102,15,58,15,243,1
+++	movdqa	%xmm6,%xmm3
+++	psrldq	$1,%xmm2
+++
+++
+++
+++
+++	movdqa	%xmm4,%xmm5
+++.byte	102,15,56,0,224
+++.byte	102,15,56,0,233
+++
+++
+++	pxor	%xmm5,%xmm2
+++
+++
+++
+++	movdqa	%xmm4,%xmm5
+++	psllq	$60,%xmm5
+++	movdqa	%xmm5,%xmm6
+++	pslldq	$8,%xmm6
+++	pxor	%xmm6,%xmm3
+++
+++
+++	psrldq	$8,%xmm5
+++	pxor	%xmm5,%xmm2
+++	psrlq	$4,%xmm4
+++	pxor	%xmm4,%xmm2
+++
+++	subq	$1,%rax
+++	jnz	.Loop_row_3
+++
+++
+++
+++	pxor	%xmm3,%xmm2
+++	psrlq	$1,%xmm3
+++	pxor	%xmm3,%xmm2
+++	psrlq	$1,%xmm3
+++	pxor	%xmm3,%xmm2
+++	psrlq	$5,%xmm3
+++	pxor	%xmm3,%xmm2
+++	pxor	%xmm3,%xmm3
+++
+++.byte	102,65,15,56,0,210
+++	movdqu	%xmm2,(%rdi)
+++
+++
+++	pxor	%xmm0,%xmm0
+++	pxor	%xmm1,%xmm1
+++	pxor	%xmm2,%xmm2
+++	pxor	%xmm3,%xmm3
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm5,%xmm5
+++	pxor	%xmm6,%xmm6
+++	.byte	0xf3,0xc3
+++.Lgmult_seh_end:
+++.cfi_endproc	
+++.size	gcm_gmult_ssse3,.-gcm_gmult_ssse3
+++
+++
+++
+++
+++
+++.type	gcm_ghash_ssse3, @function
+++.globl	gcm_ghash_ssse3
+++.hidden gcm_ghash_ssse3
+++.align	16
+++gcm_ghash_ssse3:
+++.Lghash_seh_begin:
+++.cfi_startproc	
+++	movdqu	(%rdi),%xmm0
+++	movdqa	.Lreverse_bytes(%rip),%xmm10
+++	movdqa	.Llow4_mask(%rip),%xmm11
+++
+++
+++	andq	$-16,%rcx
+++
+++
+++
+++.byte	102,65,15,56,0,194
+++
+++
+++	pxor	%xmm3,%xmm3
+++.Loop_ghash:
+++
+++	movdqu	(%rdx),%xmm1
+++.byte	102,65,15,56,0,202
+++	pxor	%xmm1,%xmm0
+++
+++
+++	movdqa	%xmm11,%xmm1
+++	pandn	%xmm0,%xmm1
+++	psrld	$4,%xmm1
+++	pand	%xmm11,%xmm0
+++
+++
+++
+++
+++	pxor	%xmm2,%xmm2
+++
+++	movq	$5,%rax
+++.Loop_row_4:
+++	movdqa	(%rsi),%xmm4
+++	leaq	16(%rsi),%rsi
+++
+++
+++	movdqa	%xmm2,%xmm6
+++.byte	102,15,58,15,243,1
+++	movdqa	%xmm6,%xmm3
+++	psrldq	$1,%xmm2
+++
+++
+++
+++
+++	movdqa	%xmm4,%xmm5
+++.byte	102,15,56,0,224
+++.byte	102,15,56,0,233
+++
+++
+++	pxor	%xmm5,%xmm2
+++
+++
+++
+++	movdqa	%xmm4,%xmm5
+++	psllq	$60,%xmm5
+++	movdqa	%xmm5,%xmm6
+++	pslldq	$8,%xmm6
+++	pxor	%xmm6,%xmm3
+++
+++
+++	psrldq	$8,%xmm5
+++	pxor	%xmm5,%xmm2
+++	psrlq	$4,%xmm4
+++	pxor	%xmm4,%xmm2
+++
+++	subq	$1,%rax
+++	jnz	.Loop_row_4
+++
+++
+++
+++	pxor	%xmm3,%xmm2
+++	psrlq	$1,%xmm3
+++	pxor	%xmm3,%xmm2
+++	psrlq	$1,%xmm3
+++	pxor	%xmm3,%xmm2
+++	psrlq	$5,%xmm3
+++	pxor	%xmm3,%xmm2
+++	pxor	%xmm3,%xmm3
+++	movq	$5,%rax
+++.Loop_row_5:
+++	movdqa	(%rsi),%xmm4
+++	leaq	16(%rsi),%rsi
+++
+++
+++	movdqa	%xmm2,%xmm6
+++.byte	102,15,58,15,243,1
+++	movdqa	%xmm6,%xmm3
+++	psrldq	$1,%xmm2
+++
+++
+++
+++
+++	movdqa	%xmm4,%xmm5
+++.byte	102,15,56,0,224
+++.byte	102,15,56,0,233
+++
+++
+++	pxor	%xmm5,%xmm2
+++
+++
+++
+++	movdqa	%xmm4,%xmm5
+++	psllq	$60,%xmm5
+++	movdqa	%xmm5,%xmm6
+++	pslldq	$8,%xmm6
+++	pxor	%xmm6,%xmm3
+++
+++
+++	psrldq	$8,%xmm5
+++	pxor	%xmm5,%xmm2
+++	psrlq	$4,%xmm4
+++	pxor	%xmm4,%xmm2
+++
+++	subq	$1,%rax
+++	jnz	.Loop_row_5
+++
+++
+++
+++	pxor	%xmm3,%xmm2
+++	psrlq	$1,%xmm3
+++	pxor	%xmm3,%xmm2
+++	psrlq	$1,%xmm3
+++	pxor	%xmm3,%xmm2
+++	psrlq	$5,%xmm3
+++	pxor	%xmm3,%xmm2
+++	pxor	%xmm3,%xmm3
+++	movq	$6,%rax
+++.Loop_row_6:
+++	movdqa	(%rsi),%xmm4
+++	leaq	16(%rsi),%rsi
+++
+++
+++	movdqa	%xmm2,%xmm6
+++.byte	102,15,58,15,243,1
+++	movdqa	%xmm6,%xmm3
+++	psrldq	$1,%xmm2
+++
+++
+++
+++
+++	movdqa	%xmm4,%xmm5
+++.byte	102,15,56,0,224
+++.byte	102,15,56,0,233
+++
+++
+++	pxor	%xmm5,%xmm2
+++
+++
+++
+++	movdqa	%xmm4,%xmm5
+++	psllq	$60,%xmm5
+++	movdqa	%xmm5,%xmm6
+++	pslldq	$8,%xmm6
+++	pxor	%xmm6,%xmm3
+++
+++
+++	psrldq	$8,%xmm5
+++	pxor	%xmm5,%xmm2
+++	psrlq	$4,%xmm4
+++	pxor	%xmm4,%xmm2
+++
+++	subq	$1,%rax
+++	jnz	.Loop_row_6
+++
+++
+++
+++	pxor	%xmm3,%xmm2
+++	psrlq	$1,%xmm3
+++	pxor	%xmm3,%xmm2
+++	psrlq	$1,%xmm3
+++	pxor	%xmm3,%xmm2
+++	psrlq	$5,%xmm3
+++	pxor	%xmm3,%xmm2
+++	pxor	%xmm3,%xmm3
+++	movdqa	%xmm2,%xmm0
+++
+++
+++	leaq	-256(%rsi),%rsi
+++
+++
+++	leaq	16(%rdx),%rdx
+++	subq	$16,%rcx
+++	jnz	.Loop_ghash
+++
+++
+++.byte	102,65,15,56,0,194
+++	movdqu	%xmm0,(%rdi)
+++
+++
+++	pxor	%xmm0,%xmm0
+++	pxor	%xmm1,%xmm1
+++	pxor	%xmm2,%xmm2
+++	pxor	%xmm3,%xmm3
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm5,%xmm5
+++	pxor	%xmm6,%xmm6
+++	.byte	0xf3,0xc3
+++.Lghash_seh_end:
+++.cfi_endproc	
+++.size	gcm_ghash_ssse3,.-gcm_ghash_ssse3
+++
+++.align	16
+++
+++
+++.Lreverse_bytes:
+++.byte	15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0
+++
+++.Llow4_mask:
+++.quad	0x0f0f0f0f0f0f0f0f, 0x0f0f0f0f0f0f0f0f
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/ghash-x86_64.S b/linux-x86_64/ypto/fipsmodule/ghash-x86_64.S
++new file mode 100644
++index 000000000..3eb1af435
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/ghash-x86_64.S
++@@ -0,0 +1,1127 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++.extern	OPENSSL_ia32cap_P
+++.hidden OPENSSL_ia32cap_P
+++.globl	gcm_init_clmul
+++.hidden gcm_init_clmul
+++.type	gcm_init_clmul,@function
+++.align	16
+++gcm_init_clmul:
+++.cfi_startproc	
+++.L_init_clmul:
+++	movdqu	(%rsi),%xmm2
+++	pshufd	$78,%xmm2,%xmm2
+++
+++
+++	pshufd	$255,%xmm2,%xmm4
+++	movdqa	%xmm2,%xmm3
+++	psllq	$1,%xmm2
+++	pxor	%xmm5,%xmm5
+++	psrlq	$63,%xmm3
+++	pcmpgtd	%xmm4,%xmm5
+++	pslldq	$8,%xmm3
+++	por	%xmm3,%xmm2
+++
+++
+++	pand	.L0x1c2_polynomial(%rip),%xmm5
+++	pxor	%xmm5,%xmm2
+++
+++
+++	pshufd	$78,%xmm2,%xmm6
+++	movdqa	%xmm2,%xmm0
+++	pxor	%xmm2,%xmm6
+++	movdqa	%xmm0,%xmm1
+++	pshufd	$78,%xmm0,%xmm3
+++	pxor	%xmm0,%xmm3
+++.byte	102,15,58,68,194,0
+++.byte	102,15,58,68,202,17
+++.byte	102,15,58,68,222,0
+++	pxor	%xmm0,%xmm3
+++	pxor	%xmm1,%xmm3
+++
+++	movdqa	%xmm3,%xmm4
+++	psrldq	$8,%xmm3
+++	pslldq	$8,%xmm4
+++	pxor	%xmm3,%xmm1
+++	pxor	%xmm4,%xmm0
+++
+++	movdqa	%xmm0,%xmm4
+++	movdqa	%xmm0,%xmm3
+++	psllq	$5,%xmm0
+++	pxor	%xmm0,%xmm3
+++	psllq	$1,%xmm0
+++	pxor	%xmm3,%xmm0
+++	psllq	$57,%xmm0
+++	movdqa	%xmm0,%xmm3
+++	pslldq	$8,%xmm0
+++	psrldq	$8,%xmm3
+++	pxor	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm1
+++
+++
+++	movdqa	%xmm0,%xmm4
+++	psrlq	$1,%xmm0
+++	pxor	%xmm4,%xmm1
+++	pxor	%xmm0,%xmm4
+++	psrlq	$5,%xmm0
+++	pxor	%xmm4,%xmm0
+++	psrlq	$1,%xmm0
+++	pxor	%xmm1,%xmm0
+++	pshufd	$78,%xmm2,%xmm3
+++	pshufd	$78,%xmm0,%xmm4
+++	pxor	%xmm2,%xmm3
+++	movdqu	%xmm2,0(%rdi)
+++	pxor	%xmm0,%xmm4
+++	movdqu	%xmm0,16(%rdi)
+++.byte	102,15,58,15,227,8
+++	movdqu	%xmm4,32(%rdi)
+++	movdqa	%xmm0,%xmm1
+++	pshufd	$78,%xmm0,%xmm3
+++	pxor	%xmm0,%xmm3
+++.byte	102,15,58,68,194,0
+++.byte	102,15,58,68,202,17
+++.byte	102,15,58,68,222,0
+++	pxor	%xmm0,%xmm3
+++	pxor	%xmm1,%xmm3
+++
+++	movdqa	%xmm3,%xmm4
+++	psrldq	$8,%xmm3
+++	pslldq	$8,%xmm4
+++	pxor	%xmm3,%xmm1
+++	pxor	%xmm4,%xmm0
+++
+++	movdqa	%xmm0,%xmm4
+++	movdqa	%xmm0,%xmm3
+++	psllq	$5,%xmm0
+++	pxor	%xmm0,%xmm3
+++	psllq	$1,%xmm0
+++	pxor	%xmm3,%xmm0
+++	psllq	$57,%xmm0
+++	movdqa	%xmm0,%xmm3
+++	pslldq	$8,%xmm0
+++	psrldq	$8,%xmm3
+++	pxor	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm1
+++
+++
+++	movdqa	%xmm0,%xmm4
+++	psrlq	$1,%xmm0
+++	pxor	%xmm4,%xmm1
+++	pxor	%xmm0,%xmm4
+++	psrlq	$5,%xmm0
+++	pxor	%xmm4,%xmm0
+++	psrlq	$1,%xmm0
+++	pxor	%xmm1,%xmm0
+++	movdqa	%xmm0,%xmm5
+++	movdqa	%xmm0,%xmm1
+++	pshufd	$78,%xmm0,%xmm3
+++	pxor	%xmm0,%xmm3
+++.byte	102,15,58,68,194,0
+++.byte	102,15,58,68,202,17
+++.byte	102,15,58,68,222,0
+++	pxor	%xmm0,%xmm3
+++	pxor	%xmm1,%xmm3
+++
+++	movdqa	%xmm3,%xmm4
+++	psrldq	$8,%xmm3
+++	pslldq	$8,%xmm4
+++	pxor	%xmm3,%xmm1
+++	pxor	%xmm4,%xmm0
+++
+++	movdqa	%xmm0,%xmm4
+++	movdqa	%xmm0,%xmm3
+++	psllq	$5,%xmm0
+++	pxor	%xmm0,%xmm3
+++	psllq	$1,%xmm0
+++	pxor	%xmm3,%xmm0
+++	psllq	$57,%xmm0
+++	movdqa	%xmm0,%xmm3
+++	pslldq	$8,%xmm0
+++	psrldq	$8,%xmm3
+++	pxor	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm1
+++
+++
+++	movdqa	%xmm0,%xmm4
+++	psrlq	$1,%xmm0
+++	pxor	%xmm4,%xmm1
+++	pxor	%xmm0,%xmm4
+++	psrlq	$5,%xmm0
+++	pxor	%xmm4,%xmm0
+++	psrlq	$1,%xmm0
+++	pxor	%xmm1,%xmm0
+++	pshufd	$78,%xmm5,%xmm3
+++	pshufd	$78,%xmm0,%xmm4
+++	pxor	%xmm5,%xmm3
+++	movdqu	%xmm5,48(%rdi)
+++	pxor	%xmm0,%xmm4
+++	movdqu	%xmm0,64(%rdi)
+++.byte	102,15,58,15,227,8
+++	movdqu	%xmm4,80(%rdi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	gcm_init_clmul,.-gcm_init_clmul
+++.globl	gcm_gmult_clmul
+++.hidden gcm_gmult_clmul
+++.type	gcm_gmult_clmul,@function
+++.align	16
+++gcm_gmult_clmul:
+++.cfi_startproc	
+++.L_gmult_clmul:
+++	movdqu	(%rdi),%xmm0
+++	movdqa	.Lbswap_mask(%rip),%xmm5
+++	movdqu	(%rsi),%xmm2
+++	movdqu	32(%rsi),%xmm4
+++.byte	102,15,56,0,197
+++	movdqa	%xmm0,%xmm1
+++	pshufd	$78,%xmm0,%xmm3
+++	pxor	%xmm0,%xmm3
+++.byte	102,15,58,68,194,0
+++.byte	102,15,58,68,202,17
+++.byte	102,15,58,68,220,0
+++	pxor	%xmm0,%xmm3
+++	pxor	%xmm1,%xmm3
+++
+++	movdqa	%xmm3,%xmm4
+++	psrldq	$8,%xmm3
+++	pslldq	$8,%xmm4
+++	pxor	%xmm3,%xmm1
+++	pxor	%xmm4,%xmm0
+++
+++	movdqa	%xmm0,%xmm4
+++	movdqa	%xmm0,%xmm3
+++	psllq	$5,%xmm0
+++	pxor	%xmm0,%xmm3
+++	psllq	$1,%xmm0
+++	pxor	%xmm3,%xmm0
+++	psllq	$57,%xmm0
+++	movdqa	%xmm0,%xmm3
+++	pslldq	$8,%xmm0
+++	psrldq	$8,%xmm3
+++	pxor	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm1
+++
+++
+++	movdqa	%xmm0,%xmm4
+++	psrlq	$1,%xmm0
+++	pxor	%xmm4,%xmm1
+++	pxor	%xmm0,%xmm4
+++	psrlq	$5,%xmm0
+++	pxor	%xmm4,%xmm0
+++	psrlq	$1,%xmm0
+++	pxor	%xmm1,%xmm0
+++.byte	102,15,56,0,197
+++	movdqu	%xmm0,(%rdi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	gcm_gmult_clmul,.-gcm_gmult_clmul
+++.globl	gcm_ghash_clmul
+++.hidden gcm_ghash_clmul
+++.type	gcm_ghash_clmul,@function
+++.align	32
+++gcm_ghash_clmul:
+++.cfi_startproc	
+++.L_ghash_clmul:
+++	movdqa	.Lbswap_mask(%rip),%xmm10
+++
+++	movdqu	(%rdi),%xmm0
+++	movdqu	(%rsi),%xmm2
+++	movdqu	32(%rsi),%xmm7
+++.byte	102,65,15,56,0,194
+++
+++	subq	$0x10,%rcx
+++	jz	.Lodd_tail
+++
+++	movdqu	16(%rsi),%xmm6
+++	leaq	OPENSSL_ia32cap_P(%rip),%rax
+++	movl	4(%rax),%eax
+++	cmpq	$0x30,%rcx
+++	jb	.Lskip4x
+++
+++	andl	$71303168,%eax
+++	cmpl	$4194304,%eax
+++	je	.Lskip4x
+++
+++	subq	$0x30,%rcx
+++	movq	$0xA040608020C0E000,%rax
+++	movdqu	48(%rsi),%xmm14
+++	movdqu	64(%rsi),%xmm15
+++
+++
+++
+++
+++	movdqu	48(%rdx),%xmm3
+++	movdqu	32(%rdx),%xmm11
+++.byte	102,65,15,56,0,218
+++.byte	102,69,15,56,0,218
+++	movdqa	%xmm3,%xmm5
+++	pshufd	$78,%xmm3,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,68,218,0
+++.byte	102,15,58,68,234,17
+++.byte	102,15,58,68,231,0
+++
+++	movdqa	%xmm11,%xmm13
+++	pshufd	$78,%xmm11,%xmm12
+++	pxor	%xmm11,%xmm12
+++.byte	102,68,15,58,68,222,0
+++.byte	102,68,15,58,68,238,17
+++.byte	102,68,15,58,68,231,16
+++	xorps	%xmm11,%xmm3
+++	xorps	%xmm13,%xmm5
+++	movups	80(%rsi),%xmm7
+++	xorps	%xmm12,%xmm4
+++
+++	movdqu	16(%rdx),%xmm11
+++	movdqu	0(%rdx),%xmm8
+++.byte	102,69,15,56,0,218
+++.byte	102,69,15,56,0,194
+++	movdqa	%xmm11,%xmm13
+++	pshufd	$78,%xmm11,%xmm12
+++	pxor	%xmm8,%xmm0
+++	pxor	%xmm11,%xmm12
+++.byte	102,69,15,58,68,222,0
+++	movdqa	%xmm0,%xmm1
+++	pshufd	$78,%xmm0,%xmm8
+++	pxor	%xmm0,%xmm8
+++.byte	102,69,15,58,68,238,17
+++.byte	102,68,15,58,68,231,0
+++	xorps	%xmm11,%xmm3
+++	xorps	%xmm13,%xmm5
+++
+++	leaq	64(%rdx),%rdx
+++	subq	$0x40,%rcx
+++	jc	.Ltail4x
+++
+++	jmp	.Lmod4_loop
+++.align	32
+++.Lmod4_loop:
+++.byte	102,65,15,58,68,199,0
+++	xorps	%xmm12,%xmm4
+++	movdqu	48(%rdx),%xmm11
+++.byte	102,69,15,56,0,218
+++.byte	102,65,15,58,68,207,17
+++	xorps	%xmm3,%xmm0
+++	movdqu	32(%rdx),%xmm3
+++	movdqa	%xmm11,%xmm13
+++.byte	102,68,15,58,68,199,16
+++	pshufd	$78,%xmm11,%xmm12
+++	xorps	%xmm5,%xmm1
+++	pxor	%xmm11,%xmm12
+++.byte	102,65,15,56,0,218
+++	movups	32(%rsi),%xmm7
+++	xorps	%xmm4,%xmm8
+++.byte	102,68,15,58,68,218,0
+++	pshufd	$78,%xmm3,%xmm4
+++
+++	pxor	%xmm0,%xmm8
+++	movdqa	%xmm3,%xmm5
+++	pxor	%xmm1,%xmm8
+++	pxor	%xmm3,%xmm4
+++	movdqa	%xmm8,%xmm9
+++.byte	102,68,15,58,68,234,17
+++	pslldq	$8,%xmm8
+++	psrldq	$8,%xmm9
+++	pxor	%xmm8,%xmm0
+++	movdqa	.L7_mask(%rip),%xmm8
+++	pxor	%xmm9,%xmm1
+++.byte	102,76,15,110,200
+++
+++	pand	%xmm0,%xmm8
+++.byte	102,69,15,56,0,200
+++	pxor	%xmm0,%xmm9
+++.byte	102,68,15,58,68,231,0
+++	psllq	$57,%xmm9
+++	movdqa	%xmm9,%xmm8
+++	pslldq	$8,%xmm9
+++.byte	102,15,58,68,222,0
+++	psrldq	$8,%xmm8
+++	pxor	%xmm9,%xmm0
+++	pxor	%xmm8,%xmm1
+++	movdqu	0(%rdx),%xmm8
+++
+++	movdqa	%xmm0,%xmm9
+++	psrlq	$1,%xmm0
+++.byte	102,15,58,68,238,17
+++	xorps	%xmm11,%xmm3
+++	movdqu	16(%rdx),%xmm11
+++.byte	102,69,15,56,0,218
+++.byte	102,15,58,68,231,16
+++	xorps	%xmm13,%xmm5
+++	movups	80(%rsi),%xmm7
+++.byte	102,69,15,56,0,194
+++	pxor	%xmm9,%xmm1
+++	pxor	%xmm0,%xmm9
+++	psrlq	$5,%xmm0
+++
+++	movdqa	%xmm11,%xmm13
+++	pxor	%xmm12,%xmm4
+++	pshufd	$78,%xmm11,%xmm12
+++	pxor	%xmm9,%xmm0
+++	pxor	%xmm8,%xmm1
+++	pxor	%xmm11,%xmm12
+++.byte	102,69,15,58,68,222,0
+++	psrlq	$1,%xmm0
+++	pxor	%xmm1,%xmm0
+++	movdqa	%xmm0,%xmm1
+++.byte	102,69,15,58,68,238,17
+++	xorps	%xmm11,%xmm3
+++	pshufd	$78,%xmm0,%xmm8
+++	pxor	%xmm0,%xmm8
+++
+++.byte	102,68,15,58,68,231,0
+++	xorps	%xmm13,%xmm5
+++
+++	leaq	64(%rdx),%rdx
+++	subq	$0x40,%rcx
+++	jnc	.Lmod4_loop
+++
+++.Ltail4x:
+++.byte	102,65,15,58,68,199,0
+++.byte	102,65,15,58,68,207,17
+++.byte	102,68,15,58,68,199,16
+++	xorps	%xmm12,%xmm4
+++	xorps	%xmm3,%xmm0
+++	xorps	%xmm5,%xmm1
+++	pxor	%xmm0,%xmm1
+++	pxor	%xmm4,%xmm8
+++
+++	pxor	%xmm1,%xmm8
+++	pxor	%xmm0,%xmm1
+++
+++	movdqa	%xmm8,%xmm9
+++	psrldq	$8,%xmm8
+++	pslldq	$8,%xmm9
+++	pxor	%xmm8,%xmm1
+++	pxor	%xmm9,%xmm0
+++
+++	movdqa	%xmm0,%xmm4
+++	movdqa	%xmm0,%xmm3
+++	psllq	$5,%xmm0
+++	pxor	%xmm0,%xmm3
+++	psllq	$1,%xmm0
+++	pxor	%xmm3,%xmm0
+++	psllq	$57,%xmm0
+++	movdqa	%xmm0,%xmm3
+++	pslldq	$8,%xmm0
+++	psrldq	$8,%xmm3
+++	pxor	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm1
+++
+++
+++	movdqa	%xmm0,%xmm4
+++	psrlq	$1,%xmm0
+++	pxor	%xmm4,%xmm1
+++	pxor	%xmm0,%xmm4
+++	psrlq	$5,%xmm0
+++	pxor	%xmm4,%xmm0
+++	psrlq	$1,%xmm0
+++	pxor	%xmm1,%xmm0
+++	addq	$0x40,%rcx
+++	jz	.Ldone
+++	movdqu	32(%rsi),%xmm7
+++	subq	$0x10,%rcx
+++	jz	.Lodd_tail
+++.Lskip4x:
+++
+++
+++
+++
+++
+++	movdqu	(%rdx),%xmm8
+++	movdqu	16(%rdx),%xmm3
+++.byte	102,69,15,56,0,194
+++.byte	102,65,15,56,0,218
+++	pxor	%xmm8,%xmm0
+++
+++	movdqa	%xmm3,%xmm5
+++	pshufd	$78,%xmm3,%xmm4
+++	pxor	%xmm3,%xmm4
+++.byte	102,15,58,68,218,0
+++.byte	102,15,58,68,234,17
+++.byte	102,15,58,68,231,0
+++
+++	leaq	32(%rdx),%rdx
+++	nop
+++	subq	$0x20,%rcx
+++	jbe	.Leven_tail
+++	nop
+++	jmp	.Lmod_loop
+++
+++.align	32
+++.Lmod_loop:
+++	movdqa	%xmm0,%xmm1
+++	movdqa	%xmm4,%xmm8
+++	pshufd	$78,%xmm0,%xmm4
+++	pxor	%xmm0,%xmm4
+++
+++.byte	102,15,58,68,198,0
+++.byte	102,15,58,68,206,17
+++.byte	102,15,58,68,231,16
+++
+++	pxor	%xmm3,%xmm0
+++	pxor	%xmm5,%xmm1
+++	movdqu	(%rdx),%xmm9
+++	pxor	%xmm0,%xmm8
+++.byte	102,69,15,56,0,202
+++	movdqu	16(%rdx),%xmm3
+++
+++	pxor	%xmm1,%xmm8
+++	pxor	%xmm9,%xmm1
+++	pxor	%xmm8,%xmm4
+++.byte	102,65,15,56,0,218
+++	movdqa	%xmm4,%xmm8
+++	psrldq	$8,%xmm8
+++	pslldq	$8,%xmm4
+++	pxor	%xmm8,%xmm1
+++	pxor	%xmm4,%xmm0
+++
+++	movdqa	%xmm3,%xmm5
+++
+++	movdqa	%xmm0,%xmm9
+++	movdqa	%xmm0,%xmm8
+++	psllq	$5,%xmm0
+++	pxor	%xmm0,%xmm8
+++.byte	102,15,58,68,218,0
+++	psllq	$1,%xmm0
+++	pxor	%xmm8,%xmm0
+++	psllq	$57,%xmm0
+++	movdqa	%xmm0,%xmm8
+++	pslldq	$8,%xmm0
+++	psrldq	$8,%xmm8
+++	pxor	%xmm9,%xmm0
+++	pshufd	$78,%xmm5,%xmm4
+++	pxor	%xmm8,%xmm1
+++	pxor	%xmm5,%xmm4
+++
+++	movdqa	%xmm0,%xmm9
+++	psrlq	$1,%xmm0
+++.byte	102,15,58,68,234,17
+++	pxor	%xmm9,%xmm1
+++	pxor	%xmm0,%xmm9
+++	psrlq	$5,%xmm0
+++	pxor	%xmm9,%xmm0
+++	leaq	32(%rdx),%rdx
+++	psrlq	$1,%xmm0
+++.byte	102,15,58,68,231,0
+++	pxor	%xmm1,%xmm0
+++
+++	subq	$0x20,%rcx
+++	ja	.Lmod_loop
+++
+++.Leven_tail:
+++	movdqa	%xmm0,%xmm1
+++	movdqa	%xmm4,%xmm8
+++	pshufd	$78,%xmm0,%xmm4
+++	pxor	%xmm0,%xmm4
+++
+++.byte	102,15,58,68,198,0
+++.byte	102,15,58,68,206,17
+++.byte	102,15,58,68,231,16
+++
+++	pxor	%xmm3,%xmm0
+++	pxor	%xmm5,%xmm1
+++	pxor	%xmm0,%xmm8
+++	pxor	%xmm1,%xmm8
+++	pxor	%xmm8,%xmm4
+++	movdqa	%xmm4,%xmm8
+++	psrldq	$8,%xmm8
+++	pslldq	$8,%xmm4
+++	pxor	%xmm8,%xmm1
+++	pxor	%xmm4,%xmm0
+++
+++	movdqa	%xmm0,%xmm4
+++	movdqa	%xmm0,%xmm3
+++	psllq	$5,%xmm0
+++	pxor	%xmm0,%xmm3
+++	psllq	$1,%xmm0
+++	pxor	%xmm3,%xmm0
+++	psllq	$57,%xmm0
+++	movdqa	%xmm0,%xmm3
+++	pslldq	$8,%xmm0
+++	psrldq	$8,%xmm3
+++	pxor	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm1
+++
+++
+++	movdqa	%xmm0,%xmm4
+++	psrlq	$1,%xmm0
+++	pxor	%xmm4,%xmm1
+++	pxor	%xmm0,%xmm4
+++	psrlq	$5,%xmm0
+++	pxor	%xmm4,%xmm0
+++	psrlq	$1,%xmm0
+++	pxor	%xmm1,%xmm0
+++	testq	%rcx,%rcx
+++	jnz	.Ldone
+++
+++.Lodd_tail:
+++	movdqu	(%rdx),%xmm8
+++.byte	102,69,15,56,0,194
+++	pxor	%xmm8,%xmm0
+++	movdqa	%xmm0,%xmm1
+++	pshufd	$78,%xmm0,%xmm3
+++	pxor	%xmm0,%xmm3
+++.byte	102,15,58,68,194,0
+++.byte	102,15,58,68,202,17
+++.byte	102,15,58,68,223,0
+++	pxor	%xmm0,%xmm3
+++	pxor	%xmm1,%xmm3
+++
+++	movdqa	%xmm3,%xmm4
+++	psrldq	$8,%xmm3
+++	pslldq	$8,%xmm4
+++	pxor	%xmm3,%xmm1
+++	pxor	%xmm4,%xmm0
+++
+++	movdqa	%xmm0,%xmm4
+++	movdqa	%xmm0,%xmm3
+++	psllq	$5,%xmm0
+++	pxor	%xmm0,%xmm3
+++	psllq	$1,%xmm0
+++	pxor	%xmm3,%xmm0
+++	psllq	$57,%xmm0
+++	movdqa	%xmm0,%xmm3
+++	pslldq	$8,%xmm0
+++	psrldq	$8,%xmm3
+++	pxor	%xmm4,%xmm0
+++	pxor	%xmm3,%xmm1
+++
+++
+++	movdqa	%xmm0,%xmm4
+++	psrlq	$1,%xmm0
+++	pxor	%xmm4,%xmm1
+++	pxor	%xmm0,%xmm4
+++	psrlq	$5,%xmm0
+++	pxor	%xmm4,%xmm0
+++	psrlq	$1,%xmm0
+++	pxor	%xmm1,%xmm0
+++.Ldone:
+++.byte	102,65,15,56,0,194
+++	movdqu	%xmm0,(%rdi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	gcm_ghash_clmul,.-gcm_ghash_clmul
+++.globl	gcm_init_avx
+++.hidden gcm_init_avx
+++.type	gcm_init_avx,@function
+++.align	32
+++gcm_init_avx:
+++.cfi_startproc	
+++	vzeroupper
+++
+++	vmovdqu	(%rsi),%xmm2
+++	vpshufd	$78,%xmm2,%xmm2
+++
+++
+++	vpshufd	$255,%xmm2,%xmm4
+++	vpsrlq	$63,%xmm2,%xmm3
+++	vpsllq	$1,%xmm2,%xmm2
+++	vpxor	%xmm5,%xmm5,%xmm5
+++	vpcmpgtd	%xmm4,%xmm5,%xmm5
+++	vpslldq	$8,%xmm3,%xmm3
+++	vpor	%xmm3,%xmm2,%xmm2
+++
+++
+++	vpand	.L0x1c2_polynomial(%rip),%xmm5,%xmm5
+++	vpxor	%xmm5,%xmm2,%xmm2
+++
+++	vpunpckhqdq	%xmm2,%xmm2,%xmm6
+++	vmovdqa	%xmm2,%xmm0
+++	vpxor	%xmm2,%xmm6,%xmm6
+++	movq	$4,%r10
+++	jmp	.Linit_start_avx
+++.align	32
+++.Linit_loop_avx:
+++	vpalignr	$8,%xmm3,%xmm4,%xmm5
+++	vmovdqu	%xmm5,-16(%rdi)
+++	vpunpckhqdq	%xmm0,%xmm0,%xmm3
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x11,%xmm2,%xmm0,%xmm1
+++	vpclmulqdq	$0x00,%xmm2,%xmm0,%xmm0
+++	vpclmulqdq	$0x00,%xmm6,%xmm3,%xmm3
+++	vpxor	%xmm0,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++
+++	vpslldq	$8,%xmm3,%xmm4
+++	vpsrldq	$8,%xmm3,%xmm3
+++	vpxor	%xmm4,%xmm0,%xmm0
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpsllq	$57,%xmm0,%xmm3
+++	vpsllq	$62,%xmm0,%xmm4
+++	vpxor	%xmm3,%xmm4,%xmm4
+++	vpsllq	$63,%xmm0,%xmm3
+++	vpxor	%xmm3,%xmm4,%xmm4
+++	vpslldq	$8,%xmm4,%xmm3
+++	vpsrldq	$8,%xmm4,%xmm4
+++	vpxor	%xmm3,%xmm0,%xmm0
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vpsrlq	$1,%xmm0,%xmm4
+++	vpxor	%xmm0,%xmm1,%xmm1
+++	vpxor	%xmm4,%xmm0,%xmm0
+++	vpsrlq	$5,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm0,%xmm0
+++	vpsrlq	$1,%xmm0,%xmm0
+++	vpxor	%xmm1,%xmm0,%xmm0
+++.Linit_start_avx:
+++	vmovdqa	%xmm0,%xmm5
+++	vpunpckhqdq	%xmm0,%xmm0,%xmm3
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x11,%xmm2,%xmm0,%xmm1
+++	vpclmulqdq	$0x00,%xmm2,%xmm0,%xmm0
+++	vpclmulqdq	$0x00,%xmm6,%xmm3,%xmm3
+++	vpxor	%xmm0,%xmm1,%xmm4
+++	vpxor	%xmm4,%xmm3,%xmm3
+++
+++	vpslldq	$8,%xmm3,%xmm4
+++	vpsrldq	$8,%xmm3,%xmm3
+++	vpxor	%xmm4,%xmm0,%xmm0
+++	vpxor	%xmm3,%xmm1,%xmm1
+++	vpsllq	$57,%xmm0,%xmm3
+++	vpsllq	$62,%xmm0,%xmm4
+++	vpxor	%xmm3,%xmm4,%xmm4
+++	vpsllq	$63,%xmm0,%xmm3
+++	vpxor	%xmm3,%xmm4,%xmm4
+++	vpslldq	$8,%xmm4,%xmm3
+++	vpsrldq	$8,%xmm4,%xmm4
+++	vpxor	%xmm3,%xmm0,%xmm0
+++	vpxor	%xmm4,%xmm1,%xmm1
+++
+++	vpsrlq	$1,%xmm0,%xmm4
+++	vpxor	%xmm0,%xmm1,%xmm1
+++	vpxor	%xmm4,%xmm0,%xmm0
+++	vpsrlq	$5,%xmm4,%xmm4
+++	vpxor	%xmm4,%xmm0,%xmm0
+++	vpsrlq	$1,%xmm0,%xmm0
+++	vpxor	%xmm1,%xmm0,%xmm0
+++	vpshufd	$78,%xmm5,%xmm3
+++	vpshufd	$78,%xmm0,%xmm4
+++	vpxor	%xmm5,%xmm3,%xmm3
+++	vmovdqu	%xmm5,0(%rdi)
+++	vpxor	%xmm0,%xmm4,%xmm4
+++	vmovdqu	%xmm0,16(%rdi)
+++	leaq	48(%rdi),%rdi
+++	subq	$1,%r10
+++	jnz	.Linit_loop_avx
+++
+++	vpalignr	$8,%xmm4,%xmm3,%xmm5
+++	vmovdqu	%xmm5,-16(%rdi)
+++
+++	vzeroupper
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	gcm_init_avx,.-gcm_init_avx
+++.globl	gcm_gmult_avx
+++.hidden gcm_gmult_avx
+++.type	gcm_gmult_avx,@function
+++.align	32
+++gcm_gmult_avx:
+++.cfi_startproc	
+++	jmp	.L_gmult_clmul
+++.cfi_endproc	
+++.size	gcm_gmult_avx,.-gcm_gmult_avx
+++.globl	gcm_ghash_avx
+++.hidden gcm_ghash_avx
+++.type	gcm_ghash_avx,@function
+++.align	32
+++gcm_ghash_avx:
+++.cfi_startproc	
+++	vzeroupper
+++
+++	vmovdqu	(%rdi),%xmm10
+++	leaq	.L0x1c2_polynomial(%rip),%r10
+++	leaq	64(%rsi),%rsi
+++	vmovdqu	.Lbswap_mask(%rip),%xmm13
+++	vpshufb	%xmm13,%xmm10,%xmm10
+++	cmpq	$0x80,%rcx
+++	jb	.Lshort_avx
+++	subq	$0x80,%rcx
+++
+++	vmovdqu	112(%rdx),%xmm14
+++	vmovdqu	0-64(%rsi),%xmm6
+++	vpshufb	%xmm13,%xmm14,%xmm14
+++	vmovdqu	32-64(%rsi),%xmm7
+++
+++	vpunpckhqdq	%xmm14,%xmm14,%xmm9
+++	vmovdqu	96(%rdx),%xmm15
+++	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
+++	vpxor	%xmm14,%xmm9,%xmm9
+++	vpshufb	%xmm13,%xmm15,%xmm15
+++	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
+++	vmovdqu	16-64(%rsi),%xmm6
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vmovdqu	80(%rdx),%xmm14
+++	vpclmulqdq	$0x00,%xmm7,%xmm9,%xmm2
+++	vpxor	%xmm15,%xmm8,%xmm8
+++
+++	vpshufb	%xmm13,%xmm14,%xmm14
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm3
+++	vpunpckhqdq	%xmm14,%xmm14,%xmm9
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm4
+++	vmovdqu	48-64(%rsi),%xmm6
+++	vpxor	%xmm14,%xmm9,%xmm9
+++	vmovdqu	64(%rdx),%xmm15
+++	vpclmulqdq	$0x10,%xmm7,%xmm8,%xmm5
+++	vmovdqu	80-64(%rsi),%xmm7
+++
+++	vpshufb	%xmm13,%xmm15,%xmm15
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
+++	vmovdqu	64-64(%rsi),%xmm6
+++	vpxor	%xmm2,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,%xmm7,%xmm9,%xmm2
+++	vpxor	%xmm15,%xmm8,%xmm8
+++
+++	vmovdqu	48(%rdx),%xmm14
+++	vpxor	%xmm3,%xmm0,%xmm0
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm3
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpshufb	%xmm13,%xmm14,%xmm14
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm4
+++	vmovdqu	96-64(%rsi),%xmm6
+++	vpxor	%xmm5,%xmm2,%xmm2
+++	vpunpckhqdq	%xmm14,%xmm14,%xmm9
+++	vpclmulqdq	$0x10,%xmm7,%xmm8,%xmm5
+++	vmovdqu	128-64(%rsi),%xmm7
+++	vpxor	%xmm14,%xmm9,%xmm9
+++
+++	vmovdqu	32(%rdx),%xmm15
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpshufb	%xmm13,%xmm15,%xmm15
+++	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
+++	vmovdqu	112-64(%rsi),%xmm6
+++	vpxor	%xmm2,%xmm5,%xmm5
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vpclmulqdq	$0x00,%xmm7,%xmm9,%xmm2
+++	vpxor	%xmm15,%xmm8,%xmm8
+++
+++	vmovdqu	16(%rdx),%xmm14
+++	vpxor	%xmm3,%xmm0,%xmm0
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm3
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpshufb	%xmm13,%xmm14,%xmm14
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm4
+++	vmovdqu	144-64(%rsi),%xmm6
+++	vpxor	%xmm5,%xmm2,%xmm2
+++	vpunpckhqdq	%xmm14,%xmm14,%xmm9
+++	vpclmulqdq	$0x10,%xmm7,%xmm8,%xmm5
+++	vmovdqu	176-64(%rsi),%xmm7
+++	vpxor	%xmm14,%xmm9,%xmm9
+++
+++	vmovdqu	(%rdx),%xmm15
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpshufb	%xmm13,%xmm15,%xmm15
+++	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
+++	vmovdqu	160-64(%rsi),%xmm6
+++	vpxor	%xmm2,%xmm5,%xmm5
+++	vpclmulqdq	$0x10,%xmm7,%xmm9,%xmm2
+++
+++	leaq	128(%rdx),%rdx
+++	cmpq	$0x80,%rcx
+++	jb	.Ltail_avx
+++
+++	vpxor	%xmm10,%xmm15,%xmm15
+++	subq	$0x80,%rcx
+++	jmp	.Loop8x_avx
+++
+++.align	32
+++.Loop8x_avx:
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vmovdqu	112(%rdx),%xmm14
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpxor	%xmm15,%xmm8,%xmm8
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm10
+++	vpshufb	%xmm13,%xmm14,%xmm14
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm11
+++	vmovdqu	0-64(%rsi),%xmm6
+++	vpunpckhqdq	%xmm14,%xmm14,%xmm9
+++	vpxor	%xmm2,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm12
+++	vmovdqu	32-64(%rsi),%xmm7
+++	vpxor	%xmm14,%xmm9,%xmm9
+++
+++	vmovdqu	96(%rdx),%xmm15
+++	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
+++	vpxor	%xmm3,%xmm10,%xmm10
+++	vpshufb	%xmm13,%xmm15,%xmm15
+++	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
+++	vxorps	%xmm4,%xmm11,%xmm11
+++	vmovdqu	16-64(%rsi),%xmm6
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vpclmulqdq	$0x00,%xmm7,%xmm9,%xmm2
+++	vpxor	%xmm5,%xmm12,%xmm12
+++	vxorps	%xmm15,%xmm8,%xmm8
+++
+++	vmovdqu	80(%rdx),%xmm14
+++	vpxor	%xmm10,%xmm12,%xmm12
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm3
+++	vpxor	%xmm11,%xmm12,%xmm12
+++	vpslldq	$8,%xmm12,%xmm9
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm4
+++	vpsrldq	$8,%xmm12,%xmm12
+++	vpxor	%xmm9,%xmm10,%xmm10
+++	vmovdqu	48-64(%rsi),%xmm6
+++	vpshufb	%xmm13,%xmm14,%xmm14
+++	vxorps	%xmm12,%xmm11,%xmm11
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpunpckhqdq	%xmm14,%xmm14,%xmm9
+++	vpclmulqdq	$0x10,%xmm7,%xmm8,%xmm5
+++	vmovdqu	80-64(%rsi),%xmm7
+++	vpxor	%xmm14,%xmm9,%xmm9
+++	vpxor	%xmm2,%xmm5,%xmm5
+++
+++	vmovdqu	64(%rdx),%xmm15
+++	vpalignr	$8,%xmm10,%xmm10,%xmm12
+++	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
+++	vpshufb	%xmm13,%xmm15,%xmm15
+++	vpxor	%xmm3,%xmm0,%xmm0
+++	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
+++	vmovdqu	64-64(%rsi),%xmm6
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x00,%xmm7,%xmm9,%xmm2
+++	vxorps	%xmm15,%xmm8,%xmm8
+++	vpxor	%xmm5,%xmm2,%xmm2
+++
+++	vmovdqu	48(%rdx),%xmm14
+++	vpclmulqdq	$0x10,(%r10),%xmm10,%xmm10
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm3
+++	vpshufb	%xmm13,%xmm14,%xmm14
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm4
+++	vmovdqu	96-64(%rsi),%xmm6
+++	vpunpckhqdq	%xmm14,%xmm14,%xmm9
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpclmulqdq	$0x10,%xmm7,%xmm8,%xmm5
+++	vmovdqu	128-64(%rsi),%xmm7
+++	vpxor	%xmm14,%xmm9,%xmm9
+++	vpxor	%xmm2,%xmm5,%xmm5
+++
+++	vmovdqu	32(%rdx),%xmm15
+++	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
+++	vpshufb	%xmm13,%xmm15,%xmm15
+++	vpxor	%xmm3,%xmm0,%xmm0
+++	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
+++	vmovdqu	112-64(%rsi),%xmm6
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vpxor	%xmm4,%xmm1,%xmm1
+++	vpclmulqdq	$0x00,%xmm7,%xmm9,%xmm2
+++	vpxor	%xmm15,%xmm8,%xmm8
+++	vpxor	%xmm5,%xmm2,%xmm2
+++	vxorps	%xmm12,%xmm10,%xmm10
+++
+++	vmovdqu	16(%rdx),%xmm14
+++	vpalignr	$8,%xmm10,%xmm10,%xmm12
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm3
+++	vpshufb	%xmm13,%xmm14,%xmm14
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm4
+++	vmovdqu	144-64(%rsi),%xmm6
+++	vpclmulqdq	$0x10,(%r10),%xmm10,%xmm10
+++	vxorps	%xmm11,%xmm12,%xmm12
+++	vpunpckhqdq	%xmm14,%xmm14,%xmm9
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpclmulqdq	$0x10,%xmm7,%xmm8,%xmm5
+++	vmovdqu	176-64(%rsi),%xmm7
+++	vpxor	%xmm14,%xmm9,%xmm9
+++	vpxor	%xmm2,%xmm5,%xmm5
+++
+++	vmovdqu	(%rdx),%xmm15
+++	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
+++	vpshufb	%xmm13,%xmm15,%xmm15
+++	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
+++	vmovdqu	160-64(%rsi),%xmm6
+++	vpxor	%xmm12,%xmm15,%xmm15
+++	vpclmulqdq	$0x10,%xmm7,%xmm9,%xmm2
+++	vpxor	%xmm10,%xmm15,%xmm15
+++
+++	leaq	128(%rdx),%rdx
+++	subq	$0x80,%rcx
+++	jnc	.Loop8x_avx
+++
+++	addq	$0x80,%rcx
+++	jmp	.Ltail_no_xor_avx
+++
+++.align	32
+++.Lshort_avx:
+++	vmovdqu	-16(%rdx,%rcx,1),%xmm14
+++	leaq	(%rdx,%rcx,1),%rdx
+++	vmovdqu	0-64(%rsi),%xmm6
+++	vmovdqu	32-64(%rsi),%xmm7
+++	vpshufb	%xmm13,%xmm14,%xmm15
+++
+++	vmovdqa	%xmm0,%xmm3
+++	vmovdqa	%xmm1,%xmm4
+++	vmovdqa	%xmm2,%xmm5
+++	subq	$0x10,%rcx
+++	jz	.Ltail_avx
+++
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
+++	vpxor	%xmm15,%xmm8,%xmm8
+++	vmovdqu	-32(%rdx),%xmm14
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
+++	vmovdqu	16-64(%rsi),%xmm6
+++	vpshufb	%xmm13,%xmm14,%xmm15
+++	vpxor	%xmm2,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2
+++	vpsrldq	$8,%xmm7,%xmm7
+++	subq	$0x10,%rcx
+++	jz	.Ltail_avx
+++
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
+++	vpxor	%xmm15,%xmm8,%xmm8
+++	vmovdqu	-48(%rdx),%xmm14
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
+++	vmovdqu	48-64(%rsi),%xmm6
+++	vpshufb	%xmm13,%xmm14,%xmm15
+++	vpxor	%xmm2,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2
+++	vmovdqu	80-64(%rsi),%xmm7
+++	subq	$0x10,%rcx
+++	jz	.Ltail_avx
+++
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
+++	vpxor	%xmm15,%xmm8,%xmm8
+++	vmovdqu	-64(%rdx),%xmm14
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
+++	vmovdqu	64-64(%rsi),%xmm6
+++	vpshufb	%xmm13,%xmm14,%xmm15
+++	vpxor	%xmm2,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2
+++	vpsrldq	$8,%xmm7,%xmm7
+++	subq	$0x10,%rcx
+++	jz	.Ltail_avx
+++
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
+++	vpxor	%xmm15,%xmm8,%xmm8
+++	vmovdqu	-80(%rdx),%xmm14
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
+++	vmovdqu	96-64(%rsi),%xmm6
+++	vpshufb	%xmm13,%xmm14,%xmm15
+++	vpxor	%xmm2,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2
+++	vmovdqu	128-64(%rsi),%xmm7
+++	subq	$0x10,%rcx
+++	jz	.Ltail_avx
+++
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
+++	vpxor	%xmm15,%xmm8,%xmm8
+++	vmovdqu	-96(%rdx),%xmm14
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
+++	vmovdqu	112-64(%rsi),%xmm6
+++	vpshufb	%xmm13,%xmm14,%xmm15
+++	vpxor	%xmm2,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2
+++	vpsrldq	$8,%xmm7,%xmm7
+++	subq	$0x10,%rcx
+++	jz	.Ltail_avx
+++
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
+++	vpxor	%xmm15,%xmm8,%xmm8
+++	vmovdqu	-112(%rdx),%xmm14
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
+++	vmovdqu	144-64(%rsi),%xmm6
+++	vpshufb	%xmm13,%xmm14,%xmm15
+++	vpxor	%xmm2,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2
+++	vmovq	184-64(%rsi),%xmm7
+++	subq	$0x10,%rcx
+++	jmp	.Ltail_avx
+++
+++.align	32
+++.Ltail_avx:
+++	vpxor	%xmm10,%xmm15,%xmm15
+++.Ltail_no_xor_avx:
+++	vpunpckhqdq	%xmm15,%xmm15,%xmm8
+++	vpxor	%xmm0,%xmm3,%xmm3
+++	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
+++	vpxor	%xmm15,%xmm8,%xmm8
+++	vpxor	%xmm1,%xmm4,%xmm4
+++	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
+++	vpxor	%xmm2,%xmm5,%xmm5
+++	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2
+++
+++	vmovdqu	(%r10),%xmm12
+++
+++	vpxor	%xmm0,%xmm3,%xmm10
+++	vpxor	%xmm1,%xmm4,%xmm11
+++	vpxor	%xmm2,%xmm5,%xmm5
+++
+++	vpxor	%xmm10,%xmm5,%xmm5
+++	vpxor	%xmm11,%xmm5,%xmm5
+++	vpslldq	$8,%xmm5,%xmm9
+++	vpsrldq	$8,%xmm5,%xmm5
+++	vpxor	%xmm9,%xmm10,%xmm10
+++	vpxor	%xmm5,%xmm11,%xmm11
+++
+++	vpclmulqdq	$0x10,%xmm12,%xmm10,%xmm9
+++	vpalignr	$8,%xmm10,%xmm10,%xmm10
+++	vpxor	%xmm9,%xmm10,%xmm10
+++
+++	vpclmulqdq	$0x10,%xmm12,%xmm10,%xmm9
+++	vpalignr	$8,%xmm10,%xmm10,%xmm10
+++	vpxor	%xmm11,%xmm10,%xmm10
+++	vpxor	%xmm9,%xmm10,%xmm10
+++
+++	cmpq	$0,%rcx
+++	jne	.Lshort_avx
+++
+++	vpshufb	%xmm13,%xmm10,%xmm10
+++	vmovdqu	%xmm10,(%rdi)
+++	vzeroupper
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	gcm_ghash_avx,.-gcm_ghash_avx
+++.align	64
+++.Lbswap_mask:
+++.byte	15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0
+++.L0x1c2_polynomial:
+++.byte	1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0xc2
+++.L7_mask:
+++.long	7,0,7,0
+++.align	64
+++
+++.byte	71,72,65,83,72,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
+++.align	64
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/md5-x86_64.S b/linux-x86_64/ypto/fipsmodule/md5-x86_64.S
++new file mode 100644
++index 000000000..04aaf057e
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/md5-x86_64.S
++@@ -0,0 +1,702 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++.align	16
+++
+++.globl	md5_block_asm_data_order
+++.hidden md5_block_asm_data_order
+++.type	md5_block_asm_data_order,@function
+++md5_block_asm_data_order:
+++.cfi_startproc	
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	r12,-32
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	r14,-40
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	r15,-48
+++.Lprologue:
+++
+++
+++
+++
+++	movq	%rdi,%rbp
+++	shlq	$6,%rdx
+++	leaq	(%rsi,%rdx,1),%rdi
+++	movl	0(%rbp),%eax
+++	movl	4(%rbp),%ebx
+++	movl	8(%rbp),%ecx
+++	movl	12(%rbp),%edx
+++
+++
+++
+++
+++
+++
+++
+++	cmpq	%rdi,%rsi
+++	je	.Lend
+++
+++
+++.Lloop:
+++	movl	%eax,%r8d
+++	movl	%ebx,%r9d
+++	movl	%ecx,%r14d
+++	movl	%edx,%r15d
+++	movl	0(%rsi),%r10d
+++	movl	%edx,%r11d
+++	xorl	%ecx,%r11d
+++	leal	-680876936(%rax,%r10,1),%eax
+++	andl	%ebx,%r11d
+++	xorl	%edx,%r11d
+++	movl	4(%rsi),%r10d
+++	addl	%r11d,%eax
+++	roll	$7,%eax
+++	movl	%ecx,%r11d
+++	addl	%ebx,%eax
+++	xorl	%ebx,%r11d
+++	leal	-389564586(%rdx,%r10,1),%edx
+++	andl	%eax,%r11d
+++	xorl	%ecx,%r11d
+++	movl	8(%rsi),%r10d
+++	addl	%r11d,%edx
+++	roll	$12,%edx
+++	movl	%ebx,%r11d
+++	addl	%eax,%edx
+++	xorl	%eax,%r11d
+++	leal	606105819(%rcx,%r10,1),%ecx
+++	andl	%edx,%r11d
+++	xorl	%ebx,%r11d
+++	movl	12(%rsi),%r10d
+++	addl	%r11d,%ecx
+++	roll	$17,%ecx
+++	movl	%eax,%r11d
+++	addl	%edx,%ecx
+++	xorl	%edx,%r11d
+++	leal	-1044525330(%rbx,%r10,1),%ebx
+++	andl	%ecx,%r11d
+++	xorl	%eax,%r11d
+++	movl	16(%rsi),%r10d
+++	addl	%r11d,%ebx
+++	roll	$22,%ebx
+++	movl	%edx,%r11d
+++	addl	%ecx,%ebx
+++	xorl	%ecx,%r11d
+++	leal	-176418897(%rax,%r10,1),%eax
+++	andl	%ebx,%r11d
+++	xorl	%edx,%r11d
+++	movl	20(%rsi),%r10d
+++	addl	%r11d,%eax
+++	roll	$7,%eax
+++	movl	%ecx,%r11d
+++	addl	%ebx,%eax
+++	xorl	%ebx,%r11d
+++	leal	1200080426(%rdx,%r10,1),%edx
+++	andl	%eax,%r11d
+++	xorl	%ecx,%r11d
+++	movl	24(%rsi),%r10d
+++	addl	%r11d,%edx
+++	roll	$12,%edx
+++	movl	%ebx,%r11d
+++	addl	%eax,%edx
+++	xorl	%eax,%r11d
+++	leal	-1473231341(%rcx,%r10,1),%ecx
+++	andl	%edx,%r11d
+++	xorl	%ebx,%r11d
+++	movl	28(%rsi),%r10d
+++	addl	%r11d,%ecx
+++	roll	$17,%ecx
+++	movl	%eax,%r11d
+++	addl	%edx,%ecx
+++	xorl	%edx,%r11d
+++	leal	-45705983(%rbx,%r10,1),%ebx
+++	andl	%ecx,%r11d
+++	xorl	%eax,%r11d
+++	movl	32(%rsi),%r10d
+++	addl	%r11d,%ebx
+++	roll	$22,%ebx
+++	movl	%edx,%r11d
+++	addl	%ecx,%ebx
+++	xorl	%ecx,%r11d
+++	leal	1770035416(%rax,%r10,1),%eax
+++	andl	%ebx,%r11d
+++	xorl	%edx,%r11d
+++	movl	36(%rsi),%r10d
+++	addl	%r11d,%eax
+++	roll	$7,%eax
+++	movl	%ecx,%r11d
+++	addl	%ebx,%eax
+++	xorl	%ebx,%r11d
+++	leal	-1958414417(%rdx,%r10,1),%edx
+++	andl	%eax,%r11d
+++	xorl	%ecx,%r11d
+++	movl	40(%rsi),%r10d
+++	addl	%r11d,%edx
+++	roll	$12,%edx
+++	movl	%ebx,%r11d
+++	addl	%eax,%edx
+++	xorl	%eax,%r11d
+++	leal	-42063(%rcx,%r10,1),%ecx
+++	andl	%edx,%r11d
+++	xorl	%ebx,%r11d
+++	movl	44(%rsi),%r10d
+++	addl	%r11d,%ecx
+++	roll	$17,%ecx
+++	movl	%eax,%r11d
+++	addl	%edx,%ecx
+++	xorl	%edx,%r11d
+++	leal	-1990404162(%rbx,%r10,1),%ebx
+++	andl	%ecx,%r11d
+++	xorl	%eax,%r11d
+++	movl	48(%rsi),%r10d
+++	addl	%r11d,%ebx
+++	roll	$22,%ebx
+++	movl	%edx,%r11d
+++	addl	%ecx,%ebx
+++	xorl	%ecx,%r11d
+++	leal	1804603682(%rax,%r10,1),%eax
+++	andl	%ebx,%r11d
+++	xorl	%edx,%r11d
+++	movl	52(%rsi),%r10d
+++	addl	%r11d,%eax
+++	roll	$7,%eax
+++	movl	%ecx,%r11d
+++	addl	%ebx,%eax
+++	xorl	%ebx,%r11d
+++	leal	-40341101(%rdx,%r10,1),%edx
+++	andl	%eax,%r11d
+++	xorl	%ecx,%r11d
+++	movl	56(%rsi),%r10d
+++	addl	%r11d,%edx
+++	roll	$12,%edx
+++	movl	%ebx,%r11d
+++	addl	%eax,%edx
+++	xorl	%eax,%r11d
+++	leal	-1502002290(%rcx,%r10,1),%ecx
+++	andl	%edx,%r11d
+++	xorl	%ebx,%r11d
+++	movl	60(%rsi),%r10d
+++	addl	%r11d,%ecx
+++	roll	$17,%ecx
+++	movl	%eax,%r11d
+++	addl	%edx,%ecx
+++	xorl	%edx,%r11d
+++	leal	1236535329(%rbx,%r10,1),%ebx
+++	andl	%ecx,%r11d
+++	xorl	%eax,%r11d
+++	movl	0(%rsi),%r10d
+++	addl	%r11d,%ebx
+++	roll	$22,%ebx
+++	movl	%edx,%r11d
+++	addl	%ecx,%ebx
+++	movl	4(%rsi),%r10d
+++	movl	%edx,%r11d
+++	movl	%edx,%r12d
+++	notl	%r11d
+++	leal	-165796510(%rax,%r10,1),%eax
+++	andl	%ebx,%r12d
+++	andl	%ecx,%r11d
+++	movl	24(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%ecx,%r11d
+++	addl	%r12d,%eax
+++	movl	%ecx,%r12d
+++	roll	$5,%eax
+++	addl	%ebx,%eax
+++	notl	%r11d
+++	leal	-1069501632(%rdx,%r10,1),%edx
+++	andl	%eax,%r12d
+++	andl	%ebx,%r11d
+++	movl	44(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%ebx,%r11d
+++	addl	%r12d,%edx
+++	movl	%ebx,%r12d
+++	roll	$9,%edx
+++	addl	%eax,%edx
+++	notl	%r11d
+++	leal	643717713(%rcx,%r10,1),%ecx
+++	andl	%edx,%r12d
+++	andl	%eax,%r11d
+++	movl	0(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%eax,%r11d
+++	addl	%r12d,%ecx
+++	movl	%eax,%r12d
+++	roll	$14,%ecx
+++	addl	%edx,%ecx
+++	notl	%r11d
+++	leal	-373897302(%rbx,%r10,1),%ebx
+++	andl	%ecx,%r12d
+++	andl	%edx,%r11d
+++	movl	20(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%edx,%r11d
+++	addl	%r12d,%ebx
+++	movl	%edx,%r12d
+++	roll	$20,%ebx
+++	addl	%ecx,%ebx
+++	notl	%r11d
+++	leal	-701558691(%rax,%r10,1),%eax
+++	andl	%ebx,%r12d
+++	andl	%ecx,%r11d
+++	movl	40(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%ecx,%r11d
+++	addl	%r12d,%eax
+++	movl	%ecx,%r12d
+++	roll	$5,%eax
+++	addl	%ebx,%eax
+++	notl	%r11d
+++	leal	38016083(%rdx,%r10,1),%edx
+++	andl	%eax,%r12d
+++	andl	%ebx,%r11d
+++	movl	60(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%ebx,%r11d
+++	addl	%r12d,%edx
+++	movl	%ebx,%r12d
+++	roll	$9,%edx
+++	addl	%eax,%edx
+++	notl	%r11d
+++	leal	-660478335(%rcx,%r10,1),%ecx
+++	andl	%edx,%r12d
+++	andl	%eax,%r11d
+++	movl	16(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%eax,%r11d
+++	addl	%r12d,%ecx
+++	movl	%eax,%r12d
+++	roll	$14,%ecx
+++	addl	%edx,%ecx
+++	notl	%r11d
+++	leal	-405537848(%rbx,%r10,1),%ebx
+++	andl	%ecx,%r12d
+++	andl	%edx,%r11d
+++	movl	36(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%edx,%r11d
+++	addl	%r12d,%ebx
+++	movl	%edx,%r12d
+++	roll	$20,%ebx
+++	addl	%ecx,%ebx
+++	notl	%r11d
+++	leal	568446438(%rax,%r10,1),%eax
+++	andl	%ebx,%r12d
+++	andl	%ecx,%r11d
+++	movl	56(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%ecx,%r11d
+++	addl	%r12d,%eax
+++	movl	%ecx,%r12d
+++	roll	$5,%eax
+++	addl	%ebx,%eax
+++	notl	%r11d
+++	leal	-1019803690(%rdx,%r10,1),%edx
+++	andl	%eax,%r12d
+++	andl	%ebx,%r11d
+++	movl	12(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%ebx,%r11d
+++	addl	%r12d,%edx
+++	movl	%ebx,%r12d
+++	roll	$9,%edx
+++	addl	%eax,%edx
+++	notl	%r11d
+++	leal	-187363961(%rcx,%r10,1),%ecx
+++	andl	%edx,%r12d
+++	andl	%eax,%r11d
+++	movl	32(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%eax,%r11d
+++	addl	%r12d,%ecx
+++	movl	%eax,%r12d
+++	roll	$14,%ecx
+++	addl	%edx,%ecx
+++	notl	%r11d
+++	leal	1163531501(%rbx,%r10,1),%ebx
+++	andl	%ecx,%r12d
+++	andl	%edx,%r11d
+++	movl	52(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%edx,%r11d
+++	addl	%r12d,%ebx
+++	movl	%edx,%r12d
+++	roll	$20,%ebx
+++	addl	%ecx,%ebx
+++	notl	%r11d
+++	leal	-1444681467(%rax,%r10,1),%eax
+++	andl	%ebx,%r12d
+++	andl	%ecx,%r11d
+++	movl	8(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%ecx,%r11d
+++	addl	%r12d,%eax
+++	movl	%ecx,%r12d
+++	roll	$5,%eax
+++	addl	%ebx,%eax
+++	notl	%r11d
+++	leal	-51403784(%rdx,%r10,1),%edx
+++	andl	%eax,%r12d
+++	andl	%ebx,%r11d
+++	movl	28(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%ebx,%r11d
+++	addl	%r12d,%edx
+++	movl	%ebx,%r12d
+++	roll	$9,%edx
+++	addl	%eax,%edx
+++	notl	%r11d
+++	leal	1735328473(%rcx,%r10,1),%ecx
+++	andl	%edx,%r12d
+++	andl	%eax,%r11d
+++	movl	48(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%eax,%r11d
+++	addl	%r12d,%ecx
+++	movl	%eax,%r12d
+++	roll	$14,%ecx
+++	addl	%edx,%ecx
+++	notl	%r11d
+++	leal	-1926607734(%rbx,%r10,1),%ebx
+++	andl	%ecx,%r12d
+++	andl	%edx,%r11d
+++	movl	0(%rsi),%r10d
+++	orl	%r11d,%r12d
+++	movl	%edx,%r11d
+++	addl	%r12d,%ebx
+++	movl	%edx,%r12d
+++	roll	$20,%ebx
+++	addl	%ecx,%ebx
+++	movl	20(%rsi),%r10d
+++	movl	%ecx,%r11d
+++	leal	-378558(%rax,%r10,1),%eax
+++	movl	32(%rsi),%r10d
+++	xorl	%edx,%r11d
+++	xorl	%ebx,%r11d
+++	addl	%r11d,%eax
+++	roll	$4,%eax
+++	movl	%ebx,%r11d
+++	addl	%ebx,%eax
+++	leal	-2022574463(%rdx,%r10,1),%edx
+++	movl	44(%rsi),%r10d
+++	xorl	%ecx,%r11d
+++	xorl	%eax,%r11d
+++	addl	%r11d,%edx
+++	roll	$11,%edx
+++	movl	%eax,%r11d
+++	addl	%eax,%edx
+++	leal	1839030562(%rcx,%r10,1),%ecx
+++	movl	56(%rsi),%r10d
+++	xorl	%ebx,%r11d
+++	xorl	%edx,%r11d
+++	addl	%r11d,%ecx
+++	roll	$16,%ecx
+++	movl	%edx,%r11d
+++	addl	%edx,%ecx
+++	leal	-35309556(%rbx,%r10,1),%ebx
+++	movl	4(%rsi),%r10d
+++	xorl	%eax,%r11d
+++	xorl	%ecx,%r11d
+++	addl	%r11d,%ebx
+++	roll	$23,%ebx
+++	movl	%ecx,%r11d
+++	addl	%ecx,%ebx
+++	leal	-1530992060(%rax,%r10,1),%eax
+++	movl	16(%rsi),%r10d
+++	xorl	%edx,%r11d
+++	xorl	%ebx,%r11d
+++	addl	%r11d,%eax
+++	roll	$4,%eax
+++	movl	%ebx,%r11d
+++	addl	%ebx,%eax
+++	leal	1272893353(%rdx,%r10,1),%edx
+++	movl	28(%rsi),%r10d
+++	xorl	%ecx,%r11d
+++	xorl	%eax,%r11d
+++	addl	%r11d,%edx
+++	roll	$11,%edx
+++	movl	%eax,%r11d
+++	addl	%eax,%edx
+++	leal	-155497632(%rcx,%r10,1),%ecx
+++	movl	40(%rsi),%r10d
+++	xorl	%ebx,%r11d
+++	xorl	%edx,%r11d
+++	addl	%r11d,%ecx
+++	roll	$16,%ecx
+++	movl	%edx,%r11d
+++	addl	%edx,%ecx
+++	leal	-1094730640(%rbx,%r10,1),%ebx
+++	movl	52(%rsi),%r10d
+++	xorl	%eax,%r11d
+++	xorl	%ecx,%r11d
+++	addl	%r11d,%ebx
+++	roll	$23,%ebx
+++	movl	%ecx,%r11d
+++	addl	%ecx,%ebx
+++	leal	681279174(%rax,%r10,1),%eax
+++	movl	0(%rsi),%r10d
+++	xorl	%edx,%r11d
+++	xorl	%ebx,%r11d
+++	addl	%r11d,%eax
+++	roll	$4,%eax
+++	movl	%ebx,%r11d
+++	addl	%ebx,%eax
+++	leal	-358537222(%rdx,%r10,1),%edx
+++	movl	12(%rsi),%r10d
+++	xorl	%ecx,%r11d
+++	xorl	%eax,%r11d
+++	addl	%r11d,%edx
+++	roll	$11,%edx
+++	movl	%eax,%r11d
+++	addl	%eax,%edx
+++	leal	-722521979(%rcx,%r10,1),%ecx
+++	movl	24(%rsi),%r10d
+++	xorl	%ebx,%r11d
+++	xorl	%edx,%r11d
+++	addl	%r11d,%ecx
+++	roll	$16,%ecx
+++	movl	%edx,%r11d
+++	addl	%edx,%ecx
+++	leal	76029189(%rbx,%r10,1),%ebx
+++	movl	36(%rsi),%r10d
+++	xorl	%eax,%r11d
+++	xorl	%ecx,%r11d
+++	addl	%r11d,%ebx
+++	roll	$23,%ebx
+++	movl	%ecx,%r11d
+++	addl	%ecx,%ebx
+++	leal	-640364487(%rax,%r10,1),%eax
+++	movl	48(%rsi),%r10d
+++	xorl	%edx,%r11d
+++	xorl	%ebx,%r11d
+++	addl	%r11d,%eax
+++	roll	$4,%eax
+++	movl	%ebx,%r11d
+++	addl	%ebx,%eax
+++	leal	-421815835(%rdx,%r10,1),%edx
+++	movl	60(%rsi),%r10d
+++	xorl	%ecx,%r11d
+++	xorl	%eax,%r11d
+++	addl	%r11d,%edx
+++	roll	$11,%edx
+++	movl	%eax,%r11d
+++	addl	%eax,%edx
+++	leal	530742520(%rcx,%r10,1),%ecx
+++	movl	8(%rsi),%r10d
+++	xorl	%ebx,%r11d
+++	xorl	%edx,%r11d
+++	addl	%r11d,%ecx
+++	roll	$16,%ecx
+++	movl	%edx,%r11d
+++	addl	%edx,%ecx
+++	leal	-995338651(%rbx,%r10,1),%ebx
+++	movl	0(%rsi),%r10d
+++	xorl	%eax,%r11d
+++	xorl	%ecx,%r11d
+++	addl	%r11d,%ebx
+++	roll	$23,%ebx
+++	movl	%ecx,%r11d
+++	addl	%ecx,%ebx
+++	movl	0(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	xorl	%edx,%r11d
+++	leal	-198630844(%rax,%r10,1),%eax
+++	orl	%ebx,%r11d
+++	xorl	%ecx,%r11d
+++	addl	%r11d,%eax
+++	movl	28(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$6,%eax
+++	xorl	%ecx,%r11d
+++	addl	%ebx,%eax
+++	leal	1126891415(%rdx,%r10,1),%edx
+++	orl	%eax,%r11d
+++	xorl	%ebx,%r11d
+++	addl	%r11d,%edx
+++	movl	56(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$10,%edx
+++	xorl	%ebx,%r11d
+++	addl	%eax,%edx
+++	leal	-1416354905(%rcx,%r10,1),%ecx
+++	orl	%edx,%r11d
+++	xorl	%eax,%r11d
+++	addl	%r11d,%ecx
+++	movl	20(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$15,%ecx
+++	xorl	%eax,%r11d
+++	addl	%edx,%ecx
+++	leal	-57434055(%rbx,%r10,1),%ebx
+++	orl	%ecx,%r11d
+++	xorl	%edx,%r11d
+++	addl	%r11d,%ebx
+++	movl	48(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$21,%ebx
+++	xorl	%edx,%r11d
+++	addl	%ecx,%ebx
+++	leal	1700485571(%rax,%r10,1),%eax
+++	orl	%ebx,%r11d
+++	xorl	%ecx,%r11d
+++	addl	%r11d,%eax
+++	movl	12(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$6,%eax
+++	xorl	%ecx,%r11d
+++	addl	%ebx,%eax
+++	leal	-1894986606(%rdx,%r10,1),%edx
+++	orl	%eax,%r11d
+++	xorl	%ebx,%r11d
+++	addl	%r11d,%edx
+++	movl	40(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$10,%edx
+++	xorl	%ebx,%r11d
+++	addl	%eax,%edx
+++	leal	-1051523(%rcx,%r10,1),%ecx
+++	orl	%edx,%r11d
+++	xorl	%eax,%r11d
+++	addl	%r11d,%ecx
+++	movl	4(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$15,%ecx
+++	xorl	%eax,%r11d
+++	addl	%edx,%ecx
+++	leal	-2054922799(%rbx,%r10,1),%ebx
+++	orl	%ecx,%r11d
+++	xorl	%edx,%r11d
+++	addl	%r11d,%ebx
+++	movl	32(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$21,%ebx
+++	xorl	%edx,%r11d
+++	addl	%ecx,%ebx
+++	leal	1873313359(%rax,%r10,1),%eax
+++	orl	%ebx,%r11d
+++	xorl	%ecx,%r11d
+++	addl	%r11d,%eax
+++	movl	60(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$6,%eax
+++	xorl	%ecx,%r11d
+++	addl	%ebx,%eax
+++	leal	-30611744(%rdx,%r10,1),%edx
+++	orl	%eax,%r11d
+++	xorl	%ebx,%r11d
+++	addl	%r11d,%edx
+++	movl	24(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$10,%edx
+++	xorl	%ebx,%r11d
+++	addl	%eax,%edx
+++	leal	-1560198380(%rcx,%r10,1),%ecx
+++	orl	%edx,%r11d
+++	xorl	%eax,%r11d
+++	addl	%r11d,%ecx
+++	movl	52(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$15,%ecx
+++	xorl	%eax,%r11d
+++	addl	%edx,%ecx
+++	leal	1309151649(%rbx,%r10,1),%ebx
+++	orl	%ecx,%r11d
+++	xorl	%edx,%r11d
+++	addl	%r11d,%ebx
+++	movl	16(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$21,%ebx
+++	xorl	%edx,%r11d
+++	addl	%ecx,%ebx
+++	leal	-145523070(%rax,%r10,1),%eax
+++	orl	%ebx,%r11d
+++	xorl	%ecx,%r11d
+++	addl	%r11d,%eax
+++	movl	44(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$6,%eax
+++	xorl	%ecx,%r11d
+++	addl	%ebx,%eax
+++	leal	-1120210379(%rdx,%r10,1),%edx
+++	orl	%eax,%r11d
+++	xorl	%ebx,%r11d
+++	addl	%r11d,%edx
+++	movl	8(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$10,%edx
+++	xorl	%ebx,%r11d
+++	addl	%eax,%edx
+++	leal	718787259(%rcx,%r10,1),%ecx
+++	orl	%edx,%r11d
+++	xorl	%eax,%r11d
+++	addl	%r11d,%ecx
+++	movl	36(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$15,%ecx
+++	xorl	%eax,%r11d
+++	addl	%edx,%ecx
+++	leal	-343485551(%rbx,%r10,1),%ebx
+++	orl	%ecx,%r11d
+++	xorl	%edx,%r11d
+++	addl	%r11d,%ebx
+++	movl	0(%rsi),%r10d
+++	movl	$0xffffffff,%r11d
+++	roll	$21,%ebx
+++	xorl	%edx,%r11d
+++	addl	%ecx,%ebx
+++
+++	addl	%r8d,%eax
+++	addl	%r9d,%ebx
+++	addl	%r14d,%ecx
+++	addl	%r15d,%edx
+++
+++
+++	addq	$64,%rsi
+++	cmpq	%rdi,%rsi
+++	jb	.Lloop
+++
+++
+++.Lend:
+++	movl	%eax,0(%rbp)
+++	movl	%ebx,4(%rbp)
+++	movl	%ecx,8(%rbp)
+++	movl	%edx,12(%rbp)
+++
+++	movq	(%rsp),%r15
+++.cfi_restore	r15
+++	movq	8(%rsp),%r14
+++.cfi_restore	r14
+++	movq	16(%rsp),%r12
+++.cfi_restore	r12
+++	movq	24(%rsp),%rbx
+++.cfi_restore	rbx
+++	movq	32(%rsp),%rbp
+++.cfi_restore	rbp
+++	addq	$40,%rsp
+++.cfi_adjust_cfa_offset	-40
+++.Lepilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	md5_block_asm_data_order,.-md5_block_asm_data_order
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/p256-x86_64-asm.S b/linux-x86_64/ypto/fipsmodule/p256-x86_64-asm.S
++new file mode 100644
++index 000000000..85f489901
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/p256-x86_64-asm.S
++@@ -0,0 +1,4543 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++.extern	OPENSSL_ia32cap_P
+++.hidden OPENSSL_ia32cap_P
+++
+++
+++.align	64
+++.Lpoly:
+++.quad	0xffffffffffffffff, 0x00000000ffffffff, 0x0000000000000000, 0xffffffff00000001
+++
+++.LOne:
+++.long	1,1,1,1,1,1,1,1
+++.LTwo:
+++.long	2,2,2,2,2,2,2,2
+++.LThree:
+++.long	3,3,3,3,3,3,3,3
+++.LONE_mont:
+++.quad	0x0000000000000001, 0xffffffff00000000, 0xffffffffffffffff, 0x00000000fffffffe
+++
+++
+++.Lord:
+++.quad	0xf3b9cac2fc632551, 0xbce6faada7179e84, 0xffffffffffffffff, 0xffffffff00000000
+++.LordK:
+++.quad	0xccd1c8aaee00bc4f
+++
+++
+++
+++.globl	ecp_nistz256_neg
+++.hidden ecp_nistz256_neg
+++.type	ecp_nistz256_neg,@function
+++.align	32
+++ecp_nistz256_neg:
+++.cfi_startproc	
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-16
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-24
+++.Lneg_body:
+++
+++	xorq	%r8,%r8
+++	xorq	%r9,%r9
+++	xorq	%r10,%r10
+++	xorq	%r11,%r11
+++	xorq	%r13,%r13
+++
+++	subq	0(%rsi),%r8
+++	sbbq	8(%rsi),%r9
+++	sbbq	16(%rsi),%r10
+++	movq	%r8,%rax
+++	sbbq	24(%rsi),%r11
+++	leaq	.Lpoly(%rip),%rsi
+++	movq	%r9,%rdx
+++	sbbq	$0,%r13
+++
+++	addq	0(%rsi),%r8
+++	movq	%r10,%rcx
+++	adcq	8(%rsi),%r9
+++	adcq	16(%rsi),%r10
+++	movq	%r11,%r12
+++	adcq	24(%rsi),%r11
+++	testq	%r13,%r13
+++
+++	cmovzq	%rax,%r8
+++	cmovzq	%rdx,%r9
+++	movq	%r8,0(%rdi)
+++	cmovzq	%rcx,%r10
+++	movq	%r9,8(%rdi)
+++	cmovzq	%r12,%r11
+++	movq	%r10,16(%rdi)
+++	movq	%r11,24(%rdi)
+++
+++	movq	0(%rsp),%r13
+++.cfi_restore	%r13
+++	movq	8(%rsp),%r12
+++.cfi_restore	%r12
+++	leaq	16(%rsp),%rsp
+++.cfi_adjust_cfa_offset	-16
+++.Lneg_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ecp_nistz256_neg,.-ecp_nistz256_neg
+++
+++
+++
+++
+++
+++
+++.globl	ecp_nistz256_ord_mul_mont
+++.hidden ecp_nistz256_ord_mul_mont
+++.type	ecp_nistz256_ord_mul_mont,@function
+++.align	32
+++ecp_nistz256_ord_mul_mont:
+++.cfi_startproc	
+++	leaq	OPENSSL_ia32cap_P(%rip),%rcx
+++	movq	8(%rcx),%rcx
+++	andl	$0x80100,%ecx
+++	cmpl	$0x80100,%ecx
+++	je	.Lecp_nistz256_ord_mul_montx
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++.Lord_mul_body:
+++
+++	movq	0(%rdx),%rax
+++	movq	%rdx,%rbx
+++	leaq	.Lord(%rip),%r14
+++	movq	.LordK(%rip),%r15
+++
+++
+++	movq	%rax,%rcx
+++	mulq	0(%rsi)
+++	movq	%rax,%r8
+++	movq	%rcx,%rax
+++	movq	%rdx,%r9
+++
+++	mulq	8(%rsi)
+++	addq	%rax,%r9
+++	movq	%rcx,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	16(%rsi)
+++	addq	%rax,%r10
+++	movq	%rcx,%rax
+++	adcq	$0,%rdx
+++
+++	movq	%r8,%r13
+++	imulq	%r15,%r8
+++
+++	movq	%rdx,%r11
+++	mulq	24(%rsi)
+++	addq	%rax,%r11
+++	movq	%r8,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r12
+++
+++
+++	mulq	0(%r14)
+++	movq	%r8,%rbp
+++	addq	%rax,%r13
+++	movq	%r8,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rcx
+++
+++	subq	%r8,%r10
+++	sbbq	$0,%r8
+++
+++	mulq	8(%r14)
+++	addq	%rcx,%r9
+++	adcq	$0,%rdx
+++	addq	%rax,%r9
+++	movq	%rbp,%rax
+++	adcq	%rdx,%r10
+++	movq	%rbp,%rdx
+++	adcq	$0,%r8
+++
+++	shlq	$32,%rax
+++	shrq	$32,%rdx
+++	subq	%rax,%r11
+++	movq	8(%rbx),%rax
+++	sbbq	%rdx,%rbp
+++
+++	addq	%r8,%r11
+++	adcq	%rbp,%r12
+++	adcq	$0,%r13
+++
+++
+++	movq	%rax,%rcx
+++	mulq	0(%rsi)
+++	addq	%rax,%r9
+++	movq	%rcx,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rbp
+++
+++	mulq	8(%rsi)
+++	addq	%rbp,%r10
+++	adcq	$0,%rdx
+++	addq	%rax,%r10
+++	movq	%rcx,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rbp
+++
+++	mulq	16(%rsi)
+++	addq	%rbp,%r11
+++	adcq	$0,%rdx
+++	addq	%rax,%r11
+++	movq	%rcx,%rax
+++	adcq	$0,%rdx
+++
+++	movq	%r9,%rcx
+++	imulq	%r15,%r9
+++
+++	movq	%rdx,%rbp
+++	mulq	24(%rsi)
+++	addq	%rbp,%r12
+++	adcq	$0,%rdx
+++	xorq	%r8,%r8
+++	addq	%rax,%r12
+++	movq	%r9,%rax
+++	adcq	%rdx,%r13
+++	adcq	$0,%r8
+++
+++
+++	mulq	0(%r14)
+++	movq	%r9,%rbp
+++	addq	%rax,%rcx
+++	movq	%r9,%rax
+++	adcq	%rdx,%rcx
+++
+++	subq	%r9,%r11
+++	sbbq	$0,%r9
+++
+++	mulq	8(%r14)
+++	addq	%rcx,%r10
+++	adcq	$0,%rdx
+++	addq	%rax,%r10
+++	movq	%rbp,%rax
+++	adcq	%rdx,%r11
+++	movq	%rbp,%rdx
+++	adcq	$0,%r9
+++
+++	shlq	$32,%rax
+++	shrq	$32,%rdx
+++	subq	%rax,%r12
+++	movq	16(%rbx),%rax
+++	sbbq	%rdx,%rbp
+++
+++	addq	%r9,%r12
+++	adcq	%rbp,%r13
+++	adcq	$0,%r8
+++
+++
+++	movq	%rax,%rcx
+++	mulq	0(%rsi)
+++	addq	%rax,%r10
+++	movq	%rcx,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rbp
+++
+++	mulq	8(%rsi)
+++	addq	%rbp,%r11
+++	adcq	$0,%rdx
+++	addq	%rax,%r11
+++	movq	%rcx,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rbp
+++
+++	mulq	16(%rsi)
+++	addq	%rbp,%r12
+++	adcq	$0,%rdx
+++	addq	%rax,%r12
+++	movq	%rcx,%rax
+++	adcq	$0,%rdx
+++
+++	movq	%r10,%rcx
+++	imulq	%r15,%r10
+++
+++	movq	%rdx,%rbp
+++	mulq	24(%rsi)
+++	addq	%rbp,%r13
+++	adcq	$0,%rdx
+++	xorq	%r9,%r9
+++	addq	%rax,%r13
+++	movq	%r10,%rax
+++	adcq	%rdx,%r8
+++	adcq	$0,%r9
+++
+++
+++	mulq	0(%r14)
+++	movq	%r10,%rbp
+++	addq	%rax,%rcx
+++	movq	%r10,%rax
+++	adcq	%rdx,%rcx
+++
+++	subq	%r10,%r12
+++	sbbq	$0,%r10
+++
+++	mulq	8(%r14)
+++	addq	%rcx,%r11
+++	adcq	$0,%rdx
+++	addq	%rax,%r11
+++	movq	%rbp,%rax
+++	adcq	%rdx,%r12
+++	movq	%rbp,%rdx
+++	adcq	$0,%r10
+++
+++	shlq	$32,%rax
+++	shrq	$32,%rdx
+++	subq	%rax,%r13
+++	movq	24(%rbx),%rax
+++	sbbq	%rdx,%rbp
+++
+++	addq	%r10,%r13
+++	adcq	%rbp,%r8
+++	adcq	$0,%r9
+++
+++
+++	movq	%rax,%rcx
+++	mulq	0(%rsi)
+++	addq	%rax,%r11
+++	movq	%rcx,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rbp
+++
+++	mulq	8(%rsi)
+++	addq	%rbp,%r12
+++	adcq	$0,%rdx
+++	addq	%rax,%r12
+++	movq	%rcx,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rbp
+++
+++	mulq	16(%rsi)
+++	addq	%rbp,%r13
+++	adcq	$0,%rdx
+++	addq	%rax,%r13
+++	movq	%rcx,%rax
+++	adcq	$0,%rdx
+++
+++	movq	%r11,%rcx
+++	imulq	%r15,%r11
+++
+++	movq	%rdx,%rbp
+++	mulq	24(%rsi)
+++	addq	%rbp,%r8
+++	adcq	$0,%rdx
+++	xorq	%r10,%r10
+++	addq	%rax,%r8
+++	movq	%r11,%rax
+++	adcq	%rdx,%r9
+++	adcq	$0,%r10
+++
+++
+++	mulq	0(%r14)
+++	movq	%r11,%rbp
+++	addq	%rax,%rcx
+++	movq	%r11,%rax
+++	adcq	%rdx,%rcx
+++
+++	subq	%r11,%r13
+++	sbbq	$0,%r11
+++
+++	mulq	8(%r14)
+++	addq	%rcx,%r12
+++	adcq	$0,%rdx
+++	addq	%rax,%r12
+++	movq	%rbp,%rax
+++	adcq	%rdx,%r13
+++	movq	%rbp,%rdx
+++	adcq	$0,%r11
+++
+++	shlq	$32,%rax
+++	shrq	$32,%rdx
+++	subq	%rax,%r8
+++	sbbq	%rdx,%rbp
+++
+++	addq	%r11,%r8
+++	adcq	%rbp,%r9
+++	adcq	$0,%r10
+++
+++
+++	movq	%r12,%rsi
+++	subq	0(%r14),%r12
+++	movq	%r13,%r11
+++	sbbq	8(%r14),%r13
+++	movq	%r8,%rcx
+++	sbbq	16(%r14),%r8
+++	movq	%r9,%rbp
+++	sbbq	24(%r14),%r9
+++	sbbq	$0,%r10
+++
+++	cmovcq	%rsi,%r12
+++	cmovcq	%r11,%r13
+++	cmovcq	%rcx,%r8
+++	cmovcq	%rbp,%r9
+++
+++	movq	%r12,0(%rdi)
+++	movq	%r13,8(%rdi)
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++
+++	movq	0(%rsp),%r15
+++.cfi_restore	%r15
+++	movq	8(%rsp),%r14
+++.cfi_restore	%r14
+++	movq	16(%rsp),%r13
+++.cfi_restore	%r13
+++	movq	24(%rsp),%r12
+++.cfi_restore	%r12
+++	movq	32(%rsp),%rbx
+++.cfi_restore	%rbx
+++	movq	40(%rsp),%rbp
+++.cfi_restore	%rbp
+++	leaq	48(%rsp),%rsp
+++.cfi_adjust_cfa_offset	-48
+++.Lord_mul_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ecp_nistz256_ord_mul_mont,.-ecp_nistz256_ord_mul_mont
+++
+++
+++
+++
+++
+++
+++
+++.globl	ecp_nistz256_ord_sqr_mont
+++.hidden ecp_nistz256_ord_sqr_mont
+++.type	ecp_nistz256_ord_sqr_mont,@function
+++.align	32
+++ecp_nistz256_ord_sqr_mont:
+++.cfi_startproc	
+++	leaq	OPENSSL_ia32cap_P(%rip),%rcx
+++	movq	8(%rcx),%rcx
+++	andl	$0x80100,%ecx
+++	cmpl	$0x80100,%ecx
+++	je	.Lecp_nistz256_ord_sqr_montx
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++.Lord_sqr_body:
+++
+++	movq	0(%rsi),%r8
+++	movq	8(%rsi),%rax
+++	movq	16(%rsi),%r14
+++	movq	24(%rsi),%r15
+++	leaq	.Lord(%rip),%rsi
+++	movq	%rdx,%rbx
+++	jmp	.Loop_ord_sqr
+++
+++.align	32
+++.Loop_ord_sqr:
+++
+++	movq	%rax,%rbp
+++	mulq	%r8
+++	movq	%rax,%r9
+++.byte	102,72,15,110,205
+++	movq	%r14,%rax
+++	movq	%rdx,%r10
+++
+++	mulq	%r8
+++	addq	%rax,%r10
+++	movq	%r15,%rax
+++.byte	102,73,15,110,214
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%r8
+++	addq	%rax,%r11
+++	movq	%r15,%rax
+++.byte	102,73,15,110,223
+++	adcq	$0,%rdx
+++	movq	%rdx,%r12
+++
+++
+++	mulq	%r14
+++	movq	%rax,%r13
+++	movq	%r14,%rax
+++	movq	%rdx,%r14
+++
+++
+++	mulq	%rbp
+++	addq	%rax,%r11
+++	movq	%r15,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r15
+++
+++	mulq	%rbp
+++	addq	%rax,%r12
+++	adcq	$0,%rdx
+++
+++	addq	%r15,%r12
+++	adcq	%rdx,%r13
+++	adcq	$0,%r14
+++
+++
+++	xorq	%r15,%r15
+++	movq	%r8,%rax
+++	addq	%r9,%r9
+++	adcq	%r10,%r10
+++	adcq	%r11,%r11
+++	adcq	%r12,%r12
+++	adcq	%r13,%r13
+++	adcq	%r14,%r14
+++	adcq	$0,%r15
+++
+++
+++	mulq	%rax
+++	movq	%rax,%r8
+++.byte	102,72,15,126,200
+++	movq	%rdx,%rbp
+++
+++	mulq	%rax
+++	addq	%rbp,%r9
+++	adcq	%rax,%r10
+++.byte	102,72,15,126,208
+++	adcq	$0,%rdx
+++	movq	%rdx,%rbp
+++
+++	mulq	%rax
+++	addq	%rbp,%r11
+++	adcq	%rax,%r12
+++.byte	102,72,15,126,216
+++	adcq	$0,%rdx
+++	movq	%rdx,%rbp
+++
+++	movq	%r8,%rcx
+++	imulq	32(%rsi),%r8
+++
+++	mulq	%rax
+++	addq	%rbp,%r13
+++	adcq	%rax,%r14
+++	movq	0(%rsi),%rax
+++	adcq	%rdx,%r15
+++
+++
+++	mulq	%r8
+++	movq	%r8,%rbp
+++	addq	%rax,%rcx
+++	movq	8(%rsi),%rax
+++	adcq	%rdx,%rcx
+++
+++	subq	%r8,%r10
+++	sbbq	$0,%rbp
+++
+++	mulq	%r8
+++	addq	%rcx,%r9
+++	adcq	$0,%rdx
+++	addq	%rax,%r9
+++	movq	%r8,%rax
+++	adcq	%rdx,%r10
+++	movq	%r8,%rdx
+++	adcq	$0,%rbp
+++
+++	movq	%r9,%rcx
+++	imulq	32(%rsi),%r9
+++
+++	shlq	$32,%rax
+++	shrq	$32,%rdx
+++	subq	%rax,%r11
+++	movq	0(%rsi),%rax
+++	sbbq	%rdx,%r8
+++
+++	addq	%rbp,%r11
+++	adcq	$0,%r8
+++
+++
+++	mulq	%r9
+++	movq	%r9,%rbp
+++	addq	%rax,%rcx
+++	movq	8(%rsi),%rax
+++	adcq	%rdx,%rcx
+++
+++	subq	%r9,%r11
+++	sbbq	$0,%rbp
+++
+++	mulq	%r9
+++	addq	%rcx,%r10
+++	adcq	$0,%rdx
+++	addq	%rax,%r10
+++	movq	%r9,%rax
+++	adcq	%rdx,%r11
+++	movq	%r9,%rdx
+++	adcq	$0,%rbp
+++
+++	movq	%r10,%rcx
+++	imulq	32(%rsi),%r10
+++
+++	shlq	$32,%rax
+++	shrq	$32,%rdx
+++	subq	%rax,%r8
+++	movq	0(%rsi),%rax
+++	sbbq	%rdx,%r9
+++
+++	addq	%rbp,%r8
+++	adcq	$0,%r9
+++
+++
+++	mulq	%r10
+++	movq	%r10,%rbp
+++	addq	%rax,%rcx
+++	movq	8(%rsi),%rax
+++	adcq	%rdx,%rcx
+++
+++	subq	%r10,%r8
+++	sbbq	$0,%rbp
+++
+++	mulq	%r10
+++	addq	%rcx,%r11
+++	adcq	$0,%rdx
+++	addq	%rax,%r11
+++	movq	%r10,%rax
+++	adcq	%rdx,%r8
+++	movq	%r10,%rdx
+++	adcq	$0,%rbp
+++
+++	movq	%r11,%rcx
+++	imulq	32(%rsi),%r11
+++
+++	shlq	$32,%rax
+++	shrq	$32,%rdx
+++	subq	%rax,%r9
+++	movq	0(%rsi),%rax
+++	sbbq	%rdx,%r10
+++
+++	addq	%rbp,%r9
+++	adcq	$0,%r10
+++
+++
+++	mulq	%r11
+++	movq	%r11,%rbp
+++	addq	%rax,%rcx
+++	movq	8(%rsi),%rax
+++	adcq	%rdx,%rcx
+++
+++	subq	%r11,%r9
+++	sbbq	$0,%rbp
+++
+++	mulq	%r11
+++	addq	%rcx,%r8
+++	adcq	$0,%rdx
+++	addq	%rax,%r8
+++	movq	%r11,%rax
+++	adcq	%rdx,%r9
+++	movq	%r11,%rdx
+++	adcq	$0,%rbp
+++
+++	shlq	$32,%rax
+++	shrq	$32,%rdx
+++	subq	%rax,%r10
+++	sbbq	%rdx,%r11
+++
+++	addq	%rbp,%r10
+++	adcq	$0,%r11
+++
+++
+++	xorq	%rdx,%rdx
+++	addq	%r12,%r8
+++	adcq	%r13,%r9
+++	movq	%r8,%r12
+++	adcq	%r14,%r10
+++	adcq	%r15,%r11
+++	movq	%r9,%rax
+++	adcq	$0,%rdx
+++
+++
+++	subq	0(%rsi),%r8
+++	movq	%r10,%r14
+++	sbbq	8(%rsi),%r9
+++	sbbq	16(%rsi),%r10
+++	movq	%r11,%r15
+++	sbbq	24(%rsi),%r11
+++	sbbq	$0,%rdx
+++
+++	cmovcq	%r12,%r8
+++	cmovncq	%r9,%rax
+++	cmovncq	%r10,%r14
+++	cmovncq	%r11,%r15
+++
+++	decq	%rbx
+++	jnz	.Loop_ord_sqr
+++
+++	movq	%r8,0(%rdi)
+++	movq	%rax,8(%rdi)
+++	pxor	%xmm1,%xmm1
+++	movq	%r14,16(%rdi)
+++	pxor	%xmm2,%xmm2
+++	movq	%r15,24(%rdi)
+++	pxor	%xmm3,%xmm3
+++
+++	movq	0(%rsp),%r15
+++.cfi_restore	%r15
+++	movq	8(%rsp),%r14
+++.cfi_restore	%r14
+++	movq	16(%rsp),%r13
+++.cfi_restore	%r13
+++	movq	24(%rsp),%r12
+++.cfi_restore	%r12
+++	movq	32(%rsp),%rbx
+++.cfi_restore	%rbx
+++	movq	40(%rsp),%rbp
+++.cfi_restore	%rbp
+++	leaq	48(%rsp),%rsp
+++.cfi_adjust_cfa_offset	-48
+++.Lord_sqr_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ecp_nistz256_ord_sqr_mont,.-ecp_nistz256_ord_sqr_mont
+++
+++.type	ecp_nistz256_ord_mul_montx,@function
+++.align	32
+++ecp_nistz256_ord_mul_montx:
+++.cfi_startproc	
+++.Lecp_nistz256_ord_mul_montx:
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++.Lord_mulx_body:
+++
+++	movq	%rdx,%rbx
+++	movq	0(%rdx),%rdx
+++	movq	0(%rsi),%r9
+++	movq	8(%rsi),%r10
+++	movq	16(%rsi),%r11
+++	movq	24(%rsi),%r12
+++	leaq	-128(%rsi),%rsi
+++	leaq	.Lord-128(%rip),%r14
+++	movq	.LordK(%rip),%r15
+++
+++
+++	mulxq	%r9,%r8,%r9
+++	mulxq	%r10,%rcx,%r10
+++	mulxq	%r11,%rbp,%r11
+++	addq	%rcx,%r9
+++	mulxq	%r12,%rcx,%r12
+++	movq	%r8,%rdx
+++	mulxq	%r15,%rdx,%rax
+++	adcq	%rbp,%r10
+++	adcq	%rcx,%r11
+++	adcq	$0,%r12
+++
+++
+++	xorq	%r13,%r13
+++	mulxq	0+128(%r14),%rcx,%rbp
+++	adcxq	%rcx,%r8
+++	adoxq	%rbp,%r9
+++
+++	mulxq	8+128(%r14),%rcx,%rbp
+++	adcxq	%rcx,%r9
+++	adoxq	%rbp,%r10
+++
+++	mulxq	16+128(%r14),%rcx,%rbp
+++	adcxq	%rcx,%r10
+++	adoxq	%rbp,%r11
+++
+++	mulxq	24+128(%r14),%rcx,%rbp
+++	movq	8(%rbx),%rdx
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r12
+++	adcxq	%r8,%r12
+++	adoxq	%r8,%r13
+++	adcq	$0,%r13
+++
+++
+++	mulxq	0+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r9
+++	adoxq	%rbp,%r10
+++
+++	mulxq	8+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r10
+++	adoxq	%rbp,%r11
+++
+++	mulxq	16+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r12
+++
+++	mulxq	24+128(%rsi),%rcx,%rbp
+++	movq	%r9,%rdx
+++	mulxq	%r15,%rdx,%rax
+++	adcxq	%rcx,%r12
+++	adoxq	%rbp,%r13
+++
+++	adcxq	%r8,%r13
+++	adoxq	%r8,%r8
+++	adcq	$0,%r8
+++
+++
+++	mulxq	0+128(%r14),%rcx,%rbp
+++	adcxq	%rcx,%r9
+++	adoxq	%rbp,%r10
+++
+++	mulxq	8+128(%r14),%rcx,%rbp
+++	adcxq	%rcx,%r10
+++	adoxq	%rbp,%r11
+++
+++	mulxq	16+128(%r14),%rcx,%rbp
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r12
+++
+++	mulxq	24+128(%r14),%rcx,%rbp
+++	movq	16(%rbx),%rdx
+++	adcxq	%rcx,%r12
+++	adoxq	%rbp,%r13
+++	adcxq	%r9,%r13
+++	adoxq	%r9,%r8
+++	adcq	$0,%r8
+++
+++
+++	mulxq	0+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r10
+++	adoxq	%rbp,%r11
+++
+++	mulxq	8+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r12
+++
+++	mulxq	16+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r12
+++	adoxq	%rbp,%r13
+++
+++	mulxq	24+128(%rsi),%rcx,%rbp
+++	movq	%r10,%rdx
+++	mulxq	%r15,%rdx,%rax
+++	adcxq	%rcx,%r13
+++	adoxq	%rbp,%r8
+++
+++	adcxq	%r9,%r8
+++	adoxq	%r9,%r9
+++	adcq	$0,%r9
+++
+++
+++	mulxq	0+128(%r14),%rcx,%rbp
+++	adcxq	%rcx,%r10
+++	adoxq	%rbp,%r11
+++
+++	mulxq	8+128(%r14),%rcx,%rbp
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r12
+++
+++	mulxq	16+128(%r14),%rcx,%rbp
+++	adcxq	%rcx,%r12
+++	adoxq	%rbp,%r13
+++
+++	mulxq	24+128(%r14),%rcx,%rbp
+++	movq	24(%rbx),%rdx
+++	adcxq	%rcx,%r13
+++	adoxq	%rbp,%r8
+++	adcxq	%r10,%r8
+++	adoxq	%r10,%r9
+++	adcq	$0,%r9
+++
+++
+++	mulxq	0+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r12
+++
+++	mulxq	8+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r12
+++	adoxq	%rbp,%r13
+++
+++	mulxq	16+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r13
+++	adoxq	%rbp,%r8
+++
+++	mulxq	24+128(%rsi),%rcx,%rbp
+++	movq	%r11,%rdx
+++	mulxq	%r15,%rdx,%rax
+++	adcxq	%rcx,%r8
+++	adoxq	%rbp,%r9
+++
+++	adcxq	%r10,%r9
+++	adoxq	%r10,%r10
+++	adcq	$0,%r10
+++
+++
+++	mulxq	0+128(%r14),%rcx,%rbp
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r12
+++
+++	mulxq	8+128(%r14),%rcx,%rbp
+++	adcxq	%rcx,%r12
+++	adoxq	%rbp,%r13
+++
+++	mulxq	16+128(%r14),%rcx,%rbp
+++	adcxq	%rcx,%r13
+++	adoxq	%rbp,%r8
+++
+++	mulxq	24+128(%r14),%rcx,%rbp
+++	leaq	128(%r14),%r14
+++	movq	%r12,%rbx
+++	adcxq	%rcx,%r8
+++	adoxq	%rbp,%r9
+++	movq	%r13,%rdx
+++	adcxq	%r11,%r9
+++	adoxq	%r11,%r10
+++	adcq	$0,%r10
+++
+++
+++
+++	movq	%r8,%rcx
+++	subq	0(%r14),%r12
+++	sbbq	8(%r14),%r13
+++	sbbq	16(%r14),%r8
+++	movq	%r9,%rbp
+++	sbbq	24(%r14),%r9
+++	sbbq	$0,%r10
+++
+++	cmovcq	%rbx,%r12
+++	cmovcq	%rdx,%r13
+++	cmovcq	%rcx,%r8
+++	cmovcq	%rbp,%r9
+++
+++	movq	%r12,0(%rdi)
+++	movq	%r13,8(%rdi)
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++
+++	movq	0(%rsp),%r15
+++.cfi_restore	%r15
+++	movq	8(%rsp),%r14
+++.cfi_restore	%r14
+++	movq	16(%rsp),%r13
+++.cfi_restore	%r13
+++	movq	24(%rsp),%r12
+++.cfi_restore	%r12
+++	movq	32(%rsp),%rbx
+++.cfi_restore	%rbx
+++	movq	40(%rsp),%rbp
+++.cfi_restore	%rbp
+++	leaq	48(%rsp),%rsp
+++.cfi_adjust_cfa_offset	-48
+++.Lord_mulx_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ecp_nistz256_ord_mul_montx,.-ecp_nistz256_ord_mul_montx
+++
+++.type	ecp_nistz256_ord_sqr_montx,@function
+++.align	32
+++ecp_nistz256_ord_sqr_montx:
+++.cfi_startproc	
+++.Lecp_nistz256_ord_sqr_montx:
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++.Lord_sqrx_body:
+++
+++	movq	%rdx,%rbx
+++	movq	0(%rsi),%rdx
+++	movq	8(%rsi),%r14
+++	movq	16(%rsi),%r15
+++	movq	24(%rsi),%r8
+++	leaq	.Lord(%rip),%rsi
+++	jmp	.Loop_ord_sqrx
+++
+++.align	32
+++.Loop_ord_sqrx:
+++	mulxq	%r14,%r9,%r10
+++	mulxq	%r15,%rcx,%r11
+++	movq	%rdx,%rax
+++.byte	102,73,15,110,206
+++	mulxq	%r8,%rbp,%r12
+++	movq	%r14,%rdx
+++	addq	%rcx,%r10
+++.byte	102,73,15,110,215
+++	adcq	%rbp,%r11
+++	adcq	$0,%r12
+++	xorq	%r13,%r13
+++
+++	mulxq	%r15,%rcx,%rbp
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r12
+++
+++	mulxq	%r8,%rcx,%rbp
+++	movq	%r15,%rdx
+++	adcxq	%rcx,%r12
+++	adoxq	%rbp,%r13
+++	adcq	$0,%r13
+++
+++	mulxq	%r8,%rcx,%r14
+++	movq	%rax,%rdx
+++.byte	102,73,15,110,216
+++	xorq	%r15,%r15
+++	adcxq	%r9,%r9
+++	adoxq	%rcx,%r13
+++	adcxq	%r10,%r10
+++	adoxq	%r15,%r14
+++
+++
+++	mulxq	%rdx,%r8,%rbp
+++.byte	102,72,15,126,202
+++	adcxq	%r11,%r11
+++	adoxq	%rbp,%r9
+++	adcxq	%r12,%r12
+++	mulxq	%rdx,%rcx,%rax
+++.byte	102,72,15,126,210
+++	adcxq	%r13,%r13
+++	adoxq	%rcx,%r10
+++	adcxq	%r14,%r14
+++	mulxq	%rdx,%rcx,%rbp
+++.byte	0x67
+++.byte	102,72,15,126,218
+++	adoxq	%rax,%r11
+++	adcxq	%r15,%r15
+++	adoxq	%rcx,%r12
+++	adoxq	%rbp,%r13
+++	mulxq	%rdx,%rcx,%rax
+++	adoxq	%rcx,%r14
+++	adoxq	%rax,%r15
+++
+++
+++	movq	%r8,%rdx
+++	mulxq	32(%rsi),%rdx,%rcx
+++
+++	xorq	%rax,%rax
+++	mulxq	0(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r8
+++	adoxq	%rbp,%r9
+++	mulxq	8(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r9
+++	adoxq	%rbp,%r10
+++	mulxq	16(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r10
+++	adoxq	%rbp,%r11
+++	mulxq	24(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r8
+++	adcxq	%rax,%r8
+++
+++
+++	movq	%r9,%rdx
+++	mulxq	32(%rsi),%rdx,%rcx
+++
+++	mulxq	0(%rsi),%rcx,%rbp
+++	adoxq	%rcx,%r9
+++	adcxq	%rbp,%r10
+++	mulxq	8(%rsi),%rcx,%rbp
+++	adoxq	%rcx,%r10
+++	adcxq	%rbp,%r11
+++	mulxq	16(%rsi),%rcx,%rbp
+++	adoxq	%rcx,%r11
+++	adcxq	%rbp,%r8
+++	mulxq	24(%rsi),%rcx,%rbp
+++	adoxq	%rcx,%r8
+++	adcxq	%rbp,%r9
+++	adoxq	%rax,%r9
+++
+++
+++	movq	%r10,%rdx
+++	mulxq	32(%rsi),%rdx,%rcx
+++
+++	mulxq	0(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r10
+++	adoxq	%rbp,%r11
+++	mulxq	8(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r8
+++	mulxq	16(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r8
+++	adoxq	%rbp,%r9
+++	mulxq	24(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r9
+++	adoxq	%rbp,%r10
+++	adcxq	%rax,%r10
+++
+++
+++	movq	%r11,%rdx
+++	mulxq	32(%rsi),%rdx,%rcx
+++
+++	mulxq	0(%rsi),%rcx,%rbp
+++	adoxq	%rcx,%r11
+++	adcxq	%rbp,%r8
+++	mulxq	8(%rsi),%rcx,%rbp
+++	adoxq	%rcx,%r8
+++	adcxq	%rbp,%r9
+++	mulxq	16(%rsi),%rcx,%rbp
+++	adoxq	%rcx,%r9
+++	adcxq	%rbp,%r10
+++	mulxq	24(%rsi),%rcx,%rbp
+++	adoxq	%rcx,%r10
+++	adcxq	%rbp,%r11
+++	adoxq	%rax,%r11
+++
+++
+++	addq	%r8,%r12
+++	adcq	%r13,%r9
+++	movq	%r12,%rdx
+++	adcq	%r14,%r10
+++	adcq	%r15,%r11
+++	movq	%r9,%r14
+++	adcq	$0,%rax
+++
+++
+++	subq	0(%rsi),%r12
+++	movq	%r10,%r15
+++	sbbq	8(%rsi),%r9
+++	sbbq	16(%rsi),%r10
+++	movq	%r11,%r8
+++	sbbq	24(%rsi),%r11
+++	sbbq	$0,%rax
+++
+++	cmovncq	%r12,%rdx
+++	cmovncq	%r9,%r14
+++	cmovncq	%r10,%r15
+++	cmovncq	%r11,%r8
+++
+++	decq	%rbx
+++	jnz	.Loop_ord_sqrx
+++
+++	movq	%rdx,0(%rdi)
+++	movq	%r14,8(%rdi)
+++	pxor	%xmm1,%xmm1
+++	movq	%r15,16(%rdi)
+++	pxor	%xmm2,%xmm2
+++	movq	%r8,24(%rdi)
+++	pxor	%xmm3,%xmm3
+++
+++	movq	0(%rsp),%r15
+++.cfi_restore	%r15
+++	movq	8(%rsp),%r14
+++.cfi_restore	%r14
+++	movq	16(%rsp),%r13
+++.cfi_restore	%r13
+++	movq	24(%rsp),%r12
+++.cfi_restore	%r12
+++	movq	32(%rsp),%rbx
+++.cfi_restore	%rbx
+++	movq	40(%rsp),%rbp
+++.cfi_restore	%rbp
+++	leaq	48(%rsp),%rsp
+++.cfi_adjust_cfa_offset	-48
+++.Lord_sqrx_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ecp_nistz256_ord_sqr_montx,.-ecp_nistz256_ord_sqr_montx
+++
+++
+++
+++
+++
+++
+++.globl	ecp_nistz256_mul_mont
+++.hidden ecp_nistz256_mul_mont
+++.type	ecp_nistz256_mul_mont,@function
+++.align	32
+++ecp_nistz256_mul_mont:
+++.cfi_startproc	
+++	leaq	OPENSSL_ia32cap_P(%rip),%rcx
+++	movq	8(%rcx),%rcx
+++	andl	$0x80100,%ecx
+++.Lmul_mont:
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++.Lmul_body:
+++	cmpl	$0x80100,%ecx
+++	je	.Lmul_montx
+++	movq	%rdx,%rbx
+++	movq	0(%rdx),%rax
+++	movq	0(%rsi),%r9
+++	movq	8(%rsi),%r10
+++	movq	16(%rsi),%r11
+++	movq	24(%rsi),%r12
+++
+++	call	__ecp_nistz256_mul_montq
+++	jmp	.Lmul_mont_done
+++
+++.align	32
+++.Lmul_montx:
+++	movq	%rdx,%rbx
+++	movq	0(%rdx),%rdx
+++	movq	0(%rsi),%r9
+++	movq	8(%rsi),%r10
+++	movq	16(%rsi),%r11
+++	movq	24(%rsi),%r12
+++	leaq	-128(%rsi),%rsi
+++
+++	call	__ecp_nistz256_mul_montx
+++.Lmul_mont_done:
+++	movq	0(%rsp),%r15
+++.cfi_restore	%r15
+++	movq	8(%rsp),%r14
+++.cfi_restore	%r14
+++	movq	16(%rsp),%r13
+++.cfi_restore	%r13
+++	movq	24(%rsp),%r12
+++.cfi_restore	%r12
+++	movq	32(%rsp),%rbx
+++.cfi_restore	%rbx
+++	movq	40(%rsp),%rbp
+++.cfi_restore	%rbp
+++	leaq	48(%rsp),%rsp
+++.cfi_adjust_cfa_offset	-48
+++.Lmul_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ecp_nistz256_mul_mont,.-ecp_nistz256_mul_mont
+++
+++.type	__ecp_nistz256_mul_montq,@function
+++.align	32
+++__ecp_nistz256_mul_montq:
+++.cfi_startproc	
+++
+++
+++	movq	%rax,%rbp
+++	mulq	%r9
+++	movq	.Lpoly+8(%rip),%r14
+++	movq	%rax,%r8
+++	movq	%rbp,%rax
+++	movq	%rdx,%r9
+++
+++	mulq	%r10
+++	movq	.Lpoly+24(%rip),%r15
+++	addq	%rax,%r9
+++	movq	%rbp,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%r11
+++	addq	%rax,%r10
+++	movq	%rbp,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%r12
+++	addq	%rax,%r11
+++	movq	%r8,%rax
+++	adcq	$0,%rdx
+++	xorq	%r13,%r13
+++	movq	%rdx,%r12
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	movq	%r8,%rbp
+++	shlq	$32,%r8
+++	mulq	%r15
+++	shrq	$32,%rbp
+++	addq	%r8,%r9
+++	adcq	%rbp,%r10
+++	adcq	%rax,%r11
+++	movq	8(%rbx),%rax
+++	adcq	%rdx,%r12
+++	adcq	$0,%r13
+++	xorq	%r8,%r8
+++
+++
+++
+++	movq	%rax,%rbp
+++	mulq	0(%rsi)
+++	addq	%rax,%r9
+++	movq	%rbp,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rcx
+++
+++	mulq	8(%rsi)
+++	addq	%rcx,%r10
+++	adcq	$0,%rdx
+++	addq	%rax,%r10
+++	movq	%rbp,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rcx
+++
+++	mulq	16(%rsi)
+++	addq	%rcx,%r11
+++	adcq	$0,%rdx
+++	addq	%rax,%r11
+++	movq	%rbp,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rcx
+++
+++	mulq	24(%rsi)
+++	addq	%rcx,%r12
+++	adcq	$0,%rdx
+++	addq	%rax,%r12
+++	movq	%r9,%rax
+++	adcq	%rdx,%r13
+++	adcq	$0,%r8
+++
+++
+++
+++	movq	%r9,%rbp
+++	shlq	$32,%r9
+++	mulq	%r15
+++	shrq	$32,%rbp
+++	addq	%r9,%r10
+++	adcq	%rbp,%r11
+++	adcq	%rax,%r12
+++	movq	16(%rbx),%rax
+++	adcq	%rdx,%r13
+++	adcq	$0,%r8
+++	xorq	%r9,%r9
+++
+++
+++
+++	movq	%rax,%rbp
+++	mulq	0(%rsi)
+++	addq	%rax,%r10
+++	movq	%rbp,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rcx
+++
+++	mulq	8(%rsi)
+++	addq	%rcx,%r11
+++	adcq	$0,%rdx
+++	addq	%rax,%r11
+++	movq	%rbp,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rcx
+++
+++	mulq	16(%rsi)
+++	addq	%rcx,%r12
+++	adcq	$0,%rdx
+++	addq	%rax,%r12
+++	movq	%rbp,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rcx
+++
+++	mulq	24(%rsi)
+++	addq	%rcx,%r13
+++	adcq	$0,%rdx
+++	addq	%rax,%r13
+++	movq	%r10,%rax
+++	adcq	%rdx,%r8
+++	adcq	$0,%r9
+++
+++
+++
+++	movq	%r10,%rbp
+++	shlq	$32,%r10
+++	mulq	%r15
+++	shrq	$32,%rbp
+++	addq	%r10,%r11
+++	adcq	%rbp,%r12
+++	adcq	%rax,%r13
+++	movq	24(%rbx),%rax
+++	adcq	%rdx,%r8
+++	adcq	$0,%r9
+++	xorq	%r10,%r10
+++
+++
+++
+++	movq	%rax,%rbp
+++	mulq	0(%rsi)
+++	addq	%rax,%r11
+++	movq	%rbp,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rcx
+++
+++	mulq	8(%rsi)
+++	addq	%rcx,%r12
+++	adcq	$0,%rdx
+++	addq	%rax,%r12
+++	movq	%rbp,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rcx
+++
+++	mulq	16(%rsi)
+++	addq	%rcx,%r13
+++	adcq	$0,%rdx
+++	addq	%rax,%r13
+++	movq	%rbp,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rcx
+++
+++	mulq	24(%rsi)
+++	addq	%rcx,%r8
+++	adcq	$0,%rdx
+++	addq	%rax,%r8
+++	movq	%r11,%rax
+++	adcq	%rdx,%r9
+++	adcq	$0,%r10
+++
+++
+++
+++	movq	%r11,%rbp
+++	shlq	$32,%r11
+++	mulq	%r15
+++	shrq	$32,%rbp
+++	addq	%r11,%r12
+++	adcq	%rbp,%r13
+++	movq	%r12,%rcx
+++	adcq	%rax,%r8
+++	adcq	%rdx,%r9
+++	movq	%r13,%rbp
+++	adcq	$0,%r10
+++
+++
+++
+++	subq	$-1,%r12
+++	movq	%r8,%rbx
+++	sbbq	%r14,%r13
+++	sbbq	$0,%r8
+++	movq	%r9,%rdx
+++	sbbq	%r15,%r9
+++	sbbq	$0,%r10
+++
+++	cmovcq	%rcx,%r12
+++	cmovcq	%rbp,%r13
+++	movq	%r12,0(%rdi)
+++	cmovcq	%rbx,%r8
+++	movq	%r13,8(%rdi)
+++	cmovcq	%rdx,%r9
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__ecp_nistz256_mul_montq,.-__ecp_nistz256_mul_montq
+++
+++
+++
+++
+++
+++
+++
+++
+++.globl	ecp_nistz256_sqr_mont
+++.hidden ecp_nistz256_sqr_mont
+++.type	ecp_nistz256_sqr_mont,@function
+++.align	32
+++ecp_nistz256_sqr_mont:
+++.cfi_startproc	
+++	leaq	OPENSSL_ia32cap_P(%rip),%rcx
+++	movq	8(%rcx),%rcx
+++	andl	$0x80100,%ecx
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++.Lsqr_body:
+++	cmpl	$0x80100,%ecx
+++	je	.Lsqr_montx
+++	movq	0(%rsi),%rax
+++	movq	8(%rsi),%r14
+++	movq	16(%rsi),%r15
+++	movq	24(%rsi),%r8
+++
+++	call	__ecp_nistz256_sqr_montq
+++	jmp	.Lsqr_mont_done
+++
+++.align	32
+++.Lsqr_montx:
+++	movq	0(%rsi),%rdx
+++	movq	8(%rsi),%r14
+++	movq	16(%rsi),%r15
+++	movq	24(%rsi),%r8
+++	leaq	-128(%rsi),%rsi
+++
+++	call	__ecp_nistz256_sqr_montx
+++.Lsqr_mont_done:
+++	movq	0(%rsp),%r15
+++.cfi_restore	%r15
+++	movq	8(%rsp),%r14
+++.cfi_restore	%r14
+++	movq	16(%rsp),%r13
+++.cfi_restore	%r13
+++	movq	24(%rsp),%r12
+++.cfi_restore	%r12
+++	movq	32(%rsp),%rbx
+++.cfi_restore	%rbx
+++	movq	40(%rsp),%rbp
+++.cfi_restore	%rbp
+++	leaq	48(%rsp),%rsp
+++.cfi_adjust_cfa_offset	-48
+++.Lsqr_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ecp_nistz256_sqr_mont,.-ecp_nistz256_sqr_mont
+++
+++.type	__ecp_nistz256_sqr_montq,@function
+++.align	32
+++__ecp_nistz256_sqr_montq:
+++.cfi_startproc	
+++	movq	%rax,%r13
+++	mulq	%r14
+++	movq	%rax,%r9
+++	movq	%r15,%rax
+++	movq	%rdx,%r10
+++
+++	mulq	%r13
+++	addq	%rax,%r10
+++	movq	%r8,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%r13
+++	addq	%rax,%r11
+++	movq	%r15,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r12
+++
+++
+++	mulq	%r14
+++	addq	%rax,%r11
+++	movq	%r8,%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rbp
+++
+++	mulq	%r14
+++	addq	%rax,%r12
+++	movq	%r8,%rax
+++	adcq	$0,%rdx
+++	addq	%rbp,%r12
+++	movq	%rdx,%r13
+++	adcq	$0,%r13
+++
+++
+++	mulq	%r15
+++	xorq	%r15,%r15
+++	addq	%rax,%r13
+++	movq	0(%rsi),%rax
+++	movq	%rdx,%r14
+++	adcq	$0,%r14
+++
+++	addq	%r9,%r9
+++	adcq	%r10,%r10
+++	adcq	%r11,%r11
+++	adcq	%r12,%r12
+++	adcq	%r13,%r13
+++	adcq	%r14,%r14
+++	adcq	$0,%r15
+++
+++	mulq	%rax
+++	movq	%rax,%r8
+++	movq	8(%rsi),%rax
+++	movq	%rdx,%rcx
+++
+++	mulq	%rax
+++	addq	%rcx,%r9
+++	adcq	%rax,%r10
+++	movq	16(%rsi),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rcx
+++
+++	mulq	%rax
+++	addq	%rcx,%r11
+++	adcq	%rax,%r12
+++	movq	24(%rsi),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rcx
+++
+++	mulq	%rax
+++	addq	%rcx,%r13
+++	adcq	%rax,%r14
+++	movq	%r8,%rax
+++	adcq	%rdx,%r15
+++
+++	movq	.Lpoly+8(%rip),%rsi
+++	movq	.Lpoly+24(%rip),%rbp
+++
+++
+++
+++
+++	movq	%r8,%rcx
+++	shlq	$32,%r8
+++	mulq	%rbp
+++	shrq	$32,%rcx
+++	addq	%r8,%r9
+++	adcq	%rcx,%r10
+++	adcq	%rax,%r11
+++	movq	%r9,%rax
+++	adcq	$0,%rdx
+++
+++
+++
+++	movq	%r9,%rcx
+++	shlq	$32,%r9
+++	movq	%rdx,%r8
+++	mulq	%rbp
+++	shrq	$32,%rcx
+++	addq	%r9,%r10
+++	adcq	%rcx,%r11
+++	adcq	%rax,%r8
+++	movq	%r10,%rax
+++	adcq	$0,%rdx
+++
+++
+++
+++	movq	%r10,%rcx
+++	shlq	$32,%r10
+++	movq	%rdx,%r9
+++	mulq	%rbp
+++	shrq	$32,%rcx
+++	addq	%r10,%r11
+++	adcq	%rcx,%r8
+++	adcq	%rax,%r9
+++	movq	%r11,%rax
+++	adcq	$0,%rdx
+++
+++
+++
+++	movq	%r11,%rcx
+++	shlq	$32,%r11
+++	movq	%rdx,%r10
+++	mulq	%rbp
+++	shrq	$32,%rcx
+++	addq	%r11,%r8
+++	adcq	%rcx,%r9
+++	adcq	%rax,%r10
+++	adcq	$0,%rdx
+++	xorq	%r11,%r11
+++
+++
+++
+++	addq	%r8,%r12
+++	adcq	%r9,%r13
+++	movq	%r12,%r8
+++	adcq	%r10,%r14
+++	adcq	%rdx,%r15
+++	movq	%r13,%r9
+++	adcq	$0,%r11
+++
+++	subq	$-1,%r12
+++	movq	%r14,%r10
+++	sbbq	%rsi,%r13
+++	sbbq	$0,%r14
+++	movq	%r15,%rcx
+++	sbbq	%rbp,%r15
+++	sbbq	$0,%r11
+++
+++	cmovcq	%r8,%r12
+++	cmovcq	%r9,%r13
+++	movq	%r12,0(%rdi)
+++	cmovcq	%r10,%r14
+++	movq	%r13,8(%rdi)
+++	cmovcq	%rcx,%r15
+++	movq	%r14,16(%rdi)
+++	movq	%r15,24(%rdi)
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__ecp_nistz256_sqr_montq,.-__ecp_nistz256_sqr_montq
+++.type	__ecp_nistz256_mul_montx,@function
+++.align	32
+++__ecp_nistz256_mul_montx:
+++.cfi_startproc	
+++
+++
+++	mulxq	%r9,%r8,%r9
+++	mulxq	%r10,%rcx,%r10
+++	movq	$32,%r14
+++	xorq	%r13,%r13
+++	mulxq	%r11,%rbp,%r11
+++	movq	.Lpoly+24(%rip),%r15
+++	adcq	%rcx,%r9
+++	mulxq	%r12,%rcx,%r12
+++	movq	%r8,%rdx
+++	adcq	%rbp,%r10
+++	shlxq	%r14,%r8,%rbp
+++	adcq	%rcx,%r11
+++	shrxq	%r14,%r8,%rcx
+++	adcq	$0,%r12
+++
+++
+++
+++	addq	%rbp,%r9
+++	adcq	%rcx,%r10
+++
+++	mulxq	%r15,%rcx,%rbp
+++	movq	8(%rbx),%rdx
+++	adcq	%rcx,%r11
+++	adcq	%rbp,%r12
+++	adcq	$0,%r13
+++	xorq	%r8,%r8
+++
+++
+++
+++	mulxq	0+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r9
+++	adoxq	%rbp,%r10
+++
+++	mulxq	8+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r10
+++	adoxq	%rbp,%r11
+++
+++	mulxq	16+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r12
+++
+++	mulxq	24+128(%rsi),%rcx,%rbp
+++	movq	%r9,%rdx
+++	adcxq	%rcx,%r12
+++	shlxq	%r14,%r9,%rcx
+++	adoxq	%rbp,%r13
+++	shrxq	%r14,%r9,%rbp
+++
+++	adcxq	%r8,%r13
+++	adoxq	%r8,%r8
+++	adcq	$0,%r8
+++
+++
+++
+++	addq	%rcx,%r10
+++	adcq	%rbp,%r11
+++
+++	mulxq	%r15,%rcx,%rbp
+++	movq	16(%rbx),%rdx
+++	adcq	%rcx,%r12
+++	adcq	%rbp,%r13
+++	adcq	$0,%r8
+++	xorq	%r9,%r9
+++
+++
+++
+++	mulxq	0+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r10
+++	adoxq	%rbp,%r11
+++
+++	mulxq	8+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r12
+++
+++	mulxq	16+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r12
+++	adoxq	%rbp,%r13
+++
+++	mulxq	24+128(%rsi),%rcx,%rbp
+++	movq	%r10,%rdx
+++	adcxq	%rcx,%r13
+++	shlxq	%r14,%r10,%rcx
+++	adoxq	%rbp,%r8
+++	shrxq	%r14,%r10,%rbp
+++
+++	adcxq	%r9,%r8
+++	adoxq	%r9,%r9
+++	adcq	$0,%r9
+++
+++
+++
+++	addq	%rcx,%r11
+++	adcq	%rbp,%r12
+++
+++	mulxq	%r15,%rcx,%rbp
+++	movq	24(%rbx),%rdx
+++	adcq	%rcx,%r13
+++	adcq	%rbp,%r8
+++	adcq	$0,%r9
+++	xorq	%r10,%r10
+++
+++
+++
+++	mulxq	0+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r12
+++
+++	mulxq	8+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r12
+++	adoxq	%rbp,%r13
+++
+++	mulxq	16+128(%rsi),%rcx,%rbp
+++	adcxq	%rcx,%r13
+++	adoxq	%rbp,%r8
+++
+++	mulxq	24+128(%rsi),%rcx,%rbp
+++	movq	%r11,%rdx
+++	adcxq	%rcx,%r8
+++	shlxq	%r14,%r11,%rcx
+++	adoxq	%rbp,%r9
+++	shrxq	%r14,%r11,%rbp
+++
+++	adcxq	%r10,%r9
+++	adoxq	%r10,%r10
+++	adcq	$0,%r10
+++
+++
+++
+++	addq	%rcx,%r12
+++	adcq	%rbp,%r13
+++
+++	mulxq	%r15,%rcx,%rbp
+++	movq	%r12,%rbx
+++	movq	.Lpoly+8(%rip),%r14
+++	adcq	%rcx,%r8
+++	movq	%r13,%rdx
+++	adcq	%rbp,%r9
+++	adcq	$0,%r10
+++
+++
+++
+++	xorl	%eax,%eax
+++	movq	%r8,%rcx
+++	sbbq	$-1,%r12
+++	sbbq	%r14,%r13
+++	sbbq	$0,%r8
+++	movq	%r9,%rbp
+++	sbbq	%r15,%r9
+++	sbbq	$0,%r10
+++
+++	cmovcq	%rbx,%r12
+++	cmovcq	%rdx,%r13
+++	movq	%r12,0(%rdi)
+++	cmovcq	%rcx,%r8
+++	movq	%r13,8(%rdi)
+++	cmovcq	%rbp,%r9
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__ecp_nistz256_mul_montx,.-__ecp_nistz256_mul_montx
+++
+++.type	__ecp_nistz256_sqr_montx,@function
+++.align	32
+++__ecp_nistz256_sqr_montx:
+++.cfi_startproc	
+++	mulxq	%r14,%r9,%r10
+++	mulxq	%r15,%rcx,%r11
+++	xorl	%eax,%eax
+++	adcq	%rcx,%r10
+++	mulxq	%r8,%rbp,%r12
+++	movq	%r14,%rdx
+++	adcq	%rbp,%r11
+++	adcq	$0,%r12
+++	xorq	%r13,%r13
+++
+++
+++	mulxq	%r15,%rcx,%rbp
+++	adcxq	%rcx,%r11
+++	adoxq	%rbp,%r12
+++
+++	mulxq	%r8,%rcx,%rbp
+++	movq	%r15,%rdx
+++	adcxq	%rcx,%r12
+++	adoxq	%rbp,%r13
+++	adcq	$0,%r13
+++
+++
+++	mulxq	%r8,%rcx,%r14
+++	movq	0+128(%rsi),%rdx
+++	xorq	%r15,%r15
+++	adcxq	%r9,%r9
+++	adoxq	%rcx,%r13
+++	adcxq	%r10,%r10
+++	adoxq	%r15,%r14
+++
+++	mulxq	%rdx,%r8,%rbp
+++	movq	8+128(%rsi),%rdx
+++	adcxq	%r11,%r11
+++	adoxq	%rbp,%r9
+++	adcxq	%r12,%r12
+++	mulxq	%rdx,%rcx,%rax
+++	movq	16+128(%rsi),%rdx
+++	adcxq	%r13,%r13
+++	adoxq	%rcx,%r10
+++	adcxq	%r14,%r14
+++.byte	0x67
+++	mulxq	%rdx,%rcx,%rbp
+++	movq	24+128(%rsi),%rdx
+++	adoxq	%rax,%r11
+++	adcxq	%r15,%r15
+++	adoxq	%rcx,%r12
+++	movq	$32,%rsi
+++	adoxq	%rbp,%r13
+++.byte	0x67,0x67
+++	mulxq	%rdx,%rcx,%rax
+++	movq	.Lpoly+24(%rip),%rdx
+++	adoxq	%rcx,%r14
+++	shlxq	%rsi,%r8,%rcx
+++	adoxq	%rax,%r15
+++	shrxq	%rsi,%r8,%rax
+++	movq	%rdx,%rbp
+++
+++
+++	addq	%rcx,%r9
+++	adcq	%rax,%r10
+++
+++	mulxq	%r8,%rcx,%r8
+++	adcq	%rcx,%r11
+++	shlxq	%rsi,%r9,%rcx
+++	adcq	$0,%r8
+++	shrxq	%rsi,%r9,%rax
+++
+++
+++	addq	%rcx,%r10
+++	adcq	%rax,%r11
+++
+++	mulxq	%r9,%rcx,%r9
+++	adcq	%rcx,%r8
+++	shlxq	%rsi,%r10,%rcx
+++	adcq	$0,%r9
+++	shrxq	%rsi,%r10,%rax
+++
+++
+++	addq	%rcx,%r11
+++	adcq	%rax,%r8
+++
+++	mulxq	%r10,%rcx,%r10
+++	adcq	%rcx,%r9
+++	shlxq	%rsi,%r11,%rcx
+++	adcq	$0,%r10
+++	shrxq	%rsi,%r11,%rax
+++
+++
+++	addq	%rcx,%r8
+++	adcq	%rax,%r9
+++
+++	mulxq	%r11,%rcx,%r11
+++	adcq	%rcx,%r10
+++	adcq	$0,%r11
+++
+++	xorq	%rdx,%rdx
+++	addq	%r8,%r12
+++	movq	.Lpoly+8(%rip),%rsi
+++	adcq	%r9,%r13
+++	movq	%r12,%r8
+++	adcq	%r10,%r14
+++	adcq	%r11,%r15
+++	movq	%r13,%r9
+++	adcq	$0,%rdx
+++
+++	subq	$-1,%r12
+++	movq	%r14,%r10
+++	sbbq	%rsi,%r13
+++	sbbq	$0,%r14
+++	movq	%r15,%r11
+++	sbbq	%rbp,%r15
+++	sbbq	$0,%rdx
+++
+++	cmovcq	%r8,%r12
+++	cmovcq	%r9,%r13
+++	movq	%r12,0(%rdi)
+++	cmovcq	%r10,%r14
+++	movq	%r13,8(%rdi)
+++	cmovcq	%r11,%r15
+++	movq	%r14,16(%rdi)
+++	movq	%r15,24(%rdi)
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__ecp_nistz256_sqr_montx,.-__ecp_nistz256_sqr_montx
+++
+++
+++.globl	ecp_nistz256_select_w5
+++.hidden ecp_nistz256_select_w5
+++.type	ecp_nistz256_select_w5,@function
+++.align	32
+++ecp_nistz256_select_w5:
+++.cfi_startproc	
+++	leaq	OPENSSL_ia32cap_P(%rip),%rax
+++	movq	8(%rax),%rax
+++	testl	$32,%eax
+++	jnz	.Lavx2_select_w5
+++	movdqa	.LOne(%rip),%xmm0
+++	movd	%edx,%xmm1
+++
+++	pxor	%xmm2,%xmm2
+++	pxor	%xmm3,%xmm3
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm5,%xmm5
+++	pxor	%xmm6,%xmm6
+++	pxor	%xmm7,%xmm7
+++
+++	movdqa	%xmm0,%xmm8
+++	pshufd	$0,%xmm1,%xmm1
+++
+++	movq	$16,%rax
+++.Lselect_loop_sse_w5:
+++
+++	movdqa	%xmm8,%xmm15
+++	paddd	%xmm0,%xmm8
+++	pcmpeqd	%xmm1,%xmm15
+++
+++	movdqa	0(%rsi),%xmm9
+++	movdqa	16(%rsi),%xmm10
+++	movdqa	32(%rsi),%xmm11
+++	movdqa	48(%rsi),%xmm12
+++	movdqa	64(%rsi),%xmm13
+++	movdqa	80(%rsi),%xmm14
+++	leaq	96(%rsi),%rsi
+++
+++	pand	%xmm15,%xmm9
+++	pand	%xmm15,%xmm10
+++	por	%xmm9,%xmm2
+++	pand	%xmm15,%xmm11
+++	por	%xmm10,%xmm3
+++	pand	%xmm15,%xmm12
+++	por	%xmm11,%xmm4
+++	pand	%xmm15,%xmm13
+++	por	%xmm12,%xmm5
+++	pand	%xmm15,%xmm14
+++	por	%xmm13,%xmm6
+++	por	%xmm14,%xmm7
+++
+++	decq	%rax
+++	jnz	.Lselect_loop_sse_w5
+++
+++	movdqu	%xmm2,0(%rdi)
+++	movdqu	%xmm3,16(%rdi)
+++	movdqu	%xmm4,32(%rdi)
+++	movdqu	%xmm5,48(%rdi)
+++	movdqu	%xmm6,64(%rdi)
+++	movdqu	%xmm7,80(%rdi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.LSEH_end_ecp_nistz256_select_w5:
+++.size	ecp_nistz256_select_w5,.-ecp_nistz256_select_w5
+++
+++
+++
+++.globl	ecp_nistz256_select_w7
+++.hidden ecp_nistz256_select_w7
+++.type	ecp_nistz256_select_w7,@function
+++.align	32
+++ecp_nistz256_select_w7:
+++.cfi_startproc	
+++	leaq	OPENSSL_ia32cap_P(%rip),%rax
+++	movq	8(%rax),%rax
+++	testl	$32,%eax
+++	jnz	.Lavx2_select_w7
+++	movdqa	.LOne(%rip),%xmm8
+++	movd	%edx,%xmm1
+++
+++	pxor	%xmm2,%xmm2
+++	pxor	%xmm3,%xmm3
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm5,%xmm5
+++
+++	movdqa	%xmm8,%xmm0
+++	pshufd	$0,%xmm1,%xmm1
+++	movq	$64,%rax
+++
+++.Lselect_loop_sse_w7:
+++	movdqa	%xmm8,%xmm15
+++	paddd	%xmm0,%xmm8
+++	movdqa	0(%rsi),%xmm9
+++	movdqa	16(%rsi),%xmm10
+++	pcmpeqd	%xmm1,%xmm15
+++	movdqa	32(%rsi),%xmm11
+++	movdqa	48(%rsi),%xmm12
+++	leaq	64(%rsi),%rsi
+++
+++	pand	%xmm15,%xmm9
+++	pand	%xmm15,%xmm10
+++	por	%xmm9,%xmm2
+++	pand	%xmm15,%xmm11
+++	por	%xmm10,%xmm3
+++	pand	%xmm15,%xmm12
+++	por	%xmm11,%xmm4
+++	prefetcht0	255(%rsi)
+++	por	%xmm12,%xmm5
+++
+++	decq	%rax
+++	jnz	.Lselect_loop_sse_w7
+++
+++	movdqu	%xmm2,0(%rdi)
+++	movdqu	%xmm3,16(%rdi)
+++	movdqu	%xmm4,32(%rdi)
+++	movdqu	%xmm5,48(%rdi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.LSEH_end_ecp_nistz256_select_w7:
+++.size	ecp_nistz256_select_w7,.-ecp_nistz256_select_w7
+++
+++
+++.type	ecp_nistz256_avx2_select_w5,@function
+++.align	32
+++ecp_nistz256_avx2_select_w5:
+++.cfi_startproc	
+++.Lavx2_select_w5:
+++	vzeroupper
+++	vmovdqa	.LTwo(%rip),%ymm0
+++
+++	vpxor	%ymm2,%ymm2,%ymm2
+++	vpxor	%ymm3,%ymm3,%ymm3
+++	vpxor	%ymm4,%ymm4,%ymm4
+++
+++	vmovdqa	.LOne(%rip),%ymm5
+++	vmovdqa	.LTwo(%rip),%ymm10
+++
+++	vmovd	%edx,%xmm1
+++	vpermd	%ymm1,%ymm2,%ymm1
+++
+++	movq	$8,%rax
+++.Lselect_loop_avx2_w5:
+++
+++	vmovdqa	0(%rsi),%ymm6
+++	vmovdqa	32(%rsi),%ymm7
+++	vmovdqa	64(%rsi),%ymm8
+++
+++	vmovdqa	96(%rsi),%ymm11
+++	vmovdqa	128(%rsi),%ymm12
+++	vmovdqa	160(%rsi),%ymm13
+++
+++	vpcmpeqd	%ymm1,%ymm5,%ymm9
+++	vpcmpeqd	%ymm1,%ymm10,%ymm14
+++
+++	vpaddd	%ymm0,%ymm5,%ymm5
+++	vpaddd	%ymm0,%ymm10,%ymm10
+++	leaq	192(%rsi),%rsi
+++
+++	vpand	%ymm9,%ymm6,%ymm6
+++	vpand	%ymm9,%ymm7,%ymm7
+++	vpand	%ymm9,%ymm8,%ymm8
+++	vpand	%ymm14,%ymm11,%ymm11
+++	vpand	%ymm14,%ymm12,%ymm12
+++	vpand	%ymm14,%ymm13,%ymm13
+++
+++	vpxor	%ymm6,%ymm2,%ymm2
+++	vpxor	%ymm7,%ymm3,%ymm3
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpxor	%ymm11,%ymm2,%ymm2
+++	vpxor	%ymm12,%ymm3,%ymm3
+++	vpxor	%ymm13,%ymm4,%ymm4
+++
+++	decq	%rax
+++	jnz	.Lselect_loop_avx2_w5
+++
+++	vmovdqu	%ymm2,0(%rdi)
+++	vmovdqu	%ymm3,32(%rdi)
+++	vmovdqu	%ymm4,64(%rdi)
+++	vzeroupper
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.LSEH_end_ecp_nistz256_avx2_select_w5:
+++.size	ecp_nistz256_avx2_select_w5,.-ecp_nistz256_avx2_select_w5
+++
+++
+++
+++.globl	ecp_nistz256_avx2_select_w7
+++.hidden ecp_nistz256_avx2_select_w7
+++.type	ecp_nistz256_avx2_select_w7,@function
+++.align	32
+++ecp_nistz256_avx2_select_w7:
+++.cfi_startproc	
+++.Lavx2_select_w7:
+++	vzeroupper
+++	vmovdqa	.LThree(%rip),%ymm0
+++
+++	vpxor	%ymm2,%ymm2,%ymm2
+++	vpxor	%ymm3,%ymm3,%ymm3
+++
+++	vmovdqa	.LOne(%rip),%ymm4
+++	vmovdqa	.LTwo(%rip),%ymm8
+++	vmovdqa	.LThree(%rip),%ymm12
+++
+++	vmovd	%edx,%xmm1
+++	vpermd	%ymm1,%ymm2,%ymm1
+++
+++
+++	movq	$21,%rax
+++.Lselect_loop_avx2_w7:
+++
+++	vmovdqa	0(%rsi),%ymm5
+++	vmovdqa	32(%rsi),%ymm6
+++
+++	vmovdqa	64(%rsi),%ymm9
+++	vmovdqa	96(%rsi),%ymm10
+++
+++	vmovdqa	128(%rsi),%ymm13
+++	vmovdqa	160(%rsi),%ymm14
+++
+++	vpcmpeqd	%ymm1,%ymm4,%ymm7
+++	vpcmpeqd	%ymm1,%ymm8,%ymm11
+++	vpcmpeqd	%ymm1,%ymm12,%ymm15
+++
+++	vpaddd	%ymm0,%ymm4,%ymm4
+++	vpaddd	%ymm0,%ymm8,%ymm8
+++	vpaddd	%ymm0,%ymm12,%ymm12
+++	leaq	192(%rsi),%rsi
+++
+++	vpand	%ymm7,%ymm5,%ymm5
+++	vpand	%ymm7,%ymm6,%ymm6
+++	vpand	%ymm11,%ymm9,%ymm9
+++	vpand	%ymm11,%ymm10,%ymm10
+++	vpand	%ymm15,%ymm13,%ymm13
+++	vpand	%ymm15,%ymm14,%ymm14
+++
+++	vpxor	%ymm5,%ymm2,%ymm2
+++	vpxor	%ymm6,%ymm3,%ymm3
+++	vpxor	%ymm9,%ymm2,%ymm2
+++	vpxor	%ymm10,%ymm3,%ymm3
+++	vpxor	%ymm13,%ymm2,%ymm2
+++	vpxor	%ymm14,%ymm3,%ymm3
+++
+++	decq	%rax
+++	jnz	.Lselect_loop_avx2_w7
+++
+++
+++	vmovdqa	0(%rsi),%ymm5
+++	vmovdqa	32(%rsi),%ymm6
+++
+++	vpcmpeqd	%ymm1,%ymm4,%ymm7
+++
+++	vpand	%ymm7,%ymm5,%ymm5
+++	vpand	%ymm7,%ymm6,%ymm6
+++
+++	vpxor	%ymm5,%ymm2,%ymm2
+++	vpxor	%ymm6,%ymm3,%ymm3
+++
+++	vmovdqu	%ymm2,0(%rdi)
+++	vmovdqu	%ymm3,32(%rdi)
+++	vzeroupper
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.LSEH_end_ecp_nistz256_avx2_select_w7:
+++.size	ecp_nistz256_avx2_select_w7,.-ecp_nistz256_avx2_select_w7
+++.type	__ecp_nistz256_add_toq,@function
+++.align	32
+++__ecp_nistz256_add_toq:
+++.cfi_startproc	
+++	xorq	%r11,%r11
+++	addq	0(%rbx),%r12
+++	adcq	8(%rbx),%r13
+++	movq	%r12,%rax
+++	adcq	16(%rbx),%r8
+++	adcq	24(%rbx),%r9
+++	movq	%r13,%rbp
+++	adcq	$0,%r11
+++
+++	subq	$-1,%r12
+++	movq	%r8,%rcx
+++	sbbq	%r14,%r13
+++	sbbq	$0,%r8
+++	movq	%r9,%r10
+++	sbbq	%r15,%r9
+++	sbbq	$0,%r11
+++
+++	cmovcq	%rax,%r12
+++	cmovcq	%rbp,%r13
+++	movq	%r12,0(%rdi)
+++	cmovcq	%rcx,%r8
+++	movq	%r13,8(%rdi)
+++	cmovcq	%r10,%r9
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__ecp_nistz256_add_toq,.-__ecp_nistz256_add_toq
+++
+++.type	__ecp_nistz256_sub_fromq,@function
+++.align	32
+++__ecp_nistz256_sub_fromq:
+++.cfi_startproc	
+++	subq	0(%rbx),%r12
+++	sbbq	8(%rbx),%r13
+++	movq	%r12,%rax
+++	sbbq	16(%rbx),%r8
+++	sbbq	24(%rbx),%r9
+++	movq	%r13,%rbp
+++	sbbq	%r11,%r11
+++
+++	addq	$-1,%r12
+++	movq	%r8,%rcx
+++	adcq	%r14,%r13
+++	adcq	$0,%r8
+++	movq	%r9,%r10
+++	adcq	%r15,%r9
+++	testq	%r11,%r11
+++
+++	cmovzq	%rax,%r12
+++	cmovzq	%rbp,%r13
+++	movq	%r12,0(%rdi)
+++	cmovzq	%rcx,%r8
+++	movq	%r13,8(%rdi)
+++	cmovzq	%r10,%r9
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__ecp_nistz256_sub_fromq,.-__ecp_nistz256_sub_fromq
+++
+++.type	__ecp_nistz256_subq,@function
+++.align	32
+++__ecp_nistz256_subq:
+++.cfi_startproc	
+++	subq	%r12,%rax
+++	sbbq	%r13,%rbp
+++	movq	%rax,%r12
+++	sbbq	%r8,%rcx
+++	sbbq	%r9,%r10
+++	movq	%rbp,%r13
+++	sbbq	%r11,%r11
+++
+++	addq	$-1,%rax
+++	movq	%rcx,%r8
+++	adcq	%r14,%rbp
+++	adcq	$0,%rcx
+++	movq	%r10,%r9
+++	adcq	%r15,%r10
+++	testq	%r11,%r11
+++
+++	cmovnzq	%rax,%r12
+++	cmovnzq	%rbp,%r13
+++	cmovnzq	%rcx,%r8
+++	cmovnzq	%r10,%r9
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__ecp_nistz256_subq,.-__ecp_nistz256_subq
+++
+++.type	__ecp_nistz256_mul_by_2q,@function
+++.align	32
+++__ecp_nistz256_mul_by_2q:
+++.cfi_startproc	
+++	xorq	%r11,%r11
+++	addq	%r12,%r12
+++	adcq	%r13,%r13
+++	movq	%r12,%rax
+++	adcq	%r8,%r8
+++	adcq	%r9,%r9
+++	movq	%r13,%rbp
+++	adcq	$0,%r11
+++
+++	subq	$-1,%r12
+++	movq	%r8,%rcx
+++	sbbq	%r14,%r13
+++	sbbq	$0,%r8
+++	movq	%r9,%r10
+++	sbbq	%r15,%r9
+++	sbbq	$0,%r11
+++
+++	cmovcq	%rax,%r12
+++	cmovcq	%rbp,%r13
+++	movq	%r12,0(%rdi)
+++	cmovcq	%rcx,%r8
+++	movq	%r13,8(%rdi)
+++	cmovcq	%r10,%r9
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__ecp_nistz256_mul_by_2q,.-__ecp_nistz256_mul_by_2q
+++.globl	ecp_nistz256_point_double
+++.hidden ecp_nistz256_point_double
+++.type	ecp_nistz256_point_double,@function
+++.align	32
+++ecp_nistz256_point_double:
+++.cfi_startproc	
+++	leaq	OPENSSL_ia32cap_P(%rip),%rcx
+++	movq	8(%rcx),%rcx
+++	andl	$0x80100,%ecx
+++	cmpl	$0x80100,%ecx
+++	je	.Lpoint_doublex
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++	subq	$160+8,%rsp
+++.cfi_adjust_cfa_offset	32*5+8
+++.Lpoint_doubleq_body:
+++
+++.Lpoint_double_shortcutq:
+++	movdqu	0(%rsi),%xmm0
+++	movq	%rsi,%rbx
+++	movdqu	16(%rsi),%xmm1
+++	movq	32+0(%rsi),%r12
+++	movq	32+8(%rsi),%r13
+++	movq	32+16(%rsi),%r8
+++	movq	32+24(%rsi),%r9
+++	movq	.Lpoly+8(%rip),%r14
+++	movq	.Lpoly+24(%rip),%r15
+++	movdqa	%xmm0,96(%rsp)
+++	movdqa	%xmm1,96+16(%rsp)
+++	leaq	32(%rdi),%r10
+++	leaq	64(%rdi),%r11
+++.byte	102,72,15,110,199
+++.byte	102,73,15,110,202
+++.byte	102,73,15,110,211
+++
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_mul_by_2q
+++
+++	movq	64+0(%rsi),%rax
+++	movq	64+8(%rsi),%r14
+++	movq	64+16(%rsi),%r15
+++	movq	64+24(%rsi),%r8
+++	leaq	64-0(%rsi),%rsi
+++	leaq	64(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montq
+++
+++	movq	0+0(%rsp),%rax
+++	movq	8+0(%rsp),%r14
+++	leaq	0+0(%rsp),%rsi
+++	movq	16+0(%rsp),%r15
+++	movq	24+0(%rsp),%r8
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montq
+++
+++	movq	32(%rbx),%rax
+++	movq	64+0(%rbx),%r9
+++	movq	64+8(%rbx),%r10
+++	movq	64+16(%rbx),%r11
+++	movq	64+24(%rbx),%r12
+++	leaq	64-0(%rbx),%rsi
+++	leaq	32(%rbx),%rbx
+++.byte	102,72,15,126,215
+++	call	__ecp_nistz256_mul_montq
+++	call	__ecp_nistz256_mul_by_2q
+++
+++	movq	96+0(%rsp),%r12
+++	movq	96+8(%rsp),%r13
+++	leaq	64(%rsp),%rbx
+++	movq	96+16(%rsp),%r8
+++	movq	96+24(%rsp),%r9
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_add_toq
+++
+++	movq	96+0(%rsp),%r12
+++	movq	96+8(%rsp),%r13
+++	leaq	64(%rsp),%rbx
+++	movq	96+16(%rsp),%r8
+++	movq	96+24(%rsp),%r9
+++	leaq	64(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromq
+++
+++	movq	0+0(%rsp),%rax
+++	movq	8+0(%rsp),%r14
+++	leaq	0+0(%rsp),%rsi
+++	movq	16+0(%rsp),%r15
+++	movq	24+0(%rsp),%r8
+++.byte	102,72,15,126,207
+++	call	__ecp_nistz256_sqr_montq
+++	xorq	%r9,%r9
+++	movq	%r12,%rax
+++	addq	$-1,%r12
+++	movq	%r13,%r10
+++	adcq	%rsi,%r13
+++	movq	%r14,%rcx
+++	adcq	$0,%r14
+++	movq	%r15,%r8
+++	adcq	%rbp,%r15
+++	adcq	$0,%r9
+++	xorq	%rsi,%rsi
+++	testq	$1,%rax
+++
+++	cmovzq	%rax,%r12
+++	cmovzq	%r10,%r13
+++	cmovzq	%rcx,%r14
+++	cmovzq	%r8,%r15
+++	cmovzq	%rsi,%r9
+++
+++	movq	%r13,%rax
+++	shrq	$1,%r12
+++	shlq	$63,%rax
+++	movq	%r14,%r10
+++	shrq	$1,%r13
+++	orq	%rax,%r12
+++	shlq	$63,%r10
+++	movq	%r15,%rcx
+++	shrq	$1,%r14
+++	orq	%r10,%r13
+++	shlq	$63,%rcx
+++	movq	%r12,0(%rdi)
+++	shrq	$1,%r15
+++	movq	%r13,8(%rdi)
+++	shlq	$63,%r9
+++	orq	%rcx,%r14
+++	orq	%r9,%r15
+++	movq	%r14,16(%rdi)
+++	movq	%r15,24(%rdi)
+++	movq	64(%rsp),%rax
+++	leaq	64(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	0+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	leaq	128(%rsp),%rdi
+++	call	__ecp_nistz256_mul_by_2q
+++
+++	leaq	32(%rsp),%rbx
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_add_toq
+++
+++	movq	96(%rsp),%rax
+++	leaq	96(%rsp),%rbx
+++	movq	0+0(%rsp),%r9
+++	movq	8+0(%rsp),%r10
+++	leaq	0+0(%rsp),%rsi
+++	movq	16+0(%rsp),%r11
+++	movq	24+0(%rsp),%r12
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	leaq	128(%rsp),%rdi
+++	call	__ecp_nistz256_mul_by_2q
+++
+++	movq	0+32(%rsp),%rax
+++	movq	8+32(%rsp),%r14
+++	leaq	0+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r15
+++	movq	24+32(%rsp),%r8
+++.byte	102,72,15,126,199
+++	call	__ecp_nistz256_sqr_montq
+++
+++	leaq	128(%rsp),%rbx
+++	movq	%r14,%r8
+++	movq	%r15,%r9
+++	movq	%rsi,%r14
+++	movq	%rbp,%r15
+++	call	__ecp_nistz256_sub_fromq
+++
+++	movq	0+0(%rsp),%rax
+++	movq	0+8(%rsp),%rbp
+++	movq	0+16(%rsp),%rcx
+++	movq	0+24(%rsp),%r10
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_subq
+++
+++	movq	32(%rsp),%rax
+++	leaq	32(%rsp),%rbx
+++	movq	%r12,%r14
+++	xorl	%ecx,%ecx
+++	movq	%r12,0+0(%rsp)
+++	movq	%r13,%r10
+++	movq	%r13,0+8(%rsp)
+++	cmovzq	%r8,%r11
+++	movq	%r8,0+16(%rsp)
+++	leaq	0-0(%rsp),%rsi
+++	cmovzq	%r9,%r12
+++	movq	%r9,0+24(%rsp)
+++	movq	%r14,%r9
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++.byte	102,72,15,126,203
+++.byte	102,72,15,126,207
+++	call	__ecp_nistz256_sub_fromq
+++
+++	leaq	160+56(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbx
+++.cfi_restore	%rbx
+++	movq	-8(%rsi),%rbp
+++.cfi_restore	%rbp
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lpoint_doubleq_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ecp_nistz256_point_double,.-ecp_nistz256_point_double
+++.globl	ecp_nistz256_point_add
+++.hidden ecp_nistz256_point_add
+++.type	ecp_nistz256_point_add,@function
+++.align	32
+++ecp_nistz256_point_add:
+++.cfi_startproc	
+++	leaq	OPENSSL_ia32cap_P(%rip),%rcx
+++	movq	8(%rcx),%rcx
+++	andl	$0x80100,%ecx
+++	cmpl	$0x80100,%ecx
+++	je	.Lpoint_addx
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++	subq	$576+8,%rsp
+++.cfi_adjust_cfa_offset	32*18+8
+++.Lpoint_addq_body:
+++
+++	movdqu	0(%rsi),%xmm0
+++	movdqu	16(%rsi),%xmm1
+++	movdqu	32(%rsi),%xmm2
+++	movdqu	48(%rsi),%xmm3
+++	movdqu	64(%rsi),%xmm4
+++	movdqu	80(%rsi),%xmm5
+++	movq	%rsi,%rbx
+++	movq	%rdx,%rsi
+++	movdqa	%xmm0,384(%rsp)
+++	movdqa	%xmm1,384+16(%rsp)
+++	movdqa	%xmm2,416(%rsp)
+++	movdqa	%xmm3,416+16(%rsp)
+++	movdqa	%xmm4,448(%rsp)
+++	movdqa	%xmm5,448+16(%rsp)
+++	por	%xmm4,%xmm5
+++
+++	movdqu	0(%rsi),%xmm0
+++	pshufd	$0xb1,%xmm5,%xmm3
+++	movdqu	16(%rsi),%xmm1
+++	movdqu	32(%rsi),%xmm2
+++	por	%xmm3,%xmm5
+++	movdqu	48(%rsi),%xmm3
+++	movq	64+0(%rsi),%rax
+++	movq	64+8(%rsi),%r14
+++	movq	64+16(%rsi),%r15
+++	movq	64+24(%rsi),%r8
+++	movdqa	%xmm0,480(%rsp)
+++	pshufd	$0x1e,%xmm5,%xmm4
+++	movdqa	%xmm1,480+16(%rsp)
+++	movdqu	64(%rsi),%xmm0
+++	movdqu	80(%rsi),%xmm1
+++	movdqa	%xmm2,512(%rsp)
+++	movdqa	%xmm3,512+16(%rsp)
+++	por	%xmm4,%xmm5
+++	pxor	%xmm4,%xmm4
+++	por	%xmm0,%xmm1
+++.byte	102,72,15,110,199
+++
+++	leaq	64-0(%rsi),%rsi
+++	movq	%rax,544+0(%rsp)
+++	movq	%r14,544+8(%rsp)
+++	movq	%r15,544+16(%rsp)
+++	movq	%r8,544+24(%rsp)
+++	leaq	96(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montq
+++
+++	pcmpeqd	%xmm4,%xmm5
+++	pshufd	$0xb1,%xmm1,%xmm4
+++	por	%xmm1,%xmm4
+++	pshufd	$0,%xmm5,%xmm5
+++	pshufd	$0x1e,%xmm4,%xmm3
+++	por	%xmm3,%xmm4
+++	pxor	%xmm3,%xmm3
+++	pcmpeqd	%xmm3,%xmm4
+++	pshufd	$0,%xmm4,%xmm4
+++	movq	64+0(%rbx),%rax
+++	movq	64+8(%rbx),%r14
+++	movq	64+16(%rbx),%r15
+++	movq	64+24(%rbx),%r8
+++.byte	102,72,15,110,203
+++
+++	leaq	64-0(%rbx),%rsi
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montq
+++
+++	movq	544(%rsp),%rax
+++	leaq	544(%rsp),%rbx
+++	movq	0+96(%rsp),%r9
+++	movq	8+96(%rsp),%r10
+++	leaq	0+96(%rsp),%rsi
+++	movq	16+96(%rsp),%r11
+++	movq	24+96(%rsp),%r12
+++	leaq	224(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	movq	448(%rsp),%rax
+++	leaq	448(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	0+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	256(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	movq	416(%rsp),%rax
+++	leaq	416(%rsp),%rbx
+++	movq	0+224(%rsp),%r9
+++	movq	8+224(%rsp),%r10
+++	leaq	0+224(%rsp),%rsi
+++	movq	16+224(%rsp),%r11
+++	movq	24+224(%rsp),%r12
+++	leaq	224(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	movq	512(%rsp),%rax
+++	leaq	512(%rsp),%rbx
+++	movq	0+256(%rsp),%r9
+++	movq	8+256(%rsp),%r10
+++	leaq	0+256(%rsp),%rsi
+++	movq	16+256(%rsp),%r11
+++	movq	24+256(%rsp),%r12
+++	leaq	256(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	leaq	224(%rsp),%rbx
+++	leaq	64(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromq
+++
+++	orq	%r13,%r12
+++	movdqa	%xmm4,%xmm2
+++	orq	%r8,%r12
+++	orq	%r9,%r12
+++	por	%xmm5,%xmm2
+++.byte	102,73,15,110,220
+++
+++	movq	384(%rsp),%rax
+++	leaq	384(%rsp),%rbx
+++	movq	0+96(%rsp),%r9
+++	movq	8+96(%rsp),%r10
+++	leaq	0+96(%rsp),%rsi
+++	movq	16+96(%rsp),%r11
+++	movq	24+96(%rsp),%r12
+++	leaq	160(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	movq	480(%rsp),%rax
+++	leaq	480(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	0+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	192(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	leaq	160(%rsp),%rbx
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromq
+++
+++	orq	%r13,%r12
+++	orq	%r8,%r12
+++	orq	%r9,%r12
+++
+++.byte	102,73,15,126,208
+++.byte	102,73,15,126,217
+++	orq	%r8,%r12
+++.byte	0x3e
+++	jnz	.Ladd_proceedq
+++
+++
+++
+++	testq	%r9,%r9
+++	jz	.Ladd_doubleq
+++
+++
+++
+++
+++
+++
+++.byte	102,72,15,126,199
+++	pxor	%xmm0,%xmm0
+++	movdqu	%xmm0,0(%rdi)
+++	movdqu	%xmm0,16(%rdi)
+++	movdqu	%xmm0,32(%rdi)
+++	movdqu	%xmm0,48(%rdi)
+++	movdqu	%xmm0,64(%rdi)
+++	movdqu	%xmm0,80(%rdi)
+++	jmp	.Ladd_doneq
+++
+++.align	32
+++.Ladd_doubleq:
+++.byte	102,72,15,126,206
+++.byte	102,72,15,126,199
+++	addq	$416,%rsp
+++.cfi_adjust_cfa_offset	-416
+++	jmp	.Lpoint_double_shortcutq
+++.cfi_adjust_cfa_offset	416
+++
+++.align	32
+++.Ladd_proceedq:
+++	movq	0+64(%rsp),%rax
+++	movq	8+64(%rsp),%r14
+++	leaq	0+64(%rsp),%rsi
+++	movq	16+64(%rsp),%r15
+++	movq	24+64(%rsp),%r8
+++	leaq	96(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montq
+++
+++	movq	448(%rsp),%rax
+++	leaq	448(%rsp),%rbx
+++	movq	0+0(%rsp),%r9
+++	movq	8+0(%rsp),%r10
+++	leaq	0+0(%rsp),%rsi
+++	movq	16+0(%rsp),%r11
+++	movq	24+0(%rsp),%r12
+++	leaq	352(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	movq	0+0(%rsp),%rax
+++	movq	8+0(%rsp),%r14
+++	leaq	0+0(%rsp),%rsi
+++	movq	16+0(%rsp),%r15
+++	movq	24+0(%rsp),%r8
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montq
+++
+++	movq	544(%rsp),%rax
+++	leaq	544(%rsp),%rbx
+++	movq	0+352(%rsp),%r9
+++	movq	8+352(%rsp),%r10
+++	leaq	0+352(%rsp),%rsi
+++	movq	16+352(%rsp),%r11
+++	movq	24+352(%rsp),%r12
+++	leaq	352(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	movq	0(%rsp),%rax
+++	leaq	0(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	0+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	128(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	movq	160(%rsp),%rax
+++	leaq	160(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	0+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	192(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++
+++
+++
+++	xorq	%r11,%r11
+++	addq	%r12,%r12
+++	leaq	96(%rsp),%rsi
+++	adcq	%r13,%r13
+++	movq	%r12,%rax
+++	adcq	%r8,%r8
+++	adcq	%r9,%r9
+++	movq	%r13,%rbp
+++	adcq	$0,%r11
+++
+++	subq	$-1,%r12
+++	movq	%r8,%rcx
+++	sbbq	%r14,%r13
+++	sbbq	$0,%r8
+++	movq	%r9,%r10
+++	sbbq	%r15,%r9
+++	sbbq	$0,%r11
+++
+++	cmovcq	%rax,%r12
+++	movq	0(%rsi),%rax
+++	cmovcq	%rbp,%r13
+++	movq	8(%rsi),%rbp
+++	cmovcq	%rcx,%r8
+++	movq	16(%rsi),%rcx
+++	cmovcq	%r10,%r9
+++	movq	24(%rsi),%r10
+++
+++	call	__ecp_nistz256_subq
+++
+++	leaq	128(%rsp),%rbx
+++	leaq	288(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromq
+++
+++	movq	192+0(%rsp),%rax
+++	movq	192+8(%rsp),%rbp
+++	movq	192+16(%rsp),%rcx
+++	movq	192+24(%rsp),%r10
+++	leaq	320(%rsp),%rdi
+++
+++	call	__ecp_nistz256_subq
+++
+++	movq	%r12,0(%rdi)
+++	movq	%r13,8(%rdi)
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++	movq	128(%rsp),%rax
+++	leaq	128(%rsp),%rbx
+++	movq	0+224(%rsp),%r9
+++	movq	8+224(%rsp),%r10
+++	leaq	0+224(%rsp),%rsi
+++	movq	16+224(%rsp),%r11
+++	movq	24+224(%rsp),%r12
+++	leaq	256(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	movq	320(%rsp),%rax
+++	leaq	320(%rsp),%rbx
+++	movq	0+64(%rsp),%r9
+++	movq	8+64(%rsp),%r10
+++	leaq	0+64(%rsp),%rsi
+++	movq	16+64(%rsp),%r11
+++	movq	24+64(%rsp),%r12
+++	leaq	320(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	leaq	256(%rsp),%rbx
+++	leaq	320(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromq
+++
+++.byte	102,72,15,126,199
+++
+++	movdqa	%xmm5,%xmm0
+++	movdqa	%xmm5,%xmm1
+++	pandn	352(%rsp),%xmm0
+++	movdqa	%xmm5,%xmm2
+++	pandn	352+16(%rsp),%xmm1
+++	movdqa	%xmm5,%xmm3
+++	pand	544(%rsp),%xmm2
+++	pand	544+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm4,%xmm1
+++	pandn	%xmm2,%xmm0
+++	movdqa	%xmm4,%xmm2
+++	pandn	%xmm3,%xmm1
+++	movdqa	%xmm4,%xmm3
+++	pand	448(%rsp),%xmm2
+++	pand	448+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++	movdqu	%xmm2,64(%rdi)
+++	movdqu	%xmm3,80(%rdi)
+++
+++	movdqa	%xmm5,%xmm0
+++	movdqa	%xmm5,%xmm1
+++	pandn	288(%rsp),%xmm0
+++	movdqa	%xmm5,%xmm2
+++	pandn	288+16(%rsp),%xmm1
+++	movdqa	%xmm5,%xmm3
+++	pand	480(%rsp),%xmm2
+++	pand	480+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm4,%xmm1
+++	pandn	%xmm2,%xmm0
+++	movdqa	%xmm4,%xmm2
+++	pandn	%xmm3,%xmm1
+++	movdqa	%xmm4,%xmm3
+++	pand	384(%rsp),%xmm2
+++	pand	384+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++	movdqu	%xmm2,0(%rdi)
+++	movdqu	%xmm3,16(%rdi)
+++
+++	movdqa	%xmm5,%xmm0
+++	movdqa	%xmm5,%xmm1
+++	pandn	320(%rsp),%xmm0
+++	movdqa	%xmm5,%xmm2
+++	pandn	320+16(%rsp),%xmm1
+++	movdqa	%xmm5,%xmm3
+++	pand	512(%rsp),%xmm2
+++	pand	512+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm4,%xmm1
+++	pandn	%xmm2,%xmm0
+++	movdqa	%xmm4,%xmm2
+++	pandn	%xmm3,%xmm1
+++	movdqa	%xmm4,%xmm3
+++	pand	416(%rsp),%xmm2
+++	pand	416+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++	movdqu	%xmm2,32(%rdi)
+++	movdqu	%xmm3,48(%rdi)
+++
+++.Ladd_doneq:
+++	leaq	576+56(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbx
+++.cfi_restore	%rbx
+++	movq	-8(%rsi),%rbp
+++.cfi_restore	%rbp
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lpoint_addq_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ecp_nistz256_point_add,.-ecp_nistz256_point_add
+++.globl	ecp_nistz256_point_add_affine
+++.hidden ecp_nistz256_point_add_affine
+++.type	ecp_nistz256_point_add_affine,@function
+++.align	32
+++ecp_nistz256_point_add_affine:
+++.cfi_startproc	
+++	leaq	OPENSSL_ia32cap_P(%rip),%rcx
+++	movq	8(%rcx),%rcx
+++	andl	$0x80100,%ecx
+++	cmpl	$0x80100,%ecx
+++	je	.Lpoint_add_affinex
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++	subq	$480+8,%rsp
+++.cfi_adjust_cfa_offset	32*15+8
+++.Ladd_affineq_body:
+++
+++	movdqu	0(%rsi),%xmm0
+++	movq	%rdx,%rbx
+++	movdqu	16(%rsi),%xmm1
+++	movdqu	32(%rsi),%xmm2
+++	movdqu	48(%rsi),%xmm3
+++	movdqu	64(%rsi),%xmm4
+++	movdqu	80(%rsi),%xmm5
+++	movq	64+0(%rsi),%rax
+++	movq	64+8(%rsi),%r14
+++	movq	64+16(%rsi),%r15
+++	movq	64+24(%rsi),%r8
+++	movdqa	%xmm0,320(%rsp)
+++	movdqa	%xmm1,320+16(%rsp)
+++	movdqa	%xmm2,352(%rsp)
+++	movdqa	%xmm3,352+16(%rsp)
+++	movdqa	%xmm4,384(%rsp)
+++	movdqa	%xmm5,384+16(%rsp)
+++	por	%xmm4,%xmm5
+++
+++	movdqu	0(%rbx),%xmm0
+++	pshufd	$0xb1,%xmm5,%xmm3
+++	movdqu	16(%rbx),%xmm1
+++	movdqu	32(%rbx),%xmm2
+++	por	%xmm3,%xmm5
+++	movdqu	48(%rbx),%xmm3
+++	movdqa	%xmm0,416(%rsp)
+++	pshufd	$0x1e,%xmm5,%xmm4
+++	movdqa	%xmm1,416+16(%rsp)
+++	por	%xmm0,%xmm1
+++.byte	102,72,15,110,199
+++	movdqa	%xmm2,448(%rsp)
+++	movdqa	%xmm3,448+16(%rsp)
+++	por	%xmm2,%xmm3
+++	por	%xmm4,%xmm5
+++	pxor	%xmm4,%xmm4
+++	por	%xmm1,%xmm3
+++
+++	leaq	64-0(%rsi),%rsi
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montq
+++
+++	pcmpeqd	%xmm4,%xmm5
+++	pshufd	$0xb1,%xmm3,%xmm4
+++	movq	0(%rbx),%rax
+++
+++	movq	%r12,%r9
+++	por	%xmm3,%xmm4
+++	pshufd	$0,%xmm5,%xmm5
+++	pshufd	$0x1e,%xmm4,%xmm3
+++	movq	%r13,%r10
+++	por	%xmm3,%xmm4
+++	pxor	%xmm3,%xmm3
+++	movq	%r14,%r11
+++	pcmpeqd	%xmm3,%xmm4
+++	pshufd	$0,%xmm4,%xmm4
+++
+++	leaq	32-0(%rsp),%rsi
+++	movq	%r15,%r12
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	leaq	320(%rsp),%rbx
+++	leaq	64(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromq
+++
+++	movq	384(%rsp),%rax
+++	leaq	384(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	0+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	movq	384(%rsp),%rax
+++	leaq	384(%rsp),%rbx
+++	movq	0+64(%rsp),%r9
+++	movq	8+64(%rsp),%r10
+++	leaq	0+64(%rsp),%rsi
+++	movq	16+64(%rsp),%r11
+++	movq	24+64(%rsp),%r12
+++	leaq	288(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	movq	448(%rsp),%rax
+++	leaq	448(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	0+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	leaq	352(%rsp),%rbx
+++	leaq	96(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromq
+++
+++	movq	0+64(%rsp),%rax
+++	movq	8+64(%rsp),%r14
+++	leaq	0+64(%rsp),%rsi
+++	movq	16+64(%rsp),%r15
+++	movq	24+64(%rsp),%r8
+++	leaq	128(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montq
+++
+++	movq	0+96(%rsp),%rax
+++	movq	8+96(%rsp),%r14
+++	leaq	0+96(%rsp),%rsi
+++	movq	16+96(%rsp),%r15
+++	movq	24+96(%rsp),%r8
+++	leaq	192(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montq
+++
+++	movq	128(%rsp),%rax
+++	leaq	128(%rsp),%rbx
+++	movq	0+64(%rsp),%r9
+++	movq	8+64(%rsp),%r10
+++	leaq	0+64(%rsp),%rsi
+++	movq	16+64(%rsp),%r11
+++	movq	24+64(%rsp),%r12
+++	leaq	160(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	movq	320(%rsp),%rax
+++	leaq	320(%rsp),%rbx
+++	movq	0+128(%rsp),%r9
+++	movq	8+128(%rsp),%r10
+++	leaq	0+128(%rsp),%rsi
+++	movq	16+128(%rsp),%r11
+++	movq	24+128(%rsp),%r12
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++
+++
+++
+++	xorq	%r11,%r11
+++	addq	%r12,%r12
+++	leaq	192(%rsp),%rsi
+++	adcq	%r13,%r13
+++	movq	%r12,%rax
+++	adcq	%r8,%r8
+++	adcq	%r9,%r9
+++	movq	%r13,%rbp
+++	adcq	$0,%r11
+++
+++	subq	$-1,%r12
+++	movq	%r8,%rcx
+++	sbbq	%r14,%r13
+++	sbbq	$0,%r8
+++	movq	%r9,%r10
+++	sbbq	%r15,%r9
+++	sbbq	$0,%r11
+++
+++	cmovcq	%rax,%r12
+++	movq	0(%rsi),%rax
+++	cmovcq	%rbp,%r13
+++	movq	8(%rsi),%rbp
+++	cmovcq	%rcx,%r8
+++	movq	16(%rsi),%rcx
+++	cmovcq	%r10,%r9
+++	movq	24(%rsi),%r10
+++
+++	call	__ecp_nistz256_subq
+++
+++	leaq	160(%rsp),%rbx
+++	leaq	224(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromq
+++
+++	movq	0+0(%rsp),%rax
+++	movq	0+8(%rsp),%rbp
+++	movq	0+16(%rsp),%rcx
+++	movq	0+24(%rsp),%r10
+++	leaq	64(%rsp),%rdi
+++
+++	call	__ecp_nistz256_subq
+++
+++	movq	%r12,0(%rdi)
+++	movq	%r13,8(%rdi)
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++	movq	352(%rsp),%rax
+++	leaq	352(%rsp),%rbx
+++	movq	0+160(%rsp),%r9
+++	movq	8+160(%rsp),%r10
+++	leaq	0+160(%rsp),%rsi
+++	movq	16+160(%rsp),%r11
+++	movq	24+160(%rsp),%r12
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	movq	96(%rsp),%rax
+++	leaq	96(%rsp),%rbx
+++	movq	0+64(%rsp),%r9
+++	movq	8+64(%rsp),%r10
+++	leaq	0+64(%rsp),%rsi
+++	movq	16+64(%rsp),%r11
+++	movq	24+64(%rsp),%r12
+++	leaq	64(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montq
+++
+++	leaq	32(%rsp),%rbx
+++	leaq	256(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromq
+++
+++.byte	102,72,15,126,199
+++
+++	movdqa	%xmm5,%xmm0
+++	movdqa	%xmm5,%xmm1
+++	pandn	288(%rsp),%xmm0
+++	movdqa	%xmm5,%xmm2
+++	pandn	288+16(%rsp),%xmm1
+++	movdqa	%xmm5,%xmm3
+++	pand	.LONE_mont(%rip),%xmm2
+++	pand	.LONE_mont+16(%rip),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm4,%xmm1
+++	pandn	%xmm2,%xmm0
+++	movdqa	%xmm4,%xmm2
+++	pandn	%xmm3,%xmm1
+++	movdqa	%xmm4,%xmm3
+++	pand	384(%rsp),%xmm2
+++	pand	384+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++	movdqu	%xmm2,64(%rdi)
+++	movdqu	%xmm3,80(%rdi)
+++
+++	movdqa	%xmm5,%xmm0
+++	movdqa	%xmm5,%xmm1
+++	pandn	224(%rsp),%xmm0
+++	movdqa	%xmm5,%xmm2
+++	pandn	224+16(%rsp),%xmm1
+++	movdqa	%xmm5,%xmm3
+++	pand	416(%rsp),%xmm2
+++	pand	416+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm4,%xmm1
+++	pandn	%xmm2,%xmm0
+++	movdqa	%xmm4,%xmm2
+++	pandn	%xmm3,%xmm1
+++	movdqa	%xmm4,%xmm3
+++	pand	320(%rsp),%xmm2
+++	pand	320+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++	movdqu	%xmm2,0(%rdi)
+++	movdqu	%xmm3,16(%rdi)
+++
+++	movdqa	%xmm5,%xmm0
+++	movdqa	%xmm5,%xmm1
+++	pandn	256(%rsp),%xmm0
+++	movdqa	%xmm5,%xmm2
+++	pandn	256+16(%rsp),%xmm1
+++	movdqa	%xmm5,%xmm3
+++	pand	448(%rsp),%xmm2
+++	pand	448+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm4,%xmm1
+++	pandn	%xmm2,%xmm0
+++	movdqa	%xmm4,%xmm2
+++	pandn	%xmm3,%xmm1
+++	movdqa	%xmm4,%xmm3
+++	pand	352(%rsp),%xmm2
+++	pand	352+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++	movdqu	%xmm2,32(%rdi)
+++	movdqu	%xmm3,48(%rdi)
+++
+++	leaq	480+56(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbx
+++.cfi_restore	%rbx
+++	movq	-8(%rsi),%rbp
+++.cfi_restore	%rbp
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Ladd_affineq_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ecp_nistz256_point_add_affine,.-ecp_nistz256_point_add_affine
+++.type	__ecp_nistz256_add_tox,@function
+++.align	32
+++__ecp_nistz256_add_tox:
+++.cfi_startproc	
+++	xorq	%r11,%r11
+++	adcq	0(%rbx),%r12
+++	adcq	8(%rbx),%r13
+++	movq	%r12,%rax
+++	adcq	16(%rbx),%r8
+++	adcq	24(%rbx),%r9
+++	movq	%r13,%rbp
+++	adcq	$0,%r11
+++
+++	xorq	%r10,%r10
+++	sbbq	$-1,%r12
+++	movq	%r8,%rcx
+++	sbbq	%r14,%r13
+++	sbbq	$0,%r8
+++	movq	%r9,%r10
+++	sbbq	%r15,%r9
+++	sbbq	$0,%r11
+++
+++	cmovcq	%rax,%r12
+++	cmovcq	%rbp,%r13
+++	movq	%r12,0(%rdi)
+++	cmovcq	%rcx,%r8
+++	movq	%r13,8(%rdi)
+++	cmovcq	%r10,%r9
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__ecp_nistz256_add_tox,.-__ecp_nistz256_add_tox
+++
+++.type	__ecp_nistz256_sub_fromx,@function
+++.align	32
+++__ecp_nistz256_sub_fromx:
+++.cfi_startproc	
+++	xorq	%r11,%r11
+++	sbbq	0(%rbx),%r12
+++	sbbq	8(%rbx),%r13
+++	movq	%r12,%rax
+++	sbbq	16(%rbx),%r8
+++	sbbq	24(%rbx),%r9
+++	movq	%r13,%rbp
+++	sbbq	$0,%r11
+++
+++	xorq	%r10,%r10
+++	adcq	$-1,%r12
+++	movq	%r8,%rcx
+++	adcq	%r14,%r13
+++	adcq	$0,%r8
+++	movq	%r9,%r10
+++	adcq	%r15,%r9
+++
+++	btq	$0,%r11
+++	cmovncq	%rax,%r12
+++	cmovncq	%rbp,%r13
+++	movq	%r12,0(%rdi)
+++	cmovncq	%rcx,%r8
+++	movq	%r13,8(%rdi)
+++	cmovncq	%r10,%r9
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__ecp_nistz256_sub_fromx,.-__ecp_nistz256_sub_fromx
+++
+++.type	__ecp_nistz256_subx,@function
+++.align	32
+++__ecp_nistz256_subx:
+++.cfi_startproc	
+++	xorq	%r11,%r11
+++	sbbq	%r12,%rax
+++	sbbq	%r13,%rbp
+++	movq	%rax,%r12
+++	sbbq	%r8,%rcx
+++	sbbq	%r9,%r10
+++	movq	%rbp,%r13
+++	sbbq	$0,%r11
+++
+++	xorq	%r9,%r9
+++	adcq	$-1,%rax
+++	movq	%rcx,%r8
+++	adcq	%r14,%rbp
+++	adcq	$0,%rcx
+++	movq	%r10,%r9
+++	adcq	%r15,%r10
+++
+++	btq	$0,%r11
+++	cmovcq	%rax,%r12
+++	cmovcq	%rbp,%r13
+++	cmovcq	%rcx,%r8
+++	cmovcq	%r10,%r9
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__ecp_nistz256_subx,.-__ecp_nistz256_subx
+++
+++.type	__ecp_nistz256_mul_by_2x,@function
+++.align	32
+++__ecp_nistz256_mul_by_2x:
+++.cfi_startproc	
+++	xorq	%r11,%r11
+++	adcq	%r12,%r12
+++	adcq	%r13,%r13
+++	movq	%r12,%rax
+++	adcq	%r8,%r8
+++	adcq	%r9,%r9
+++	movq	%r13,%rbp
+++	adcq	$0,%r11
+++
+++	xorq	%r10,%r10
+++	sbbq	$-1,%r12
+++	movq	%r8,%rcx
+++	sbbq	%r14,%r13
+++	sbbq	$0,%r8
+++	movq	%r9,%r10
+++	sbbq	%r15,%r9
+++	sbbq	$0,%r11
+++
+++	cmovcq	%rax,%r12
+++	cmovcq	%rbp,%r13
+++	movq	%r12,0(%rdi)
+++	cmovcq	%rcx,%r8
+++	movq	%r13,8(%rdi)
+++	cmovcq	%r10,%r9
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__ecp_nistz256_mul_by_2x,.-__ecp_nistz256_mul_by_2x
+++.type	ecp_nistz256_point_doublex,@function
+++.align	32
+++ecp_nistz256_point_doublex:
+++.cfi_startproc	
+++.Lpoint_doublex:
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++	subq	$160+8,%rsp
+++.cfi_adjust_cfa_offset	32*5+8
+++.Lpoint_doublex_body:
+++
+++.Lpoint_double_shortcutx:
+++	movdqu	0(%rsi),%xmm0
+++	movq	%rsi,%rbx
+++	movdqu	16(%rsi),%xmm1
+++	movq	32+0(%rsi),%r12
+++	movq	32+8(%rsi),%r13
+++	movq	32+16(%rsi),%r8
+++	movq	32+24(%rsi),%r9
+++	movq	.Lpoly+8(%rip),%r14
+++	movq	.Lpoly+24(%rip),%r15
+++	movdqa	%xmm0,96(%rsp)
+++	movdqa	%xmm1,96+16(%rsp)
+++	leaq	32(%rdi),%r10
+++	leaq	64(%rdi),%r11
+++.byte	102,72,15,110,199
+++.byte	102,73,15,110,202
+++.byte	102,73,15,110,211
+++
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_mul_by_2x
+++
+++	movq	64+0(%rsi),%rdx
+++	movq	64+8(%rsi),%r14
+++	movq	64+16(%rsi),%r15
+++	movq	64+24(%rsi),%r8
+++	leaq	64-128(%rsi),%rsi
+++	leaq	64(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montx
+++
+++	movq	0+0(%rsp),%rdx
+++	movq	8+0(%rsp),%r14
+++	leaq	-128+0(%rsp),%rsi
+++	movq	16+0(%rsp),%r15
+++	movq	24+0(%rsp),%r8
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montx
+++
+++	movq	32(%rbx),%rdx
+++	movq	64+0(%rbx),%r9
+++	movq	64+8(%rbx),%r10
+++	movq	64+16(%rbx),%r11
+++	movq	64+24(%rbx),%r12
+++	leaq	64-128(%rbx),%rsi
+++	leaq	32(%rbx),%rbx
+++.byte	102,72,15,126,215
+++	call	__ecp_nistz256_mul_montx
+++	call	__ecp_nistz256_mul_by_2x
+++
+++	movq	96+0(%rsp),%r12
+++	movq	96+8(%rsp),%r13
+++	leaq	64(%rsp),%rbx
+++	movq	96+16(%rsp),%r8
+++	movq	96+24(%rsp),%r9
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_add_tox
+++
+++	movq	96+0(%rsp),%r12
+++	movq	96+8(%rsp),%r13
+++	leaq	64(%rsp),%rbx
+++	movq	96+16(%rsp),%r8
+++	movq	96+24(%rsp),%r9
+++	leaq	64(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromx
+++
+++	movq	0+0(%rsp),%rdx
+++	movq	8+0(%rsp),%r14
+++	leaq	-128+0(%rsp),%rsi
+++	movq	16+0(%rsp),%r15
+++	movq	24+0(%rsp),%r8
+++.byte	102,72,15,126,207
+++	call	__ecp_nistz256_sqr_montx
+++	xorq	%r9,%r9
+++	movq	%r12,%rax
+++	addq	$-1,%r12
+++	movq	%r13,%r10
+++	adcq	%rsi,%r13
+++	movq	%r14,%rcx
+++	adcq	$0,%r14
+++	movq	%r15,%r8
+++	adcq	%rbp,%r15
+++	adcq	$0,%r9
+++	xorq	%rsi,%rsi
+++	testq	$1,%rax
+++
+++	cmovzq	%rax,%r12
+++	cmovzq	%r10,%r13
+++	cmovzq	%rcx,%r14
+++	cmovzq	%r8,%r15
+++	cmovzq	%rsi,%r9
+++
+++	movq	%r13,%rax
+++	shrq	$1,%r12
+++	shlq	$63,%rax
+++	movq	%r14,%r10
+++	shrq	$1,%r13
+++	orq	%rax,%r12
+++	shlq	$63,%r10
+++	movq	%r15,%rcx
+++	shrq	$1,%r14
+++	orq	%r10,%r13
+++	shlq	$63,%rcx
+++	movq	%r12,0(%rdi)
+++	shrq	$1,%r15
+++	movq	%r13,8(%rdi)
+++	shlq	$63,%r9
+++	orq	%rcx,%r14
+++	orq	%r9,%r15
+++	movq	%r14,16(%rdi)
+++	movq	%r15,24(%rdi)
+++	movq	64(%rsp),%rdx
+++	leaq	64(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	-128+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	leaq	128(%rsp),%rdi
+++	call	__ecp_nistz256_mul_by_2x
+++
+++	leaq	32(%rsp),%rbx
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_add_tox
+++
+++	movq	96(%rsp),%rdx
+++	leaq	96(%rsp),%rbx
+++	movq	0+0(%rsp),%r9
+++	movq	8+0(%rsp),%r10
+++	leaq	-128+0(%rsp),%rsi
+++	movq	16+0(%rsp),%r11
+++	movq	24+0(%rsp),%r12
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	leaq	128(%rsp),%rdi
+++	call	__ecp_nistz256_mul_by_2x
+++
+++	movq	0+32(%rsp),%rdx
+++	movq	8+32(%rsp),%r14
+++	leaq	-128+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r15
+++	movq	24+32(%rsp),%r8
+++.byte	102,72,15,126,199
+++	call	__ecp_nistz256_sqr_montx
+++
+++	leaq	128(%rsp),%rbx
+++	movq	%r14,%r8
+++	movq	%r15,%r9
+++	movq	%rsi,%r14
+++	movq	%rbp,%r15
+++	call	__ecp_nistz256_sub_fromx
+++
+++	movq	0+0(%rsp),%rax
+++	movq	0+8(%rsp),%rbp
+++	movq	0+16(%rsp),%rcx
+++	movq	0+24(%rsp),%r10
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_subx
+++
+++	movq	32(%rsp),%rdx
+++	leaq	32(%rsp),%rbx
+++	movq	%r12,%r14
+++	xorl	%ecx,%ecx
+++	movq	%r12,0+0(%rsp)
+++	movq	%r13,%r10
+++	movq	%r13,0+8(%rsp)
+++	cmovzq	%r8,%r11
+++	movq	%r8,0+16(%rsp)
+++	leaq	0-128(%rsp),%rsi
+++	cmovzq	%r9,%r12
+++	movq	%r9,0+24(%rsp)
+++	movq	%r14,%r9
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++.byte	102,72,15,126,203
+++.byte	102,72,15,126,207
+++	call	__ecp_nistz256_sub_fromx
+++
+++	leaq	160+56(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbx
+++.cfi_restore	%rbx
+++	movq	-8(%rsi),%rbp
+++.cfi_restore	%rbp
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lpoint_doublex_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ecp_nistz256_point_doublex,.-ecp_nistz256_point_doublex
+++.type	ecp_nistz256_point_addx,@function
+++.align	32
+++ecp_nistz256_point_addx:
+++.cfi_startproc	
+++.Lpoint_addx:
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++	subq	$576+8,%rsp
+++.cfi_adjust_cfa_offset	32*18+8
+++.Lpoint_addx_body:
+++
+++	movdqu	0(%rsi),%xmm0
+++	movdqu	16(%rsi),%xmm1
+++	movdqu	32(%rsi),%xmm2
+++	movdqu	48(%rsi),%xmm3
+++	movdqu	64(%rsi),%xmm4
+++	movdqu	80(%rsi),%xmm5
+++	movq	%rsi,%rbx
+++	movq	%rdx,%rsi
+++	movdqa	%xmm0,384(%rsp)
+++	movdqa	%xmm1,384+16(%rsp)
+++	movdqa	%xmm2,416(%rsp)
+++	movdqa	%xmm3,416+16(%rsp)
+++	movdqa	%xmm4,448(%rsp)
+++	movdqa	%xmm5,448+16(%rsp)
+++	por	%xmm4,%xmm5
+++
+++	movdqu	0(%rsi),%xmm0
+++	pshufd	$0xb1,%xmm5,%xmm3
+++	movdqu	16(%rsi),%xmm1
+++	movdqu	32(%rsi),%xmm2
+++	por	%xmm3,%xmm5
+++	movdqu	48(%rsi),%xmm3
+++	movq	64+0(%rsi),%rdx
+++	movq	64+8(%rsi),%r14
+++	movq	64+16(%rsi),%r15
+++	movq	64+24(%rsi),%r8
+++	movdqa	%xmm0,480(%rsp)
+++	pshufd	$0x1e,%xmm5,%xmm4
+++	movdqa	%xmm1,480+16(%rsp)
+++	movdqu	64(%rsi),%xmm0
+++	movdqu	80(%rsi),%xmm1
+++	movdqa	%xmm2,512(%rsp)
+++	movdqa	%xmm3,512+16(%rsp)
+++	por	%xmm4,%xmm5
+++	pxor	%xmm4,%xmm4
+++	por	%xmm0,%xmm1
+++.byte	102,72,15,110,199
+++
+++	leaq	64-128(%rsi),%rsi
+++	movq	%rdx,544+0(%rsp)
+++	movq	%r14,544+8(%rsp)
+++	movq	%r15,544+16(%rsp)
+++	movq	%r8,544+24(%rsp)
+++	leaq	96(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montx
+++
+++	pcmpeqd	%xmm4,%xmm5
+++	pshufd	$0xb1,%xmm1,%xmm4
+++	por	%xmm1,%xmm4
+++	pshufd	$0,%xmm5,%xmm5
+++	pshufd	$0x1e,%xmm4,%xmm3
+++	por	%xmm3,%xmm4
+++	pxor	%xmm3,%xmm3
+++	pcmpeqd	%xmm3,%xmm4
+++	pshufd	$0,%xmm4,%xmm4
+++	movq	64+0(%rbx),%rdx
+++	movq	64+8(%rbx),%r14
+++	movq	64+16(%rbx),%r15
+++	movq	64+24(%rbx),%r8
+++.byte	102,72,15,110,203
+++
+++	leaq	64-128(%rbx),%rsi
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montx
+++
+++	movq	544(%rsp),%rdx
+++	leaq	544(%rsp),%rbx
+++	movq	0+96(%rsp),%r9
+++	movq	8+96(%rsp),%r10
+++	leaq	-128+96(%rsp),%rsi
+++	movq	16+96(%rsp),%r11
+++	movq	24+96(%rsp),%r12
+++	leaq	224(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	movq	448(%rsp),%rdx
+++	leaq	448(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	-128+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	256(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	movq	416(%rsp),%rdx
+++	leaq	416(%rsp),%rbx
+++	movq	0+224(%rsp),%r9
+++	movq	8+224(%rsp),%r10
+++	leaq	-128+224(%rsp),%rsi
+++	movq	16+224(%rsp),%r11
+++	movq	24+224(%rsp),%r12
+++	leaq	224(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	movq	512(%rsp),%rdx
+++	leaq	512(%rsp),%rbx
+++	movq	0+256(%rsp),%r9
+++	movq	8+256(%rsp),%r10
+++	leaq	-128+256(%rsp),%rsi
+++	movq	16+256(%rsp),%r11
+++	movq	24+256(%rsp),%r12
+++	leaq	256(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	leaq	224(%rsp),%rbx
+++	leaq	64(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromx
+++
+++	orq	%r13,%r12
+++	movdqa	%xmm4,%xmm2
+++	orq	%r8,%r12
+++	orq	%r9,%r12
+++	por	%xmm5,%xmm2
+++.byte	102,73,15,110,220
+++
+++	movq	384(%rsp),%rdx
+++	leaq	384(%rsp),%rbx
+++	movq	0+96(%rsp),%r9
+++	movq	8+96(%rsp),%r10
+++	leaq	-128+96(%rsp),%rsi
+++	movq	16+96(%rsp),%r11
+++	movq	24+96(%rsp),%r12
+++	leaq	160(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	movq	480(%rsp),%rdx
+++	leaq	480(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	-128+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	192(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	leaq	160(%rsp),%rbx
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromx
+++
+++	orq	%r13,%r12
+++	orq	%r8,%r12
+++	orq	%r9,%r12
+++
+++.byte	102,73,15,126,208
+++.byte	102,73,15,126,217
+++	orq	%r8,%r12
+++.byte	0x3e
+++	jnz	.Ladd_proceedx
+++
+++
+++
+++	testq	%r9,%r9
+++	jz	.Ladd_doublex
+++
+++
+++
+++
+++
+++
+++.byte	102,72,15,126,199
+++	pxor	%xmm0,%xmm0
+++	movdqu	%xmm0,0(%rdi)
+++	movdqu	%xmm0,16(%rdi)
+++	movdqu	%xmm0,32(%rdi)
+++	movdqu	%xmm0,48(%rdi)
+++	movdqu	%xmm0,64(%rdi)
+++	movdqu	%xmm0,80(%rdi)
+++	jmp	.Ladd_donex
+++
+++.align	32
+++.Ladd_doublex:
+++.byte	102,72,15,126,206
+++.byte	102,72,15,126,199
+++	addq	$416,%rsp
+++.cfi_adjust_cfa_offset	-416
+++	jmp	.Lpoint_double_shortcutx
+++.cfi_adjust_cfa_offset	416
+++
+++.align	32
+++.Ladd_proceedx:
+++	movq	0+64(%rsp),%rdx
+++	movq	8+64(%rsp),%r14
+++	leaq	-128+64(%rsp),%rsi
+++	movq	16+64(%rsp),%r15
+++	movq	24+64(%rsp),%r8
+++	leaq	96(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montx
+++
+++	movq	448(%rsp),%rdx
+++	leaq	448(%rsp),%rbx
+++	movq	0+0(%rsp),%r9
+++	movq	8+0(%rsp),%r10
+++	leaq	-128+0(%rsp),%rsi
+++	movq	16+0(%rsp),%r11
+++	movq	24+0(%rsp),%r12
+++	leaq	352(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	movq	0+0(%rsp),%rdx
+++	movq	8+0(%rsp),%r14
+++	leaq	-128+0(%rsp),%rsi
+++	movq	16+0(%rsp),%r15
+++	movq	24+0(%rsp),%r8
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montx
+++
+++	movq	544(%rsp),%rdx
+++	leaq	544(%rsp),%rbx
+++	movq	0+352(%rsp),%r9
+++	movq	8+352(%rsp),%r10
+++	leaq	-128+352(%rsp),%rsi
+++	movq	16+352(%rsp),%r11
+++	movq	24+352(%rsp),%r12
+++	leaq	352(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	movq	0(%rsp),%rdx
+++	leaq	0(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	-128+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	128(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	movq	160(%rsp),%rdx
+++	leaq	160(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	-128+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	192(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++
+++
+++
+++	xorq	%r11,%r11
+++	addq	%r12,%r12
+++	leaq	96(%rsp),%rsi
+++	adcq	%r13,%r13
+++	movq	%r12,%rax
+++	adcq	%r8,%r8
+++	adcq	%r9,%r9
+++	movq	%r13,%rbp
+++	adcq	$0,%r11
+++
+++	subq	$-1,%r12
+++	movq	%r8,%rcx
+++	sbbq	%r14,%r13
+++	sbbq	$0,%r8
+++	movq	%r9,%r10
+++	sbbq	%r15,%r9
+++	sbbq	$0,%r11
+++
+++	cmovcq	%rax,%r12
+++	movq	0(%rsi),%rax
+++	cmovcq	%rbp,%r13
+++	movq	8(%rsi),%rbp
+++	cmovcq	%rcx,%r8
+++	movq	16(%rsi),%rcx
+++	cmovcq	%r10,%r9
+++	movq	24(%rsi),%r10
+++
+++	call	__ecp_nistz256_subx
+++
+++	leaq	128(%rsp),%rbx
+++	leaq	288(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromx
+++
+++	movq	192+0(%rsp),%rax
+++	movq	192+8(%rsp),%rbp
+++	movq	192+16(%rsp),%rcx
+++	movq	192+24(%rsp),%r10
+++	leaq	320(%rsp),%rdi
+++
+++	call	__ecp_nistz256_subx
+++
+++	movq	%r12,0(%rdi)
+++	movq	%r13,8(%rdi)
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++	movq	128(%rsp),%rdx
+++	leaq	128(%rsp),%rbx
+++	movq	0+224(%rsp),%r9
+++	movq	8+224(%rsp),%r10
+++	leaq	-128+224(%rsp),%rsi
+++	movq	16+224(%rsp),%r11
+++	movq	24+224(%rsp),%r12
+++	leaq	256(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	movq	320(%rsp),%rdx
+++	leaq	320(%rsp),%rbx
+++	movq	0+64(%rsp),%r9
+++	movq	8+64(%rsp),%r10
+++	leaq	-128+64(%rsp),%rsi
+++	movq	16+64(%rsp),%r11
+++	movq	24+64(%rsp),%r12
+++	leaq	320(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	leaq	256(%rsp),%rbx
+++	leaq	320(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromx
+++
+++.byte	102,72,15,126,199
+++
+++	movdqa	%xmm5,%xmm0
+++	movdqa	%xmm5,%xmm1
+++	pandn	352(%rsp),%xmm0
+++	movdqa	%xmm5,%xmm2
+++	pandn	352+16(%rsp),%xmm1
+++	movdqa	%xmm5,%xmm3
+++	pand	544(%rsp),%xmm2
+++	pand	544+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm4,%xmm1
+++	pandn	%xmm2,%xmm0
+++	movdqa	%xmm4,%xmm2
+++	pandn	%xmm3,%xmm1
+++	movdqa	%xmm4,%xmm3
+++	pand	448(%rsp),%xmm2
+++	pand	448+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++	movdqu	%xmm2,64(%rdi)
+++	movdqu	%xmm3,80(%rdi)
+++
+++	movdqa	%xmm5,%xmm0
+++	movdqa	%xmm5,%xmm1
+++	pandn	288(%rsp),%xmm0
+++	movdqa	%xmm5,%xmm2
+++	pandn	288+16(%rsp),%xmm1
+++	movdqa	%xmm5,%xmm3
+++	pand	480(%rsp),%xmm2
+++	pand	480+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm4,%xmm1
+++	pandn	%xmm2,%xmm0
+++	movdqa	%xmm4,%xmm2
+++	pandn	%xmm3,%xmm1
+++	movdqa	%xmm4,%xmm3
+++	pand	384(%rsp),%xmm2
+++	pand	384+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++	movdqu	%xmm2,0(%rdi)
+++	movdqu	%xmm3,16(%rdi)
+++
+++	movdqa	%xmm5,%xmm0
+++	movdqa	%xmm5,%xmm1
+++	pandn	320(%rsp),%xmm0
+++	movdqa	%xmm5,%xmm2
+++	pandn	320+16(%rsp),%xmm1
+++	movdqa	%xmm5,%xmm3
+++	pand	512(%rsp),%xmm2
+++	pand	512+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm4,%xmm1
+++	pandn	%xmm2,%xmm0
+++	movdqa	%xmm4,%xmm2
+++	pandn	%xmm3,%xmm1
+++	movdqa	%xmm4,%xmm3
+++	pand	416(%rsp),%xmm2
+++	pand	416+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++	movdqu	%xmm2,32(%rdi)
+++	movdqu	%xmm3,48(%rdi)
+++
+++.Ladd_donex:
+++	leaq	576+56(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbx
+++.cfi_restore	%rbx
+++	movq	-8(%rsi),%rbp
+++.cfi_restore	%rbp
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lpoint_addx_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ecp_nistz256_point_addx,.-ecp_nistz256_point_addx
+++.type	ecp_nistz256_point_add_affinex,@function
+++.align	32
+++ecp_nistz256_point_add_affinex:
+++.cfi_startproc	
+++.Lpoint_add_affinex:
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbp,-16
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%rbx,-24
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r15,-56
+++	subq	$480+8,%rsp
+++.cfi_adjust_cfa_offset	32*15+8
+++.Ladd_affinex_body:
+++
+++	movdqu	0(%rsi),%xmm0
+++	movq	%rdx,%rbx
+++	movdqu	16(%rsi),%xmm1
+++	movdqu	32(%rsi),%xmm2
+++	movdqu	48(%rsi),%xmm3
+++	movdqu	64(%rsi),%xmm4
+++	movdqu	80(%rsi),%xmm5
+++	movq	64+0(%rsi),%rdx
+++	movq	64+8(%rsi),%r14
+++	movq	64+16(%rsi),%r15
+++	movq	64+24(%rsi),%r8
+++	movdqa	%xmm0,320(%rsp)
+++	movdqa	%xmm1,320+16(%rsp)
+++	movdqa	%xmm2,352(%rsp)
+++	movdqa	%xmm3,352+16(%rsp)
+++	movdqa	%xmm4,384(%rsp)
+++	movdqa	%xmm5,384+16(%rsp)
+++	por	%xmm4,%xmm5
+++
+++	movdqu	0(%rbx),%xmm0
+++	pshufd	$0xb1,%xmm5,%xmm3
+++	movdqu	16(%rbx),%xmm1
+++	movdqu	32(%rbx),%xmm2
+++	por	%xmm3,%xmm5
+++	movdqu	48(%rbx),%xmm3
+++	movdqa	%xmm0,416(%rsp)
+++	pshufd	$0x1e,%xmm5,%xmm4
+++	movdqa	%xmm1,416+16(%rsp)
+++	por	%xmm0,%xmm1
+++.byte	102,72,15,110,199
+++	movdqa	%xmm2,448(%rsp)
+++	movdqa	%xmm3,448+16(%rsp)
+++	por	%xmm2,%xmm3
+++	por	%xmm4,%xmm5
+++	pxor	%xmm4,%xmm4
+++	por	%xmm1,%xmm3
+++
+++	leaq	64-128(%rsi),%rsi
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montx
+++
+++	pcmpeqd	%xmm4,%xmm5
+++	pshufd	$0xb1,%xmm3,%xmm4
+++	movq	0(%rbx),%rdx
+++
+++	movq	%r12,%r9
+++	por	%xmm3,%xmm4
+++	pshufd	$0,%xmm5,%xmm5
+++	pshufd	$0x1e,%xmm4,%xmm3
+++	movq	%r13,%r10
+++	por	%xmm3,%xmm4
+++	pxor	%xmm3,%xmm3
+++	movq	%r14,%r11
+++	pcmpeqd	%xmm3,%xmm4
+++	pshufd	$0,%xmm4,%xmm4
+++
+++	leaq	32-128(%rsp),%rsi
+++	movq	%r15,%r12
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	leaq	320(%rsp),%rbx
+++	leaq	64(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromx
+++
+++	movq	384(%rsp),%rdx
+++	leaq	384(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	-128+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	movq	384(%rsp),%rdx
+++	leaq	384(%rsp),%rbx
+++	movq	0+64(%rsp),%r9
+++	movq	8+64(%rsp),%r10
+++	leaq	-128+64(%rsp),%rsi
+++	movq	16+64(%rsp),%r11
+++	movq	24+64(%rsp),%r12
+++	leaq	288(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	movq	448(%rsp),%rdx
+++	leaq	448(%rsp),%rbx
+++	movq	0+32(%rsp),%r9
+++	movq	8+32(%rsp),%r10
+++	leaq	-128+32(%rsp),%rsi
+++	movq	16+32(%rsp),%r11
+++	movq	24+32(%rsp),%r12
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	leaq	352(%rsp),%rbx
+++	leaq	96(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromx
+++
+++	movq	0+64(%rsp),%rdx
+++	movq	8+64(%rsp),%r14
+++	leaq	-128+64(%rsp),%rsi
+++	movq	16+64(%rsp),%r15
+++	movq	24+64(%rsp),%r8
+++	leaq	128(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montx
+++
+++	movq	0+96(%rsp),%rdx
+++	movq	8+96(%rsp),%r14
+++	leaq	-128+96(%rsp),%rsi
+++	movq	16+96(%rsp),%r15
+++	movq	24+96(%rsp),%r8
+++	leaq	192(%rsp),%rdi
+++	call	__ecp_nistz256_sqr_montx
+++
+++	movq	128(%rsp),%rdx
+++	leaq	128(%rsp),%rbx
+++	movq	0+64(%rsp),%r9
+++	movq	8+64(%rsp),%r10
+++	leaq	-128+64(%rsp),%rsi
+++	movq	16+64(%rsp),%r11
+++	movq	24+64(%rsp),%r12
+++	leaq	160(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	movq	320(%rsp),%rdx
+++	leaq	320(%rsp),%rbx
+++	movq	0+128(%rsp),%r9
+++	movq	8+128(%rsp),%r10
+++	leaq	-128+128(%rsp),%rsi
+++	movq	16+128(%rsp),%r11
+++	movq	24+128(%rsp),%r12
+++	leaq	0(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++
+++
+++
+++	xorq	%r11,%r11
+++	addq	%r12,%r12
+++	leaq	192(%rsp),%rsi
+++	adcq	%r13,%r13
+++	movq	%r12,%rax
+++	adcq	%r8,%r8
+++	adcq	%r9,%r9
+++	movq	%r13,%rbp
+++	adcq	$0,%r11
+++
+++	subq	$-1,%r12
+++	movq	%r8,%rcx
+++	sbbq	%r14,%r13
+++	sbbq	$0,%r8
+++	movq	%r9,%r10
+++	sbbq	%r15,%r9
+++	sbbq	$0,%r11
+++
+++	cmovcq	%rax,%r12
+++	movq	0(%rsi),%rax
+++	cmovcq	%rbp,%r13
+++	movq	8(%rsi),%rbp
+++	cmovcq	%rcx,%r8
+++	movq	16(%rsi),%rcx
+++	cmovcq	%r10,%r9
+++	movq	24(%rsi),%r10
+++
+++	call	__ecp_nistz256_subx
+++
+++	leaq	160(%rsp),%rbx
+++	leaq	224(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromx
+++
+++	movq	0+0(%rsp),%rax
+++	movq	0+8(%rsp),%rbp
+++	movq	0+16(%rsp),%rcx
+++	movq	0+24(%rsp),%r10
+++	leaq	64(%rsp),%rdi
+++
+++	call	__ecp_nistz256_subx
+++
+++	movq	%r12,0(%rdi)
+++	movq	%r13,8(%rdi)
+++	movq	%r8,16(%rdi)
+++	movq	%r9,24(%rdi)
+++	movq	352(%rsp),%rdx
+++	leaq	352(%rsp),%rbx
+++	movq	0+160(%rsp),%r9
+++	movq	8+160(%rsp),%r10
+++	leaq	-128+160(%rsp),%rsi
+++	movq	16+160(%rsp),%r11
+++	movq	24+160(%rsp),%r12
+++	leaq	32(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	movq	96(%rsp),%rdx
+++	leaq	96(%rsp),%rbx
+++	movq	0+64(%rsp),%r9
+++	movq	8+64(%rsp),%r10
+++	leaq	-128+64(%rsp),%rsi
+++	movq	16+64(%rsp),%r11
+++	movq	24+64(%rsp),%r12
+++	leaq	64(%rsp),%rdi
+++	call	__ecp_nistz256_mul_montx
+++
+++	leaq	32(%rsp),%rbx
+++	leaq	256(%rsp),%rdi
+++	call	__ecp_nistz256_sub_fromx
+++
+++.byte	102,72,15,126,199
+++
+++	movdqa	%xmm5,%xmm0
+++	movdqa	%xmm5,%xmm1
+++	pandn	288(%rsp),%xmm0
+++	movdqa	%xmm5,%xmm2
+++	pandn	288+16(%rsp),%xmm1
+++	movdqa	%xmm5,%xmm3
+++	pand	.LONE_mont(%rip),%xmm2
+++	pand	.LONE_mont+16(%rip),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm4,%xmm1
+++	pandn	%xmm2,%xmm0
+++	movdqa	%xmm4,%xmm2
+++	pandn	%xmm3,%xmm1
+++	movdqa	%xmm4,%xmm3
+++	pand	384(%rsp),%xmm2
+++	pand	384+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++	movdqu	%xmm2,64(%rdi)
+++	movdqu	%xmm3,80(%rdi)
+++
+++	movdqa	%xmm5,%xmm0
+++	movdqa	%xmm5,%xmm1
+++	pandn	224(%rsp),%xmm0
+++	movdqa	%xmm5,%xmm2
+++	pandn	224+16(%rsp),%xmm1
+++	movdqa	%xmm5,%xmm3
+++	pand	416(%rsp),%xmm2
+++	pand	416+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm4,%xmm1
+++	pandn	%xmm2,%xmm0
+++	movdqa	%xmm4,%xmm2
+++	pandn	%xmm3,%xmm1
+++	movdqa	%xmm4,%xmm3
+++	pand	320(%rsp),%xmm2
+++	pand	320+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++	movdqu	%xmm2,0(%rdi)
+++	movdqu	%xmm3,16(%rdi)
+++
+++	movdqa	%xmm5,%xmm0
+++	movdqa	%xmm5,%xmm1
+++	pandn	256(%rsp),%xmm0
+++	movdqa	%xmm5,%xmm2
+++	pandn	256+16(%rsp),%xmm1
+++	movdqa	%xmm5,%xmm3
+++	pand	448(%rsp),%xmm2
+++	pand	448+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++
+++	movdqa	%xmm4,%xmm0
+++	movdqa	%xmm4,%xmm1
+++	pandn	%xmm2,%xmm0
+++	movdqa	%xmm4,%xmm2
+++	pandn	%xmm3,%xmm1
+++	movdqa	%xmm4,%xmm3
+++	pand	352(%rsp),%xmm2
+++	pand	352+16(%rsp),%xmm3
+++	por	%xmm0,%xmm2
+++	por	%xmm1,%xmm3
+++	movdqu	%xmm2,32(%rdi)
+++	movdqu	%xmm3,48(%rdi)
+++
+++	leaq	480+56(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbx
+++.cfi_restore	%rbx
+++	movq	-8(%rsi),%rbp
+++.cfi_restore	%rbp
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Ladd_affinex_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	ecp_nistz256_point_add_affinex,.-ecp_nistz256_point_add_affinex
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/p256_beeu-x86_64-asm.S b/linux-x86_64/ypto/fipsmodule/p256_beeu-x86_64-asm.S
++new file mode 100644
++index 000000000..d072a8347
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/p256_beeu-x86_64-asm.S
++@@ -0,0 +1,343 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++
+++.type	beeu_mod_inverse_vartime,@function
+++.hidden	beeu_mod_inverse_vartime
+++.globl	beeu_mod_inverse_vartime
+++.hidden beeu_mod_inverse_vartime
+++.align	32
+++beeu_mod_inverse_vartime:
+++.cfi_startproc	
+++	pushq	%rbp
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	rbp,-16
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	r12,-24
+++	pushq	%r13
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	r13,-32
+++	pushq	%r14
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	r14,-40
+++	pushq	%r15
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	r15,-48
+++	pushq	%rbx
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	rbx,-56
+++	pushq	%rsi
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	rsi,-64
+++
+++	subq	$80,%rsp
+++.cfi_adjust_cfa_offset	80
+++	movq	%rdi,0(%rsp)
+++
+++
+++	movq	$1,%r8
+++	xorq	%r9,%r9
+++	xorq	%r10,%r10
+++	xorq	%r11,%r11
+++	xorq	%rdi,%rdi
+++
+++	xorq	%r12,%r12
+++	xorq	%r13,%r13
+++	xorq	%r14,%r14
+++	xorq	%r15,%r15
+++	xorq	%rbp,%rbp
+++
+++
+++	vmovdqu	0(%rsi),%xmm0
+++	vmovdqu	16(%rsi),%xmm1
+++	vmovdqu	%xmm0,48(%rsp)
+++	vmovdqu	%xmm1,64(%rsp)
+++
+++	vmovdqu	0(%rdx),%xmm0
+++	vmovdqu	16(%rdx),%xmm1
+++	vmovdqu	%xmm0,16(%rsp)
+++	vmovdqu	%xmm1,32(%rsp)
+++
+++.Lbeeu_loop:
+++	xorq	%rbx,%rbx
+++	orq	48(%rsp),%rbx
+++	orq	56(%rsp),%rbx
+++	orq	64(%rsp),%rbx
+++	orq	72(%rsp),%rbx
+++	jz	.Lbeeu_loop_end
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	movq	$1,%rcx
+++
+++
+++.Lbeeu_shift_loop_XB:
+++	movq	%rcx,%rbx
+++	andq	48(%rsp),%rbx
+++	jnz	.Lbeeu_shift_loop_end_XB
+++
+++
+++	movq	$1,%rbx
+++	andq	%r8,%rbx
+++	jz	.Lshift1_0
+++	addq	0(%rdx),%r8
+++	adcq	8(%rdx),%r9
+++	adcq	16(%rdx),%r10
+++	adcq	24(%rdx),%r11
+++	adcq	$0,%rdi
+++
+++.Lshift1_0:
+++	shrdq	$1,%r9,%r8
+++	shrdq	$1,%r10,%r9
+++	shrdq	$1,%r11,%r10
+++	shrdq	$1,%rdi,%r11
+++	shrq	$1,%rdi
+++
+++	shlq	$1,%rcx
+++
+++
+++
+++
+++
+++	cmpq	$0x8000000,%rcx
+++	jne	.Lbeeu_shift_loop_XB
+++
+++.Lbeeu_shift_loop_end_XB:
+++	bsfq	%rcx,%rcx
+++	testq	%rcx,%rcx
+++	jz	.Lbeeu_no_shift_XB
+++
+++
+++
+++	movq	8+48(%rsp),%rax
+++	movq	16+48(%rsp),%rbx
+++	movq	24+48(%rsp),%rsi
+++
+++	shrdq	%cl,%rax,0+48(%rsp)
+++	shrdq	%cl,%rbx,8+48(%rsp)
+++	shrdq	%cl,%rsi,16+48(%rsp)
+++
+++	shrq	%cl,%rsi
+++	movq	%rsi,24+48(%rsp)
+++
+++
+++.Lbeeu_no_shift_XB:
+++
+++	movq	$1,%rcx
+++
+++
+++.Lbeeu_shift_loop_YA:
+++	movq	%rcx,%rbx
+++	andq	16(%rsp),%rbx
+++	jnz	.Lbeeu_shift_loop_end_YA
+++
+++
+++	movq	$1,%rbx
+++	andq	%r12,%rbx
+++	jz	.Lshift1_1
+++	addq	0(%rdx),%r12
+++	adcq	8(%rdx),%r13
+++	adcq	16(%rdx),%r14
+++	adcq	24(%rdx),%r15
+++	adcq	$0,%rbp
+++
+++.Lshift1_1:
+++	shrdq	$1,%r13,%r12
+++	shrdq	$1,%r14,%r13
+++	shrdq	$1,%r15,%r14
+++	shrdq	$1,%rbp,%r15
+++	shrq	$1,%rbp
+++
+++	shlq	$1,%rcx
+++
+++
+++
+++
+++
+++	cmpq	$0x8000000,%rcx
+++	jne	.Lbeeu_shift_loop_YA
+++
+++.Lbeeu_shift_loop_end_YA:
+++	bsfq	%rcx,%rcx
+++	testq	%rcx,%rcx
+++	jz	.Lbeeu_no_shift_YA
+++
+++
+++
+++	movq	8+16(%rsp),%rax
+++	movq	16+16(%rsp),%rbx
+++	movq	24+16(%rsp),%rsi
+++
+++	shrdq	%cl,%rax,0+16(%rsp)
+++	shrdq	%cl,%rbx,8+16(%rsp)
+++	shrdq	%cl,%rsi,16+16(%rsp)
+++
+++	shrq	%cl,%rsi
+++	movq	%rsi,24+16(%rsp)
+++
+++
+++.Lbeeu_no_shift_YA:
+++
+++	movq	48(%rsp),%rax
+++	movq	56(%rsp),%rbx
+++	movq	64(%rsp),%rsi
+++	movq	72(%rsp),%rcx
+++	subq	16(%rsp),%rax
+++	sbbq	24(%rsp),%rbx
+++	sbbq	32(%rsp),%rsi
+++	sbbq	40(%rsp),%rcx
+++	jnc	.Lbeeu_B_bigger_than_A
+++
+++
+++	movq	16(%rsp),%rax
+++	movq	24(%rsp),%rbx
+++	movq	32(%rsp),%rsi
+++	movq	40(%rsp),%rcx
+++	subq	48(%rsp),%rax
+++	sbbq	56(%rsp),%rbx
+++	sbbq	64(%rsp),%rsi
+++	sbbq	72(%rsp),%rcx
+++	movq	%rax,16(%rsp)
+++	movq	%rbx,24(%rsp)
+++	movq	%rsi,32(%rsp)
+++	movq	%rcx,40(%rsp)
+++
+++
+++	addq	%r8,%r12
+++	adcq	%r9,%r13
+++	adcq	%r10,%r14
+++	adcq	%r11,%r15
+++	adcq	%rdi,%rbp
+++	jmp	.Lbeeu_loop
+++
+++.Lbeeu_B_bigger_than_A:
+++
+++	movq	%rax,48(%rsp)
+++	movq	%rbx,56(%rsp)
+++	movq	%rsi,64(%rsp)
+++	movq	%rcx,72(%rsp)
+++
+++
+++	addq	%r12,%r8
+++	adcq	%r13,%r9
+++	adcq	%r14,%r10
+++	adcq	%r15,%r11
+++	adcq	%rbp,%rdi
+++
+++	jmp	.Lbeeu_loop
+++
+++.Lbeeu_loop_end:
+++
+++
+++
+++
+++	movq	16(%rsp),%rbx
+++	subq	$1,%rbx
+++	orq	24(%rsp),%rbx
+++	orq	32(%rsp),%rbx
+++	orq	40(%rsp),%rbx
+++
+++	jnz	.Lbeeu_err
+++
+++
+++
+++
+++	movq	0(%rdx),%r8
+++	movq	8(%rdx),%r9
+++	movq	16(%rdx),%r10
+++	movq	24(%rdx),%r11
+++	xorq	%rdi,%rdi
+++
+++.Lbeeu_reduction_loop:
+++	movq	%r12,16(%rsp)
+++	movq	%r13,24(%rsp)
+++	movq	%r14,32(%rsp)
+++	movq	%r15,40(%rsp)
+++	movq	%rbp,48(%rsp)
+++
+++
+++	subq	%r8,%r12
+++	sbbq	%r9,%r13
+++	sbbq	%r10,%r14
+++	sbbq	%r11,%r15
+++	sbbq	$0,%rbp
+++
+++
+++	cmovcq	16(%rsp),%r12
+++	cmovcq	24(%rsp),%r13
+++	cmovcq	32(%rsp),%r14
+++	cmovcq	40(%rsp),%r15
+++	jnc	.Lbeeu_reduction_loop
+++
+++
+++	subq	%r12,%r8
+++	sbbq	%r13,%r9
+++	sbbq	%r14,%r10
+++	sbbq	%r15,%r11
+++
+++.Lbeeu_save:
+++
+++	movq	0(%rsp),%rdi
+++
+++	movq	%r8,0(%rdi)
+++	movq	%r9,8(%rdi)
+++	movq	%r10,16(%rdi)
+++	movq	%r11,24(%rdi)
+++
+++
+++	movq	$1,%rax
+++	jmp	.Lbeeu_finish
+++
+++.Lbeeu_err:
+++
+++	xorq	%rax,%rax
+++
+++.Lbeeu_finish:
+++	addq	$80,%rsp
+++.cfi_adjust_cfa_offset	-80
+++	popq	%rsi
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	rsi
+++	popq	%rbx
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	rbx
+++	popq	%r15
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	r15
+++	popq	%r14
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	r14
+++	popq	%r13
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	r13
+++	popq	%r12
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	r12
+++	popq	%rbp
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	rbp
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++
+++.size	beeu_mod_inverse_vartime, .-beeu_mod_inverse_vartime
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/rdrand-x86_64.S b/linux-x86_64/ypto/fipsmodule/rdrand-x86_64.S
++new file mode 100644
++index 000000000..18d66f6f7
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/rdrand-x86_64.S
++@@ -0,0 +1,63 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++
+++
+++
+++
+++.globl	CRYPTO_rdrand
+++.hidden CRYPTO_rdrand
+++.type	CRYPTO_rdrand,@function
+++.align	16
+++CRYPTO_rdrand:
+++.cfi_startproc	
+++	xorq	%rax,%rax
+++.byte	72,15,199,242
+++
+++	adcq	%rax,%rax
+++	movq	%rdx,0(%rdi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	CRYPTO_rdrand,.-CRYPTO_rdrand
+++
+++
+++
+++
+++
+++.globl	CRYPTO_rdrand_multiple8_buf
+++.hidden CRYPTO_rdrand_multiple8_buf
+++.type	CRYPTO_rdrand_multiple8_buf,@function
+++.align	16
+++CRYPTO_rdrand_multiple8_buf:
+++.cfi_startproc	
+++	testq	%rsi,%rsi
+++	jz	.Lout
+++	movq	$8,%rdx
+++.Lloop:
+++.byte	72,15,199,241
+++	jnc	.Lerr
+++	movq	%rcx,0(%rdi)
+++	addq	%rdx,%rdi
+++	subq	%rdx,%rsi
+++	jnz	.Lloop
+++.Lout:
+++	movq	$1,%rax
+++	.byte	0xf3,0xc3
+++.Lerr:
+++	xorq	%rax,%rax
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	CRYPTO_rdrand_multiple8_buf,.-CRYPTO_rdrand_multiple8_buf
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/rsaz-avx2.S b/linux-x86_64/ypto/fipsmodule/rsaz-avx2.S
++new file mode 100644
++index 000000000..faccd484b
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/rsaz-avx2.S
++@@ -0,0 +1,1749 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++
+++.globl	rsaz_1024_sqr_avx2
+++.hidden rsaz_1024_sqr_avx2
+++.type	rsaz_1024_sqr_avx2,@function
+++.align	64
+++rsaz_1024_sqr_avx2:
+++.cfi_startproc	
+++	leaq	(%rsp),%rax
+++.cfi_def_cfa_register	%rax
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++	vzeroupper
+++	movq	%rax,%rbp
+++.cfi_def_cfa_register	%rbp
+++	movq	%rdx,%r13
+++	subq	$832,%rsp
+++	movq	%r13,%r15
+++	subq	$-128,%rdi
+++	subq	$-128,%rsi
+++	subq	$-128,%r13
+++
+++	andq	$4095,%r15
+++	addq	$320,%r15
+++	shrq	$12,%r15
+++	vpxor	%ymm9,%ymm9,%ymm9
+++	jz	.Lsqr_1024_no_n_copy
+++
+++
+++
+++
+++
+++	subq	$320,%rsp
+++	vmovdqu	0-128(%r13),%ymm0
+++	andq	$-2048,%rsp
+++	vmovdqu	32-128(%r13),%ymm1
+++	vmovdqu	64-128(%r13),%ymm2
+++	vmovdqu	96-128(%r13),%ymm3
+++	vmovdqu	128-128(%r13),%ymm4
+++	vmovdqu	160-128(%r13),%ymm5
+++	vmovdqu	192-128(%r13),%ymm6
+++	vmovdqu	224-128(%r13),%ymm7
+++	vmovdqu	256-128(%r13),%ymm8
+++	leaq	832+128(%rsp),%r13
+++	vmovdqu	%ymm0,0-128(%r13)
+++	vmovdqu	%ymm1,32-128(%r13)
+++	vmovdqu	%ymm2,64-128(%r13)
+++	vmovdqu	%ymm3,96-128(%r13)
+++	vmovdqu	%ymm4,128-128(%r13)
+++	vmovdqu	%ymm5,160-128(%r13)
+++	vmovdqu	%ymm6,192-128(%r13)
+++	vmovdqu	%ymm7,224-128(%r13)
+++	vmovdqu	%ymm8,256-128(%r13)
+++	vmovdqu	%ymm9,288-128(%r13)
+++
+++.Lsqr_1024_no_n_copy:
+++	andq	$-1024,%rsp
+++
+++	vmovdqu	32-128(%rsi),%ymm1
+++	vmovdqu	64-128(%rsi),%ymm2
+++	vmovdqu	96-128(%rsi),%ymm3
+++	vmovdqu	128-128(%rsi),%ymm4
+++	vmovdqu	160-128(%rsi),%ymm5
+++	vmovdqu	192-128(%rsi),%ymm6
+++	vmovdqu	224-128(%rsi),%ymm7
+++	vmovdqu	256-128(%rsi),%ymm8
+++
+++	leaq	192(%rsp),%rbx
+++	vmovdqu	.Land_mask(%rip),%ymm15
+++	jmp	.LOOP_GRANDE_SQR_1024
+++
+++.align	32
+++.LOOP_GRANDE_SQR_1024:
+++	leaq	576+128(%rsp),%r9
+++	leaq	448(%rsp),%r12
+++
+++
+++
+++
+++	vpaddq	%ymm1,%ymm1,%ymm1
+++	vpbroadcastq	0-128(%rsi),%ymm10
+++	vpaddq	%ymm2,%ymm2,%ymm2
+++	vmovdqa	%ymm1,0-128(%r9)
+++	vpaddq	%ymm3,%ymm3,%ymm3
+++	vmovdqa	%ymm2,32-128(%r9)
+++	vpaddq	%ymm4,%ymm4,%ymm4
+++	vmovdqa	%ymm3,64-128(%r9)
+++	vpaddq	%ymm5,%ymm5,%ymm5
+++	vmovdqa	%ymm4,96-128(%r9)
+++	vpaddq	%ymm6,%ymm6,%ymm6
+++	vmovdqa	%ymm5,128-128(%r9)
+++	vpaddq	%ymm7,%ymm7,%ymm7
+++	vmovdqa	%ymm6,160-128(%r9)
+++	vpaddq	%ymm8,%ymm8,%ymm8
+++	vmovdqa	%ymm7,192-128(%r9)
+++	vpxor	%ymm9,%ymm9,%ymm9
+++	vmovdqa	%ymm8,224-128(%r9)
+++
+++	vpmuludq	0-128(%rsi),%ymm10,%ymm0
+++	vpbroadcastq	32-128(%rsi),%ymm11
+++	vmovdqu	%ymm9,288-192(%rbx)
+++	vpmuludq	%ymm10,%ymm1,%ymm1
+++	vmovdqu	%ymm9,320-448(%r12)
+++	vpmuludq	%ymm10,%ymm2,%ymm2
+++	vmovdqu	%ymm9,352-448(%r12)
+++	vpmuludq	%ymm10,%ymm3,%ymm3
+++	vmovdqu	%ymm9,384-448(%r12)
+++	vpmuludq	%ymm10,%ymm4,%ymm4
+++	vmovdqu	%ymm9,416-448(%r12)
+++	vpmuludq	%ymm10,%ymm5,%ymm5
+++	vmovdqu	%ymm9,448-448(%r12)
+++	vpmuludq	%ymm10,%ymm6,%ymm6
+++	vmovdqu	%ymm9,480-448(%r12)
+++	vpmuludq	%ymm10,%ymm7,%ymm7
+++	vmovdqu	%ymm9,512-448(%r12)
+++	vpmuludq	%ymm10,%ymm8,%ymm8
+++	vpbroadcastq	64-128(%rsi),%ymm10
+++	vmovdqu	%ymm9,544-448(%r12)
+++
+++	movq	%rsi,%r15
+++	movl	$4,%r14d
+++	jmp	.Lsqr_entry_1024
+++.align	32
+++.LOOP_SQR_1024:
+++	vpbroadcastq	32-128(%r15),%ymm11
+++	vpmuludq	0-128(%rsi),%ymm10,%ymm0
+++	vpaddq	0-192(%rbx),%ymm0,%ymm0
+++	vpmuludq	0-128(%r9),%ymm10,%ymm1
+++	vpaddq	32-192(%rbx),%ymm1,%ymm1
+++	vpmuludq	32-128(%r9),%ymm10,%ymm2
+++	vpaddq	64-192(%rbx),%ymm2,%ymm2
+++	vpmuludq	64-128(%r9),%ymm10,%ymm3
+++	vpaddq	96-192(%rbx),%ymm3,%ymm3
+++	vpmuludq	96-128(%r9),%ymm10,%ymm4
+++	vpaddq	128-192(%rbx),%ymm4,%ymm4
+++	vpmuludq	128-128(%r9),%ymm10,%ymm5
+++	vpaddq	160-192(%rbx),%ymm5,%ymm5
+++	vpmuludq	160-128(%r9),%ymm10,%ymm6
+++	vpaddq	192-192(%rbx),%ymm6,%ymm6
+++	vpmuludq	192-128(%r9),%ymm10,%ymm7
+++	vpaddq	224-192(%rbx),%ymm7,%ymm7
+++	vpmuludq	224-128(%r9),%ymm10,%ymm8
+++	vpbroadcastq	64-128(%r15),%ymm10
+++	vpaddq	256-192(%rbx),%ymm8,%ymm8
+++.Lsqr_entry_1024:
+++	vmovdqu	%ymm0,0-192(%rbx)
+++	vmovdqu	%ymm1,32-192(%rbx)
+++
+++	vpmuludq	32-128(%rsi),%ymm11,%ymm12
+++	vpaddq	%ymm12,%ymm2,%ymm2
+++	vpmuludq	32-128(%r9),%ymm11,%ymm14
+++	vpaddq	%ymm14,%ymm3,%ymm3
+++	vpmuludq	64-128(%r9),%ymm11,%ymm13
+++	vpaddq	%ymm13,%ymm4,%ymm4
+++	vpmuludq	96-128(%r9),%ymm11,%ymm12
+++	vpaddq	%ymm12,%ymm5,%ymm5
+++	vpmuludq	128-128(%r9),%ymm11,%ymm14
+++	vpaddq	%ymm14,%ymm6,%ymm6
+++	vpmuludq	160-128(%r9),%ymm11,%ymm13
+++	vpaddq	%ymm13,%ymm7,%ymm7
+++	vpmuludq	192-128(%r9),%ymm11,%ymm12
+++	vpaddq	%ymm12,%ymm8,%ymm8
+++	vpmuludq	224-128(%r9),%ymm11,%ymm0
+++	vpbroadcastq	96-128(%r15),%ymm11
+++	vpaddq	288-192(%rbx),%ymm0,%ymm0
+++
+++	vmovdqu	%ymm2,64-192(%rbx)
+++	vmovdqu	%ymm3,96-192(%rbx)
+++
+++	vpmuludq	64-128(%rsi),%ymm10,%ymm13
+++	vpaddq	%ymm13,%ymm4,%ymm4
+++	vpmuludq	64-128(%r9),%ymm10,%ymm12
+++	vpaddq	%ymm12,%ymm5,%ymm5
+++	vpmuludq	96-128(%r9),%ymm10,%ymm14
+++	vpaddq	%ymm14,%ymm6,%ymm6
+++	vpmuludq	128-128(%r9),%ymm10,%ymm13
+++	vpaddq	%ymm13,%ymm7,%ymm7
+++	vpmuludq	160-128(%r9),%ymm10,%ymm12
+++	vpaddq	%ymm12,%ymm8,%ymm8
+++	vpmuludq	192-128(%r9),%ymm10,%ymm14
+++	vpaddq	%ymm14,%ymm0,%ymm0
+++	vpmuludq	224-128(%r9),%ymm10,%ymm1
+++	vpbroadcastq	128-128(%r15),%ymm10
+++	vpaddq	320-448(%r12),%ymm1,%ymm1
+++
+++	vmovdqu	%ymm4,128-192(%rbx)
+++	vmovdqu	%ymm5,160-192(%rbx)
+++
+++	vpmuludq	96-128(%rsi),%ymm11,%ymm12
+++	vpaddq	%ymm12,%ymm6,%ymm6
+++	vpmuludq	96-128(%r9),%ymm11,%ymm14
+++	vpaddq	%ymm14,%ymm7,%ymm7
+++	vpmuludq	128-128(%r9),%ymm11,%ymm13
+++	vpaddq	%ymm13,%ymm8,%ymm8
+++	vpmuludq	160-128(%r9),%ymm11,%ymm12
+++	vpaddq	%ymm12,%ymm0,%ymm0
+++	vpmuludq	192-128(%r9),%ymm11,%ymm14
+++	vpaddq	%ymm14,%ymm1,%ymm1
+++	vpmuludq	224-128(%r9),%ymm11,%ymm2
+++	vpbroadcastq	160-128(%r15),%ymm11
+++	vpaddq	352-448(%r12),%ymm2,%ymm2
+++
+++	vmovdqu	%ymm6,192-192(%rbx)
+++	vmovdqu	%ymm7,224-192(%rbx)
+++
+++	vpmuludq	128-128(%rsi),%ymm10,%ymm12
+++	vpaddq	%ymm12,%ymm8,%ymm8
+++	vpmuludq	128-128(%r9),%ymm10,%ymm14
+++	vpaddq	%ymm14,%ymm0,%ymm0
+++	vpmuludq	160-128(%r9),%ymm10,%ymm13
+++	vpaddq	%ymm13,%ymm1,%ymm1
+++	vpmuludq	192-128(%r9),%ymm10,%ymm12
+++	vpaddq	%ymm12,%ymm2,%ymm2
+++	vpmuludq	224-128(%r9),%ymm10,%ymm3
+++	vpbroadcastq	192-128(%r15),%ymm10
+++	vpaddq	384-448(%r12),%ymm3,%ymm3
+++
+++	vmovdqu	%ymm8,256-192(%rbx)
+++	vmovdqu	%ymm0,288-192(%rbx)
+++	leaq	8(%rbx),%rbx
+++
+++	vpmuludq	160-128(%rsi),%ymm11,%ymm13
+++	vpaddq	%ymm13,%ymm1,%ymm1
+++	vpmuludq	160-128(%r9),%ymm11,%ymm12
+++	vpaddq	%ymm12,%ymm2,%ymm2
+++	vpmuludq	192-128(%r9),%ymm11,%ymm14
+++	vpaddq	%ymm14,%ymm3,%ymm3
+++	vpmuludq	224-128(%r9),%ymm11,%ymm4
+++	vpbroadcastq	224-128(%r15),%ymm11
+++	vpaddq	416-448(%r12),%ymm4,%ymm4
+++
+++	vmovdqu	%ymm1,320-448(%r12)
+++	vmovdqu	%ymm2,352-448(%r12)
+++
+++	vpmuludq	192-128(%rsi),%ymm10,%ymm12
+++	vpaddq	%ymm12,%ymm3,%ymm3
+++	vpmuludq	192-128(%r9),%ymm10,%ymm14
+++	vpbroadcastq	256-128(%r15),%ymm0
+++	vpaddq	%ymm14,%ymm4,%ymm4
+++	vpmuludq	224-128(%r9),%ymm10,%ymm5
+++	vpbroadcastq	0+8-128(%r15),%ymm10
+++	vpaddq	448-448(%r12),%ymm5,%ymm5
+++
+++	vmovdqu	%ymm3,384-448(%r12)
+++	vmovdqu	%ymm4,416-448(%r12)
+++	leaq	8(%r15),%r15
+++
+++	vpmuludq	224-128(%rsi),%ymm11,%ymm12
+++	vpaddq	%ymm12,%ymm5,%ymm5
+++	vpmuludq	224-128(%r9),%ymm11,%ymm6
+++	vpaddq	480-448(%r12),%ymm6,%ymm6
+++
+++	vpmuludq	256-128(%rsi),%ymm0,%ymm7
+++	vmovdqu	%ymm5,448-448(%r12)
+++	vpaddq	512-448(%r12),%ymm7,%ymm7
+++	vmovdqu	%ymm6,480-448(%r12)
+++	vmovdqu	%ymm7,512-448(%r12)
+++	leaq	8(%r12),%r12
+++
+++	decl	%r14d
+++	jnz	.LOOP_SQR_1024
+++
+++	vmovdqu	256(%rsp),%ymm8
+++	vmovdqu	288(%rsp),%ymm1
+++	vmovdqu	320(%rsp),%ymm2
+++	leaq	192(%rsp),%rbx
+++
+++	vpsrlq	$29,%ymm8,%ymm14
+++	vpand	%ymm15,%ymm8,%ymm8
+++	vpsrlq	$29,%ymm1,%ymm11
+++	vpand	%ymm15,%ymm1,%ymm1
+++
+++	vpermq	$0x93,%ymm14,%ymm14
+++	vpxor	%ymm9,%ymm9,%ymm9
+++	vpermq	$0x93,%ymm11,%ymm11
+++
+++	vpblendd	$3,%ymm9,%ymm14,%ymm10
+++	vpblendd	$3,%ymm14,%ymm11,%ymm14
+++	vpaddq	%ymm10,%ymm8,%ymm8
+++	vpblendd	$3,%ymm11,%ymm9,%ymm11
+++	vpaddq	%ymm14,%ymm1,%ymm1
+++	vpaddq	%ymm11,%ymm2,%ymm2
+++	vmovdqu	%ymm1,288-192(%rbx)
+++	vmovdqu	%ymm2,320-192(%rbx)
+++
+++	movq	(%rsp),%rax
+++	movq	8(%rsp),%r10
+++	movq	16(%rsp),%r11
+++	movq	24(%rsp),%r12
+++	vmovdqu	32(%rsp),%ymm1
+++	vmovdqu	64-192(%rbx),%ymm2
+++	vmovdqu	96-192(%rbx),%ymm3
+++	vmovdqu	128-192(%rbx),%ymm4
+++	vmovdqu	160-192(%rbx),%ymm5
+++	vmovdqu	192-192(%rbx),%ymm6
+++	vmovdqu	224-192(%rbx),%ymm7
+++
+++	movq	%rax,%r9
+++	imull	%ecx,%eax
+++	andl	$0x1fffffff,%eax
+++	vmovd	%eax,%xmm12
+++
+++	movq	%rax,%rdx
+++	imulq	-128(%r13),%rax
+++	vpbroadcastq	%xmm12,%ymm12
+++	addq	%rax,%r9
+++	movq	%rdx,%rax
+++	imulq	8-128(%r13),%rax
+++	shrq	$29,%r9
+++	addq	%rax,%r10
+++	movq	%rdx,%rax
+++	imulq	16-128(%r13),%rax
+++	addq	%r9,%r10
+++	addq	%rax,%r11
+++	imulq	24-128(%r13),%rdx
+++	addq	%rdx,%r12
+++
+++	movq	%r10,%rax
+++	imull	%ecx,%eax
+++	andl	$0x1fffffff,%eax
+++
+++	movl	$9,%r14d
+++	jmp	.LOOP_REDUCE_1024
+++
+++.align	32
+++.LOOP_REDUCE_1024:
+++	vmovd	%eax,%xmm13
+++	vpbroadcastq	%xmm13,%ymm13
+++
+++	vpmuludq	32-128(%r13),%ymm12,%ymm10
+++	movq	%rax,%rdx
+++	imulq	-128(%r13),%rax
+++	vpaddq	%ymm10,%ymm1,%ymm1
+++	addq	%rax,%r10
+++	vpmuludq	64-128(%r13),%ymm12,%ymm14
+++	movq	%rdx,%rax
+++	imulq	8-128(%r13),%rax
+++	vpaddq	%ymm14,%ymm2,%ymm2
+++	vpmuludq	96-128(%r13),%ymm12,%ymm11
+++.byte	0x67
+++	addq	%rax,%r11
+++.byte	0x67
+++	movq	%rdx,%rax
+++	imulq	16-128(%r13),%rax
+++	shrq	$29,%r10
+++	vpaddq	%ymm11,%ymm3,%ymm3
+++	vpmuludq	128-128(%r13),%ymm12,%ymm10
+++	addq	%rax,%r12
+++	addq	%r10,%r11
+++	vpaddq	%ymm10,%ymm4,%ymm4
+++	vpmuludq	160-128(%r13),%ymm12,%ymm14
+++	movq	%r11,%rax
+++	imull	%ecx,%eax
+++	vpaddq	%ymm14,%ymm5,%ymm5
+++	vpmuludq	192-128(%r13),%ymm12,%ymm11
+++	andl	$0x1fffffff,%eax
+++	vpaddq	%ymm11,%ymm6,%ymm6
+++	vpmuludq	224-128(%r13),%ymm12,%ymm10
+++	vpaddq	%ymm10,%ymm7,%ymm7
+++	vpmuludq	256-128(%r13),%ymm12,%ymm14
+++	vmovd	%eax,%xmm12
+++
+++	vpaddq	%ymm14,%ymm8,%ymm8
+++
+++	vpbroadcastq	%xmm12,%ymm12
+++
+++	vpmuludq	32-8-128(%r13),%ymm13,%ymm11
+++	vmovdqu	96-8-128(%r13),%ymm14
+++	movq	%rax,%rdx
+++	imulq	-128(%r13),%rax
+++	vpaddq	%ymm11,%ymm1,%ymm1
+++	vpmuludq	64-8-128(%r13),%ymm13,%ymm10
+++	vmovdqu	128-8-128(%r13),%ymm11
+++	addq	%rax,%r11
+++	movq	%rdx,%rax
+++	imulq	8-128(%r13),%rax
+++	vpaddq	%ymm10,%ymm2,%ymm2
+++	addq	%r12,%rax
+++	shrq	$29,%r11
+++	vpmuludq	%ymm13,%ymm14,%ymm14
+++	vmovdqu	160-8-128(%r13),%ymm10
+++	addq	%r11,%rax
+++	vpaddq	%ymm14,%ymm3,%ymm3
+++	vpmuludq	%ymm13,%ymm11,%ymm11
+++	vmovdqu	192-8-128(%r13),%ymm14
+++.byte	0x67
+++	movq	%rax,%r12
+++	imull	%ecx,%eax
+++	vpaddq	%ymm11,%ymm4,%ymm4
+++	vpmuludq	%ymm13,%ymm10,%ymm10
+++.byte	0xc4,0x41,0x7e,0x6f,0x9d,0x58,0x00,0x00,0x00
+++	andl	$0x1fffffff,%eax
+++	vpaddq	%ymm10,%ymm5,%ymm5
+++	vpmuludq	%ymm13,%ymm14,%ymm14
+++	vmovdqu	256-8-128(%r13),%ymm10
+++	vpaddq	%ymm14,%ymm6,%ymm6
+++	vpmuludq	%ymm13,%ymm11,%ymm11
+++	vmovdqu	288-8-128(%r13),%ymm9
+++	vmovd	%eax,%xmm0
+++	imulq	-128(%r13),%rax
+++	vpaddq	%ymm11,%ymm7,%ymm7
+++	vpmuludq	%ymm13,%ymm10,%ymm10
+++	vmovdqu	32-16-128(%r13),%ymm14
+++	vpbroadcastq	%xmm0,%ymm0
+++	vpaddq	%ymm10,%ymm8,%ymm8
+++	vpmuludq	%ymm13,%ymm9,%ymm9
+++	vmovdqu	64-16-128(%r13),%ymm11
+++	addq	%rax,%r12
+++
+++	vmovdqu	32-24-128(%r13),%ymm13
+++	vpmuludq	%ymm12,%ymm14,%ymm14
+++	vmovdqu	96-16-128(%r13),%ymm10
+++	vpaddq	%ymm14,%ymm1,%ymm1
+++	vpmuludq	%ymm0,%ymm13,%ymm13
+++	vpmuludq	%ymm12,%ymm11,%ymm11
+++.byte	0xc4,0x41,0x7e,0x6f,0xb5,0xf0,0xff,0xff,0xff
+++	vpaddq	%ymm1,%ymm13,%ymm13
+++	vpaddq	%ymm11,%ymm2,%ymm2
+++	vpmuludq	%ymm12,%ymm10,%ymm10
+++	vmovdqu	160-16-128(%r13),%ymm11
+++.byte	0x67
+++	vmovq	%xmm13,%rax
+++	vmovdqu	%ymm13,(%rsp)
+++	vpaddq	%ymm10,%ymm3,%ymm3
+++	vpmuludq	%ymm12,%ymm14,%ymm14
+++	vmovdqu	192-16-128(%r13),%ymm10
+++	vpaddq	%ymm14,%ymm4,%ymm4
+++	vpmuludq	%ymm12,%ymm11,%ymm11
+++	vmovdqu	224-16-128(%r13),%ymm14
+++	vpaddq	%ymm11,%ymm5,%ymm5
+++	vpmuludq	%ymm12,%ymm10,%ymm10
+++	vmovdqu	256-16-128(%r13),%ymm11
+++	vpaddq	%ymm10,%ymm6,%ymm6
+++	vpmuludq	%ymm12,%ymm14,%ymm14
+++	shrq	$29,%r12
+++	vmovdqu	288-16-128(%r13),%ymm10
+++	addq	%r12,%rax
+++	vpaddq	%ymm14,%ymm7,%ymm7
+++	vpmuludq	%ymm12,%ymm11,%ymm11
+++
+++	movq	%rax,%r9
+++	imull	%ecx,%eax
+++	vpaddq	%ymm11,%ymm8,%ymm8
+++	vpmuludq	%ymm12,%ymm10,%ymm10
+++	andl	$0x1fffffff,%eax
+++	vmovd	%eax,%xmm12
+++	vmovdqu	96-24-128(%r13),%ymm11
+++.byte	0x67
+++	vpaddq	%ymm10,%ymm9,%ymm9
+++	vpbroadcastq	%xmm12,%ymm12
+++
+++	vpmuludq	64-24-128(%r13),%ymm0,%ymm14
+++	vmovdqu	128-24-128(%r13),%ymm10
+++	movq	%rax,%rdx
+++	imulq	-128(%r13),%rax
+++	movq	8(%rsp),%r10
+++	vpaddq	%ymm14,%ymm2,%ymm1
+++	vpmuludq	%ymm0,%ymm11,%ymm11
+++	vmovdqu	160-24-128(%r13),%ymm14
+++	addq	%rax,%r9
+++	movq	%rdx,%rax
+++	imulq	8-128(%r13),%rax
+++.byte	0x67
+++	shrq	$29,%r9
+++	movq	16(%rsp),%r11
+++	vpaddq	%ymm11,%ymm3,%ymm2
+++	vpmuludq	%ymm0,%ymm10,%ymm10
+++	vmovdqu	192-24-128(%r13),%ymm11
+++	addq	%rax,%r10
+++	movq	%rdx,%rax
+++	imulq	16-128(%r13),%rax
+++	vpaddq	%ymm10,%ymm4,%ymm3
+++	vpmuludq	%ymm0,%ymm14,%ymm14
+++	vmovdqu	224-24-128(%r13),%ymm10
+++	imulq	24-128(%r13),%rdx
+++	addq	%rax,%r11
+++	leaq	(%r9,%r10,1),%rax
+++	vpaddq	%ymm14,%ymm5,%ymm4
+++	vpmuludq	%ymm0,%ymm11,%ymm11
+++	vmovdqu	256-24-128(%r13),%ymm14
+++	movq	%rax,%r10
+++	imull	%ecx,%eax
+++	vpmuludq	%ymm0,%ymm10,%ymm10
+++	vpaddq	%ymm11,%ymm6,%ymm5
+++	vmovdqu	288-24-128(%r13),%ymm11
+++	andl	$0x1fffffff,%eax
+++	vpaddq	%ymm10,%ymm7,%ymm6
+++	vpmuludq	%ymm0,%ymm14,%ymm14
+++	addq	24(%rsp),%rdx
+++	vpaddq	%ymm14,%ymm8,%ymm7
+++	vpmuludq	%ymm0,%ymm11,%ymm11
+++	vpaddq	%ymm11,%ymm9,%ymm8
+++	vmovq	%r12,%xmm9
+++	movq	%rdx,%r12
+++
+++	decl	%r14d
+++	jnz	.LOOP_REDUCE_1024
+++	leaq	448(%rsp),%r12
+++	vpaddq	%ymm9,%ymm13,%ymm0
+++	vpxor	%ymm9,%ymm9,%ymm9
+++
+++	vpaddq	288-192(%rbx),%ymm0,%ymm0
+++	vpaddq	320-448(%r12),%ymm1,%ymm1
+++	vpaddq	352-448(%r12),%ymm2,%ymm2
+++	vpaddq	384-448(%r12),%ymm3,%ymm3
+++	vpaddq	416-448(%r12),%ymm4,%ymm4
+++	vpaddq	448-448(%r12),%ymm5,%ymm5
+++	vpaddq	480-448(%r12),%ymm6,%ymm6
+++	vpaddq	512-448(%r12),%ymm7,%ymm7
+++	vpaddq	544-448(%r12),%ymm8,%ymm8
+++
+++	vpsrlq	$29,%ymm0,%ymm14
+++	vpand	%ymm15,%ymm0,%ymm0
+++	vpsrlq	$29,%ymm1,%ymm11
+++	vpand	%ymm15,%ymm1,%ymm1
+++	vpsrlq	$29,%ymm2,%ymm12
+++	vpermq	$0x93,%ymm14,%ymm14
+++	vpand	%ymm15,%ymm2,%ymm2
+++	vpsrlq	$29,%ymm3,%ymm13
+++	vpermq	$0x93,%ymm11,%ymm11
+++	vpand	%ymm15,%ymm3,%ymm3
+++	vpermq	$0x93,%ymm12,%ymm12
+++
+++	vpblendd	$3,%ymm9,%ymm14,%ymm10
+++	vpermq	$0x93,%ymm13,%ymm13
+++	vpblendd	$3,%ymm14,%ymm11,%ymm14
+++	vpaddq	%ymm10,%ymm0,%ymm0
+++	vpblendd	$3,%ymm11,%ymm12,%ymm11
+++	vpaddq	%ymm14,%ymm1,%ymm1
+++	vpblendd	$3,%ymm12,%ymm13,%ymm12
+++	vpaddq	%ymm11,%ymm2,%ymm2
+++	vpblendd	$3,%ymm13,%ymm9,%ymm13
+++	vpaddq	%ymm12,%ymm3,%ymm3
+++	vpaddq	%ymm13,%ymm4,%ymm4
+++
+++	vpsrlq	$29,%ymm0,%ymm14
+++	vpand	%ymm15,%ymm0,%ymm0
+++	vpsrlq	$29,%ymm1,%ymm11
+++	vpand	%ymm15,%ymm1,%ymm1
+++	vpsrlq	$29,%ymm2,%ymm12
+++	vpermq	$0x93,%ymm14,%ymm14
+++	vpand	%ymm15,%ymm2,%ymm2
+++	vpsrlq	$29,%ymm3,%ymm13
+++	vpermq	$0x93,%ymm11,%ymm11
+++	vpand	%ymm15,%ymm3,%ymm3
+++	vpermq	$0x93,%ymm12,%ymm12
+++
+++	vpblendd	$3,%ymm9,%ymm14,%ymm10
+++	vpermq	$0x93,%ymm13,%ymm13
+++	vpblendd	$3,%ymm14,%ymm11,%ymm14
+++	vpaddq	%ymm10,%ymm0,%ymm0
+++	vpblendd	$3,%ymm11,%ymm12,%ymm11
+++	vpaddq	%ymm14,%ymm1,%ymm1
+++	vmovdqu	%ymm0,0-128(%rdi)
+++	vpblendd	$3,%ymm12,%ymm13,%ymm12
+++	vpaddq	%ymm11,%ymm2,%ymm2
+++	vmovdqu	%ymm1,32-128(%rdi)
+++	vpblendd	$3,%ymm13,%ymm9,%ymm13
+++	vpaddq	%ymm12,%ymm3,%ymm3
+++	vmovdqu	%ymm2,64-128(%rdi)
+++	vpaddq	%ymm13,%ymm4,%ymm4
+++	vmovdqu	%ymm3,96-128(%rdi)
+++	vpsrlq	$29,%ymm4,%ymm14
+++	vpand	%ymm15,%ymm4,%ymm4
+++	vpsrlq	$29,%ymm5,%ymm11
+++	vpand	%ymm15,%ymm5,%ymm5
+++	vpsrlq	$29,%ymm6,%ymm12
+++	vpermq	$0x93,%ymm14,%ymm14
+++	vpand	%ymm15,%ymm6,%ymm6
+++	vpsrlq	$29,%ymm7,%ymm13
+++	vpermq	$0x93,%ymm11,%ymm11
+++	vpand	%ymm15,%ymm7,%ymm7
+++	vpsrlq	$29,%ymm8,%ymm0
+++	vpermq	$0x93,%ymm12,%ymm12
+++	vpand	%ymm15,%ymm8,%ymm8
+++	vpermq	$0x93,%ymm13,%ymm13
+++
+++	vpblendd	$3,%ymm9,%ymm14,%ymm10
+++	vpermq	$0x93,%ymm0,%ymm0
+++	vpblendd	$3,%ymm14,%ymm11,%ymm14
+++	vpaddq	%ymm10,%ymm4,%ymm4
+++	vpblendd	$3,%ymm11,%ymm12,%ymm11
+++	vpaddq	%ymm14,%ymm5,%ymm5
+++	vpblendd	$3,%ymm12,%ymm13,%ymm12
+++	vpaddq	%ymm11,%ymm6,%ymm6
+++	vpblendd	$3,%ymm13,%ymm0,%ymm13
+++	vpaddq	%ymm12,%ymm7,%ymm7
+++	vpaddq	%ymm13,%ymm8,%ymm8
+++
+++	vpsrlq	$29,%ymm4,%ymm14
+++	vpand	%ymm15,%ymm4,%ymm4
+++	vpsrlq	$29,%ymm5,%ymm11
+++	vpand	%ymm15,%ymm5,%ymm5
+++	vpsrlq	$29,%ymm6,%ymm12
+++	vpermq	$0x93,%ymm14,%ymm14
+++	vpand	%ymm15,%ymm6,%ymm6
+++	vpsrlq	$29,%ymm7,%ymm13
+++	vpermq	$0x93,%ymm11,%ymm11
+++	vpand	%ymm15,%ymm7,%ymm7
+++	vpsrlq	$29,%ymm8,%ymm0
+++	vpermq	$0x93,%ymm12,%ymm12
+++	vpand	%ymm15,%ymm8,%ymm8
+++	vpermq	$0x93,%ymm13,%ymm13
+++
+++	vpblendd	$3,%ymm9,%ymm14,%ymm10
+++	vpermq	$0x93,%ymm0,%ymm0
+++	vpblendd	$3,%ymm14,%ymm11,%ymm14
+++	vpaddq	%ymm10,%ymm4,%ymm4
+++	vpblendd	$3,%ymm11,%ymm12,%ymm11
+++	vpaddq	%ymm14,%ymm5,%ymm5
+++	vmovdqu	%ymm4,128-128(%rdi)
+++	vpblendd	$3,%ymm12,%ymm13,%ymm12
+++	vpaddq	%ymm11,%ymm6,%ymm6
+++	vmovdqu	%ymm5,160-128(%rdi)
+++	vpblendd	$3,%ymm13,%ymm0,%ymm13
+++	vpaddq	%ymm12,%ymm7,%ymm7
+++	vmovdqu	%ymm6,192-128(%rdi)
+++	vpaddq	%ymm13,%ymm8,%ymm8
+++	vmovdqu	%ymm7,224-128(%rdi)
+++	vmovdqu	%ymm8,256-128(%rdi)
+++
+++	movq	%rdi,%rsi
+++	decl	%r8d
+++	jne	.LOOP_GRANDE_SQR_1024
+++
+++	vzeroall
+++	movq	%rbp,%rax
+++.cfi_def_cfa_register	%rax
+++	movq	-48(%rax),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rax),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rax),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rax),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rax),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rax),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rax),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lsqr_1024_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	rsaz_1024_sqr_avx2,.-rsaz_1024_sqr_avx2
+++.globl	rsaz_1024_mul_avx2
+++.hidden rsaz_1024_mul_avx2
+++.type	rsaz_1024_mul_avx2,@function
+++.align	64
+++rsaz_1024_mul_avx2:
+++.cfi_startproc	
+++	leaq	(%rsp),%rax
+++.cfi_def_cfa_register	%rax
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++	movq	%rax,%rbp
+++.cfi_def_cfa_register	%rbp
+++	vzeroall
+++	movq	%rdx,%r13
+++	subq	$64,%rsp
+++
+++
+++
+++
+++
+++
+++.byte	0x67,0x67
+++	movq	%rsi,%r15
+++	andq	$4095,%r15
+++	addq	$320,%r15
+++	shrq	$12,%r15
+++	movq	%rsi,%r15
+++	cmovnzq	%r13,%rsi
+++	cmovnzq	%r15,%r13
+++
+++	movq	%rcx,%r15
+++	subq	$-128,%rsi
+++	subq	$-128,%rcx
+++	subq	$-128,%rdi
+++
+++	andq	$4095,%r15
+++	addq	$320,%r15
+++.byte	0x67,0x67
+++	shrq	$12,%r15
+++	jz	.Lmul_1024_no_n_copy
+++
+++
+++
+++
+++
+++	subq	$320,%rsp
+++	vmovdqu	0-128(%rcx),%ymm0
+++	andq	$-512,%rsp
+++	vmovdqu	32-128(%rcx),%ymm1
+++	vmovdqu	64-128(%rcx),%ymm2
+++	vmovdqu	96-128(%rcx),%ymm3
+++	vmovdqu	128-128(%rcx),%ymm4
+++	vmovdqu	160-128(%rcx),%ymm5
+++	vmovdqu	192-128(%rcx),%ymm6
+++	vmovdqu	224-128(%rcx),%ymm7
+++	vmovdqu	256-128(%rcx),%ymm8
+++	leaq	64+128(%rsp),%rcx
+++	vmovdqu	%ymm0,0-128(%rcx)
+++	vpxor	%ymm0,%ymm0,%ymm0
+++	vmovdqu	%ymm1,32-128(%rcx)
+++	vpxor	%ymm1,%ymm1,%ymm1
+++	vmovdqu	%ymm2,64-128(%rcx)
+++	vpxor	%ymm2,%ymm2,%ymm2
+++	vmovdqu	%ymm3,96-128(%rcx)
+++	vpxor	%ymm3,%ymm3,%ymm3
+++	vmovdqu	%ymm4,128-128(%rcx)
+++	vpxor	%ymm4,%ymm4,%ymm4
+++	vmovdqu	%ymm5,160-128(%rcx)
+++	vpxor	%ymm5,%ymm5,%ymm5
+++	vmovdqu	%ymm6,192-128(%rcx)
+++	vpxor	%ymm6,%ymm6,%ymm6
+++	vmovdqu	%ymm7,224-128(%rcx)
+++	vpxor	%ymm7,%ymm7,%ymm7
+++	vmovdqu	%ymm8,256-128(%rcx)
+++	vmovdqa	%ymm0,%ymm8
+++	vmovdqu	%ymm9,288-128(%rcx)
+++.Lmul_1024_no_n_copy:
+++	andq	$-64,%rsp
+++
+++	movq	(%r13),%rbx
+++	vpbroadcastq	(%r13),%ymm10
+++	vmovdqu	%ymm0,(%rsp)
+++	xorq	%r9,%r9
+++.byte	0x67
+++	xorq	%r10,%r10
+++	xorq	%r11,%r11
+++	xorq	%r12,%r12
+++
+++	vmovdqu	.Land_mask(%rip),%ymm15
+++	movl	$9,%r14d
+++	vmovdqu	%ymm9,288-128(%rdi)
+++	jmp	.Loop_mul_1024
+++
+++.align	32
+++.Loop_mul_1024:
+++	vpsrlq	$29,%ymm3,%ymm9
+++	movq	%rbx,%rax
+++	imulq	-128(%rsi),%rax
+++	addq	%r9,%rax
+++	movq	%rbx,%r10
+++	imulq	8-128(%rsi),%r10
+++	addq	8(%rsp),%r10
+++
+++	movq	%rax,%r9
+++	imull	%r8d,%eax
+++	andl	$0x1fffffff,%eax
+++
+++	movq	%rbx,%r11
+++	imulq	16-128(%rsi),%r11
+++	addq	16(%rsp),%r11
+++
+++	movq	%rbx,%r12
+++	imulq	24-128(%rsi),%r12
+++	addq	24(%rsp),%r12
+++	vpmuludq	32-128(%rsi),%ymm10,%ymm0
+++	vmovd	%eax,%xmm11
+++	vpaddq	%ymm0,%ymm1,%ymm1
+++	vpmuludq	64-128(%rsi),%ymm10,%ymm12
+++	vpbroadcastq	%xmm11,%ymm11
+++	vpaddq	%ymm12,%ymm2,%ymm2
+++	vpmuludq	96-128(%rsi),%ymm10,%ymm13
+++	vpand	%ymm15,%ymm3,%ymm3
+++	vpaddq	%ymm13,%ymm3,%ymm3
+++	vpmuludq	128-128(%rsi),%ymm10,%ymm0
+++	vpaddq	%ymm0,%ymm4,%ymm4
+++	vpmuludq	160-128(%rsi),%ymm10,%ymm12
+++	vpaddq	%ymm12,%ymm5,%ymm5
+++	vpmuludq	192-128(%rsi),%ymm10,%ymm13
+++	vpaddq	%ymm13,%ymm6,%ymm6
+++	vpmuludq	224-128(%rsi),%ymm10,%ymm0
+++	vpermq	$0x93,%ymm9,%ymm9
+++	vpaddq	%ymm0,%ymm7,%ymm7
+++	vpmuludq	256-128(%rsi),%ymm10,%ymm12
+++	vpbroadcastq	8(%r13),%ymm10
+++	vpaddq	%ymm12,%ymm8,%ymm8
+++
+++	movq	%rax,%rdx
+++	imulq	-128(%rcx),%rax
+++	addq	%rax,%r9
+++	movq	%rdx,%rax
+++	imulq	8-128(%rcx),%rax
+++	addq	%rax,%r10
+++	movq	%rdx,%rax
+++	imulq	16-128(%rcx),%rax
+++	addq	%rax,%r11
+++	shrq	$29,%r9
+++	imulq	24-128(%rcx),%rdx
+++	addq	%rdx,%r12
+++	addq	%r9,%r10
+++
+++	vpmuludq	32-128(%rcx),%ymm11,%ymm13
+++	vmovq	%xmm10,%rbx
+++	vpaddq	%ymm13,%ymm1,%ymm1
+++	vpmuludq	64-128(%rcx),%ymm11,%ymm0
+++	vpaddq	%ymm0,%ymm2,%ymm2
+++	vpmuludq	96-128(%rcx),%ymm11,%ymm12
+++	vpaddq	%ymm12,%ymm3,%ymm3
+++	vpmuludq	128-128(%rcx),%ymm11,%ymm13
+++	vpaddq	%ymm13,%ymm4,%ymm4
+++	vpmuludq	160-128(%rcx),%ymm11,%ymm0
+++	vpaddq	%ymm0,%ymm5,%ymm5
+++	vpmuludq	192-128(%rcx),%ymm11,%ymm12
+++	vpaddq	%ymm12,%ymm6,%ymm6
+++	vpmuludq	224-128(%rcx),%ymm11,%ymm13
+++	vpblendd	$3,%ymm14,%ymm9,%ymm12
+++	vpaddq	%ymm13,%ymm7,%ymm7
+++	vpmuludq	256-128(%rcx),%ymm11,%ymm0
+++	vpaddq	%ymm12,%ymm3,%ymm3
+++	vpaddq	%ymm0,%ymm8,%ymm8
+++
+++	movq	%rbx,%rax
+++	imulq	-128(%rsi),%rax
+++	addq	%rax,%r10
+++	vmovdqu	-8+32-128(%rsi),%ymm12
+++	movq	%rbx,%rax
+++	imulq	8-128(%rsi),%rax
+++	addq	%rax,%r11
+++	vmovdqu	-8+64-128(%rsi),%ymm13
+++
+++	movq	%r10,%rax
+++	vpblendd	$0xfc,%ymm14,%ymm9,%ymm9
+++	imull	%r8d,%eax
+++	vpaddq	%ymm9,%ymm4,%ymm4
+++	andl	$0x1fffffff,%eax
+++
+++	imulq	16-128(%rsi),%rbx
+++	addq	%rbx,%r12
+++	vpmuludq	%ymm10,%ymm12,%ymm12
+++	vmovd	%eax,%xmm11
+++	vmovdqu	-8+96-128(%rsi),%ymm0
+++	vpaddq	%ymm12,%ymm1,%ymm1
+++	vpmuludq	%ymm10,%ymm13,%ymm13
+++	vpbroadcastq	%xmm11,%ymm11
+++	vmovdqu	-8+128-128(%rsi),%ymm12
+++	vpaddq	%ymm13,%ymm2,%ymm2
+++	vpmuludq	%ymm10,%ymm0,%ymm0
+++	vmovdqu	-8+160-128(%rsi),%ymm13
+++	vpaddq	%ymm0,%ymm3,%ymm3
+++	vpmuludq	%ymm10,%ymm12,%ymm12
+++	vmovdqu	-8+192-128(%rsi),%ymm0
+++	vpaddq	%ymm12,%ymm4,%ymm4
+++	vpmuludq	%ymm10,%ymm13,%ymm13
+++	vmovdqu	-8+224-128(%rsi),%ymm12
+++	vpaddq	%ymm13,%ymm5,%ymm5
+++	vpmuludq	%ymm10,%ymm0,%ymm0
+++	vmovdqu	-8+256-128(%rsi),%ymm13
+++	vpaddq	%ymm0,%ymm6,%ymm6
+++	vpmuludq	%ymm10,%ymm12,%ymm12
+++	vmovdqu	-8+288-128(%rsi),%ymm9
+++	vpaddq	%ymm12,%ymm7,%ymm7
+++	vpmuludq	%ymm10,%ymm13,%ymm13
+++	vpaddq	%ymm13,%ymm8,%ymm8
+++	vpmuludq	%ymm10,%ymm9,%ymm9
+++	vpbroadcastq	16(%r13),%ymm10
+++
+++	movq	%rax,%rdx
+++	imulq	-128(%rcx),%rax
+++	addq	%rax,%r10
+++	vmovdqu	-8+32-128(%rcx),%ymm0
+++	movq	%rdx,%rax
+++	imulq	8-128(%rcx),%rax
+++	addq	%rax,%r11
+++	vmovdqu	-8+64-128(%rcx),%ymm12
+++	shrq	$29,%r10
+++	imulq	16-128(%rcx),%rdx
+++	addq	%rdx,%r12
+++	addq	%r10,%r11
+++
+++	vpmuludq	%ymm11,%ymm0,%ymm0
+++	vmovq	%xmm10,%rbx
+++	vmovdqu	-8+96-128(%rcx),%ymm13
+++	vpaddq	%ymm0,%ymm1,%ymm1
+++	vpmuludq	%ymm11,%ymm12,%ymm12
+++	vmovdqu	-8+128-128(%rcx),%ymm0
+++	vpaddq	%ymm12,%ymm2,%ymm2
+++	vpmuludq	%ymm11,%ymm13,%ymm13
+++	vmovdqu	-8+160-128(%rcx),%ymm12
+++	vpaddq	%ymm13,%ymm3,%ymm3
+++	vpmuludq	%ymm11,%ymm0,%ymm0
+++	vmovdqu	-8+192-128(%rcx),%ymm13
+++	vpaddq	%ymm0,%ymm4,%ymm4
+++	vpmuludq	%ymm11,%ymm12,%ymm12
+++	vmovdqu	-8+224-128(%rcx),%ymm0
+++	vpaddq	%ymm12,%ymm5,%ymm5
+++	vpmuludq	%ymm11,%ymm13,%ymm13
+++	vmovdqu	-8+256-128(%rcx),%ymm12
+++	vpaddq	%ymm13,%ymm6,%ymm6
+++	vpmuludq	%ymm11,%ymm0,%ymm0
+++	vmovdqu	-8+288-128(%rcx),%ymm13
+++	vpaddq	%ymm0,%ymm7,%ymm7
+++	vpmuludq	%ymm11,%ymm12,%ymm12
+++	vpaddq	%ymm12,%ymm8,%ymm8
+++	vpmuludq	%ymm11,%ymm13,%ymm13
+++	vpaddq	%ymm13,%ymm9,%ymm9
+++
+++	vmovdqu	-16+32-128(%rsi),%ymm0
+++	movq	%rbx,%rax
+++	imulq	-128(%rsi),%rax
+++	addq	%r11,%rax
+++
+++	vmovdqu	-16+64-128(%rsi),%ymm12
+++	movq	%rax,%r11
+++	imull	%r8d,%eax
+++	andl	$0x1fffffff,%eax
+++
+++	imulq	8-128(%rsi),%rbx
+++	addq	%rbx,%r12
+++	vpmuludq	%ymm10,%ymm0,%ymm0
+++	vmovd	%eax,%xmm11
+++	vmovdqu	-16+96-128(%rsi),%ymm13
+++	vpaddq	%ymm0,%ymm1,%ymm1
+++	vpmuludq	%ymm10,%ymm12,%ymm12
+++	vpbroadcastq	%xmm11,%ymm11
+++	vmovdqu	-16+128-128(%rsi),%ymm0
+++	vpaddq	%ymm12,%ymm2,%ymm2
+++	vpmuludq	%ymm10,%ymm13,%ymm13
+++	vmovdqu	-16+160-128(%rsi),%ymm12
+++	vpaddq	%ymm13,%ymm3,%ymm3
+++	vpmuludq	%ymm10,%ymm0,%ymm0
+++	vmovdqu	-16+192-128(%rsi),%ymm13
+++	vpaddq	%ymm0,%ymm4,%ymm4
+++	vpmuludq	%ymm10,%ymm12,%ymm12
+++	vmovdqu	-16+224-128(%rsi),%ymm0
+++	vpaddq	%ymm12,%ymm5,%ymm5
+++	vpmuludq	%ymm10,%ymm13,%ymm13
+++	vmovdqu	-16+256-128(%rsi),%ymm12
+++	vpaddq	%ymm13,%ymm6,%ymm6
+++	vpmuludq	%ymm10,%ymm0,%ymm0
+++	vmovdqu	-16+288-128(%rsi),%ymm13
+++	vpaddq	%ymm0,%ymm7,%ymm7
+++	vpmuludq	%ymm10,%ymm12,%ymm12
+++	vpaddq	%ymm12,%ymm8,%ymm8
+++	vpmuludq	%ymm10,%ymm13,%ymm13
+++	vpbroadcastq	24(%r13),%ymm10
+++	vpaddq	%ymm13,%ymm9,%ymm9
+++
+++	vmovdqu	-16+32-128(%rcx),%ymm0
+++	movq	%rax,%rdx
+++	imulq	-128(%rcx),%rax
+++	addq	%rax,%r11
+++	vmovdqu	-16+64-128(%rcx),%ymm12
+++	imulq	8-128(%rcx),%rdx
+++	addq	%rdx,%r12
+++	shrq	$29,%r11
+++
+++	vpmuludq	%ymm11,%ymm0,%ymm0
+++	vmovq	%xmm10,%rbx
+++	vmovdqu	-16+96-128(%rcx),%ymm13
+++	vpaddq	%ymm0,%ymm1,%ymm1
+++	vpmuludq	%ymm11,%ymm12,%ymm12
+++	vmovdqu	-16+128-128(%rcx),%ymm0
+++	vpaddq	%ymm12,%ymm2,%ymm2
+++	vpmuludq	%ymm11,%ymm13,%ymm13
+++	vmovdqu	-16+160-128(%rcx),%ymm12
+++	vpaddq	%ymm13,%ymm3,%ymm3
+++	vpmuludq	%ymm11,%ymm0,%ymm0
+++	vmovdqu	-16+192-128(%rcx),%ymm13
+++	vpaddq	%ymm0,%ymm4,%ymm4
+++	vpmuludq	%ymm11,%ymm12,%ymm12
+++	vmovdqu	-16+224-128(%rcx),%ymm0
+++	vpaddq	%ymm12,%ymm5,%ymm5
+++	vpmuludq	%ymm11,%ymm13,%ymm13
+++	vmovdqu	-16+256-128(%rcx),%ymm12
+++	vpaddq	%ymm13,%ymm6,%ymm6
+++	vpmuludq	%ymm11,%ymm0,%ymm0
+++	vmovdqu	-16+288-128(%rcx),%ymm13
+++	vpaddq	%ymm0,%ymm7,%ymm7
+++	vpmuludq	%ymm11,%ymm12,%ymm12
+++	vmovdqu	-24+32-128(%rsi),%ymm0
+++	vpaddq	%ymm12,%ymm8,%ymm8
+++	vpmuludq	%ymm11,%ymm13,%ymm13
+++	vmovdqu	-24+64-128(%rsi),%ymm12
+++	vpaddq	%ymm13,%ymm9,%ymm9
+++
+++	addq	%r11,%r12
+++	imulq	-128(%rsi),%rbx
+++	addq	%rbx,%r12
+++
+++	movq	%r12,%rax
+++	imull	%r8d,%eax
+++	andl	$0x1fffffff,%eax
+++
+++	vpmuludq	%ymm10,%ymm0,%ymm0
+++	vmovd	%eax,%xmm11
+++	vmovdqu	-24+96-128(%rsi),%ymm13
+++	vpaddq	%ymm0,%ymm1,%ymm1
+++	vpmuludq	%ymm10,%ymm12,%ymm12
+++	vpbroadcastq	%xmm11,%ymm11
+++	vmovdqu	-24+128-128(%rsi),%ymm0
+++	vpaddq	%ymm12,%ymm2,%ymm2
+++	vpmuludq	%ymm10,%ymm13,%ymm13
+++	vmovdqu	-24+160-128(%rsi),%ymm12
+++	vpaddq	%ymm13,%ymm3,%ymm3
+++	vpmuludq	%ymm10,%ymm0,%ymm0
+++	vmovdqu	-24+192-128(%rsi),%ymm13
+++	vpaddq	%ymm0,%ymm4,%ymm4
+++	vpmuludq	%ymm10,%ymm12,%ymm12
+++	vmovdqu	-24+224-128(%rsi),%ymm0
+++	vpaddq	%ymm12,%ymm5,%ymm5
+++	vpmuludq	%ymm10,%ymm13,%ymm13
+++	vmovdqu	-24+256-128(%rsi),%ymm12
+++	vpaddq	%ymm13,%ymm6,%ymm6
+++	vpmuludq	%ymm10,%ymm0,%ymm0
+++	vmovdqu	-24+288-128(%rsi),%ymm13
+++	vpaddq	%ymm0,%ymm7,%ymm7
+++	vpmuludq	%ymm10,%ymm12,%ymm12
+++	vpaddq	%ymm12,%ymm8,%ymm8
+++	vpmuludq	%ymm10,%ymm13,%ymm13
+++	vpbroadcastq	32(%r13),%ymm10
+++	vpaddq	%ymm13,%ymm9,%ymm9
+++	addq	$32,%r13
+++
+++	vmovdqu	-24+32-128(%rcx),%ymm0
+++	imulq	-128(%rcx),%rax
+++	addq	%rax,%r12
+++	shrq	$29,%r12
+++
+++	vmovdqu	-24+64-128(%rcx),%ymm12
+++	vpmuludq	%ymm11,%ymm0,%ymm0
+++	vmovq	%xmm10,%rbx
+++	vmovdqu	-24+96-128(%rcx),%ymm13
+++	vpaddq	%ymm0,%ymm1,%ymm0
+++	vpmuludq	%ymm11,%ymm12,%ymm12
+++	vmovdqu	%ymm0,(%rsp)
+++	vpaddq	%ymm12,%ymm2,%ymm1
+++	vmovdqu	-24+128-128(%rcx),%ymm0
+++	vpmuludq	%ymm11,%ymm13,%ymm13
+++	vmovdqu	-24+160-128(%rcx),%ymm12
+++	vpaddq	%ymm13,%ymm3,%ymm2
+++	vpmuludq	%ymm11,%ymm0,%ymm0
+++	vmovdqu	-24+192-128(%rcx),%ymm13
+++	vpaddq	%ymm0,%ymm4,%ymm3
+++	vpmuludq	%ymm11,%ymm12,%ymm12
+++	vmovdqu	-24+224-128(%rcx),%ymm0
+++	vpaddq	%ymm12,%ymm5,%ymm4
+++	vpmuludq	%ymm11,%ymm13,%ymm13
+++	vmovdqu	-24+256-128(%rcx),%ymm12
+++	vpaddq	%ymm13,%ymm6,%ymm5
+++	vpmuludq	%ymm11,%ymm0,%ymm0
+++	vmovdqu	-24+288-128(%rcx),%ymm13
+++	movq	%r12,%r9
+++	vpaddq	%ymm0,%ymm7,%ymm6
+++	vpmuludq	%ymm11,%ymm12,%ymm12
+++	addq	(%rsp),%r9
+++	vpaddq	%ymm12,%ymm8,%ymm7
+++	vpmuludq	%ymm11,%ymm13,%ymm13
+++	vmovq	%r12,%xmm12
+++	vpaddq	%ymm13,%ymm9,%ymm8
+++
+++	decl	%r14d
+++	jnz	.Loop_mul_1024
+++	vpaddq	(%rsp),%ymm12,%ymm0
+++
+++	vpsrlq	$29,%ymm0,%ymm12
+++	vpand	%ymm15,%ymm0,%ymm0
+++	vpsrlq	$29,%ymm1,%ymm13
+++	vpand	%ymm15,%ymm1,%ymm1
+++	vpsrlq	$29,%ymm2,%ymm10
+++	vpermq	$0x93,%ymm12,%ymm12
+++	vpand	%ymm15,%ymm2,%ymm2
+++	vpsrlq	$29,%ymm3,%ymm11
+++	vpermq	$0x93,%ymm13,%ymm13
+++	vpand	%ymm15,%ymm3,%ymm3
+++
+++	vpblendd	$3,%ymm14,%ymm12,%ymm9
+++	vpermq	$0x93,%ymm10,%ymm10
+++	vpblendd	$3,%ymm12,%ymm13,%ymm12
+++	vpermq	$0x93,%ymm11,%ymm11
+++	vpaddq	%ymm9,%ymm0,%ymm0
+++	vpblendd	$3,%ymm13,%ymm10,%ymm13
+++	vpaddq	%ymm12,%ymm1,%ymm1
+++	vpblendd	$3,%ymm10,%ymm11,%ymm10
+++	vpaddq	%ymm13,%ymm2,%ymm2
+++	vpblendd	$3,%ymm11,%ymm14,%ymm11
+++	vpaddq	%ymm10,%ymm3,%ymm3
+++	vpaddq	%ymm11,%ymm4,%ymm4
+++
+++	vpsrlq	$29,%ymm0,%ymm12
+++	vpand	%ymm15,%ymm0,%ymm0
+++	vpsrlq	$29,%ymm1,%ymm13
+++	vpand	%ymm15,%ymm1,%ymm1
+++	vpsrlq	$29,%ymm2,%ymm10
+++	vpermq	$0x93,%ymm12,%ymm12
+++	vpand	%ymm15,%ymm2,%ymm2
+++	vpsrlq	$29,%ymm3,%ymm11
+++	vpermq	$0x93,%ymm13,%ymm13
+++	vpand	%ymm15,%ymm3,%ymm3
+++	vpermq	$0x93,%ymm10,%ymm10
+++
+++	vpblendd	$3,%ymm14,%ymm12,%ymm9
+++	vpermq	$0x93,%ymm11,%ymm11
+++	vpblendd	$3,%ymm12,%ymm13,%ymm12
+++	vpaddq	%ymm9,%ymm0,%ymm0
+++	vpblendd	$3,%ymm13,%ymm10,%ymm13
+++	vpaddq	%ymm12,%ymm1,%ymm1
+++	vpblendd	$3,%ymm10,%ymm11,%ymm10
+++	vpaddq	%ymm13,%ymm2,%ymm2
+++	vpblendd	$3,%ymm11,%ymm14,%ymm11
+++	vpaddq	%ymm10,%ymm3,%ymm3
+++	vpaddq	%ymm11,%ymm4,%ymm4
+++
+++	vmovdqu	%ymm0,0-128(%rdi)
+++	vmovdqu	%ymm1,32-128(%rdi)
+++	vmovdqu	%ymm2,64-128(%rdi)
+++	vmovdqu	%ymm3,96-128(%rdi)
+++	vpsrlq	$29,%ymm4,%ymm12
+++	vpand	%ymm15,%ymm4,%ymm4
+++	vpsrlq	$29,%ymm5,%ymm13
+++	vpand	%ymm15,%ymm5,%ymm5
+++	vpsrlq	$29,%ymm6,%ymm10
+++	vpermq	$0x93,%ymm12,%ymm12
+++	vpand	%ymm15,%ymm6,%ymm6
+++	vpsrlq	$29,%ymm7,%ymm11
+++	vpermq	$0x93,%ymm13,%ymm13
+++	vpand	%ymm15,%ymm7,%ymm7
+++	vpsrlq	$29,%ymm8,%ymm0
+++	vpermq	$0x93,%ymm10,%ymm10
+++	vpand	%ymm15,%ymm8,%ymm8
+++	vpermq	$0x93,%ymm11,%ymm11
+++
+++	vpblendd	$3,%ymm14,%ymm12,%ymm9
+++	vpermq	$0x93,%ymm0,%ymm0
+++	vpblendd	$3,%ymm12,%ymm13,%ymm12
+++	vpaddq	%ymm9,%ymm4,%ymm4
+++	vpblendd	$3,%ymm13,%ymm10,%ymm13
+++	vpaddq	%ymm12,%ymm5,%ymm5
+++	vpblendd	$3,%ymm10,%ymm11,%ymm10
+++	vpaddq	%ymm13,%ymm6,%ymm6
+++	vpblendd	$3,%ymm11,%ymm0,%ymm11
+++	vpaddq	%ymm10,%ymm7,%ymm7
+++	vpaddq	%ymm11,%ymm8,%ymm8
+++
+++	vpsrlq	$29,%ymm4,%ymm12
+++	vpand	%ymm15,%ymm4,%ymm4
+++	vpsrlq	$29,%ymm5,%ymm13
+++	vpand	%ymm15,%ymm5,%ymm5
+++	vpsrlq	$29,%ymm6,%ymm10
+++	vpermq	$0x93,%ymm12,%ymm12
+++	vpand	%ymm15,%ymm6,%ymm6
+++	vpsrlq	$29,%ymm7,%ymm11
+++	vpermq	$0x93,%ymm13,%ymm13
+++	vpand	%ymm15,%ymm7,%ymm7
+++	vpsrlq	$29,%ymm8,%ymm0
+++	vpermq	$0x93,%ymm10,%ymm10
+++	vpand	%ymm15,%ymm8,%ymm8
+++	vpermq	$0x93,%ymm11,%ymm11
+++
+++	vpblendd	$3,%ymm14,%ymm12,%ymm9
+++	vpermq	$0x93,%ymm0,%ymm0
+++	vpblendd	$3,%ymm12,%ymm13,%ymm12
+++	vpaddq	%ymm9,%ymm4,%ymm4
+++	vpblendd	$3,%ymm13,%ymm10,%ymm13
+++	vpaddq	%ymm12,%ymm5,%ymm5
+++	vpblendd	$3,%ymm10,%ymm11,%ymm10
+++	vpaddq	%ymm13,%ymm6,%ymm6
+++	vpblendd	$3,%ymm11,%ymm0,%ymm11
+++	vpaddq	%ymm10,%ymm7,%ymm7
+++	vpaddq	%ymm11,%ymm8,%ymm8
+++
+++	vmovdqu	%ymm4,128-128(%rdi)
+++	vmovdqu	%ymm5,160-128(%rdi)
+++	vmovdqu	%ymm6,192-128(%rdi)
+++	vmovdqu	%ymm7,224-128(%rdi)
+++	vmovdqu	%ymm8,256-128(%rdi)
+++	vzeroupper
+++
+++	movq	%rbp,%rax
+++.cfi_def_cfa_register	%rax
+++	movq	-48(%rax),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rax),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rax),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rax),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rax),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rax),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rax),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lmul_1024_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	rsaz_1024_mul_avx2,.-rsaz_1024_mul_avx2
+++.globl	rsaz_1024_red2norm_avx2
+++.hidden rsaz_1024_red2norm_avx2
+++.type	rsaz_1024_red2norm_avx2,@function
+++.align	32
+++rsaz_1024_red2norm_avx2:
+++.cfi_startproc	
+++	subq	$-128,%rsi
+++	xorq	%rax,%rax
+++	movq	-128(%rsi),%r8
+++	movq	-120(%rsi),%r9
+++	movq	-112(%rsi),%r10
+++	shlq	$0,%r8
+++	shlq	$29,%r9
+++	movq	%r10,%r11
+++	shlq	$58,%r10
+++	shrq	$6,%r11
+++	addq	%r8,%rax
+++	addq	%r9,%rax
+++	addq	%r10,%rax
+++	adcq	$0,%r11
+++	movq	%rax,0(%rdi)
+++	movq	%r11,%rax
+++	movq	-104(%rsi),%r8
+++	movq	-96(%rsi),%r9
+++	shlq	$23,%r8
+++	movq	%r9,%r10
+++	shlq	$52,%r9
+++	shrq	$12,%r10
+++	addq	%r8,%rax
+++	addq	%r9,%rax
+++	adcq	$0,%r10
+++	movq	%rax,8(%rdi)
+++	movq	%r10,%rax
+++	movq	-88(%rsi),%r11
+++	movq	-80(%rsi),%r8
+++	shlq	$17,%r11
+++	movq	%r8,%r9
+++	shlq	$46,%r8
+++	shrq	$18,%r9
+++	addq	%r11,%rax
+++	addq	%r8,%rax
+++	adcq	$0,%r9
+++	movq	%rax,16(%rdi)
+++	movq	%r9,%rax
+++	movq	-72(%rsi),%r10
+++	movq	-64(%rsi),%r11
+++	shlq	$11,%r10
+++	movq	%r11,%r8
+++	shlq	$40,%r11
+++	shrq	$24,%r8
+++	addq	%r10,%rax
+++	addq	%r11,%rax
+++	adcq	$0,%r8
+++	movq	%rax,24(%rdi)
+++	movq	%r8,%rax
+++	movq	-56(%rsi),%r9
+++	movq	-48(%rsi),%r10
+++	movq	-40(%rsi),%r11
+++	shlq	$5,%r9
+++	shlq	$34,%r10
+++	movq	%r11,%r8
+++	shlq	$63,%r11
+++	shrq	$1,%r8
+++	addq	%r9,%rax
+++	addq	%r10,%rax
+++	addq	%r11,%rax
+++	adcq	$0,%r8
+++	movq	%rax,32(%rdi)
+++	movq	%r8,%rax
+++	movq	-32(%rsi),%r9
+++	movq	-24(%rsi),%r10
+++	shlq	$28,%r9
+++	movq	%r10,%r11
+++	shlq	$57,%r10
+++	shrq	$7,%r11
+++	addq	%r9,%rax
+++	addq	%r10,%rax
+++	adcq	$0,%r11
+++	movq	%rax,40(%rdi)
+++	movq	%r11,%rax
+++	movq	-16(%rsi),%r8
+++	movq	-8(%rsi),%r9
+++	shlq	$22,%r8
+++	movq	%r9,%r10
+++	shlq	$51,%r9
+++	shrq	$13,%r10
+++	addq	%r8,%rax
+++	addq	%r9,%rax
+++	adcq	$0,%r10
+++	movq	%rax,48(%rdi)
+++	movq	%r10,%rax
+++	movq	0(%rsi),%r11
+++	movq	8(%rsi),%r8
+++	shlq	$16,%r11
+++	movq	%r8,%r9
+++	shlq	$45,%r8
+++	shrq	$19,%r9
+++	addq	%r11,%rax
+++	addq	%r8,%rax
+++	adcq	$0,%r9
+++	movq	%rax,56(%rdi)
+++	movq	%r9,%rax
+++	movq	16(%rsi),%r10
+++	movq	24(%rsi),%r11
+++	shlq	$10,%r10
+++	movq	%r11,%r8
+++	shlq	$39,%r11
+++	shrq	$25,%r8
+++	addq	%r10,%rax
+++	addq	%r11,%rax
+++	adcq	$0,%r8
+++	movq	%rax,64(%rdi)
+++	movq	%r8,%rax
+++	movq	32(%rsi),%r9
+++	movq	40(%rsi),%r10
+++	movq	48(%rsi),%r11
+++	shlq	$4,%r9
+++	shlq	$33,%r10
+++	movq	%r11,%r8
+++	shlq	$62,%r11
+++	shrq	$2,%r8
+++	addq	%r9,%rax
+++	addq	%r10,%rax
+++	addq	%r11,%rax
+++	adcq	$0,%r8
+++	movq	%rax,72(%rdi)
+++	movq	%r8,%rax
+++	movq	56(%rsi),%r9
+++	movq	64(%rsi),%r10
+++	shlq	$27,%r9
+++	movq	%r10,%r11
+++	shlq	$56,%r10
+++	shrq	$8,%r11
+++	addq	%r9,%rax
+++	addq	%r10,%rax
+++	adcq	$0,%r11
+++	movq	%rax,80(%rdi)
+++	movq	%r11,%rax
+++	movq	72(%rsi),%r8
+++	movq	80(%rsi),%r9
+++	shlq	$21,%r8
+++	movq	%r9,%r10
+++	shlq	$50,%r9
+++	shrq	$14,%r10
+++	addq	%r8,%rax
+++	addq	%r9,%rax
+++	adcq	$0,%r10
+++	movq	%rax,88(%rdi)
+++	movq	%r10,%rax
+++	movq	88(%rsi),%r11
+++	movq	96(%rsi),%r8
+++	shlq	$15,%r11
+++	movq	%r8,%r9
+++	shlq	$44,%r8
+++	shrq	$20,%r9
+++	addq	%r11,%rax
+++	addq	%r8,%rax
+++	adcq	$0,%r9
+++	movq	%rax,96(%rdi)
+++	movq	%r9,%rax
+++	movq	104(%rsi),%r10
+++	movq	112(%rsi),%r11
+++	shlq	$9,%r10
+++	movq	%r11,%r8
+++	shlq	$38,%r11
+++	shrq	$26,%r8
+++	addq	%r10,%rax
+++	addq	%r11,%rax
+++	adcq	$0,%r8
+++	movq	%rax,104(%rdi)
+++	movq	%r8,%rax
+++	movq	120(%rsi),%r9
+++	movq	128(%rsi),%r10
+++	movq	136(%rsi),%r11
+++	shlq	$3,%r9
+++	shlq	$32,%r10
+++	movq	%r11,%r8
+++	shlq	$61,%r11
+++	shrq	$3,%r8
+++	addq	%r9,%rax
+++	addq	%r10,%rax
+++	addq	%r11,%rax
+++	adcq	$0,%r8
+++	movq	%rax,112(%rdi)
+++	movq	%r8,%rax
+++	movq	144(%rsi),%r9
+++	movq	152(%rsi),%r10
+++	shlq	$26,%r9
+++	movq	%r10,%r11
+++	shlq	$55,%r10
+++	shrq	$9,%r11
+++	addq	%r9,%rax
+++	addq	%r10,%rax
+++	adcq	$0,%r11
+++	movq	%rax,120(%rdi)
+++	movq	%r11,%rax
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	rsaz_1024_red2norm_avx2,.-rsaz_1024_red2norm_avx2
+++
+++.globl	rsaz_1024_norm2red_avx2
+++.hidden rsaz_1024_norm2red_avx2
+++.type	rsaz_1024_norm2red_avx2,@function
+++.align	32
+++rsaz_1024_norm2red_avx2:
+++.cfi_startproc	
+++	subq	$-128,%rdi
+++	movq	(%rsi),%r8
+++	movl	$0x1fffffff,%eax
+++	movq	8(%rsi),%r9
+++	movq	%r8,%r11
+++	shrq	$0,%r11
+++	andq	%rax,%r11
+++	movq	%r11,-128(%rdi)
+++	movq	%r8,%r10
+++	shrq	$29,%r10
+++	andq	%rax,%r10
+++	movq	%r10,-120(%rdi)
+++	shrdq	$58,%r9,%r8
+++	andq	%rax,%r8
+++	movq	%r8,-112(%rdi)
+++	movq	16(%rsi),%r10
+++	movq	%r9,%r8
+++	shrq	$23,%r8
+++	andq	%rax,%r8
+++	movq	%r8,-104(%rdi)
+++	shrdq	$52,%r10,%r9
+++	andq	%rax,%r9
+++	movq	%r9,-96(%rdi)
+++	movq	24(%rsi),%r11
+++	movq	%r10,%r9
+++	shrq	$17,%r9
+++	andq	%rax,%r9
+++	movq	%r9,-88(%rdi)
+++	shrdq	$46,%r11,%r10
+++	andq	%rax,%r10
+++	movq	%r10,-80(%rdi)
+++	movq	32(%rsi),%r8
+++	movq	%r11,%r10
+++	shrq	$11,%r10
+++	andq	%rax,%r10
+++	movq	%r10,-72(%rdi)
+++	shrdq	$40,%r8,%r11
+++	andq	%rax,%r11
+++	movq	%r11,-64(%rdi)
+++	movq	40(%rsi),%r9
+++	movq	%r8,%r11
+++	shrq	$5,%r11
+++	andq	%rax,%r11
+++	movq	%r11,-56(%rdi)
+++	movq	%r8,%r10
+++	shrq	$34,%r10
+++	andq	%rax,%r10
+++	movq	%r10,-48(%rdi)
+++	shrdq	$63,%r9,%r8
+++	andq	%rax,%r8
+++	movq	%r8,-40(%rdi)
+++	movq	48(%rsi),%r10
+++	movq	%r9,%r8
+++	shrq	$28,%r8
+++	andq	%rax,%r8
+++	movq	%r8,-32(%rdi)
+++	shrdq	$57,%r10,%r9
+++	andq	%rax,%r9
+++	movq	%r9,-24(%rdi)
+++	movq	56(%rsi),%r11
+++	movq	%r10,%r9
+++	shrq	$22,%r9
+++	andq	%rax,%r9
+++	movq	%r9,-16(%rdi)
+++	shrdq	$51,%r11,%r10
+++	andq	%rax,%r10
+++	movq	%r10,-8(%rdi)
+++	movq	64(%rsi),%r8
+++	movq	%r11,%r10
+++	shrq	$16,%r10
+++	andq	%rax,%r10
+++	movq	%r10,0(%rdi)
+++	shrdq	$45,%r8,%r11
+++	andq	%rax,%r11
+++	movq	%r11,8(%rdi)
+++	movq	72(%rsi),%r9
+++	movq	%r8,%r11
+++	shrq	$10,%r11
+++	andq	%rax,%r11
+++	movq	%r11,16(%rdi)
+++	shrdq	$39,%r9,%r8
+++	andq	%rax,%r8
+++	movq	%r8,24(%rdi)
+++	movq	80(%rsi),%r10
+++	movq	%r9,%r8
+++	shrq	$4,%r8
+++	andq	%rax,%r8
+++	movq	%r8,32(%rdi)
+++	movq	%r9,%r11
+++	shrq	$33,%r11
+++	andq	%rax,%r11
+++	movq	%r11,40(%rdi)
+++	shrdq	$62,%r10,%r9
+++	andq	%rax,%r9
+++	movq	%r9,48(%rdi)
+++	movq	88(%rsi),%r11
+++	movq	%r10,%r9
+++	shrq	$27,%r9
+++	andq	%rax,%r9
+++	movq	%r9,56(%rdi)
+++	shrdq	$56,%r11,%r10
+++	andq	%rax,%r10
+++	movq	%r10,64(%rdi)
+++	movq	96(%rsi),%r8
+++	movq	%r11,%r10
+++	shrq	$21,%r10
+++	andq	%rax,%r10
+++	movq	%r10,72(%rdi)
+++	shrdq	$50,%r8,%r11
+++	andq	%rax,%r11
+++	movq	%r11,80(%rdi)
+++	movq	104(%rsi),%r9
+++	movq	%r8,%r11
+++	shrq	$15,%r11
+++	andq	%rax,%r11
+++	movq	%r11,88(%rdi)
+++	shrdq	$44,%r9,%r8
+++	andq	%rax,%r8
+++	movq	%r8,96(%rdi)
+++	movq	112(%rsi),%r10
+++	movq	%r9,%r8
+++	shrq	$9,%r8
+++	andq	%rax,%r8
+++	movq	%r8,104(%rdi)
+++	shrdq	$38,%r10,%r9
+++	andq	%rax,%r9
+++	movq	%r9,112(%rdi)
+++	movq	120(%rsi),%r11
+++	movq	%r10,%r9
+++	shrq	$3,%r9
+++	andq	%rax,%r9
+++	movq	%r9,120(%rdi)
+++	movq	%r10,%r8
+++	shrq	$32,%r8
+++	andq	%rax,%r8
+++	movq	%r8,128(%rdi)
+++	shrdq	$61,%r11,%r10
+++	andq	%rax,%r10
+++	movq	%r10,136(%rdi)
+++	xorq	%r8,%r8
+++	movq	%r11,%r10
+++	shrq	$26,%r10
+++	andq	%rax,%r10
+++	movq	%r10,144(%rdi)
+++	shrdq	$55,%r8,%r11
+++	andq	%rax,%r11
+++	movq	%r11,152(%rdi)
+++	movq	%r8,160(%rdi)
+++	movq	%r8,168(%rdi)
+++	movq	%r8,176(%rdi)
+++	movq	%r8,184(%rdi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	rsaz_1024_norm2red_avx2,.-rsaz_1024_norm2red_avx2
+++.globl	rsaz_1024_scatter5_avx2
+++.hidden rsaz_1024_scatter5_avx2
+++.type	rsaz_1024_scatter5_avx2,@function
+++.align	32
+++rsaz_1024_scatter5_avx2:
+++.cfi_startproc	
+++	vzeroupper
+++	vmovdqu	.Lscatter_permd(%rip),%ymm5
+++	shll	$4,%edx
+++	leaq	(%rdi,%rdx,1),%rdi
+++	movl	$9,%eax
+++	jmp	.Loop_scatter_1024
+++
+++.align	32
+++.Loop_scatter_1024:
+++	vmovdqu	(%rsi),%ymm0
+++	leaq	32(%rsi),%rsi
+++	vpermd	%ymm0,%ymm5,%ymm0
+++	vmovdqu	%xmm0,(%rdi)
+++	leaq	512(%rdi),%rdi
+++	decl	%eax
+++	jnz	.Loop_scatter_1024
+++
+++	vzeroupper
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	rsaz_1024_scatter5_avx2,.-rsaz_1024_scatter5_avx2
+++
+++.globl	rsaz_1024_gather5_avx2
+++.hidden rsaz_1024_gather5_avx2
+++.type	rsaz_1024_gather5_avx2,@function
+++.align	32
+++rsaz_1024_gather5_avx2:
+++.cfi_startproc	
+++	vzeroupper
+++	movq	%rsp,%r11
+++.cfi_def_cfa_register	%r11
+++	leaq	-256(%rsp),%rsp
+++	andq	$-32,%rsp
+++	leaq	.Linc(%rip),%r10
+++	leaq	-128(%rsp),%rax
+++
+++	vmovd	%edx,%xmm4
+++	vmovdqa	(%r10),%ymm0
+++	vmovdqa	32(%r10),%ymm1
+++	vmovdqa	64(%r10),%ymm5
+++	vpbroadcastd	%xmm4,%ymm4
+++
+++	vpaddd	%ymm5,%ymm0,%ymm2
+++	vpcmpeqd	%ymm4,%ymm0,%ymm0
+++	vpaddd	%ymm5,%ymm1,%ymm3
+++	vpcmpeqd	%ymm4,%ymm1,%ymm1
+++	vmovdqa	%ymm0,0+128(%rax)
+++	vpaddd	%ymm5,%ymm2,%ymm0
+++	vpcmpeqd	%ymm4,%ymm2,%ymm2
+++	vmovdqa	%ymm1,32+128(%rax)
+++	vpaddd	%ymm5,%ymm3,%ymm1
+++	vpcmpeqd	%ymm4,%ymm3,%ymm3
+++	vmovdqa	%ymm2,64+128(%rax)
+++	vpaddd	%ymm5,%ymm0,%ymm2
+++	vpcmpeqd	%ymm4,%ymm0,%ymm0
+++	vmovdqa	%ymm3,96+128(%rax)
+++	vpaddd	%ymm5,%ymm1,%ymm3
+++	vpcmpeqd	%ymm4,%ymm1,%ymm1
+++	vmovdqa	%ymm0,128+128(%rax)
+++	vpaddd	%ymm5,%ymm2,%ymm8
+++	vpcmpeqd	%ymm4,%ymm2,%ymm2
+++	vmovdqa	%ymm1,160+128(%rax)
+++	vpaddd	%ymm5,%ymm3,%ymm9
+++	vpcmpeqd	%ymm4,%ymm3,%ymm3
+++	vmovdqa	%ymm2,192+128(%rax)
+++	vpaddd	%ymm5,%ymm8,%ymm10
+++	vpcmpeqd	%ymm4,%ymm8,%ymm8
+++	vmovdqa	%ymm3,224+128(%rax)
+++	vpaddd	%ymm5,%ymm9,%ymm11
+++	vpcmpeqd	%ymm4,%ymm9,%ymm9
+++	vpaddd	%ymm5,%ymm10,%ymm12
+++	vpcmpeqd	%ymm4,%ymm10,%ymm10
+++	vpaddd	%ymm5,%ymm11,%ymm13
+++	vpcmpeqd	%ymm4,%ymm11,%ymm11
+++	vpaddd	%ymm5,%ymm12,%ymm14
+++	vpcmpeqd	%ymm4,%ymm12,%ymm12
+++	vpaddd	%ymm5,%ymm13,%ymm15
+++	vpcmpeqd	%ymm4,%ymm13,%ymm13
+++	vpcmpeqd	%ymm4,%ymm14,%ymm14
+++	vpcmpeqd	%ymm4,%ymm15,%ymm15
+++
+++	vmovdqa	-32(%r10),%ymm7
+++	leaq	128(%rsi),%rsi
+++	movl	$9,%edx
+++
+++.Loop_gather_1024:
+++	vmovdqa	0-128(%rsi),%ymm0
+++	vmovdqa	32-128(%rsi),%ymm1
+++	vmovdqa	64-128(%rsi),%ymm2
+++	vmovdqa	96-128(%rsi),%ymm3
+++	vpand	0+128(%rax),%ymm0,%ymm0
+++	vpand	32+128(%rax),%ymm1,%ymm1
+++	vpand	64+128(%rax),%ymm2,%ymm2
+++	vpor	%ymm0,%ymm1,%ymm4
+++	vpand	96+128(%rax),%ymm3,%ymm3
+++	vmovdqa	128-128(%rsi),%ymm0
+++	vmovdqa	160-128(%rsi),%ymm1
+++	vpor	%ymm2,%ymm3,%ymm5
+++	vmovdqa	192-128(%rsi),%ymm2
+++	vmovdqa	224-128(%rsi),%ymm3
+++	vpand	128+128(%rax),%ymm0,%ymm0
+++	vpand	160+128(%rax),%ymm1,%ymm1
+++	vpand	192+128(%rax),%ymm2,%ymm2
+++	vpor	%ymm0,%ymm4,%ymm4
+++	vpand	224+128(%rax),%ymm3,%ymm3
+++	vpand	256-128(%rsi),%ymm8,%ymm0
+++	vpor	%ymm1,%ymm5,%ymm5
+++	vpand	288-128(%rsi),%ymm9,%ymm1
+++	vpor	%ymm2,%ymm4,%ymm4
+++	vpand	320-128(%rsi),%ymm10,%ymm2
+++	vpor	%ymm3,%ymm5,%ymm5
+++	vpand	352-128(%rsi),%ymm11,%ymm3
+++	vpor	%ymm0,%ymm4,%ymm4
+++	vpand	384-128(%rsi),%ymm12,%ymm0
+++	vpor	%ymm1,%ymm5,%ymm5
+++	vpand	416-128(%rsi),%ymm13,%ymm1
+++	vpor	%ymm2,%ymm4,%ymm4
+++	vpand	448-128(%rsi),%ymm14,%ymm2
+++	vpor	%ymm3,%ymm5,%ymm5
+++	vpand	480-128(%rsi),%ymm15,%ymm3
+++	leaq	512(%rsi),%rsi
+++	vpor	%ymm0,%ymm4,%ymm4
+++	vpor	%ymm1,%ymm5,%ymm5
+++	vpor	%ymm2,%ymm4,%ymm4
+++	vpor	%ymm3,%ymm5,%ymm5
+++
+++	vpor	%ymm5,%ymm4,%ymm4
+++	vextracti128	$1,%ymm4,%xmm5
+++	vpor	%xmm4,%xmm5,%xmm5
+++	vpermd	%ymm5,%ymm7,%ymm5
+++	vmovdqu	%ymm5,(%rdi)
+++	leaq	32(%rdi),%rdi
+++	decl	%edx
+++	jnz	.Loop_gather_1024
+++
+++	vpxor	%ymm0,%ymm0,%ymm0
+++	vmovdqu	%ymm0,(%rdi)
+++	vzeroupper
+++	leaq	(%r11),%rsp
+++.cfi_def_cfa_register	%rsp
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.LSEH_end_rsaz_1024_gather5:
+++.size	rsaz_1024_gather5_avx2,.-rsaz_1024_gather5_avx2
+++.align	64
+++.Land_mask:
+++.quad	0x1fffffff,0x1fffffff,0x1fffffff,0x1fffffff
+++.Lscatter_permd:
+++.long	0,2,4,6,7,7,7,7
+++.Lgather_permd:
+++.long	0,7,1,7,2,7,3,7
+++.Linc:
+++.long	0,0,0,0, 1,1,1,1
+++.long	2,2,2,2, 3,3,3,3
+++.long	4,4,4,4, 4,4,4,4
+++.align	64
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/sha1-x86_64.S b/linux-x86_64/ypto/fipsmodule/sha1-x86_64.S
++new file mode 100644
++index 000000000..964687dc7
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/sha1-x86_64.S
++@@ -0,0 +1,5468 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++.extern	OPENSSL_ia32cap_P
+++.hidden OPENSSL_ia32cap_P
+++
+++.globl	sha1_block_data_order
+++.hidden sha1_block_data_order
+++.type	sha1_block_data_order,@function
+++.align	16
+++sha1_block_data_order:
+++.cfi_startproc	
+++	leaq	OPENSSL_ia32cap_P(%rip),%r10
+++	movl	0(%r10),%r9d
+++	movl	4(%r10),%r8d
+++	movl	8(%r10),%r10d
+++	testl	$512,%r8d
+++	jz	.Lialu
+++	testl	$536870912,%r10d
+++	jnz	_shaext_shortcut
+++	andl	$296,%r10d
+++	cmpl	$296,%r10d
+++	je	_avx2_shortcut
+++	andl	$268435456,%r8d
+++	andl	$1073741824,%r9d
+++	orl	%r9d,%r8d
+++	cmpl	$1342177280,%r8d
+++	je	_avx_shortcut
+++	jmp	_ssse3_shortcut
+++
+++.align	16
+++.Lialu:
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	movq	%rdi,%r8
+++	subq	$72,%rsp
+++	movq	%rsi,%r9
+++	andq	$-64,%rsp
+++	movq	%rdx,%r10
+++	movq	%rax,64(%rsp)
+++.cfi_escape	0x0f,0x06,0x77,0xc0,0x00,0x06,0x23,0x08
+++.Lprologue:
+++
+++	movl	0(%r8),%esi
+++	movl	4(%r8),%edi
+++	movl	8(%r8),%r11d
+++	movl	12(%r8),%r12d
+++	movl	16(%r8),%r13d
+++	jmp	.Lloop
+++
+++.align	16
+++.Lloop:
+++	movl	0(%r9),%edx
+++	bswapl	%edx
+++	movl	4(%r9),%ebp
+++	movl	%r12d,%eax
+++	movl	%edx,0(%rsp)
+++	movl	%esi,%ecx
+++	bswapl	%ebp
+++	xorl	%r11d,%eax
+++	roll	$5,%ecx
+++	andl	%edi,%eax
+++	leal	1518500249(%rdx,%r13,1),%r13d
+++	addl	%ecx,%r13d
+++	xorl	%r12d,%eax
+++	roll	$30,%edi
+++	addl	%eax,%r13d
+++	movl	8(%r9),%r14d
+++	movl	%r11d,%eax
+++	movl	%ebp,4(%rsp)
+++	movl	%r13d,%ecx
+++	bswapl	%r14d
+++	xorl	%edi,%eax
+++	roll	$5,%ecx
+++	andl	%esi,%eax
+++	leal	1518500249(%rbp,%r12,1),%r12d
+++	addl	%ecx,%r12d
+++	xorl	%r11d,%eax
+++	roll	$30,%esi
+++	addl	%eax,%r12d
+++	movl	12(%r9),%edx
+++	movl	%edi,%eax
+++	movl	%r14d,8(%rsp)
+++	movl	%r12d,%ecx
+++	bswapl	%edx
+++	xorl	%esi,%eax
+++	roll	$5,%ecx
+++	andl	%r13d,%eax
+++	leal	1518500249(%r14,%r11,1),%r11d
+++	addl	%ecx,%r11d
+++	xorl	%edi,%eax
+++	roll	$30,%r13d
+++	addl	%eax,%r11d
+++	movl	16(%r9),%ebp
+++	movl	%esi,%eax
+++	movl	%edx,12(%rsp)
+++	movl	%r11d,%ecx
+++	bswapl	%ebp
+++	xorl	%r13d,%eax
+++	roll	$5,%ecx
+++	andl	%r12d,%eax
+++	leal	1518500249(%rdx,%rdi,1),%edi
+++	addl	%ecx,%edi
+++	xorl	%esi,%eax
+++	roll	$30,%r12d
+++	addl	%eax,%edi
+++	movl	20(%r9),%r14d
+++	movl	%r13d,%eax
+++	movl	%ebp,16(%rsp)
+++	movl	%edi,%ecx
+++	bswapl	%r14d
+++	xorl	%r12d,%eax
+++	roll	$5,%ecx
+++	andl	%r11d,%eax
+++	leal	1518500249(%rbp,%rsi,1),%esi
+++	addl	%ecx,%esi
+++	xorl	%r13d,%eax
+++	roll	$30,%r11d
+++	addl	%eax,%esi
+++	movl	24(%r9),%edx
+++	movl	%r12d,%eax
+++	movl	%r14d,20(%rsp)
+++	movl	%esi,%ecx
+++	bswapl	%edx
+++	xorl	%r11d,%eax
+++	roll	$5,%ecx
+++	andl	%edi,%eax
+++	leal	1518500249(%r14,%r13,1),%r13d
+++	addl	%ecx,%r13d
+++	xorl	%r12d,%eax
+++	roll	$30,%edi
+++	addl	%eax,%r13d
+++	movl	28(%r9),%ebp
+++	movl	%r11d,%eax
+++	movl	%edx,24(%rsp)
+++	movl	%r13d,%ecx
+++	bswapl	%ebp
+++	xorl	%edi,%eax
+++	roll	$5,%ecx
+++	andl	%esi,%eax
+++	leal	1518500249(%rdx,%r12,1),%r12d
+++	addl	%ecx,%r12d
+++	xorl	%r11d,%eax
+++	roll	$30,%esi
+++	addl	%eax,%r12d
+++	movl	32(%r9),%r14d
+++	movl	%edi,%eax
+++	movl	%ebp,28(%rsp)
+++	movl	%r12d,%ecx
+++	bswapl	%r14d
+++	xorl	%esi,%eax
+++	roll	$5,%ecx
+++	andl	%r13d,%eax
+++	leal	1518500249(%rbp,%r11,1),%r11d
+++	addl	%ecx,%r11d
+++	xorl	%edi,%eax
+++	roll	$30,%r13d
+++	addl	%eax,%r11d
+++	movl	36(%r9),%edx
+++	movl	%esi,%eax
+++	movl	%r14d,32(%rsp)
+++	movl	%r11d,%ecx
+++	bswapl	%edx
+++	xorl	%r13d,%eax
+++	roll	$5,%ecx
+++	andl	%r12d,%eax
+++	leal	1518500249(%r14,%rdi,1),%edi
+++	addl	%ecx,%edi
+++	xorl	%esi,%eax
+++	roll	$30,%r12d
+++	addl	%eax,%edi
+++	movl	40(%r9),%ebp
+++	movl	%r13d,%eax
+++	movl	%edx,36(%rsp)
+++	movl	%edi,%ecx
+++	bswapl	%ebp
+++	xorl	%r12d,%eax
+++	roll	$5,%ecx
+++	andl	%r11d,%eax
+++	leal	1518500249(%rdx,%rsi,1),%esi
+++	addl	%ecx,%esi
+++	xorl	%r13d,%eax
+++	roll	$30,%r11d
+++	addl	%eax,%esi
+++	movl	44(%r9),%r14d
+++	movl	%r12d,%eax
+++	movl	%ebp,40(%rsp)
+++	movl	%esi,%ecx
+++	bswapl	%r14d
+++	xorl	%r11d,%eax
+++	roll	$5,%ecx
+++	andl	%edi,%eax
+++	leal	1518500249(%rbp,%r13,1),%r13d
+++	addl	%ecx,%r13d
+++	xorl	%r12d,%eax
+++	roll	$30,%edi
+++	addl	%eax,%r13d
+++	movl	48(%r9),%edx
+++	movl	%r11d,%eax
+++	movl	%r14d,44(%rsp)
+++	movl	%r13d,%ecx
+++	bswapl	%edx
+++	xorl	%edi,%eax
+++	roll	$5,%ecx
+++	andl	%esi,%eax
+++	leal	1518500249(%r14,%r12,1),%r12d
+++	addl	%ecx,%r12d
+++	xorl	%r11d,%eax
+++	roll	$30,%esi
+++	addl	%eax,%r12d
+++	movl	52(%r9),%ebp
+++	movl	%edi,%eax
+++	movl	%edx,48(%rsp)
+++	movl	%r12d,%ecx
+++	bswapl	%ebp
+++	xorl	%esi,%eax
+++	roll	$5,%ecx
+++	andl	%r13d,%eax
+++	leal	1518500249(%rdx,%r11,1),%r11d
+++	addl	%ecx,%r11d
+++	xorl	%edi,%eax
+++	roll	$30,%r13d
+++	addl	%eax,%r11d
+++	movl	56(%r9),%r14d
+++	movl	%esi,%eax
+++	movl	%ebp,52(%rsp)
+++	movl	%r11d,%ecx
+++	bswapl	%r14d
+++	xorl	%r13d,%eax
+++	roll	$5,%ecx
+++	andl	%r12d,%eax
+++	leal	1518500249(%rbp,%rdi,1),%edi
+++	addl	%ecx,%edi
+++	xorl	%esi,%eax
+++	roll	$30,%r12d
+++	addl	%eax,%edi
+++	movl	60(%r9),%edx
+++	movl	%r13d,%eax
+++	movl	%r14d,56(%rsp)
+++	movl	%edi,%ecx
+++	bswapl	%edx
+++	xorl	%r12d,%eax
+++	roll	$5,%ecx
+++	andl	%r11d,%eax
+++	leal	1518500249(%r14,%rsi,1),%esi
+++	addl	%ecx,%esi
+++	xorl	%r13d,%eax
+++	roll	$30,%r11d
+++	addl	%eax,%esi
+++	xorl	0(%rsp),%ebp
+++	movl	%r12d,%eax
+++	movl	%edx,60(%rsp)
+++	movl	%esi,%ecx
+++	xorl	8(%rsp),%ebp
+++	xorl	%r11d,%eax
+++	roll	$5,%ecx
+++	xorl	32(%rsp),%ebp
+++	andl	%edi,%eax
+++	leal	1518500249(%rdx,%r13,1),%r13d
+++	roll	$30,%edi
+++	xorl	%r12d,%eax
+++	addl	%ecx,%r13d
+++	roll	$1,%ebp
+++	addl	%eax,%r13d
+++	xorl	4(%rsp),%r14d
+++	movl	%r11d,%eax
+++	movl	%ebp,0(%rsp)
+++	movl	%r13d,%ecx
+++	xorl	12(%rsp),%r14d
+++	xorl	%edi,%eax
+++	roll	$5,%ecx
+++	xorl	36(%rsp),%r14d
+++	andl	%esi,%eax
+++	leal	1518500249(%rbp,%r12,1),%r12d
+++	roll	$30,%esi
+++	xorl	%r11d,%eax
+++	addl	%ecx,%r12d
+++	roll	$1,%r14d
+++	addl	%eax,%r12d
+++	xorl	8(%rsp),%edx
+++	movl	%edi,%eax
+++	movl	%r14d,4(%rsp)
+++	movl	%r12d,%ecx
+++	xorl	16(%rsp),%edx
+++	xorl	%esi,%eax
+++	roll	$5,%ecx
+++	xorl	40(%rsp),%edx
+++	andl	%r13d,%eax
+++	leal	1518500249(%r14,%r11,1),%r11d
+++	roll	$30,%r13d
+++	xorl	%edi,%eax
+++	addl	%ecx,%r11d
+++	roll	$1,%edx
+++	addl	%eax,%r11d
+++	xorl	12(%rsp),%ebp
+++	movl	%esi,%eax
+++	movl	%edx,8(%rsp)
+++	movl	%r11d,%ecx
+++	xorl	20(%rsp),%ebp
+++	xorl	%r13d,%eax
+++	roll	$5,%ecx
+++	xorl	44(%rsp),%ebp
+++	andl	%r12d,%eax
+++	leal	1518500249(%rdx,%rdi,1),%edi
+++	roll	$30,%r12d
+++	xorl	%esi,%eax
+++	addl	%ecx,%edi
+++	roll	$1,%ebp
+++	addl	%eax,%edi
+++	xorl	16(%rsp),%r14d
+++	movl	%r13d,%eax
+++	movl	%ebp,12(%rsp)
+++	movl	%edi,%ecx
+++	xorl	24(%rsp),%r14d
+++	xorl	%r12d,%eax
+++	roll	$5,%ecx
+++	xorl	48(%rsp),%r14d
+++	andl	%r11d,%eax
+++	leal	1518500249(%rbp,%rsi,1),%esi
+++	roll	$30,%r11d
+++	xorl	%r13d,%eax
+++	addl	%ecx,%esi
+++	roll	$1,%r14d
+++	addl	%eax,%esi
+++	xorl	20(%rsp),%edx
+++	movl	%edi,%eax
+++	movl	%r14d,16(%rsp)
+++	movl	%esi,%ecx
+++	xorl	28(%rsp),%edx
+++	xorl	%r12d,%eax
+++	roll	$5,%ecx
+++	xorl	52(%rsp),%edx
+++	leal	1859775393(%r14,%r13,1),%r13d
+++	xorl	%r11d,%eax
+++	addl	%ecx,%r13d
+++	roll	$30,%edi
+++	addl	%eax,%r13d
+++	roll	$1,%edx
+++	xorl	24(%rsp),%ebp
+++	movl	%esi,%eax
+++	movl	%edx,20(%rsp)
+++	movl	%r13d,%ecx
+++	xorl	32(%rsp),%ebp
+++	xorl	%r11d,%eax
+++	roll	$5,%ecx
+++	xorl	56(%rsp),%ebp
+++	leal	1859775393(%rdx,%r12,1),%r12d
+++	xorl	%edi,%eax
+++	addl	%ecx,%r12d
+++	roll	$30,%esi
+++	addl	%eax,%r12d
+++	roll	$1,%ebp
+++	xorl	28(%rsp),%r14d
+++	movl	%r13d,%eax
+++	movl	%ebp,24(%rsp)
+++	movl	%r12d,%ecx
+++	xorl	36(%rsp),%r14d
+++	xorl	%edi,%eax
+++	roll	$5,%ecx
+++	xorl	60(%rsp),%r14d
+++	leal	1859775393(%rbp,%r11,1),%r11d
+++	xorl	%esi,%eax
+++	addl	%ecx,%r11d
+++	roll	$30,%r13d
+++	addl	%eax,%r11d
+++	roll	$1,%r14d
+++	xorl	32(%rsp),%edx
+++	movl	%r12d,%eax
+++	movl	%r14d,28(%rsp)
+++	movl	%r11d,%ecx
+++	xorl	40(%rsp),%edx
+++	xorl	%esi,%eax
+++	roll	$5,%ecx
+++	xorl	0(%rsp),%edx
+++	leal	1859775393(%r14,%rdi,1),%edi
+++	xorl	%r13d,%eax
+++	addl	%ecx,%edi
+++	roll	$30,%r12d
+++	addl	%eax,%edi
+++	roll	$1,%edx
+++	xorl	36(%rsp),%ebp
+++	movl	%r11d,%eax
+++	movl	%edx,32(%rsp)
+++	movl	%edi,%ecx
+++	xorl	44(%rsp),%ebp
+++	xorl	%r13d,%eax
+++	roll	$5,%ecx
+++	xorl	4(%rsp),%ebp
+++	leal	1859775393(%rdx,%rsi,1),%esi
+++	xorl	%r12d,%eax
+++	addl	%ecx,%esi
+++	roll	$30,%r11d
+++	addl	%eax,%esi
+++	roll	$1,%ebp
+++	xorl	40(%rsp),%r14d
+++	movl	%edi,%eax
+++	movl	%ebp,36(%rsp)
+++	movl	%esi,%ecx
+++	xorl	48(%rsp),%r14d
+++	xorl	%r12d,%eax
+++	roll	$5,%ecx
+++	xorl	8(%rsp),%r14d
+++	leal	1859775393(%rbp,%r13,1),%r13d
+++	xorl	%r11d,%eax
+++	addl	%ecx,%r13d
+++	roll	$30,%edi
+++	addl	%eax,%r13d
+++	roll	$1,%r14d
+++	xorl	44(%rsp),%edx
+++	movl	%esi,%eax
+++	movl	%r14d,40(%rsp)
+++	movl	%r13d,%ecx
+++	xorl	52(%rsp),%edx
+++	xorl	%r11d,%eax
+++	roll	$5,%ecx
+++	xorl	12(%rsp),%edx
+++	leal	1859775393(%r14,%r12,1),%r12d
+++	xorl	%edi,%eax
+++	addl	%ecx,%r12d
+++	roll	$30,%esi
+++	addl	%eax,%r12d
+++	roll	$1,%edx
+++	xorl	48(%rsp),%ebp
+++	movl	%r13d,%eax
+++	movl	%edx,44(%rsp)
+++	movl	%r12d,%ecx
+++	xorl	56(%rsp),%ebp
+++	xorl	%edi,%eax
+++	roll	$5,%ecx
+++	xorl	16(%rsp),%ebp
+++	leal	1859775393(%rdx,%r11,1),%r11d
+++	xorl	%esi,%eax
+++	addl	%ecx,%r11d
+++	roll	$30,%r13d
+++	addl	%eax,%r11d
+++	roll	$1,%ebp
+++	xorl	52(%rsp),%r14d
+++	movl	%r12d,%eax
+++	movl	%ebp,48(%rsp)
+++	movl	%r11d,%ecx
+++	xorl	60(%rsp),%r14d
+++	xorl	%esi,%eax
+++	roll	$5,%ecx
+++	xorl	20(%rsp),%r14d
+++	leal	1859775393(%rbp,%rdi,1),%edi
+++	xorl	%r13d,%eax
+++	addl	%ecx,%edi
+++	roll	$30,%r12d
+++	addl	%eax,%edi
+++	roll	$1,%r14d
+++	xorl	56(%rsp),%edx
+++	movl	%r11d,%eax
+++	movl	%r14d,52(%rsp)
+++	movl	%edi,%ecx
+++	xorl	0(%rsp),%edx
+++	xorl	%r13d,%eax
+++	roll	$5,%ecx
+++	xorl	24(%rsp),%edx
+++	leal	1859775393(%r14,%rsi,1),%esi
+++	xorl	%r12d,%eax
+++	addl	%ecx,%esi
+++	roll	$30,%r11d
+++	addl	%eax,%esi
+++	roll	$1,%edx
+++	xorl	60(%rsp),%ebp
+++	movl	%edi,%eax
+++	movl	%edx,56(%rsp)
+++	movl	%esi,%ecx
+++	xorl	4(%rsp),%ebp
+++	xorl	%r12d,%eax
+++	roll	$5,%ecx
+++	xorl	28(%rsp),%ebp
+++	leal	1859775393(%rdx,%r13,1),%r13d
+++	xorl	%r11d,%eax
+++	addl	%ecx,%r13d
+++	roll	$30,%edi
+++	addl	%eax,%r13d
+++	roll	$1,%ebp
+++	xorl	0(%rsp),%r14d
+++	movl	%esi,%eax
+++	movl	%ebp,60(%rsp)
+++	movl	%r13d,%ecx
+++	xorl	8(%rsp),%r14d
+++	xorl	%r11d,%eax
+++	roll	$5,%ecx
+++	xorl	32(%rsp),%r14d
+++	leal	1859775393(%rbp,%r12,1),%r12d
+++	xorl	%edi,%eax
+++	addl	%ecx,%r12d
+++	roll	$30,%esi
+++	addl	%eax,%r12d
+++	roll	$1,%r14d
+++	xorl	4(%rsp),%edx
+++	movl	%r13d,%eax
+++	movl	%r14d,0(%rsp)
+++	movl	%r12d,%ecx
+++	xorl	12(%rsp),%edx
+++	xorl	%edi,%eax
+++	roll	$5,%ecx
+++	xorl	36(%rsp),%edx
+++	leal	1859775393(%r14,%r11,1),%r11d
+++	xorl	%esi,%eax
+++	addl	%ecx,%r11d
+++	roll	$30,%r13d
+++	addl	%eax,%r11d
+++	roll	$1,%edx
+++	xorl	8(%rsp),%ebp
+++	movl	%r12d,%eax
+++	movl	%edx,4(%rsp)
+++	movl	%r11d,%ecx
+++	xorl	16(%rsp),%ebp
+++	xorl	%esi,%eax
+++	roll	$5,%ecx
+++	xorl	40(%rsp),%ebp
+++	leal	1859775393(%rdx,%rdi,1),%edi
+++	xorl	%r13d,%eax
+++	addl	%ecx,%edi
+++	roll	$30,%r12d
+++	addl	%eax,%edi
+++	roll	$1,%ebp
+++	xorl	12(%rsp),%r14d
+++	movl	%r11d,%eax
+++	movl	%ebp,8(%rsp)
+++	movl	%edi,%ecx
+++	xorl	20(%rsp),%r14d
+++	xorl	%r13d,%eax
+++	roll	$5,%ecx
+++	xorl	44(%rsp),%r14d
+++	leal	1859775393(%rbp,%rsi,1),%esi
+++	xorl	%r12d,%eax
+++	addl	%ecx,%esi
+++	roll	$30,%r11d
+++	addl	%eax,%esi
+++	roll	$1,%r14d
+++	xorl	16(%rsp),%edx
+++	movl	%edi,%eax
+++	movl	%r14d,12(%rsp)
+++	movl	%esi,%ecx
+++	xorl	24(%rsp),%edx
+++	xorl	%r12d,%eax
+++	roll	$5,%ecx
+++	xorl	48(%rsp),%edx
+++	leal	1859775393(%r14,%r13,1),%r13d
+++	xorl	%r11d,%eax
+++	addl	%ecx,%r13d
+++	roll	$30,%edi
+++	addl	%eax,%r13d
+++	roll	$1,%edx
+++	xorl	20(%rsp),%ebp
+++	movl	%esi,%eax
+++	movl	%edx,16(%rsp)
+++	movl	%r13d,%ecx
+++	xorl	28(%rsp),%ebp
+++	xorl	%r11d,%eax
+++	roll	$5,%ecx
+++	xorl	52(%rsp),%ebp
+++	leal	1859775393(%rdx,%r12,1),%r12d
+++	xorl	%edi,%eax
+++	addl	%ecx,%r12d
+++	roll	$30,%esi
+++	addl	%eax,%r12d
+++	roll	$1,%ebp
+++	xorl	24(%rsp),%r14d
+++	movl	%r13d,%eax
+++	movl	%ebp,20(%rsp)
+++	movl	%r12d,%ecx
+++	xorl	32(%rsp),%r14d
+++	xorl	%edi,%eax
+++	roll	$5,%ecx
+++	xorl	56(%rsp),%r14d
+++	leal	1859775393(%rbp,%r11,1),%r11d
+++	xorl	%esi,%eax
+++	addl	%ecx,%r11d
+++	roll	$30,%r13d
+++	addl	%eax,%r11d
+++	roll	$1,%r14d
+++	xorl	28(%rsp),%edx
+++	movl	%r12d,%eax
+++	movl	%r14d,24(%rsp)
+++	movl	%r11d,%ecx
+++	xorl	36(%rsp),%edx
+++	xorl	%esi,%eax
+++	roll	$5,%ecx
+++	xorl	60(%rsp),%edx
+++	leal	1859775393(%r14,%rdi,1),%edi
+++	xorl	%r13d,%eax
+++	addl	%ecx,%edi
+++	roll	$30,%r12d
+++	addl	%eax,%edi
+++	roll	$1,%edx
+++	xorl	32(%rsp),%ebp
+++	movl	%r11d,%eax
+++	movl	%edx,28(%rsp)
+++	movl	%edi,%ecx
+++	xorl	40(%rsp),%ebp
+++	xorl	%r13d,%eax
+++	roll	$5,%ecx
+++	xorl	0(%rsp),%ebp
+++	leal	1859775393(%rdx,%rsi,1),%esi
+++	xorl	%r12d,%eax
+++	addl	%ecx,%esi
+++	roll	$30,%r11d
+++	addl	%eax,%esi
+++	roll	$1,%ebp
+++	xorl	36(%rsp),%r14d
+++	movl	%r12d,%eax
+++	movl	%ebp,32(%rsp)
+++	movl	%r12d,%ebx
+++	xorl	44(%rsp),%r14d
+++	andl	%r11d,%eax
+++	movl	%esi,%ecx
+++	xorl	4(%rsp),%r14d
+++	leal	-1894007588(%rbp,%r13,1),%r13d
+++	xorl	%r11d,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%r13d
+++	roll	$1,%r14d
+++	andl	%edi,%ebx
+++	addl	%ecx,%r13d
+++	roll	$30,%edi
+++	addl	%ebx,%r13d
+++	xorl	40(%rsp),%edx
+++	movl	%r11d,%eax
+++	movl	%r14d,36(%rsp)
+++	movl	%r11d,%ebx
+++	xorl	48(%rsp),%edx
+++	andl	%edi,%eax
+++	movl	%r13d,%ecx
+++	xorl	8(%rsp),%edx
+++	leal	-1894007588(%r14,%r12,1),%r12d
+++	xorl	%edi,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%r12d
+++	roll	$1,%edx
+++	andl	%esi,%ebx
+++	addl	%ecx,%r12d
+++	roll	$30,%esi
+++	addl	%ebx,%r12d
+++	xorl	44(%rsp),%ebp
+++	movl	%edi,%eax
+++	movl	%edx,40(%rsp)
+++	movl	%edi,%ebx
+++	xorl	52(%rsp),%ebp
+++	andl	%esi,%eax
+++	movl	%r12d,%ecx
+++	xorl	12(%rsp),%ebp
+++	leal	-1894007588(%rdx,%r11,1),%r11d
+++	xorl	%esi,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%r11d
+++	roll	$1,%ebp
+++	andl	%r13d,%ebx
+++	addl	%ecx,%r11d
+++	roll	$30,%r13d
+++	addl	%ebx,%r11d
+++	xorl	48(%rsp),%r14d
+++	movl	%esi,%eax
+++	movl	%ebp,44(%rsp)
+++	movl	%esi,%ebx
+++	xorl	56(%rsp),%r14d
+++	andl	%r13d,%eax
+++	movl	%r11d,%ecx
+++	xorl	16(%rsp),%r14d
+++	leal	-1894007588(%rbp,%rdi,1),%edi
+++	xorl	%r13d,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%edi
+++	roll	$1,%r14d
+++	andl	%r12d,%ebx
+++	addl	%ecx,%edi
+++	roll	$30,%r12d
+++	addl	%ebx,%edi
+++	xorl	52(%rsp),%edx
+++	movl	%r13d,%eax
+++	movl	%r14d,48(%rsp)
+++	movl	%r13d,%ebx
+++	xorl	60(%rsp),%edx
+++	andl	%r12d,%eax
+++	movl	%edi,%ecx
+++	xorl	20(%rsp),%edx
+++	leal	-1894007588(%r14,%rsi,1),%esi
+++	xorl	%r12d,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%esi
+++	roll	$1,%edx
+++	andl	%r11d,%ebx
+++	addl	%ecx,%esi
+++	roll	$30,%r11d
+++	addl	%ebx,%esi
+++	xorl	56(%rsp),%ebp
+++	movl	%r12d,%eax
+++	movl	%edx,52(%rsp)
+++	movl	%r12d,%ebx
+++	xorl	0(%rsp),%ebp
+++	andl	%r11d,%eax
+++	movl	%esi,%ecx
+++	xorl	24(%rsp),%ebp
+++	leal	-1894007588(%rdx,%r13,1),%r13d
+++	xorl	%r11d,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%r13d
+++	roll	$1,%ebp
+++	andl	%edi,%ebx
+++	addl	%ecx,%r13d
+++	roll	$30,%edi
+++	addl	%ebx,%r13d
+++	xorl	60(%rsp),%r14d
+++	movl	%r11d,%eax
+++	movl	%ebp,56(%rsp)
+++	movl	%r11d,%ebx
+++	xorl	4(%rsp),%r14d
+++	andl	%edi,%eax
+++	movl	%r13d,%ecx
+++	xorl	28(%rsp),%r14d
+++	leal	-1894007588(%rbp,%r12,1),%r12d
+++	xorl	%edi,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%r12d
+++	roll	$1,%r14d
+++	andl	%esi,%ebx
+++	addl	%ecx,%r12d
+++	roll	$30,%esi
+++	addl	%ebx,%r12d
+++	xorl	0(%rsp),%edx
+++	movl	%edi,%eax
+++	movl	%r14d,60(%rsp)
+++	movl	%edi,%ebx
+++	xorl	8(%rsp),%edx
+++	andl	%esi,%eax
+++	movl	%r12d,%ecx
+++	xorl	32(%rsp),%edx
+++	leal	-1894007588(%r14,%r11,1),%r11d
+++	xorl	%esi,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%r11d
+++	roll	$1,%edx
+++	andl	%r13d,%ebx
+++	addl	%ecx,%r11d
+++	roll	$30,%r13d
+++	addl	%ebx,%r11d
+++	xorl	4(%rsp),%ebp
+++	movl	%esi,%eax
+++	movl	%edx,0(%rsp)
+++	movl	%esi,%ebx
+++	xorl	12(%rsp),%ebp
+++	andl	%r13d,%eax
+++	movl	%r11d,%ecx
+++	xorl	36(%rsp),%ebp
+++	leal	-1894007588(%rdx,%rdi,1),%edi
+++	xorl	%r13d,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%edi
+++	roll	$1,%ebp
+++	andl	%r12d,%ebx
+++	addl	%ecx,%edi
+++	roll	$30,%r12d
+++	addl	%ebx,%edi
+++	xorl	8(%rsp),%r14d
+++	movl	%r13d,%eax
+++	movl	%ebp,4(%rsp)
+++	movl	%r13d,%ebx
+++	xorl	16(%rsp),%r14d
+++	andl	%r12d,%eax
+++	movl	%edi,%ecx
+++	xorl	40(%rsp),%r14d
+++	leal	-1894007588(%rbp,%rsi,1),%esi
+++	xorl	%r12d,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%esi
+++	roll	$1,%r14d
+++	andl	%r11d,%ebx
+++	addl	%ecx,%esi
+++	roll	$30,%r11d
+++	addl	%ebx,%esi
+++	xorl	12(%rsp),%edx
+++	movl	%r12d,%eax
+++	movl	%r14d,8(%rsp)
+++	movl	%r12d,%ebx
+++	xorl	20(%rsp),%edx
+++	andl	%r11d,%eax
+++	movl	%esi,%ecx
+++	xorl	44(%rsp),%edx
+++	leal	-1894007588(%r14,%r13,1),%r13d
+++	xorl	%r11d,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%r13d
+++	roll	$1,%edx
+++	andl	%edi,%ebx
+++	addl	%ecx,%r13d
+++	roll	$30,%edi
+++	addl	%ebx,%r13d
+++	xorl	16(%rsp),%ebp
+++	movl	%r11d,%eax
+++	movl	%edx,12(%rsp)
+++	movl	%r11d,%ebx
+++	xorl	24(%rsp),%ebp
+++	andl	%edi,%eax
+++	movl	%r13d,%ecx
+++	xorl	48(%rsp),%ebp
+++	leal	-1894007588(%rdx,%r12,1),%r12d
+++	xorl	%edi,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%r12d
+++	roll	$1,%ebp
+++	andl	%esi,%ebx
+++	addl	%ecx,%r12d
+++	roll	$30,%esi
+++	addl	%ebx,%r12d
+++	xorl	20(%rsp),%r14d
+++	movl	%edi,%eax
+++	movl	%ebp,16(%rsp)
+++	movl	%edi,%ebx
+++	xorl	28(%rsp),%r14d
+++	andl	%esi,%eax
+++	movl	%r12d,%ecx
+++	xorl	52(%rsp),%r14d
+++	leal	-1894007588(%rbp,%r11,1),%r11d
+++	xorl	%esi,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%r11d
+++	roll	$1,%r14d
+++	andl	%r13d,%ebx
+++	addl	%ecx,%r11d
+++	roll	$30,%r13d
+++	addl	%ebx,%r11d
+++	xorl	24(%rsp),%edx
+++	movl	%esi,%eax
+++	movl	%r14d,20(%rsp)
+++	movl	%esi,%ebx
+++	xorl	32(%rsp),%edx
+++	andl	%r13d,%eax
+++	movl	%r11d,%ecx
+++	xorl	56(%rsp),%edx
+++	leal	-1894007588(%r14,%rdi,1),%edi
+++	xorl	%r13d,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%edi
+++	roll	$1,%edx
+++	andl	%r12d,%ebx
+++	addl	%ecx,%edi
+++	roll	$30,%r12d
+++	addl	%ebx,%edi
+++	xorl	28(%rsp),%ebp
+++	movl	%r13d,%eax
+++	movl	%edx,24(%rsp)
+++	movl	%r13d,%ebx
+++	xorl	36(%rsp),%ebp
+++	andl	%r12d,%eax
+++	movl	%edi,%ecx
+++	xorl	60(%rsp),%ebp
+++	leal	-1894007588(%rdx,%rsi,1),%esi
+++	xorl	%r12d,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%esi
+++	roll	$1,%ebp
+++	andl	%r11d,%ebx
+++	addl	%ecx,%esi
+++	roll	$30,%r11d
+++	addl	%ebx,%esi
+++	xorl	32(%rsp),%r14d
+++	movl	%r12d,%eax
+++	movl	%ebp,28(%rsp)
+++	movl	%r12d,%ebx
+++	xorl	40(%rsp),%r14d
+++	andl	%r11d,%eax
+++	movl	%esi,%ecx
+++	xorl	0(%rsp),%r14d
+++	leal	-1894007588(%rbp,%r13,1),%r13d
+++	xorl	%r11d,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%r13d
+++	roll	$1,%r14d
+++	andl	%edi,%ebx
+++	addl	%ecx,%r13d
+++	roll	$30,%edi
+++	addl	%ebx,%r13d
+++	xorl	36(%rsp),%edx
+++	movl	%r11d,%eax
+++	movl	%r14d,32(%rsp)
+++	movl	%r11d,%ebx
+++	xorl	44(%rsp),%edx
+++	andl	%edi,%eax
+++	movl	%r13d,%ecx
+++	xorl	4(%rsp),%edx
+++	leal	-1894007588(%r14,%r12,1),%r12d
+++	xorl	%edi,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%r12d
+++	roll	$1,%edx
+++	andl	%esi,%ebx
+++	addl	%ecx,%r12d
+++	roll	$30,%esi
+++	addl	%ebx,%r12d
+++	xorl	40(%rsp),%ebp
+++	movl	%edi,%eax
+++	movl	%edx,36(%rsp)
+++	movl	%edi,%ebx
+++	xorl	48(%rsp),%ebp
+++	andl	%esi,%eax
+++	movl	%r12d,%ecx
+++	xorl	8(%rsp),%ebp
+++	leal	-1894007588(%rdx,%r11,1),%r11d
+++	xorl	%esi,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%r11d
+++	roll	$1,%ebp
+++	andl	%r13d,%ebx
+++	addl	%ecx,%r11d
+++	roll	$30,%r13d
+++	addl	%ebx,%r11d
+++	xorl	44(%rsp),%r14d
+++	movl	%esi,%eax
+++	movl	%ebp,40(%rsp)
+++	movl	%esi,%ebx
+++	xorl	52(%rsp),%r14d
+++	andl	%r13d,%eax
+++	movl	%r11d,%ecx
+++	xorl	12(%rsp),%r14d
+++	leal	-1894007588(%rbp,%rdi,1),%edi
+++	xorl	%r13d,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%edi
+++	roll	$1,%r14d
+++	andl	%r12d,%ebx
+++	addl	%ecx,%edi
+++	roll	$30,%r12d
+++	addl	%ebx,%edi
+++	xorl	48(%rsp),%edx
+++	movl	%r13d,%eax
+++	movl	%r14d,44(%rsp)
+++	movl	%r13d,%ebx
+++	xorl	56(%rsp),%edx
+++	andl	%r12d,%eax
+++	movl	%edi,%ecx
+++	xorl	16(%rsp),%edx
+++	leal	-1894007588(%r14,%rsi,1),%esi
+++	xorl	%r12d,%ebx
+++	roll	$5,%ecx
+++	addl	%eax,%esi
+++	roll	$1,%edx
+++	andl	%r11d,%ebx
+++	addl	%ecx,%esi
+++	roll	$30,%r11d
+++	addl	%ebx,%esi
+++	xorl	52(%rsp),%ebp
+++	movl	%edi,%eax
+++	movl	%edx,48(%rsp)
+++	movl	%esi,%ecx
+++	xorl	60(%rsp),%ebp
+++	xorl	%r12d,%eax
+++	roll	$5,%ecx
+++	xorl	20(%rsp),%ebp
+++	leal	-899497514(%rdx,%r13,1),%r13d
+++	xorl	%r11d,%eax
+++	addl	%ecx,%r13d
+++	roll	$30,%edi
+++	addl	%eax,%r13d
+++	roll	$1,%ebp
+++	xorl	56(%rsp),%r14d
+++	movl	%esi,%eax
+++	movl	%ebp,52(%rsp)
+++	movl	%r13d,%ecx
+++	xorl	0(%rsp),%r14d
+++	xorl	%r11d,%eax
+++	roll	$5,%ecx
+++	xorl	24(%rsp),%r14d
+++	leal	-899497514(%rbp,%r12,1),%r12d
+++	xorl	%edi,%eax
+++	addl	%ecx,%r12d
+++	roll	$30,%esi
+++	addl	%eax,%r12d
+++	roll	$1,%r14d
+++	xorl	60(%rsp),%edx
+++	movl	%r13d,%eax
+++	movl	%r14d,56(%rsp)
+++	movl	%r12d,%ecx
+++	xorl	4(%rsp),%edx
+++	xorl	%edi,%eax
+++	roll	$5,%ecx
+++	xorl	28(%rsp),%edx
+++	leal	-899497514(%r14,%r11,1),%r11d
+++	xorl	%esi,%eax
+++	addl	%ecx,%r11d
+++	roll	$30,%r13d
+++	addl	%eax,%r11d
+++	roll	$1,%edx
+++	xorl	0(%rsp),%ebp
+++	movl	%r12d,%eax
+++	movl	%edx,60(%rsp)
+++	movl	%r11d,%ecx
+++	xorl	8(%rsp),%ebp
+++	xorl	%esi,%eax
+++	roll	$5,%ecx
+++	xorl	32(%rsp),%ebp
+++	leal	-899497514(%rdx,%rdi,1),%edi
+++	xorl	%r13d,%eax
+++	addl	%ecx,%edi
+++	roll	$30,%r12d
+++	addl	%eax,%edi
+++	roll	$1,%ebp
+++	xorl	4(%rsp),%r14d
+++	movl	%r11d,%eax
+++	movl	%ebp,0(%rsp)
+++	movl	%edi,%ecx
+++	xorl	12(%rsp),%r14d
+++	xorl	%r13d,%eax
+++	roll	$5,%ecx
+++	xorl	36(%rsp),%r14d
+++	leal	-899497514(%rbp,%rsi,1),%esi
+++	xorl	%r12d,%eax
+++	addl	%ecx,%esi
+++	roll	$30,%r11d
+++	addl	%eax,%esi
+++	roll	$1,%r14d
+++	xorl	8(%rsp),%edx
+++	movl	%edi,%eax
+++	movl	%r14d,4(%rsp)
+++	movl	%esi,%ecx
+++	xorl	16(%rsp),%edx
+++	xorl	%r12d,%eax
+++	roll	$5,%ecx
+++	xorl	40(%rsp),%edx
+++	leal	-899497514(%r14,%r13,1),%r13d
+++	xorl	%r11d,%eax
+++	addl	%ecx,%r13d
+++	roll	$30,%edi
+++	addl	%eax,%r13d
+++	roll	$1,%edx
+++	xorl	12(%rsp),%ebp
+++	movl	%esi,%eax
+++	movl	%edx,8(%rsp)
+++	movl	%r13d,%ecx
+++	xorl	20(%rsp),%ebp
+++	xorl	%r11d,%eax
+++	roll	$5,%ecx
+++	xorl	44(%rsp),%ebp
+++	leal	-899497514(%rdx,%r12,1),%r12d
+++	xorl	%edi,%eax
+++	addl	%ecx,%r12d
+++	roll	$30,%esi
+++	addl	%eax,%r12d
+++	roll	$1,%ebp
+++	xorl	16(%rsp),%r14d
+++	movl	%r13d,%eax
+++	movl	%ebp,12(%rsp)
+++	movl	%r12d,%ecx
+++	xorl	24(%rsp),%r14d
+++	xorl	%edi,%eax
+++	roll	$5,%ecx
+++	xorl	48(%rsp),%r14d
+++	leal	-899497514(%rbp,%r11,1),%r11d
+++	xorl	%esi,%eax
+++	addl	%ecx,%r11d
+++	roll	$30,%r13d
+++	addl	%eax,%r11d
+++	roll	$1,%r14d
+++	xorl	20(%rsp),%edx
+++	movl	%r12d,%eax
+++	movl	%r14d,16(%rsp)
+++	movl	%r11d,%ecx
+++	xorl	28(%rsp),%edx
+++	xorl	%esi,%eax
+++	roll	$5,%ecx
+++	xorl	52(%rsp),%edx
+++	leal	-899497514(%r14,%rdi,1),%edi
+++	xorl	%r13d,%eax
+++	addl	%ecx,%edi
+++	roll	$30,%r12d
+++	addl	%eax,%edi
+++	roll	$1,%edx
+++	xorl	24(%rsp),%ebp
+++	movl	%r11d,%eax
+++	movl	%edx,20(%rsp)
+++	movl	%edi,%ecx
+++	xorl	32(%rsp),%ebp
+++	xorl	%r13d,%eax
+++	roll	$5,%ecx
+++	xorl	56(%rsp),%ebp
+++	leal	-899497514(%rdx,%rsi,1),%esi
+++	xorl	%r12d,%eax
+++	addl	%ecx,%esi
+++	roll	$30,%r11d
+++	addl	%eax,%esi
+++	roll	$1,%ebp
+++	xorl	28(%rsp),%r14d
+++	movl	%edi,%eax
+++	movl	%ebp,24(%rsp)
+++	movl	%esi,%ecx
+++	xorl	36(%rsp),%r14d
+++	xorl	%r12d,%eax
+++	roll	$5,%ecx
+++	xorl	60(%rsp),%r14d
+++	leal	-899497514(%rbp,%r13,1),%r13d
+++	xorl	%r11d,%eax
+++	addl	%ecx,%r13d
+++	roll	$30,%edi
+++	addl	%eax,%r13d
+++	roll	$1,%r14d
+++	xorl	32(%rsp),%edx
+++	movl	%esi,%eax
+++	movl	%r14d,28(%rsp)
+++	movl	%r13d,%ecx
+++	xorl	40(%rsp),%edx
+++	xorl	%r11d,%eax
+++	roll	$5,%ecx
+++	xorl	0(%rsp),%edx
+++	leal	-899497514(%r14,%r12,1),%r12d
+++	xorl	%edi,%eax
+++	addl	%ecx,%r12d
+++	roll	$30,%esi
+++	addl	%eax,%r12d
+++	roll	$1,%edx
+++	xorl	36(%rsp),%ebp
+++	movl	%r13d,%eax
+++
+++	movl	%r12d,%ecx
+++	xorl	44(%rsp),%ebp
+++	xorl	%edi,%eax
+++	roll	$5,%ecx
+++	xorl	4(%rsp),%ebp
+++	leal	-899497514(%rdx,%r11,1),%r11d
+++	xorl	%esi,%eax
+++	addl	%ecx,%r11d
+++	roll	$30,%r13d
+++	addl	%eax,%r11d
+++	roll	$1,%ebp
+++	xorl	40(%rsp),%r14d
+++	movl	%r12d,%eax
+++
+++	movl	%r11d,%ecx
+++	xorl	48(%rsp),%r14d
+++	xorl	%esi,%eax
+++	roll	$5,%ecx
+++	xorl	8(%rsp),%r14d
+++	leal	-899497514(%rbp,%rdi,1),%edi
+++	xorl	%r13d,%eax
+++	addl	%ecx,%edi
+++	roll	$30,%r12d
+++	addl	%eax,%edi
+++	roll	$1,%r14d
+++	xorl	44(%rsp),%edx
+++	movl	%r11d,%eax
+++
+++	movl	%edi,%ecx
+++	xorl	52(%rsp),%edx
+++	xorl	%r13d,%eax
+++	roll	$5,%ecx
+++	xorl	12(%rsp),%edx
+++	leal	-899497514(%r14,%rsi,1),%esi
+++	xorl	%r12d,%eax
+++	addl	%ecx,%esi
+++	roll	$30,%r11d
+++	addl	%eax,%esi
+++	roll	$1,%edx
+++	xorl	48(%rsp),%ebp
+++	movl	%edi,%eax
+++
+++	movl	%esi,%ecx
+++	xorl	56(%rsp),%ebp
+++	xorl	%r12d,%eax
+++	roll	$5,%ecx
+++	xorl	16(%rsp),%ebp
+++	leal	-899497514(%rdx,%r13,1),%r13d
+++	xorl	%r11d,%eax
+++	addl	%ecx,%r13d
+++	roll	$30,%edi
+++	addl	%eax,%r13d
+++	roll	$1,%ebp
+++	xorl	52(%rsp),%r14d
+++	movl	%esi,%eax
+++
+++	movl	%r13d,%ecx
+++	xorl	60(%rsp),%r14d
+++	xorl	%r11d,%eax
+++	roll	$5,%ecx
+++	xorl	20(%rsp),%r14d
+++	leal	-899497514(%rbp,%r12,1),%r12d
+++	xorl	%edi,%eax
+++	addl	%ecx,%r12d
+++	roll	$30,%esi
+++	addl	%eax,%r12d
+++	roll	$1,%r14d
+++	xorl	56(%rsp),%edx
+++	movl	%r13d,%eax
+++
+++	movl	%r12d,%ecx
+++	xorl	0(%rsp),%edx
+++	xorl	%edi,%eax
+++	roll	$5,%ecx
+++	xorl	24(%rsp),%edx
+++	leal	-899497514(%r14,%r11,1),%r11d
+++	xorl	%esi,%eax
+++	addl	%ecx,%r11d
+++	roll	$30,%r13d
+++	addl	%eax,%r11d
+++	roll	$1,%edx
+++	xorl	60(%rsp),%ebp
+++	movl	%r12d,%eax
+++
+++	movl	%r11d,%ecx
+++	xorl	4(%rsp),%ebp
+++	xorl	%esi,%eax
+++	roll	$5,%ecx
+++	xorl	28(%rsp),%ebp
+++	leal	-899497514(%rdx,%rdi,1),%edi
+++	xorl	%r13d,%eax
+++	addl	%ecx,%edi
+++	roll	$30,%r12d
+++	addl	%eax,%edi
+++	roll	$1,%ebp
+++	movl	%r11d,%eax
+++	movl	%edi,%ecx
+++	xorl	%r13d,%eax
+++	leal	-899497514(%rbp,%rsi,1),%esi
+++	roll	$5,%ecx
+++	xorl	%r12d,%eax
+++	addl	%ecx,%esi
+++	roll	$30,%r11d
+++	addl	%eax,%esi
+++	addl	0(%r8),%esi
+++	addl	4(%r8),%edi
+++	addl	8(%r8),%r11d
+++	addl	12(%r8),%r12d
+++	addl	16(%r8),%r13d
+++	movl	%esi,0(%r8)
+++	movl	%edi,4(%r8)
+++	movl	%r11d,8(%r8)
+++	movl	%r12d,12(%r8)
+++	movl	%r13d,16(%r8)
+++
+++	subq	$1,%r10
+++	leaq	64(%r9),%r9
+++	jnz	.Lloop
+++
+++	movq	64(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lepilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	sha1_block_data_order,.-sha1_block_data_order
+++.type	sha1_block_data_order_shaext,@function
+++.align	32
+++sha1_block_data_order_shaext:
+++_shaext_shortcut:
+++.cfi_startproc	
+++	movdqu	(%rdi),%xmm0
+++	movd	16(%rdi),%xmm1
+++	movdqa	K_XX_XX+160(%rip),%xmm3
+++
+++	movdqu	(%rsi),%xmm4
+++	pshufd	$27,%xmm0,%xmm0
+++	movdqu	16(%rsi),%xmm5
+++	pshufd	$27,%xmm1,%xmm1
+++	movdqu	32(%rsi),%xmm6
+++.byte	102,15,56,0,227
+++	movdqu	48(%rsi),%xmm7
+++.byte	102,15,56,0,235
+++.byte	102,15,56,0,243
+++	movdqa	%xmm1,%xmm9
+++.byte	102,15,56,0,251
+++	jmp	.Loop_shaext
+++
+++.align	16
+++.Loop_shaext:
+++	decq	%rdx
+++	leaq	64(%rsi),%r8
+++	paddd	%xmm4,%xmm1
+++	cmovneq	%r8,%rsi
+++	movdqa	%xmm0,%xmm8
+++.byte	15,56,201,229
+++	movdqa	%xmm0,%xmm2
+++.byte	15,58,204,193,0
+++.byte	15,56,200,213
+++	pxor	%xmm6,%xmm4
+++.byte	15,56,201,238
+++.byte	15,56,202,231
+++
+++	movdqa	%xmm0,%xmm1
+++.byte	15,58,204,194,0
+++.byte	15,56,200,206
+++	pxor	%xmm7,%xmm5
+++.byte	15,56,202,236
+++.byte	15,56,201,247
+++	movdqa	%xmm0,%xmm2
+++.byte	15,58,204,193,0
+++.byte	15,56,200,215
+++	pxor	%xmm4,%xmm6
+++.byte	15,56,201,252
+++.byte	15,56,202,245
+++
+++	movdqa	%xmm0,%xmm1
+++.byte	15,58,204,194,0
+++.byte	15,56,200,204
+++	pxor	%xmm5,%xmm7
+++.byte	15,56,202,254
+++.byte	15,56,201,229
+++	movdqa	%xmm0,%xmm2
+++.byte	15,58,204,193,0
+++.byte	15,56,200,213
+++	pxor	%xmm6,%xmm4
+++.byte	15,56,201,238
+++.byte	15,56,202,231
+++
+++	movdqa	%xmm0,%xmm1
+++.byte	15,58,204,194,1
+++.byte	15,56,200,206
+++	pxor	%xmm7,%xmm5
+++.byte	15,56,202,236
+++.byte	15,56,201,247
+++	movdqa	%xmm0,%xmm2
+++.byte	15,58,204,193,1
+++.byte	15,56,200,215
+++	pxor	%xmm4,%xmm6
+++.byte	15,56,201,252
+++.byte	15,56,202,245
+++
+++	movdqa	%xmm0,%xmm1
+++.byte	15,58,204,194,1
+++.byte	15,56,200,204
+++	pxor	%xmm5,%xmm7
+++.byte	15,56,202,254
+++.byte	15,56,201,229
+++	movdqa	%xmm0,%xmm2
+++.byte	15,58,204,193,1
+++.byte	15,56,200,213
+++	pxor	%xmm6,%xmm4
+++.byte	15,56,201,238
+++.byte	15,56,202,231
+++
+++	movdqa	%xmm0,%xmm1
+++.byte	15,58,204,194,1
+++.byte	15,56,200,206
+++	pxor	%xmm7,%xmm5
+++.byte	15,56,202,236
+++.byte	15,56,201,247
+++	movdqa	%xmm0,%xmm2
+++.byte	15,58,204,193,2
+++.byte	15,56,200,215
+++	pxor	%xmm4,%xmm6
+++.byte	15,56,201,252
+++.byte	15,56,202,245
+++
+++	movdqa	%xmm0,%xmm1
+++.byte	15,58,204,194,2
+++.byte	15,56,200,204
+++	pxor	%xmm5,%xmm7
+++.byte	15,56,202,254
+++.byte	15,56,201,229
+++	movdqa	%xmm0,%xmm2
+++.byte	15,58,204,193,2
+++.byte	15,56,200,213
+++	pxor	%xmm6,%xmm4
+++.byte	15,56,201,238
+++.byte	15,56,202,231
+++
+++	movdqa	%xmm0,%xmm1
+++.byte	15,58,204,194,2
+++.byte	15,56,200,206
+++	pxor	%xmm7,%xmm5
+++.byte	15,56,202,236
+++.byte	15,56,201,247
+++	movdqa	%xmm0,%xmm2
+++.byte	15,58,204,193,2
+++.byte	15,56,200,215
+++	pxor	%xmm4,%xmm6
+++.byte	15,56,201,252
+++.byte	15,56,202,245
+++
+++	movdqa	%xmm0,%xmm1
+++.byte	15,58,204,194,3
+++.byte	15,56,200,204
+++	pxor	%xmm5,%xmm7
+++.byte	15,56,202,254
+++	movdqu	(%rsi),%xmm4
+++	movdqa	%xmm0,%xmm2
+++.byte	15,58,204,193,3
+++.byte	15,56,200,213
+++	movdqu	16(%rsi),%xmm5
+++.byte	102,15,56,0,227
+++
+++	movdqa	%xmm0,%xmm1
+++.byte	15,58,204,194,3
+++.byte	15,56,200,206
+++	movdqu	32(%rsi),%xmm6
+++.byte	102,15,56,0,235
+++
+++	movdqa	%xmm0,%xmm2
+++.byte	15,58,204,193,3
+++.byte	15,56,200,215
+++	movdqu	48(%rsi),%xmm7
+++.byte	102,15,56,0,243
+++
+++	movdqa	%xmm0,%xmm1
+++.byte	15,58,204,194,3
+++.byte	65,15,56,200,201
+++.byte	102,15,56,0,251
+++
+++	paddd	%xmm8,%xmm0
+++	movdqa	%xmm1,%xmm9
+++
+++	jnz	.Loop_shaext
+++
+++	pshufd	$27,%xmm0,%xmm0
+++	pshufd	$27,%xmm1,%xmm1
+++	movdqu	%xmm0,(%rdi)
+++	movd	%xmm1,16(%rdi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	sha1_block_data_order_shaext,.-sha1_block_data_order_shaext
+++.type	sha1_block_data_order_ssse3,@function
+++.align	16
+++sha1_block_data_order_ssse3:
+++_ssse3_shortcut:
+++.cfi_startproc	
+++	movq	%rsp,%r11
+++.cfi_def_cfa_register	%r11
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	leaq	-64(%rsp),%rsp
+++	andq	$-64,%rsp
+++	movq	%rdi,%r8
+++	movq	%rsi,%r9
+++	movq	%rdx,%r10
+++
+++	shlq	$6,%r10
+++	addq	%r9,%r10
+++	leaq	K_XX_XX+64(%rip),%r14
+++
+++	movl	0(%r8),%eax
+++	movl	4(%r8),%ebx
+++	movl	8(%r8),%ecx
+++	movl	12(%r8),%edx
+++	movl	%ebx,%esi
+++	movl	16(%r8),%ebp
+++	movl	%ecx,%edi
+++	xorl	%edx,%edi
+++	andl	%edi,%esi
+++
+++	movdqa	64(%r14),%xmm6
+++	movdqa	-64(%r14),%xmm9
+++	movdqu	0(%r9),%xmm0
+++	movdqu	16(%r9),%xmm1
+++	movdqu	32(%r9),%xmm2
+++	movdqu	48(%r9),%xmm3
+++.byte	102,15,56,0,198
+++.byte	102,15,56,0,206
+++.byte	102,15,56,0,214
+++	addq	$64,%r9
+++	paddd	%xmm9,%xmm0
+++.byte	102,15,56,0,222
+++	paddd	%xmm9,%xmm1
+++	paddd	%xmm9,%xmm2
+++	movdqa	%xmm0,0(%rsp)
+++	psubd	%xmm9,%xmm0
+++	movdqa	%xmm1,16(%rsp)
+++	psubd	%xmm9,%xmm1
+++	movdqa	%xmm2,32(%rsp)
+++	psubd	%xmm9,%xmm2
+++	jmp	.Loop_ssse3
+++.align	16
+++.Loop_ssse3:
+++	rorl	$2,%ebx
+++	pshufd	$238,%xmm0,%xmm4
+++	xorl	%edx,%esi
+++	movdqa	%xmm3,%xmm8
+++	paddd	%xmm3,%xmm9
+++	movl	%eax,%edi
+++	addl	0(%rsp),%ebp
+++	punpcklqdq	%xmm1,%xmm4
+++	xorl	%ecx,%ebx
+++	roll	$5,%eax
+++	addl	%esi,%ebp
+++	psrldq	$4,%xmm8
+++	andl	%ebx,%edi
+++	xorl	%ecx,%ebx
+++	pxor	%xmm0,%xmm4
+++	addl	%eax,%ebp
+++	rorl	$7,%eax
+++	pxor	%xmm2,%xmm8
+++	xorl	%ecx,%edi
+++	movl	%ebp,%esi
+++	addl	4(%rsp),%edx
+++	pxor	%xmm8,%xmm4
+++	xorl	%ebx,%eax
+++	roll	$5,%ebp
+++	movdqa	%xmm9,48(%rsp)
+++	addl	%edi,%edx
+++	andl	%eax,%esi
+++	movdqa	%xmm4,%xmm10
+++	xorl	%ebx,%eax
+++	addl	%ebp,%edx
+++	rorl	$7,%ebp
+++	movdqa	%xmm4,%xmm8
+++	xorl	%ebx,%esi
+++	pslldq	$12,%xmm10
+++	paddd	%xmm4,%xmm4
+++	movl	%edx,%edi
+++	addl	8(%rsp),%ecx
+++	psrld	$31,%xmm8
+++	xorl	%eax,%ebp
+++	roll	$5,%edx
+++	addl	%esi,%ecx
+++	movdqa	%xmm10,%xmm9
+++	andl	%ebp,%edi
+++	xorl	%eax,%ebp
+++	psrld	$30,%xmm10
+++	addl	%edx,%ecx
+++	rorl	$7,%edx
+++	por	%xmm8,%xmm4
+++	xorl	%eax,%edi
+++	movl	%ecx,%esi
+++	addl	12(%rsp),%ebx
+++	pslld	$2,%xmm9
+++	pxor	%xmm10,%xmm4
+++	xorl	%ebp,%edx
+++	movdqa	-64(%r14),%xmm10
+++	roll	$5,%ecx
+++	addl	%edi,%ebx
+++	andl	%edx,%esi
+++	pxor	%xmm9,%xmm4
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	rorl	$7,%ecx
+++	pshufd	$238,%xmm1,%xmm5
+++	xorl	%ebp,%esi
+++	movdqa	%xmm4,%xmm9
+++	paddd	%xmm4,%xmm10
+++	movl	%ebx,%edi
+++	addl	16(%rsp),%eax
+++	punpcklqdq	%xmm2,%xmm5
+++	xorl	%edx,%ecx
+++	roll	$5,%ebx
+++	addl	%esi,%eax
+++	psrldq	$4,%xmm9
+++	andl	%ecx,%edi
+++	xorl	%edx,%ecx
+++	pxor	%xmm1,%xmm5
+++	addl	%ebx,%eax
+++	rorl	$7,%ebx
+++	pxor	%xmm3,%xmm9
+++	xorl	%edx,%edi
+++	movl	%eax,%esi
+++	addl	20(%rsp),%ebp
+++	pxor	%xmm9,%xmm5
+++	xorl	%ecx,%ebx
+++	roll	$5,%eax
+++	movdqa	%xmm10,0(%rsp)
+++	addl	%edi,%ebp
+++	andl	%ebx,%esi
+++	movdqa	%xmm5,%xmm8
+++	xorl	%ecx,%ebx
+++	addl	%eax,%ebp
+++	rorl	$7,%eax
+++	movdqa	%xmm5,%xmm9
+++	xorl	%ecx,%esi
+++	pslldq	$12,%xmm8
+++	paddd	%xmm5,%xmm5
+++	movl	%ebp,%edi
+++	addl	24(%rsp),%edx
+++	psrld	$31,%xmm9
+++	xorl	%ebx,%eax
+++	roll	$5,%ebp
+++	addl	%esi,%edx
+++	movdqa	%xmm8,%xmm10
+++	andl	%eax,%edi
+++	xorl	%ebx,%eax
+++	psrld	$30,%xmm8
+++	addl	%ebp,%edx
+++	rorl	$7,%ebp
+++	por	%xmm9,%xmm5
+++	xorl	%ebx,%edi
+++	movl	%edx,%esi
+++	addl	28(%rsp),%ecx
+++	pslld	$2,%xmm10
+++	pxor	%xmm8,%xmm5
+++	xorl	%eax,%ebp
+++	movdqa	-32(%r14),%xmm8
+++	roll	$5,%edx
+++	addl	%edi,%ecx
+++	andl	%ebp,%esi
+++	pxor	%xmm10,%xmm5
+++	xorl	%eax,%ebp
+++	addl	%edx,%ecx
+++	rorl	$7,%edx
+++	pshufd	$238,%xmm2,%xmm6
+++	xorl	%eax,%esi
+++	movdqa	%xmm5,%xmm10
+++	paddd	%xmm5,%xmm8
+++	movl	%ecx,%edi
+++	addl	32(%rsp),%ebx
+++	punpcklqdq	%xmm3,%xmm6
+++	xorl	%ebp,%edx
+++	roll	$5,%ecx
+++	addl	%esi,%ebx
+++	psrldq	$4,%xmm10
+++	andl	%edx,%edi
+++	xorl	%ebp,%edx
+++	pxor	%xmm2,%xmm6
+++	addl	%ecx,%ebx
+++	rorl	$7,%ecx
+++	pxor	%xmm4,%xmm10
+++	xorl	%ebp,%edi
+++	movl	%ebx,%esi
+++	addl	36(%rsp),%eax
+++	pxor	%xmm10,%xmm6
+++	xorl	%edx,%ecx
+++	roll	$5,%ebx
+++	movdqa	%xmm8,16(%rsp)
+++	addl	%edi,%eax
+++	andl	%ecx,%esi
+++	movdqa	%xmm6,%xmm9
+++	xorl	%edx,%ecx
+++	addl	%ebx,%eax
+++	rorl	$7,%ebx
+++	movdqa	%xmm6,%xmm10
+++	xorl	%edx,%esi
+++	pslldq	$12,%xmm9
+++	paddd	%xmm6,%xmm6
+++	movl	%eax,%edi
+++	addl	40(%rsp),%ebp
+++	psrld	$31,%xmm10
+++	xorl	%ecx,%ebx
+++	roll	$5,%eax
+++	addl	%esi,%ebp
+++	movdqa	%xmm9,%xmm8
+++	andl	%ebx,%edi
+++	xorl	%ecx,%ebx
+++	psrld	$30,%xmm9
+++	addl	%eax,%ebp
+++	rorl	$7,%eax
+++	por	%xmm10,%xmm6
+++	xorl	%ecx,%edi
+++	movl	%ebp,%esi
+++	addl	44(%rsp),%edx
+++	pslld	$2,%xmm8
+++	pxor	%xmm9,%xmm6
+++	xorl	%ebx,%eax
+++	movdqa	-32(%r14),%xmm9
+++	roll	$5,%ebp
+++	addl	%edi,%edx
+++	andl	%eax,%esi
+++	pxor	%xmm8,%xmm6
+++	xorl	%ebx,%eax
+++	addl	%ebp,%edx
+++	rorl	$7,%ebp
+++	pshufd	$238,%xmm3,%xmm7
+++	xorl	%ebx,%esi
+++	movdqa	%xmm6,%xmm8
+++	paddd	%xmm6,%xmm9
+++	movl	%edx,%edi
+++	addl	48(%rsp),%ecx
+++	punpcklqdq	%xmm4,%xmm7
+++	xorl	%eax,%ebp
+++	roll	$5,%edx
+++	addl	%esi,%ecx
+++	psrldq	$4,%xmm8
+++	andl	%ebp,%edi
+++	xorl	%eax,%ebp
+++	pxor	%xmm3,%xmm7
+++	addl	%edx,%ecx
+++	rorl	$7,%edx
+++	pxor	%xmm5,%xmm8
+++	xorl	%eax,%edi
+++	movl	%ecx,%esi
+++	addl	52(%rsp),%ebx
+++	pxor	%xmm8,%xmm7
+++	xorl	%ebp,%edx
+++	roll	$5,%ecx
+++	movdqa	%xmm9,32(%rsp)
+++	addl	%edi,%ebx
+++	andl	%edx,%esi
+++	movdqa	%xmm7,%xmm10
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	rorl	$7,%ecx
+++	movdqa	%xmm7,%xmm8
+++	xorl	%ebp,%esi
+++	pslldq	$12,%xmm10
+++	paddd	%xmm7,%xmm7
+++	movl	%ebx,%edi
+++	addl	56(%rsp),%eax
+++	psrld	$31,%xmm8
+++	xorl	%edx,%ecx
+++	roll	$5,%ebx
+++	addl	%esi,%eax
+++	movdqa	%xmm10,%xmm9
+++	andl	%ecx,%edi
+++	xorl	%edx,%ecx
+++	psrld	$30,%xmm10
+++	addl	%ebx,%eax
+++	rorl	$7,%ebx
+++	por	%xmm8,%xmm7
+++	xorl	%edx,%edi
+++	movl	%eax,%esi
+++	addl	60(%rsp),%ebp
+++	pslld	$2,%xmm9
+++	pxor	%xmm10,%xmm7
+++	xorl	%ecx,%ebx
+++	movdqa	-32(%r14),%xmm10
+++	roll	$5,%eax
+++	addl	%edi,%ebp
+++	andl	%ebx,%esi
+++	pxor	%xmm9,%xmm7
+++	pshufd	$238,%xmm6,%xmm9
+++	xorl	%ecx,%ebx
+++	addl	%eax,%ebp
+++	rorl	$7,%eax
+++	pxor	%xmm4,%xmm0
+++	xorl	%ecx,%esi
+++	movl	%ebp,%edi
+++	addl	0(%rsp),%edx
+++	punpcklqdq	%xmm7,%xmm9
+++	xorl	%ebx,%eax
+++	roll	$5,%ebp
+++	pxor	%xmm1,%xmm0
+++	addl	%esi,%edx
+++	andl	%eax,%edi
+++	movdqa	%xmm10,%xmm8
+++	xorl	%ebx,%eax
+++	paddd	%xmm7,%xmm10
+++	addl	%ebp,%edx
+++	pxor	%xmm9,%xmm0
+++	rorl	$7,%ebp
+++	xorl	%ebx,%edi
+++	movl	%edx,%esi
+++	addl	4(%rsp),%ecx
+++	movdqa	%xmm0,%xmm9
+++	xorl	%eax,%ebp
+++	roll	$5,%edx
+++	movdqa	%xmm10,48(%rsp)
+++	addl	%edi,%ecx
+++	andl	%ebp,%esi
+++	xorl	%eax,%ebp
+++	pslld	$2,%xmm0
+++	addl	%edx,%ecx
+++	rorl	$7,%edx
+++	psrld	$30,%xmm9
+++	xorl	%eax,%esi
+++	movl	%ecx,%edi
+++	addl	8(%rsp),%ebx
+++	por	%xmm9,%xmm0
+++	xorl	%ebp,%edx
+++	roll	$5,%ecx
+++	pshufd	$238,%xmm7,%xmm10
+++	addl	%esi,%ebx
+++	andl	%edx,%edi
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	addl	12(%rsp),%eax
+++	xorl	%ebp,%edi
+++	movl	%ebx,%esi
+++	roll	$5,%ebx
+++	addl	%edi,%eax
+++	xorl	%edx,%esi
+++	rorl	$7,%ecx
+++	addl	%ebx,%eax
+++	pxor	%xmm5,%xmm1
+++	addl	16(%rsp),%ebp
+++	xorl	%ecx,%esi
+++	punpcklqdq	%xmm0,%xmm10
+++	movl	%eax,%edi
+++	roll	$5,%eax
+++	pxor	%xmm2,%xmm1
+++	addl	%esi,%ebp
+++	xorl	%ecx,%edi
+++	movdqa	%xmm8,%xmm9
+++	rorl	$7,%ebx
+++	paddd	%xmm0,%xmm8
+++	addl	%eax,%ebp
+++	pxor	%xmm10,%xmm1
+++	addl	20(%rsp),%edx
+++	xorl	%ebx,%edi
+++	movl	%ebp,%esi
+++	roll	$5,%ebp
+++	movdqa	%xmm1,%xmm10
+++	addl	%edi,%edx
+++	xorl	%ebx,%esi
+++	movdqa	%xmm8,0(%rsp)
+++	rorl	$7,%eax
+++	addl	%ebp,%edx
+++	addl	24(%rsp),%ecx
+++	pslld	$2,%xmm1
+++	xorl	%eax,%esi
+++	movl	%edx,%edi
+++	psrld	$30,%xmm10
+++	roll	$5,%edx
+++	addl	%esi,%ecx
+++	xorl	%eax,%edi
+++	rorl	$7,%ebp
+++	por	%xmm10,%xmm1
+++	addl	%edx,%ecx
+++	addl	28(%rsp),%ebx
+++	pshufd	$238,%xmm0,%xmm8
+++	xorl	%ebp,%edi
+++	movl	%ecx,%esi
+++	roll	$5,%ecx
+++	addl	%edi,%ebx
+++	xorl	%ebp,%esi
+++	rorl	$7,%edx
+++	addl	%ecx,%ebx
+++	pxor	%xmm6,%xmm2
+++	addl	32(%rsp),%eax
+++	xorl	%edx,%esi
+++	punpcklqdq	%xmm1,%xmm8
+++	movl	%ebx,%edi
+++	roll	$5,%ebx
+++	pxor	%xmm3,%xmm2
+++	addl	%esi,%eax
+++	xorl	%edx,%edi
+++	movdqa	0(%r14),%xmm10
+++	rorl	$7,%ecx
+++	paddd	%xmm1,%xmm9
+++	addl	%ebx,%eax
+++	pxor	%xmm8,%xmm2
+++	addl	36(%rsp),%ebp
+++	xorl	%ecx,%edi
+++	movl	%eax,%esi
+++	roll	$5,%eax
+++	movdqa	%xmm2,%xmm8
+++	addl	%edi,%ebp
+++	xorl	%ecx,%esi
+++	movdqa	%xmm9,16(%rsp)
+++	rorl	$7,%ebx
+++	addl	%eax,%ebp
+++	addl	40(%rsp),%edx
+++	pslld	$2,%xmm2
+++	xorl	%ebx,%esi
+++	movl	%ebp,%edi
+++	psrld	$30,%xmm8
+++	roll	$5,%ebp
+++	addl	%esi,%edx
+++	xorl	%ebx,%edi
+++	rorl	$7,%eax
+++	por	%xmm8,%xmm2
+++	addl	%ebp,%edx
+++	addl	44(%rsp),%ecx
+++	pshufd	$238,%xmm1,%xmm9
+++	xorl	%eax,%edi
+++	movl	%edx,%esi
+++	roll	$5,%edx
+++	addl	%edi,%ecx
+++	xorl	%eax,%esi
+++	rorl	$7,%ebp
+++	addl	%edx,%ecx
+++	pxor	%xmm7,%xmm3
+++	addl	48(%rsp),%ebx
+++	xorl	%ebp,%esi
+++	punpcklqdq	%xmm2,%xmm9
+++	movl	%ecx,%edi
+++	roll	$5,%ecx
+++	pxor	%xmm4,%xmm3
+++	addl	%esi,%ebx
+++	xorl	%ebp,%edi
+++	movdqa	%xmm10,%xmm8
+++	rorl	$7,%edx
+++	paddd	%xmm2,%xmm10
+++	addl	%ecx,%ebx
+++	pxor	%xmm9,%xmm3
+++	addl	52(%rsp),%eax
+++	xorl	%edx,%edi
+++	movl	%ebx,%esi
+++	roll	$5,%ebx
+++	movdqa	%xmm3,%xmm9
+++	addl	%edi,%eax
+++	xorl	%edx,%esi
+++	movdqa	%xmm10,32(%rsp)
+++	rorl	$7,%ecx
+++	addl	%ebx,%eax
+++	addl	56(%rsp),%ebp
+++	pslld	$2,%xmm3
+++	xorl	%ecx,%esi
+++	movl	%eax,%edi
+++	psrld	$30,%xmm9
+++	roll	$5,%eax
+++	addl	%esi,%ebp
+++	xorl	%ecx,%edi
+++	rorl	$7,%ebx
+++	por	%xmm9,%xmm3
+++	addl	%eax,%ebp
+++	addl	60(%rsp),%edx
+++	pshufd	$238,%xmm2,%xmm10
+++	xorl	%ebx,%edi
+++	movl	%ebp,%esi
+++	roll	$5,%ebp
+++	addl	%edi,%edx
+++	xorl	%ebx,%esi
+++	rorl	$7,%eax
+++	addl	%ebp,%edx
+++	pxor	%xmm0,%xmm4
+++	addl	0(%rsp),%ecx
+++	xorl	%eax,%esi
+++	punpcklqdq	%xmm3,%xmm10
+++	movl	%edx,%edi
+++	roll	$5,%edx
+++	pxor	%xmm5,%xmm4
+++	addl	%esi,%ecx
+++	xorl	%eax,%edi
+++	movdqa	%xmm8,%xmm9
+++	rorl	$7,%ebp
+++	paddd	%xmm3,%xmm8
+++	addl	%edx,%ecx
+++	pxor	%xmm10,%xmm4
+++	addl	4(%rsp),%ebx
+++	xorl	%ebp,%edi
+++	movl	%ecx,%esi
+++	roll	$5,%ecx
+++	movdqa	%xmm4,%xmm10
+++	addl	%edi,%ebx
+++	xorl	%ebp,%esi
+++	movdqa	%xmm8,48(%rsp)
+++	rorl	$7,%edx
+++	addl	%ecx,%ebx
+++	addl	8(%rsp),%eax
+++	pslld	$2,%xmm4
+++	xorl	%edx,%esi
+++	movl	%ebx,%edi
+++	psrld	$30,%xmm10
+++	roll	$5,%ebx
+++	addl	%esi,%eax
+++	xorl	%edx,%edi
+++	rorl	$7,%ecx
+++	por	%xmm10,%xmm4
+++	addl	%ebx,%eax
+++	addl	12(%rsp),%ebp
+++	pshufd	$238,%xmm3,%xmm8
+++	xorl	%ecx,%edi
+++	movl	%eax,%esi
+++	roll	$5,%eax
+++	addl	%edi,%ebp
+++	xorl	%ecx,%esi
+++	rorl	$7,%ebx
+++	addl	%eax,%ebp
+++	pxor	%xmm1,%xmm5
+++	addl	16(%rsp),%edx
+++	xorl	%ebx,%esi
+++	punpcklqdq	%xmm4,%xmm8
+++	movl	%ebp,%edi
+++	roll	$5,%ebp
+++	pxor	%xmm6,%xmm5
+++	addl	%esi,%edx
+++	xorl	%ebx,%edi
+++	movdqa	%xmm9,%xmm10
+++	rorl	$7,%eax
+++	paddd	%xmm4,%xmm9
+++	addl	%ebp,%edx
+++	pxor	%xmm8,%xmm5
+++	addl	20(%rsp),%ecx
+++	xorl	%eax,%edi
+++	movl	%edx,%esi
+++	roll	$5,%edx
+++	movdqa	%xmm5,%xmm8
+++	addl	%edi,%ecx
+++	xorl	%eax,%esi
+++	movdqa	%xmm9,0(%rsp)
+++	rorl	$7,%ebp
+++	addl	%edx,%ecx
+++	addl	24(%rsp),%ebx
+++	pslld	$2,%xmm5
+++	xorl	%ebp,%esi
+++	movl	%ecx,%edi
+++	psrld	$30,%xmm8
+++	roll	$5,%ecx
+++	addl	%esi,%ebx
+++	xorl	%ebp,%edi
+++	rorl	$7,%edx
+++	por	%xmm8,%xmm5
+++	addl	%ecx,%ebx
+++	addl	28(%rsp),%eax
+++	pshufd	$238,%xmm4,%xmm9
+++	rorl	$7,%ecx
+++	movl	%ebx,%esi
+++	xorl	%edx,%edi
+++	roll	$5,%ebx
+++	addl	%edi,%eax
+++	xorl	%ecx,%esi
+++	xorl	%edx,%ecx
+++	addl	%ebx,%eax
+++	pxor	%xmm2,%xmm6
+++	addl	32(%rsp),%ebp
+++	andl	%ecx,%esi
+++	xorl	%edx,%ecx
+++	rorl	$7,%ebx
+++	punpcklqdq	%xmm5,%xmm9
+++	movl	%eax,%edi
+++	xorl	%ecx,%esi
+++	pxor	%xmm7,%xmm6
+++	roll	$5,%eax
+++	addl	%esi,%ebp
+++	movdqa	%xmm10,%xmm8
+++	xorl	%ebx,%edi
+++	paddd	%xmm5,%xmm10
+++	xorl	%ecx,%ebx
+++	pxor	%xmm9,%xmm6
+++	addl	%eax,%ebp
+++	addl	36(%rsp),%edx
+++	andl	%ebx,%edi
+++	xorl	%ecx,%ebx
+++	rorl	$7,%eax
+++	movdqa	%xmm6,%xmm9
+++	movl	%ebp,%esi
+++	xorl	%ebx,%edi
+++	movdqa	%xmm10,16(%rsp)
+++	roll	$5,%ebp
+++	addl	%edi,%edx
+++	xorl	%eax,%esi
+++	pslld	$2,%xmm6
+++	xorl	%ebx,%eax
+++	addl	%ebp,%edx
+++	psrld	$30,%xmm9
+++	addl	40(%rsp),%ecx
+++	andl	%eax,%esi
+++	xorl	%ebx,%eax
+++	por	%xmm9,%xmm6
+++	rorl	$7,%ebp
+++	movl	%edx,%edi
+++	xorl	%eax,%esi
+++	roll	$5,%edx
+++	pshufd	$238,%xmm5,%xmm10
+++	addl	%esi,%ecx
+++	xorl	%ebp,%edi
+++	xorl	%eax,%ebp
+++	addl	%edx,%ecx
+++	addl	44(%rsp),%ebx
+++	andl	%ebp,%edi
+++	xorl	%eax,%ebp
+++	rorl	$7,%edx
+++	movl	%ecx,%esi
+++	xorl	%ebp,%edi
+++	roll	$5,%ecx
+++	addl	%edi,%ebx
+++	xorl	%edx,%esi
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	pxor	%xmm3,%xmm7
+++	addl	48(%rsp),%eax
+++	andl	%edx,%esi
+++	xorl	%ebp,%edx
+++	rorl	$7,%ecx
+++	punpcklqdq	%xmm6,%xmm10
+++	movl	%ebx,%edi
+++	xorl	%edx,%esi
+++	pxor	%xmm0,%xmm7
+++	roll	$5,%ebx
+++	addl	%esi,%eax
+++	movdqa	32(%r14),%xmm9
+++	xorl	%ecx,%edi
+++	paddd	%xmm6,%xmm8
+++	xorl	%edx,%ecx
+++	pxor	%xmm10,%xmm7
+++	addl	%ebx,%eax
+++	addl	52(%rsp),%ebp
+++	andl	%ecx,%edi
+++	xorl	%edx,%ecx
+++	rorl	$7,%ebx
+++	movdqa	%xmm7,%xmm10
+++	movl	%eax,%esi
+++	xorl	%ecx,%edi
+++	movdqa	%xmm8,32(%rsp)
+++	roll	$5,%eax
+++	addl	%edi,%ebp
+++	xorl	%ebx,%esi
+++	pslld	$2,%xmm7
+++	xorl	%ecx,%ebx
+++	addl	%eax,%ebp
+++	psrld	$30,%xmm10
+++	addl	56(%rsp),%edx
+++	andl	%ebx,%esi
+++	xorl	%ecx,%ebx
+++	por	%xmm10,%xmm7
+++	rorl	$7,%eax
+++	movl	%ebp,%edi
+++	xorl	%ebx,%esi
+++	roll	$5,%ebp
+++	pshufd	$238,%xmm6,%xmm8
+++	addl	%esi,%edx
+++	xorl	%eax,%edi
+++	xorl	%ebx,%eax
+++	addl	%ebp,%edx
+++	addl	60(%rsp),%ecx
+++	andl	%eax,%edi
+++	xorl	%ebx,%eax
+++	rorl	$7,%ebp
+++	movl	%edx,%esi
+++	xorl	%eax,%edi
+++	roll	$5,%edx
+++	addl	%edi,%ecx
+++	xorl	%ebp,%esi
+++	xorl	%eax,%ebp
+++	addl	%edx,%ecx
+++	pxor	%xmm4,%xmm0
+++	addl	0(%rsp),%ebx
+++	andl	%ebp,%esi
+++	xorl	%eax,%ebp
+++	rorl	$7,%edx
+++	punpcklqdq	%xmm7,%xmm8
+++	movl	%ecx,%edi
+++	xorl	%ebp,%esi
+++	pxor	%xmm1,%xmm0
+++	roll	$5,%ecx
+++	addl	%esi,%ebx
+++	movdqa	%xmm9,%xmm10
+++	xorl	%edx,%edi
+++	paddd	%xmm7,%xmm9
+++	xorl	%ebp,%edx
+++	pxor	%xmm8,%xmm0
+++	addl	%ecx,%ebx
+++	addl	4(%rsp),%eax
+++	andl	%edx,%edi
+++	xorl	%ebp,%edx
+++	rorl	$7,%ecx
+++	movdqa	%xmm0,%xmm8
+++	movl	%ebx,%esi
+++	xorl	%edx,%edi
+++	movdqa	%xmm9,48(%rsp)
+++	roll	$5,%ebx
+++	addl	%edi,%eax
+++	xorl	%ecx,%esi
+++	pslld	$2,%xmm0
+++	xorl	%edx,%ecx
+++	addl	%ebx,%eax
+++	psrld	$30,%xmm8
+++	addl	8(%rsp),%ebp
+++	andl	%ecx,%esi
+++	xorl	%edx,%ecx
+++	por	%xmm8,%xmm0
+++	rorl	$7,%ebx
+++	movl	%eax,%edi
+++	xorl	%ecx,%esi
+++	roll	$5,%eax
+++	pshufd	$238,%xmm7,%xmm9
+++	addl	%esi,%ebp
+++	xorl	%ebx,%edi
+++	xorl	%ecx,%ebx
+++	addl	%eax,%ebp
+++	addl	12(%rsp),%edx
+++	andl	%ebx,%edi
+++	xorl	%ecx,%ebx
+++	rorl	$7,%eax
+++	movl	%ebp,%esi
+++	xorl	%ebx,%edi
+++	roll	$5,%ebp
+++	addl	%edi,%edx
+++	xorl	%eax,%esi
+++	xorl	%ebx,%eax
+++	addl	%ebp,%edx
+++	pxor	%xmm5,%xmm1
+++	addl	16(%rsp),%ecx
+++	andl	%eax,%esi
+++	xorl	%ebx,%eax
+++	rorl	$7,%ebp
+++	punpcklqdq	%xmm0,%xmm9
+++	movl	%edx,%edi
+++	xorl	%eax,%esi
+++	pxor	%xmm2,%xmm1
+++	roll	$5,%edx
+++	addl	%esi,%ecx
+++	movdqa	%xmm10,%xmm8
+++	xorl	%ebp,%edi
+++	paddd	%xmm0,%xmm10
+++	xorl	%eax,%ebp
+++	pxor	%xmm9,%xmm1
+++	addl	%edx,%ecx
+++	addl	20(%rsp),%ebx
+++	andl	%ebp,%edi
+++	xorl	%eax,%ebp
+++	rorl	$7,%edx
+++	movdqa	%xmm1,%xmm9
+++	movl	%ecx,%esi
+++	xorl	%ebp,%edi
+++	movdqa	%xmm10,0(%rsp)
+++	roll	$5,%ecx
+++	addl	%edi,%ebx
+++	xorl	%edx,%esi
+++	pslld	$2,%xmm1
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	psrld	$30,%xmm9
+++	addl	24(%rsp),%eax
+++	andl	%edx,%esi
+++	xorl	%ebp,%edx
+++	por	%xmm9,%xmm1
+++	rorl	$7,%ecx
+++	movl	%ebx,%edi
+++	xorl	%edx,%esi
+++	roll	$5,%ebx
+++	pshufd	$238,%xmm0,%xmm10
+++	addl	%esi,%eax
+++	xorl	%ecx,%edi
+++	xorl	%edx,%ecx
+++	addl	%ebx,%eax
+++	addl	28(%rsp),%ebp
+++	andl	%ecx,%edi
+++	xorl	%edx,%ecx
+++	rorl	$7,%ebx
+++	movl	%eax,%esi
+++	xorl	%ecx,%edi
+++	roll	$5,%eax
+++	addl	%edi,%ebp
+++	xorl	%ebx,%esi
+++	xorl	%ecx,%ebx
+++	addl	%eax,%ebp
+++	pxor	%xmm6,%xmm2
+++	addl	32(%rsp),%edx
+++	andl	%ebx,%esi
+++	xorl	%ecx,%ebx
+++	rorl	$7,%eax
+++	punpcklqdq	%xmm1,%xmm10
+++	movl	%ebp,%edi
+++	xorl	%ebx,%esi
+++	pxor	%xmm3,%xmm2
+++	roll	$5,%ebp
+++	addl	%esi,%edx
+++	movdqa	%xmm8,%xmm9
+++	xorl	%eax,%edi
+++	paddd	%xmm1,%xmm8
+++	xorl	%ebx,%eax
+++	pxor	%xmm10,%xmm2
+++	addl	%ebp,%edx
+++	addl	36(%rsp),%ecx
+++	andl	%eax,%edi
+++	xorl	%ebx,%eax
+++	rorl	$7,%ebp
+++	movdqa	%xmm2,%xmm10
+++	movl	%edx,%esi
+++	xorl	%eax,%edi
+++	movdqa	%xmm8,16(%rsp)
+++	roll	$5,%edx
+++	addl	%edi,%ecx
+++	xorl	%ebp,%esi
+++	pslld	$2,%xmm2
+++	xorl	%eax,%ebp
+++	addl	%edx,%ecx
+++	psrld	$30,%xmm10
+++	addl	40(%rsp),%ebx
+++	andl	%ebp,%esi
+++	xorl	%eax,%ebp
+++	por	%xmm10,%xmm2
+++	rorl	$7,%edx
+++	movl	%ecx,%edi
+++	xorl	%ebp,%esi
+++	roll	$5,%ecx
+++	pshufd	$238,%xmm1,%xmm8
+++	addl	%esi,%ebx
+++	xorl	%edx,%edi
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	addl	44(%rsp),%eax
+++	andl	%edx,%edi
+++	xorl	%ebp,%edx
+++	rorl	$7,%ecx
+++	movl	%ebx,%esi
+++	xorl	%edx,%edi
+++	roll	$5,%ebx
+++	addl	%edi,%eax
+++	xorl	%edx,%esi
+++	addl	%ebx,%eax
+++	pxor	%xmm7,%xmm3
+++	addl	48(%rsp),%ebp
+++	xorl	%ecx,%esi
+++	punpcklqdq	%xmm2,%xmm8
+++	movl	%eax,%edi
+++	roll	$5,%eax
+++	pxor	%xmm4,%xmm3
+++	addl	%esi,%ebp
+++	xorl	%ecx,%edi
+++	movdqa	%xmm9,%xmm10
+++	rorl	$7,%ebx
+++	paddd	%xmm2,%xmm9
+++	addl	%eax,%ebp
+++	pxor	%xmm8,%xmm3
+++	addl	52(%rsp),%edx
+++	xorl	%ebx,%edi
+++	movl	%ebp,%esi
+++	roll	$5,%ebp
+++	movdqa	%xmm3,%xmm8
+++	addl	%edi,%edx
+++	xorl	%ebx,%esi
+++	movdqa	%xmm9,32(%rsp)
+++	rorl	$7,%eax
+++	addl	%ebp,%edx
+++	addl	56(%rsp),%ecx
+++	pslld	$2,%xmm3
+++	xorl	%eax,%esi
+++	movl	%edx,%edi
+++	psrld	$30,%xmm8
+++	roll	$5,%edx
+++	addl	%esi,%ecx
+++	xorl	%eax,%edi
+++	rorl	$7,%ebp
+++	por	%xmm8,%xmm3
+++	addl	%edx,%ecx
+++	addl	60(%rsp),%ebx
+++	xorl	%ebp,%edi
+++	movl	%ecx,%esi
+++	roll	$5,%ecx
+++	addl	%edi,%ebx
+++	xorl	%ebp,%esi
+++	rorl	$7,%edx
+++	addl	%ecx,%ebx
+++	addl	0(%rsp),%eax
+++	xorl	%edx,%esi
+++	movl	%ebx,%edi
+++	roll	$5,%ebx
+++	paddd	%xmm3,%xmm10
+++	addl	%esi,%eax
+++	xorl	%edx,%edi
+++	movdqa	%xmm10,48(%rsp)
+++	rorl	$7,%ecx
+++	addl	%ebx,%eax
+++	addl	4(%rsp),%ebp
+++	xorl	%ecx,%edi
+++	movl	%eax,%esi
+++	roll	$5,%eax
+++	addl	%edi,%ebp
+++	xorl	%ecx,%esi
+++	rorl	$7,%ebx
+++	addl	%eax,%ebp
+++	addl	8(%rsp),%edx
+++	xorl	%ebx,%esi
+++	movl	%ebp,%edi
+++	roll	$5,%ebp
+++	addl	%esi,%edx
+++	xorl	%ebx,%edi
+++	rorl	$7,%eax
+++	addl	%ebp,%edx
+++	addl	12(%rsp),%ecx
+++	xorl	%eax,%edi
+++	movl	%edx,%esi
+++	roll	$5,%edx
+++	addl	%edi,%ecx
+++	xorl	%eax,%esi
+++	rorl	$7,%ebp
+++	addl	%edx,%ecx
+++	cmpq	%r10,%r9
+++	je	.Ldone_ssse3
+++	movdqa	64(%r14),%xmm6
+++	movdqa	-64(%r14),%xmm9
+++	movdqu	0(%r9),%xmm0
+++	movdqu	16(%r9),%xmm1
+++	movdqu	32(%r9),%xmm2
+++	movdqu	48(%r9),%xmm3
+++.byte	102,15,56,0,198
+++	addq	$64,%r9
+++	addl	16(%rsp),%ebx
+++	xorl	%ebp,%esi
+++	movl	%ecx,%edi
+++.byte	102,15,56,0,206
+++	roll	$5,%ecx
+++	addl	%esi,%ebx
+++	xorl	%ebp,%edi
+++	rorl	$7,%edx
+++	paddd	%xmm9,%xmm0
+++	addl	%ecx,%ebx
+++	addl	20(%rsp),%eax
+++	xorl	%edx,%edi
+++	movl	%ebx,%esi
+++	movdqa	%xmm0,0(%rsp)
+++	roll	$5,%ebx
+++	addl	%edi,%eax
+++	xorl	%edx,%esi
+++	rorl	$7,%ecx
+++	psubd	%xmm9,%xmm0
+++	addl	%ebx,%eax
+++	addl	24(%rsp),%ebp
+++	xorl	%ecx,%esi
+++	movl	%eax,%edi
+++	roll	$5,%eax
+++	addl	%esi,%ebp
+++	xorl	%ecx,%edi
+++	rorl	$7,%ebx
+++	addl	%eax,%ebp
+++	addl	28(%rsp),%edx
+++	xorl	%ebx,%edi
+++	movl	%ebp,%esi
+++	roll	$5,%ebp
+++	addl	%edi,%edx
+++	xorl	%ebx,%esi
+++	rorl	$7,%eax
+++	addl	%ebp,%edx
+++	addl	32(%rsp),%ecx
+++	xorl	%eax,%esi
+++	movl	%edx,%edi
+++.byte	102,15,56,0,214
+++	roll	$5,%edx
+++	addl	%esi,%ecx
+++	xorl	%eax,%edi
+++	rorl	$7,%ebp
+++	paddd	%xmm9,%xmm1
+++	addl	%edx,%ecx
+++	addl	36(%rsp),%ebx
+++	xorl	%ebp,%edi
+++	movl	%ecx,%esi
+++	movdqa	%xmm1,16(%rsp)
+++	roll	$5,%ecx
+++	addl	%edi,%ebx
+++	xorl	%ebp,%esi
+++	rorl	$7,%edx
+++	psubd	%xmm9,%xmm1
+++	addl	%ecx,%ebx
+++	addl	40(%rsp),%eax
+++	xorl	%edx,%esi
+++	movl	%ebx,%edi
+++	roll	$5,%ebx
+++	addl	%esi,%eax
+++	xorl	%edx,%edi
+++	rorl	$7,%ecx
+++	addl	%ebx,%eax
+++	addl	44(%rsp),%ebp
+++	xorl	%ecx,%edi
+++	movl	%eax,%esi
+++	roll	$5,%eax
+++	addl	%edi,%ebp
+++	xorl	%ecx,%esi
+++	rorl	$7,%ebx
+++	addl	%eax,%ebp
+++	addl	48(%rsp),%edx
+++	xorl	%ebx,%esi
+++	movl	%ebp,%edi
+++.byte	102,15,56,0,222
+++	roll	$5,%ebp
+++	addl	%esi,%edx
+++	xorl	%ebx,%edi
+++	rorl	$7,%eax
+++	paddd	%xmm9,%xmm2
+++	addl	%ebp,%edx
+++	addl	52(%rsp),%ecx
+++	xorl	%eax,%edi
+++	movl	%edx,%esi
+++	movdqa	%xmm2,32(%rsp)
+++	roll	$5,%edx
+++	addl	%edi,%ecx
+++	xorl	%eax,%esi
+++	rorl	$7,%ebp
+++	psubd	%xmm9,%xmm2
+++	addl	%edx,%ecx
+++	addl	56(%rsp),%ebx
+++	xorl	%ebp,%esi
+++	movl	%ecx,%edi
+++	roll	$5,%ecx
+++	addl	%esi,%ebx
+++	xorl	%ebp,%edi
+++	rorl	$7,%edx
+++	addl	%ecx,%ebx
+++	addl	60(%rsp),%eax
+++	xorl	%edx,%edi
+++	movl	%ebx,%esi
+++	roll	$5,%ebx
+++	addl	%edi,%eax
+++	rorl	$7,%ecx
+++	addl	%ebx,%eax
+++	addl	0(%r8),%eax
+++	addl	4(%r8),%esi
+++	addl	8(%r8),%ecx
+++	addl	12(%r8),%edx
+++	movl	%eax,0(%r8)
+++	addl	16(%r8),%ebp
+++	movl	%esi,4(%r8)
+++	movl	%esi,%ebx
+++	movl	%ecx,8(%r8)
+++	movl	%ecx,%edi
+++	movl	%edx,12(%r8)
+++	xorl	%edx,%edi
+++	movl	%ebp,16(%r8)
+++	andl	%edi,%esi
+++	jmp	.Loop_ssse3
+++
+++.align	16
+++.Ldone_ssse3:
+++	addl	16(%rsp),%ebx
+++	xorl	%ebp,%esi
+++	movl	%ecx,%edi
+++	roll	$5,%ecx
+++	addl	%esi,%ebx
+++	xorl	%ebp,%edi
+++	rorl	$7,%edx
+++	addl	%ecx,%ebx
+++	addl	20(%rsp),%eax
+++	xorl	%edx,%edi
+++	movl	%ebx,%esi
+++	roll	$5,%ebx
+++	addl	%edi,%eax
+++	xorl	%edx,%esi
+++	rorl	$7,%ecx
+++	addl	%ebx,%eax
+++	addl	24(%rsp),%ebp
+++	xorl	%ecx,%esi
+++	movl	%eax,%edi
+++	roll	$5,%eax
+++	addl	%esi,%ebp
+++	xorl	%ecx,%edi
+++	rorl	$7,%ebx
+++	addl	%eax,%ebp
+++	addl	28(%rsp),%edx
+++	xorl	%ebx,%edi
+++	movl	%ebp,%esi
+++	roll	$5,%ebp
+++	addl	%edi,%edx
+++	xorl	%ebx,%esi
+++	rorl	$7,%eax
+++	addl	%ebp,%edx
+++	addl	32(%rsp),%ecx
+++	xorl	%eax,%esi
+++	movl	%edx,%edi
+++	roll	$5,%edx
+++	addl	%esi,%ecx
+++	xorl	%eax,%edi
+++	rorl	$7,%ebp
+++	addl	%edx,%ecx
+++	addl	36(%rsp),%ebx
+++	xorl	%ebp,%edi
+++	movl	%ecx,%esi
+++	roll	$5,%ecx
+++	addl	%edi,%ebx
+++	xorl	%ebp,%esi
+++	rorl	$7,%edx
+++	addl	%ecx,%ebx
+++	addl	40(%rsp),%eax
+++	xorl	%edx,%esi
+++	movl	%ebx,%edi
+++	roll	$5,%ebx
+++	addl	%esi,%eax
+++	xorl	%edx,%edi
+++	rorl	$7,%ecx
+++	addl	%ebx,%eax
+++	addl	44(%rsp),%ebp
+++	xorl	%ecx,%edi
+++	movl	%eax,%esi
+++	roll	$5,%eax
+++	addl	%edi,%ebp
+++	xorl	%ecx,%esi
+++	rorl	$7,%ebx
+++	addl	%eax,%ebp
+++	addl	48(%rsp),%edx
+++	xorl	%ebx,%esi
+++	movl	%ebp,%edi
+++	roll	$5,%ebp
+++	addl	%esi,%edx
+++	xorl	%ebx,%edi
+++	rorl	$7,%eax
+++	addl	%ebp,%edx
+++	addl	52(%rsp),%ecx
+++	xorl	%eax,%edi
+++	movl	%edx,%esi
+++	roll	$5,%edx
+++	addl	%edi,%ecx
+++	xorl	%eax,%esi
+++	rorl	$7,%ebp
+++	addl	%edx,%ecx
+++	addl	56(%rsp),%ebx
+++	xorl	%ebp,%esi
+++	movl	%ecx,%edi
+++	roll	$5,%ecx
+++	addl	%esi,%ebx
+++	xorl	%ebp,%edi
+++	rorl	$7,%edx
+++	addl	%ecx,%ebx
+++	addl	60(%rsp),%eax
+++	xorl	%edx,%edi
+++	movl	%ebx,%esi
+++	roll	$5,%ebx
+++	addl	%edi,%eax
+++	rorl	$7,%ecx
+++	addl	%ebx,%eax
+++	addl	0(%r8),%eax
+++	addl	4(%r8),%esi
+++	addl	8(%r8),%ecx
+++	movl	%eax,0(%r8)
+++	addl	12(%r8),%edx
+++	movl	%esi,4(%r8)
+++	addl	16(%r8),%ebp
+++	movl	%ecx,8(%r8)
+++	movl	%edx,12(%r8)
+++	movl	%ebp,16(%r8)
+++	movq	-40(%r11),%r14
+++.cfi_restore	%r14
+++	movq	-32(%r11),%r13
+++.cfi_restore	%r13
+++	movq	-24(%r11),%r12
+++.cfi_restore	%r12
+++	movq	-16(%r11),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%r11),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%r11),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lepilogue_ssse3:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	sha1_block_data_order_ssse3,.-sha1_block_data_order_ssse3
+++.type	sha1_block_data_order_avx,@function
+++.align	16
+++sha1_block_data_order_avx:
+++_avx_shortcut:
+++.cfi_startproc	
+++	movq	%rsp,%r11
+++.cfi_def_cfa_register	%r11
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	leaq	-64(%rsp),%rsp
+++	vzeroupper
+++	andq	$-64,%rsp
+++	movq	%rdi,%r8
+++	movq	%rsi,%r9
+++	movq	%rdx,%r10
+++
+++	shlq	$6,%r10
+++	addq	%r9,%r10
+++	leaq	K_XX_XX+64(%rip),%r14
+++
+++	movl	0(%r8),%eax
+++	movl	4(%r8),%ebx
+++	movl	8(%r8),%ecx
+++	movl	12(%r8),%edx
+++	movl	%ebx,%esi
+++	movl	16(%r8),%ebp
+++	movl	%ecx,%edi
+++	xorl	%edx,%edi
+++	andl	%edi,%esi
+++
+++	vmovdqa	64(%r14),%xmm6
+++	vmovdqa	-64(%r14),%xmm11
+++	vmovdqu	0(%r9),%xmm0
+++	vmovdqu	16(%r9),%xmm1
+++	vmovdqu	32(%r9),%xmm2
+++	vmovdqu	48(%r9),%xmm3
+++	vpshufb	%xmm6,%xmm0,%xmm0
+++	addq	$64,%r9
+++	vpshufb	%xmm6,%xmm1,%xmm1
+++	vpshufb	%xmm6,%xmm2,%xmm2
+++	vpshufb	%xmm6,%xmm3,%xmm3
+++	vpaddd	%xmm11,%xmm0,%xmm4
+++	vpaddd	%xmm11,%xmm1,%xmm5
+++	vpaddd	%xmm11,%xmm2,%xmm6
+++	vmovdqa	%xmm4,0(%rsp)
+++	vmovdqa	%xmm5,16(%rsp)
+++	vmovdqa	%xmm6,32(%rsp)
+++	jmp	.Loop_avx
+++.align	16
+++.Loop_avx:
+++	shrdl	$2,%ebx,%ebx
+++	xorl	%edx,%esi
+++	vpalignr	$8,%xmm0,%xmm1,%xmm4
+++	movl	%eax,%edi
+++	addl	0(%rsp),%ebp
+++	vpaddd	%xmm3,%xmm11,%xmm9
+++	xorl	%ecx,%ebx
+++	shldl	$5,%eax,%eax
+++	vpsrldq	$4,%xmm3,%xmm8
+++	addl	%esi,%ebp
+++	andl	%ebx,%edi
+++	vpxor	%xmm0,%xmm4,%xmm4
+++	xorl	%ecx,%ebx
+++	addl	%eax,%ebp
+++	vpxor	%xmm2,%xmm8,%xmm8
+++	shrdl	$7,%eax,%eax
+++	xorl	%ecx,%edi
+++	movl	%ebp,%esi
+++	addl	4(%rsp),%edx
+++	vpxor	%xmm8,%xmm4,%xmm4
+++	xorl	%ebx,%eax
+++	shldl	$5,%ebp,%ebp
+++	vmovdqa	%xmm9,48(%rsp)
+++	addl	%edi,%edx
+++	andl	%eax,%esi
+++	vpsrld	$31,%xmm4,%xmm8
+++	xorl	%ebx,%eax
+++	addl	%ebp,%edx
+++	shrdl	$7,%ebp,%ebp
+++	xorl	%ebx,%esi
+++	vpslldq	$12,%xmm4,%xmm10
+++	vpaddd	%xmm4,%xmm4,%xmm4
+++	movl	%edx,%edi
+++	addl	8(%rsp),%ecx
+++	xorl	%eax,%ebp
+++	shldl	$5,%edx,%edx
+++	vpsrld	$30,%xmm10,%xmm9
+++	vpor	%xmm8,%xmm4,%xmm4
+++	addl	%esi,%ecx
+++	andl	%ebp,%edi
+++	xorl	%eax,%ebp
+++	addl	%edx,%ecx
+++	vpslld	$2,%xmm10,%xmm10
+++	vpxor	%xmm9,%xmm4,%xmm4
+++	shrdl	$7,%edx,%edx
+++	xorl	%eax,%edi
+++	movl	%ecx,%esi
+++	addl	12(%rsp),%ebx
+++	vpxor	%xmm10,%xmm4,%xmm4
+++	xorl	%ebp,%edx
+++	shldl	$5,%ecx,%ecx
+++	addl	%edi,%ebx
+++	andl	%edx,%esi
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	shrdl	$7,%ecx,%ecx
+++	xorl	%ebp,%esi
+++	vpalignr	$8,%xmm1,%xmm2,%xmm5
+++	movl	%ebx,%edi
+++	addl	16(%rsp),%eax
+++	vpaddd	%xmm4,%xmm11,%xmm9
+++	xorl	%edx,%ecx
+++	shldl	$5,%ebx,%ebx
+++	vpsrldq	$4,%xmm4,%xmm8
+++	addl	%esi,%eax
+++	andl	%ecx,%edi
+++	vpxor	%xmm1,%xmm5,%xmm5
+++	xorl	%edx,%ecx
+++	addl	%ebx,%eax
+++	vpxor	%xmm3,%xmm8,%xmm8
+++	shrdl	$7,%ebx,%ebx
+++	xorl	%edx,%edi
+++	movl	%eax,%esi
+++	addl	20(%rsp),%ebp
+++	vpxor	%xmm8,%xmm5,%xmm5
+++	xorl	%ecx,%ebx
+++	shldl	$5,%eax,%eax
+++	vmovdqa	%xmm9,0(%rsp)
+++	addl	%edi,%ebp
+++	andl	%ebx,%esi
+++	vpsrld	$31,%xmm5,%xmm8
+++	xorl	%ecx,%ebx
+++	addl	%eax,%ebp
+++	shrdl	$7,%eax,%eax
+++	xorl	%ecx,%esi
+++	vpslldq	$12,%xmm5,%xmm10
+++	vpaddd	%xmm5,%xmm5,%xmm5
+++	movl	%ebp,%edi
+++	addl	24(%rsp),%edx
+++	xorl	%ebx,%eax
+++	shldl	$5,%ebp,%ebp
+++	vpsrld	$30,%xmm10,%xmm9
+++	vpor	%xmm8,%xmm5,%xmm5
+++	addl	%esi,%edx
+++	andl	%eax,%edi
+++	xorl	%ebx,%eax
+++	addl	%ebp,%edx
+++	vpslld	$2,%xmm10,%xmm10
+++	vpxor	%xmm9,%xmm5,%xmm5
+++	shrdl	$7,%ebp,%ebp
+++	xorl	%ebx,%edi
+++	movl	%edx,%esi
+++	addl	28(%rsp),%ecx
+++	vpxor	%xmm10,%xmm5,%xmm5
+++	xorl	%eax,%ebp
+++	shldl	$5,%edx,%edx
+++	vmovdqa	-32(%r14),%xmm11
+++	addl	%edi,%ecx
+++	andl	%ebp,%esi
+++	xorl	%eax,%ebp
+++	addl	%edx,%ecx
+++	shrdl	$7,%edx,%edx
+++	xorl	%eax,%esi
+++	vpalignr	$8,%xmm2,%xmm3,%xmm6
+++	movl	%ecx,%edi
+++	addl	32(%rsp),%ebx
+++	vpaddd	%xmm5,%xmm11,%xmm9
+++	xorl	%ebp,%edx
+++	shldl	$5,%ecx,%ecx
+++	vpsrldq	$4,%xmm5,%xmm8
+++	addl	%esi,%ebx
+++	andl	%edx,%edi
+++	vpxor	%xmm2,%xmm6,%xmm6
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	vpxor	%xmm4,%xmm8,%xmm8
+++	shrdl	$7,%ecx,%ecx
+++	xorl	%ebp,%edi
+++	movl	%ebx,%esi
+++	addl	36(%rsp),%eax
+++	vpxor	%xmm8,%xmm6,%xmm6
+++	xorl	%edx,%ecx
+++	shldl	$5,%ebx,%ebx
+++	vmovdqa	%xmm9,16(%rsp)
+++	addl	%edi,%eax
+++	andl	%ecx,%esi
+++	vpsrld	$31,%xmm6,%xmm8
+++	xorl	%edx,%ecx
+++	addl	%ebx,%eax
+++	shrdl	$7,%ebx,%ebx
+++	xorl	%edx,%esi
+++	vpslldq	$12,%xmm6,%xmm10
+++	vpaddd	%xmm6,%xmm6,%xmm6
+++	movl	%eax,%edi
+++	addl	40(%rsp),%ebp
+++	xorl	%ecx,%ebx
+++	shldl	$5,%eax,%eax
+++	vpsrld	$30,%xmm10,%xmm9
+++	vpor	%xmm8,%xmm6,%xmm6
+++	addl	%esi,%ebp
+++	andl	%ebx,%edi
+++	xorl	%ecx,%ebx
+++	addl	%eax,%ebp
+++	vpslld	$2,%xmm10,%xmm10
+++	vpxor	%xmm9,%xmm6,%xmm6
+++	shrdl	$7,%eax,%eax
+++	xorl	%ecx,%edi
+++	movl	%ebp,%esi
+++	addl	44(%rsp),%edx
+++	vpxor	%xmm10,%xmm6,%xmm6
+++	xorl	%ebx,%eax
+++	shldl	$5,%ebp,%ebp
+++	addl	%edi,%edx
+++	andl	%eax,%esi
+++	xorl	%ebx,%eax
+++	addl	%ebp,%edx
+++	shrdl	$7,%ebp,%ebp
+++	xorl	%ebx,%esi
+++	vpalignr	$8,%xmm3,%xmm4,%xmm7
+++	movl	%edx,%edi
+++	addl	48(%rsp),%ecx
+++	vpaddd	%xmm6,%xmm11,%xmm9
+++	xorl	%eax,%ebp
+++	shldl	$5,%edx,%edx
+++	vpsrldq	$4,%xmm6,%xmm8
+++	addl	%esi,%ecx
+++	andl	%ebp,%edi
+++	vpxor	%xmm3,%xmm7,%xmm7
+++	xorl	%eax,%ebp
+++	addl	%edx,%ecx
+++	vpxor	%xmm5,%xmm8,%xmm8
+++	shrdl	$7,%edx,%edx
+++	xorl	%eax,%edi
+++	movl	%ecx,%esi
+++	addl	52(%rsp),%ebx
+++	vpxor	%xmm8,%xmm7,%xmm7
+++	xorl	%ebp,%edx
+++	shldl	$5,%ecx,%ecx
+++	vmovdqa	%xmm9,32(%rsp)
+++	addl	%edi,%ebx
+++	andl	%edx,%esi
+++	vpsrld	$31,%xmm7,%xmm8
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	shrdl	$7,%ecx,%ecx
+++	xorl	%ebp,%esi
+++	vpslldq	$12,%xmm7,%xmm10
+++	vpaddd	%xmm7,%xmm7,%xmm7
+++	movl	%ebx,%edi
+++	addl	56(%rsp),%eax
+++	xorl	%edx,%ecx
+++	shldl	$5,%ebx,%ebx
+++	vpsrld	$30,%xmm10,%xmm9
+++	vpor	%xmm8,%xmm7,%xmm7
+++	addl	%esi,%eax
+++	andl	%ecx,%edi
+++	xorl	%edx,%ecx
+++	addl	%ebx,%eax
+++	vpslld	$2,%xmm10,%xmm10
+++	vpxor	%xmm9,%xmm7,%xmm7
+++	shrdl	$7,%ebx,%ebx
+++	xorl	%edx,%edi
+++	movl	%eax,%esi
+++	addl	60(%rsp),%ebp
+++	vpxor	%xmm10,%xmm7,%xmm7
+++	xorl	%ecx,%ebx
+++	shldl	$5,%eax,%eax
+++	addl	%edi,%ebp
+++	andl	%ebx,%esi
+++	xorl	%ecx,%ebx
+++	addl	%eax,%ebp
+++	vpalignr	$8,%xmm6,%xmm7,%xmm8
+++	vpxor	%xmm4,%xmm0,%xmm0
+++	shrdl	$7,%eax,%eax
+++	xorl	%ecx,%esi
+++	movl	%ebp,%edi
+++	addl	0(%rsp),%edx
+++	vpxor	%xmm1,%xmm0,%xmm0
+++	xorl	%ebx,%eax
+++	shldl	$5,%ebp,%ebp
+++	vpaddd	%xmm7,%xmm11,%xmm9
+++	addl	%esi,%edx
+++	andl	%eax,%edi
+++	vpxor	%xmm8,%xmm0,%xmm0
+++	xorl	%ebx,%eax
+++	addl	%ebp,%edx
+++	shrdl	$7,%ebp,%ebp
+++	xorl	%ebx,%edi
+++	vpsrld	$30,%xmm0,%xmm8
+++	vmovdqa	%xmm9,48(%rsp)
+++	movl	%edx,%esi
+++	addl	4(%rsp),%ecx
+++	xorl	%eax,%ebp
+++	shldl	$5,%edx,%edx
+++	vpslld	$2,%xmm0,%xmm0
+++	addl	%edi,%ecx
+++	andl	%ebp,%esi
+++	xorl	%eax,%ebp
+++	addl	%edx,%ecx
+++	shrdl	$7,%edx,%edx
+++	xorl	%eax,%esi
+++	movl	%ecx,%edi
+++	addl	8(%rsp),%ebx
+++	vpor	%xmm8,%xmm0,%xmm0
+++	xorl	%ebp,%edx
+++	shldl	$5,%ecx,%ecx
+++	addl	%esi,%ebx
+++	andl	%edx,%edi
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	addl	12(%rsp),%eax
+++	xorl	%ebp,%edi
+++	movl	%ebx,%esi
+++	shldl	$5,%ebx,%ebx
+++	addl	%edi,%eax
+++	xorl	%edx,%esi
+++	shrdl	$7,%ecx,%ecx
+++	addl	%ebx,%eax
+++	vpalignr	$8,%xmm7,%xmm0,%xmm8
+++	vpxor	%xmm5,%xmm1,%xmm1
+++	addl	16(%rsp),%ebp
+++	xorl	%ecx,%esi
+++	movl	%eax,%edi
+++	shldl	$5,%eax,%eax
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	addl	%esi,%ebp
+++	xorl	%ecx,%edi
+++	vpaddd	%xmm0,%xmm11,%xmm9
+++	shrdl	$7,%ebx,%ebx
+++	addl	%eax,%ebp
+++	vpxor	%xmm8,%xmm1,%xmm1
+++	addl	20(%rsp),%edx
+++	xorl	%ebx,%edi
+++	movl	%ebp,%esi
+++	shldl	$5,%ebp,%ebp
+++	vpsrld	$30,%xmm1,%xmm8
+++	vmovdqa	%xmm9,0(%rsp)
+++	addl	%edi,%edx
+++	xorl	%ebx,%esi
+++	shrdl	$7,%eax,%eax
+++	addl	%ebp,%edx
+++	vpslld	$2,%xmm1,%xmm1
+++	addl	24(%rsp),%ecx
+++	xorl	%eax,%esi
+++	movl	%edx,%edi
+++	shldl	$5,%edx,%edx
+++	addl	%esi,%ecx
+++	xorl	%eax,%edi
+++	shrdl	$7,%ebp,%ebp
+++	addl	%edx,%ecx
+++	vpor	%xmm8,%xmm1,%xmm1
+++	addl	28(%rsp),%ebx
+++	xorl	%ebp,%edi
+++	movl	%ecx,%esi
+++	shldl	$5,%ecx,%ecx
+++	addl	%edi,%ebx
+++	xorl	%ebp,%esi
+++	shrdl	$7,%edx,%edx
+++	addl	%ecx,%ebx
+++	vpalignr	$8,%xmm0,%xmm1,%xmm8
+++	vpxor	%xmm6,%xmm2,%xmm2
+++	addl	32(%rsp),%eax
+++	xorl	%edx,%esi
+++	movl	%ebx,%edi
+++	shldl	$5,%ebx,%ebx
+++	vpxor	%xmm3,%xmm2,%xmm2
+++	addl	%esi,%eax
+++	xorl	%edx,%edi
+++	vpaddd	%xmm1,%xmm11,%xmm9
+++	vmovdqa	0(%r14),%xmm11
+++	shrdl	$7,%ecx,%ecx
+++	addl	%ebx,%eax
+++	vpxor	%xmm8,%xmm2,%xmm2
+++	addl	36(%rsp),%ebp
+++	xorl	%ecx,%edi
+++	movl	%eax,%esi
+++	shldl	$5,%eax,%eax
+++	vpsrld	$30,%xmm2,%xmm8
+++	vmovdqa	%xmm9,16(%rsp)
+++	addl	%edi,%ebp
+++	xorl	%ecx,%esi
+++	shrdl	$7,%ebx,%ebx
+++	addl	%eax,%ebp
+++	vpslld	$2,%xmm2,%xmm2
+++	addl	40(%rsp),%edx
+++	xorl	%ebx,%esi
+++	movl	%ebp,%edi
+++	shldl	$5,%ebp,%ebp
+++	addl	%esi,%edx
+++	xorl	%ebx,%edi
+++	shrdl	$7,%eax,%eax
+++	addl	%ebp,%edx
+++	vpor	%xmm8,%xmm2,%xmm2
+++	addl	44(%rsp),%ecx
+++	xorl	%eax,%edi
+++	movl	%edx,%esi
+++	shldl	$5,%edx,%edx
+++	addl	%edi,%ecx
+++	xorl	%eax,%esi
+++	shrdl	$7,%ebp,%ebp
+++	addl	%edx,%ecx
+++	vpalignr	$8,%xmm1,%xmm2,%xmm8
+++	vpxor	%xmm7,%xmm3,%xmm3
+++	addl	48(%rsp),%ebx
+++	xorl	%ebp,%esi
+++	movl	%ecx,%edi
+++	shldl	$5,%ecx,%ecx
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	addl	%esi,%ebx
+++	xorl	%ebp,%edi
+++	vpaddd	%xmm2,%xmm11,%xmm9
+++	shrdl	$7,%edx,%edx
+++	addl	%ecx,%ebx
+++	vpxor	%xmm8,%xmm3,%xmm3
+++	addl	52(%rsp),%eax
+++	xorl	%edx,%edi
+++	movl	%ebx,%esi
+++	shldl	$5,%ebx,%ebx
+++	vpsrld	$30,%xmm3,%xmm8
+++	vmovdqa	%xmm9,32(%rsp)
+++	addl	%edi,%eax
+++	xorl	%edx,%esi
+++	shrdl	$7,%ecx,%ecx
+++	addl	%ebx,%eax
+++	vpslld	$2,%xmm3,%xmm3
+++	addl	56(%rsp),%ebp
+++	xorl	%ecx,%esi
+++	movl	%eax,%edi
+++	shldl	$5,%eax,%eax
+++	addl	%esi,%ebp
+++	xorl	%ecx,%edi
+++	shrdl	$7,%ebx,%ebx
+++	addl	%eax,%ebp
+++	vpor	%xmm8,%xmm3,%xmm3
+++	addl	60(%rsp),%edx
+++	xorl	%ebx,%edi
+++	movl	%ebp,%esi
+++	shldl	$5,%ebp,%ebp
+++	addl	%edi,%edx
+++	xorl	%ebx,%esi
+++	shrdl	$7,%eax,%eax
+++	addl	%ebp,%edx
+++	vpalignr	$8,%xmm2,%xmm3,%xmm8
+++	vpxor	%xmm0,%xmm4,%xmm4
+++	addl	0(%rsp),%ecx
+++	xorl	%eax,%esi
+++	movl	%edx,%edi
+++	shldl	$5,%edx,%edx
+++	vpxor	%xmm5,%xmm4,%xmm4
+++	addl	%esi,%ecx
+++	xorl	%eax,%edi
+++	vpaddd	%xmm3,%xmm11,%xmm9
+++	shrdl	$7,%ebp,%ebp
+++	addl	%edx,%ecx
+++	vpxor	%xmm8,%xmm4,%xmm4
+++	addl	4(%rsp),%ebx
+++	xorl	%ebp,%edi
+++	movl	%ecx,%esi
+++	shldl	$5,%ecx,%ecx
+++	vpsrld	$30,%xmm4,%xmm8
+++	vmovdqa	%xmm9,48(%rsp)
+++	addl	%edi,%ebx
+++	xorl	%ebp,%esi
+++	shrdl	$7,%edx,%edx
+++	addl	%ecx,%ebx
+++	vpslld	$2,%xmm4,%xmm4
+++	addl	8(%rsp),%eax
+++	xorl	%edx,%esi
+++	movl	%ebx,%edi
+++	shldl	$5,%ebx,%ebx
+++	addl	%esi,%eax
+++	xorl	%edx,%edi
+++	shrdl	$7,%ecx,%ecx
+++	addl	%ebx,%eax
+++	vpor	%xmm8,%xmm4,%xmm4
+++	addl	12(%rsp),%ebp
+++	xorl	%ecx,%edi
+++	movl	%eax,%esi
+++	shldl	$5,%eax,%eax
+++	addl	%edi,%ebp
+++	xorl	%ecx,%esi
+++	shrdl	$7,%ebx,%ebx
+++	addl	%eax,%ebp
+++	vpalignr	$8,%xmm3,%xmm4,%xmm8
+++	vpxor	%xmm1,%xmm5,%xmm5
+++	addl	16(%rsp),%edx
+++	xorl	%ebx,%esi
+++	movl	%ebp,%edi
+++	shldl	$5,%ebp,%ebp
+++	vpxor	%xmm6,%xmm5,%xmm5
+++	addl	%esi,%edx
+++	xorl	%ebx,%edi
+++	vpaddd	%xmm4,%xmm11,%xmm9
+++	shrdl	$7,%eax,%eax
+++	addl	%ebp,%edx
+++	vpxor	%xmm8,%xmm5,%xmm5
+++	addl	20(%rsp),%ecx
+++	xorl	%eax,%edi
+++	movl	%edx,%esi
+++	shldl	$5,%edx,%edx
+++	vpsrld	$30,%xmm5,%xmm8
+++	vmovdqa	%xmm9,0(%rsp)
+++	addl	%edi,%ecx
+++	xorl	%eax,%esi
+++	shrdl	$7,%ebp,%ebp
+++	addl	%edx,%ecx
+++	vpslld	$2,%xmm5,%xmm5
+++	addl	24(%rsp),%ebx
+++	xorl	%ebp,%esi
+++	movl	%ecx,%edi
+++	shldl	$5,%ecx,%ecx
+++	addl	%esi,%ebx
+++	xorl	%ebp,%edi
+++	shrdl	$7,%edx,%edx
+++	addl	%ecx,%ebx
+++	vpor	%xmm8,%xmm5,%xmm5
+++	addl	28(%rsp),%eax
+++	shrdl	$7,%ecx,%ecx
+++	movl	%ebx,%esi
+++	xorl	%edx,%edi
+++	shldl	$5,%ebx,%ebx
+++	addl	%edi,%eax
+++	xorl	%ecx,%esi
+++	xorl	%edx,%ecx
+++	addl	%ebx,%eax
+++	vpalignr	$8,%xmm4,%xmm5,%xmm8
+++	vpxor	%xmm2,%xmm6,%xmm6
+++	addl	32(%rsp),%ebp
+++	andl	%ecx,%esi
+++	xorl	%edx,%ecx
+++	shrdl	$7,%ebx,%ebx
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	movl	%eax,%edi
+++	xorl	%ecx,%esi
+++	vpaddd	%xmm5,%xmm11,%xmm9
+++	shldl	$5,%eax,%eax
+++	addl	%esi,%ebp
+++	vpxor	%xmm8,%xmm6,%xmm6
+++	xorl	%ebx,%edi
+++	xorl	%ecx,%ebx
+++	addl	%eax,%ebp
+++	addl	36(%rsp),%edx
+++	vpsrld	$30,%xmm6,%xmm8
+++	vmovdqa	%xmm9,16(%rsp)
+++	andl	%ebx,%edi
+++	xorl	%ecx,%ebx
+++	shrdl	$7,%eax,%eax
+++	movl	%ebp,%esi
+++	vpslld	$2,%xmm6,%xmm6
+++	xorl	%ebx,%edi
+++	shldl	$5,%ebp,%ebp
+++	addl	%edi,%edx
+++	xorl	%eax,%esi
+++	xorl	%ebx,%eax
+++	addl	%ebp,%edx
+++	addl	40(%rsp),%ecx
+++	andl	%eax,%esi
+++	vpor	%xmm8,%xmm6,%xmm6
+++	xorl	%ebx,%eax
+++	shrdl	$7,%ebp,%ebp
+++	movl	%edx,%edi
+++	xorl	%eax,%esi
+++	shldl	$5,%edx,%edx
+++	addl	%esi,%ecx
+++	xorl	%ebp,%edi
+++	xorl	%eax,%ebp
+++	addl	%edx,%ecx
+++	addl	44(%rsp),%ebx
+++	andl	%ebp,%edi
+++	xorl	%eax,%ebp
+++	shrdl	$7,%edx,%edx
+++	movl	%ecx,%esi
+++	xorl	%ebp,%edi
+++	shldl	$5,%ecx,%ecx
+++	addl	%edi,%ebx
+++	xorl	%edx,%esi
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	vpalignr	$8,%xmm5,%xmm6,%xmm8
+++	vpxor	%xmm3,%xmm7,%xmm7
+++	addl	48(%rsp),%eax
+++	andl	%edx,%esi
+++	xorl	%ebp,%edx
+++	shrdl	$7,%ecx,%ecx
+++	vpxor	%xmm0,%xmm7,%xmm7
+++	movl	%ebx,%edi
+++	xorl	%edx,%esi
+++	vpaddd	%xmm6,%xmm11,%xmm9
+++	vmovdqa	32(%r14),%xmm11
+++	shldl	$5,%ebx,%ebx
+++	addl	%esi,%eax
+++	vpxor	%xmm8,%xmm7,%xmm7
+++	xorl	%ecx,%edi
+++	xorl	%edx,%ecx
+++	addl	%ebx,%eax
+++	addl	52(%rsp),%ebp
+++	vpsrld	$30,%xmm7,%xmm8
+++	vmovdqa	%xmm9,32(%rsp)
+++	andl	%ecx,%edi
+++	xorl	%edx,%ecx
+++	shrdl	$7,%ebx,%ebx
+++	movl	%eax,%esi
+++	vpslld	$2,%xmm7,%xmm7
+++	xorl	%ecx,%edi
+++	shldl	$5,%eax,%eax
+++	addl	%edi,%ebp
+++	xorl	%ebx,%esi
+++	xorl	%ecx,%ebx
+++	addl	%eax,%ebp
+++	addl	56(%rsp),%edx
+++	andl	%ebx,%esi
+++	vpor	%xmm8,%xmm7,%xmm7
+++	xorl	%ecx,%ebx
+++	shrdl	$7,%eax,%eax
+++	movl	%ebp,%edi
+++	xorl	%ebx,%esi
+++	shldl	$5,%ebp,%ebp
+++	addl	%esi,%edx
+++	xorl	%eax,%edi
+++	xorl	%ebx,%eax
+++	addl	%ebp,%edx
+++	addl	60(%rsp),%ecx
+++	andl	%eax,%edi
+++	xorl	%ebx,%eax
+++	shrdl	$7,%ebp,%ebp
+++	movl	%edx,%esi
+++	xorl	%eax,%edi
+++	shldl	$5,%edx,%edx
+++	addl	%edi,%ecx
+++	xorl	%ebp,%esi
+++	xorl	%eax,%ebp
+++	addl	%edx,%ecx
+++	vpalignr	$8,%xmm6,%xmm7,%xmm8
+++	vpxor	%xmm4,%xmm0,%xmm0
+++	addl	0(%rsp),%ebx
+++	andl	%ebp,%esi
+++	xorl	%eax,%ebp
+++	shrdl	$7,%edx,%edx
+++	vpxor	%xmm1,%xmm0,%xmm0
+++	movl	%ecx,%edi
+++	xorl	%ebp,%esi
+++	vpaddd	%xmm7,%xmm11,%xmm9
+++	shldl	$5,%ecx,%ecx
+++	addl	%esi,%ebx
+++	vpxor	%xmm8,%xmm0,%xmm0
+++	xorl	%edx,%edi
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	addl	4(%rsp),%eax
+++	vpsrld	$30,%xmm0,%xmm8
+++	vmovdqa	%xmm9,48(%rsp)
+++	andl	%edx,%edi
+++	xorl	%ebp,%edx
+++	shrdl	$7,%ecx,%ecx
+++	movl	%ebx,%esi
+++	vpslld	$2,%xmm0,%xmm0
+++	xorl	%edx,%edi
+++	shldl	$5,%ebx,%ebx
+++	addl	%edi,%eax
+++	xorl	%ecx,%esi
+++	xorl	%edx,%ecx
+++	addl	%ebx,%eax
+++	addl	8(%rsp),%ebp
+++	andl	%ecx,%esi
+++	vpor	%xmm8,%xmm0,%xmm0
+++	xorl	%edx,%ecx
+++	shrdl	$7,%ebx,%ebx
+++	movl	%eax,%edi
+++	xorl	%ecx,%esi
+++	shldl	$5,%eax,%eax
+++	addl	%esi,%ebp
+++	xorl	%ebx,%edi
+++	xorl	%ecx,%ebx
+++	addl	%eax,%ebp
+++	addl	12(%rsp),%edx
+++	andl	%ebx,%edi
+++	xorl	%ecx,%ebx
+++	shrdl	$7,%eax,%eax
+++	movl	%ebp,%esi
+++	xorl	%ebx,%edi
+++	shldl	$5,%ebp,%ebp
+++	addl	%edi,%edx
+++	xorl	%eax,%esi
+++	xorl	%ebx,%eax
+++	addl	%ebp,%edx
+++	vpalignr	$8,%xmm7,%xmm0,%xmm8
+++	vpxor	%xmm5,%xmm1,%xmm1
+++	addl	16(%rsp),%ecx
+++	andl	%eax,%esi
+++	xorl	%ebx,%eax
+++	shrdl	$7,%ebp,%ebp
+++	vpxor	%xmm2,%xmm1,%xmm1
+++	movl	%edx,%edi
+++	xorl	%eax,%esi
+++	vpaddd	%xmm0,%xmm11,%xmm9
+++	shldl	$5,%edx,%edx
+++	addl	%esi,%ecx
+++	vpxor	%xmm8,%xmm1,%xmm1
+++	xorl	%ebp,%edi
+++	xorl	%eax,%ebp
+++	addl	%edx,%ecx
+++	addl	20(%rsp),%ebx
+++	vpsrld	$30,%xmm1,%xmm8
+++	vmovdqa	%xmm9,0(%rsp)
+++	andl	%ebp,%edi
+++	xorl	%eax,%ebp
+++	shrdl	$7,%edx,%edx
+++	movl	%ecx,%esi
+++	vpslld	$2,%xmm1,%xmm1
+++	xorl	%ebp,%edi
+++	shldl	$5,%ecx,%ecx
+++	addl	%edi,%ebx
+++	xorl	%edx,%esi
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	addl	24(%rsp),%eax
+++	andl	%edx,%esi
+++	vpor	%xmm8,%xmm1,%xmm1
+++	xorl	%ebp,%edx
+++	shrdl	$7,%ecx,%ecx
+++	movl	%ebx,%edi
+++	xorl	%edx,%esi
+++	shldl	$5,%ebx,%ebx
+++	addl	%esi,%eax
+++	xorl	%ecx,%edi
+++	xorl	%edx,%ecx
+++	addl	%ebx,%eax
+++	addl	28(%rsp),%ebp
+++	andl	%ecx,%edi
+++	xorl	%edx,%ecx
+++	shrdl	$7,%ebx,%ebx
+++	movl	%eax,%esi
+++	xorl	%ecx,%edi
+++	shldl	$5,%eax,%eax
+++	addl	%edi,%ebp
+++	xorl	%ebx,%esi
+++	xorl	%ecx,%ebx
+++	addl	%eax,%ebp
+++	vpalignr	$8,%xmm0,%xmm1,%xmm8
+++	vpxor	%xmm6,%xmm2,%xmm2
+++	addl	32(%rsp),%edx
+++	andl	%ebx,%esi
+++	xorl	%ecx,%ebx
+++	shrdl	$7,%eax,%eax
+++	vpxor	%xmm3,%xmm2,%xmm2
+++	movl	%ebp,%edi
+++	xorl	%ebx,%esi
+++	vpaddd	%xmm1,%xmm11,%xmm9
+++	shldl	$5,%ebp,%ebp
+++	addl	%esi,%edx
+++	vpxor	%xmm8,%xmm2,%xmm2
+++	xorl	%eax,%edi
+++	xorl	%ebx,%eax
+++	addl	%ebp,%edx
+++	addl	36(%rsp),%ecx
+++	vpsrld	$30,%xmm2,%xmm8
+++	vmovdqa	%xmm9,16(%rsp)
+++	andl	%eax,%edi
+++	xorl	%ebx,%eax
+++	shrdl	$7,%ebp,%ebp
+++	movl	%edx,%esi
+++	vpslld	$2,%xmm2,%xmm2
+++	xorl	%eax,%edi
+++	shldl	$5,%edx,%edx
+++	addl	%edi,%ecx
+++	xorl	%ebp,%esi
+++	xorl	%eax,%ebp
+++	addl	%edx,%ecx
+++	addl	40(%rsp),%ebx
+++	andl	%ebp,%esi
+++	vpor	%xmm8,%xmm2,%xmm2
+++	xorl	%eax,%ebp
+++	shrdl	$7,%edx,%edx
+++	movl	%ecx,%edi
+++	xorl	%ebp,%esi
+++	shldl	$5,%ecx,%ecx
+++	addl	%esi,%ebx
+++	xorl	%edx,%edi
+++	xorl	%ebp,%edx
+++	addl	%ecx,%ebx
+++	addl	44(%rsp),%eax
+++	andl	%edx,%edi
+++	xorl	%ebp,%edx
+++	shrdl	$7,%ecx,%ecx
+++	movl	%ebx,%esi
+++	xorl	%edx,%edi
+++	shldl	$5,%ebx,%ebx
+++	addl	%edi,%eax
+++	xorl	%edx,%esi
+++	addl	%ebx,%eax
+++	vpalignr	$8,%xmm1,%xmm2,%xmm8
+++	vpxor	%xmm7,%xmm3,%xmm3
+++	addl	48(%rsp),%ebp
+++	xorl	%ecx,%esi
+++	movl	%eax,%edi
+++	shldl	$5,%eax,%eax
+++	vpxor	%xmm4,%xmm3,%xmm3
+++	addl	%esi,%ebp
+++	xorl	%ecx,%edi
+++	vpaddd	%xmm2,%xmm11,%xmm9
+++	shrdl	$7,%ebx,%ebx
+++	addl	%eax,%ebp
+++	vpxor	%xmm8,%xmm3,%xmm3
+++	addl	52(%rsp),%edx
+++	xorl	%ebx,%edi
+++	movl	%ebp,%esi
+++	shldl	$5,%ebp,%ebp
+++	vpsrld	$30,%xmm3,%xmm8
+++	vmovdqa	%xmm9,32(%rsp)
+++	addl	%edi,%edx
+++	xorl	%ebx,%esi
+++	shrdl	$7,%eax,%eax
+++	addl	%ebp,%edx
+++	vpslld	$2,%xmm3,%xmm3
+++	addl	56(%rsp),%ecx
+++	xorl	%eax,%esi
+++	movl	%edx,%edi
+++	shldl	$5,%edx,%edx
+++	addl	%esi,%ecx
+++	xorl	%eax,%edi
+++	shrdl	$7,%ebp,%ebp
+++	addl	%edx,%ecx
+++	vpor	%xmm8,%xmm3,%xmm3
+++	addl	60(%rsp),%ebx
+++	xorl	%ebp,%edi
+++	movl	%ecx,%esi
+++	shldl	$5,%ecx,%ecx
+++	addl	%edi,%ebx
+++	xorl	%ebp,%esi
+++	shrdl	$7,%edx,%edx
+++	addl	%ecx,%ebx
+++	addl	0(%rsp),%eax
+++	vpaddd	%xmm3,%xmm11,%xmm9
+++	xorl	%edx,%esi
+++	movl	%ebx,%edi
+++	shldl	$5,%ebx,%ebx
+++	addl	%esi,%eax
+++	vmovdqa	%xmm9,48(%rsp)
+++	xorl	%edx,%edi
+++	shrdl	$7,%ecx,%ecx
+++	addl	%ebx,%eax
+++	addl	4(%rsp),%ebp
+++	xorl	%ecx,%edi
+++	movl	%eax,%esi
+++	shldl	$5,%eax,%eax
+++	addl	%edi,%ebp
+++	xorl	%ecx,%esi
+++	shrdl	$7,%ebx,%ebx
+++	addl	%eax,%ebp
+++	addl	8(%rsp),%edx
+++	xorl	%ebx,%esi
+++	movl	%ebp,%edi
+++	shldl	$5,%ebp,%ebp
+++	addl	%esi,%edx
+++	xorl	%ebx,%edi
+++	shrdl	$7,%eax,%eax
+++	addl	%ebp,%edx
+++	addl	12(%rsp),%ecx
+++	xorl	%eax,%edi
+++	movl	%edx,%esi
+++	shldl	$5,%edx,%edx
+++	addl	%edi,%ecx
+++	xorl	%eax,%esi
+++	shrdl	$7,%ebp,%ebp
+++	addl	%edx,%ecx
+++	cmpq	%r10,%r9
+++	je	.Ldone_avx
+++	vmovdqa	64(%r14),%xmm6
+++	vmovdqa	-64(%r14),%xmm11
+++	vmovdqu	0(%r9),%xmm0
+++	vmovdqu	16(%r9),%xmm1
+++	vmovdqu	32(%r9),%xmm2
+++	vmovdqu	48(%r9),%xmm3
+++	vpshufb	%xmm6,%xmm0,%xmm0
+++	addq	$64,%r9
+++	addl	16(%rsp),%ebx
+++	xorl	%ebp,%esi
+++	vpshufb	%xmm6,%xmm1,%xmm1
+++	movl	%ecx,%edi
+++	shldl	$5,%ecx,%ecx
+++	vpaddd	%xmm11,%xmm0,%xmm4
+++	addl	%esi,%ebx
+++	xorl	%ebp,%edi
+++	shrdl	$7,%edx,%edx
+++	addl	%ecx,%ebx
+++	vmovdqa	%xmm4,0(%rsp)
+++	addl	20(%rsp),%eax
+++	xorl	%edx,%edi
+++	movl	%ebx,%esi
+++	shldl	$5,%ebx,%ebx
+++	addl	%edi,%eax
+++	xorl	%edx,%esi
+++	shrdl	$7,%ecx,%ecx
+++	addl	%ebx,%eax
+++	addl	24(%rsp),%ebp
+++	xorl	%ecx,%esi
+++	movl	%eax,%edi
+++	shldl	$5,%eax,%eax
+++	addl	%esi,%ebp
+++	xorl	%ecx,%edi
+++	shrdl	$7,%ebx,%ebx
+++	addl	%eax,%ebp
+++	addl	28(%rsp),%edx
+++	xorl	%ebx,%edi
+++	movl	%ebp,%esi
+++	shldl	$5,%ebp,%ebp
+++	addl	%edi,%edx
+++	xorl	%ebx,%esi
+++	shrdl	$7,%eax,%eax
+++	addl	%ebp,%edx
+++	addl	32(%rsp),%ecx
+++	xorl	%eax,%esi
+++	vpshufb	%xmm6,%xmm2,%xmm2
+++	movl	%edx,%edi
+++	shldl	$5,%edx,%edx
+++	vpaddd	%xmm11,%xmm1,%xmm5
+++	addl	%esi,%ecx
+++	xorl	%eax,%edi
+++	shrdl	$7,%ebp,%ebp
+++	addl	%edx,%ecx
+++	vmovdqa	%xmm5,16(%rsp)
+++	addl	36(%rsp),%ebx
+++	xorl	%ebp,%edi
+++	movl	%ecx,%esi
+++	shldl	$5,%ecx,%ecx
+++	addl	%edi,%ebx
+++	xorl	%ebp,%esi
+++	shrdl	$7,%edx,%edx
+++	addl	%ecx,%ebx
+++	addl	40(%rsp),%eax
+++	xorl	%edx,%esi
+++	movl	%ebx,%edi
+++	shldl	$5,%ebx,%ebx
+++	addl	%esi,%eax
+++	xorl	%edx,%edi
+++	shrdl	$7,%ecx,%ecx
+++	addl	%ebx,%eax
+++	addl	44(%rsp),%ebp
+++	xorl	%ecx,%edi
+++	movl	%eax,%esi
+++	shldl	$5,%eax,%eax
+++	addl	%edi,%ebp
+++	xorl	%ecx,%esi
+++	shrdl	$7,%ebx,%ebx
+++	addl	%eax,%ebp
+++	addl	48(%rsp),%edx
+++	xorl	%ebx,%esi
+++	vpshufb	%xmm6,%xmm3,%xmm3
+++	movl	%ebp,%edi
+++	shldl	$5,%ebp,%ebp
+++	vpaddd	%xmm11,%xmm2,%xmm6
+++	addl	%esi,%edx
+++	xorl	%ebx,%edi
+++	shrdl	$7,%eax,%eax
+++	addl	%ebp,%edx
+++	vmovdqa	%xmm6,32(%rsp)
+++	addl	52(%rsp),%ecx
+++	xorl	%eax,%edi
+++	movl	%edx,%esi
+++	shldl	$5,%edx,%edx
+++	addl	%edi,%ecx
+++	xorl	%eax,%esi
+++	shrdl	$7,%ebp,%ebp
+++	addl	%edx,%ecx
+++	addl	56(%rsp),%ebx
+++	xorl	%ebp,%esi
+++	movl	%ecx,%edi
+++	shldl	$5,%ecx,%ecx
+++	addl	%esi,%ebx
+++	xorl	%ebp,%edi
+++	shrdl	$7,%edx,%edx
+++	addl	%ecx,%ebx
+++	addl	60(%rsp),%eax
+++	xorl	%edx,%edi
+++	movl	%ebx,%esi
+++	shldl	$5,%ebx,%ebx
+++	addl	%edi,%eax
+++	shrdl	$7,%ecx,%ecx
+++	addl	%ebx,%eax
+++	addl	0(%r8),%eax
+++	addl	4(%r8),%esi
+++	addl	8(%r8),%ecx
+++	addl	12(%r8),%edx
+++	movl	%eax,0(%r8)
+++	addl	16(%r8),%ebp
+++	movl	%esi,4(%r8)
+++	movl	%esi,%ebx
+++	movl	%ecx,8(%r8)
+++	movl	%ecx,%edi
+++	movl	%edx,12(%r8)
+++	xorl	%edx,%edi
+++	movl	%ebp,16(%r8)
+++	andl	%edi,%esi
+++	jmp	.Loop_avx
+++
+++.align	16
+++.Ldone_avx:
+++	addl	16(%rsp),%ebx
+++	xorl	%ebp,%esi
+++	movl	%ecx,%edi
+++	shldl	$5,%ecx,%ecx
+++	addl	%esi,%ebx
+++	xorl	%ebp,%edi
+++	shrdl	$7,%edx,%edx
+++	addl	%ecx,%ebx
+++	addl	20(%rsp),%eax
+++	xorl	%edx,%edi
+++	movl	%ebx,%esi
+++	shldl	$5,%ebx,%ebx
+++	addl	%edi,%eax
+++	xorl	%edx,%esi
+++	shrdl	$7,%ecx,%ecx
+++	addl	%ebx,%eax
+++	addl	24(%rsp),%ebp
+++	xorl	%ecx,%esi
+++	movl	%eax,%edi
+++	shldl	$5,%eax,%eax
+++	addl	%esi,%ebp
+++	xorl	%ecx,%edi
+++	shrdl	$7,%ebx,%ebx
+++	addl	%eax,%ebp
+++	addl	28(%rsp),%edx
+++	xorl	%ebx,%edi
+++	movl	%ebp,%esi
+++	shldl	$5,%ebp,%ebp
+++	addl	%edi,%edx
+++	xorl	%ebx,%esi
+++	shrdl	$7,%eax,%eax
+++	addl	%ebp,%edx
+++	addl	32(%rsp),%ecx
+++	xorl	%eax,%esi
+++	movl	%edx,%edi
+++	shldl	$5,%edx,%edx
+++	addl	%esi,%ecx
+++	xorl	%eax,%edi
+++	shrdl	$7,%ebp,%ebp
+++	addl	%edx,%ecx
+++	addl	36(%rsp),%ebx
+++	xorl	%ebp,%edi
+++	movl	%ecx,%esi
+++	shldl	$5,%ecx,%ecx
+++	addl	%edi,%ebx
+++	xorl	%ebp,%esi
+++	shrdl	$7,%edx,%edx
+++	addl	%ecx,%ebx
+++	addl	40(%rsp),%eax
+++	xorl	%edx,%esi
+++	movl	%ebx,%edi
+++	shldl	$5,%ebx,%ebx
+++	addl	%esi,%eax
+++	xorl	%edx,%edi
+++	shrdl	$7,%ecx,%ecx
+++	addl	%ebx,%eax
+++	addl	44(%rsp),%ebp
+++	xorl	%ecx,%edi
+++	movl	%eax,%esi
+++	shldl	$5,%eax,%eax
+++	addl	%edi,%ebp
+++	xorl	%ecx,%esi
+++	shrdl	$7,%ebx,%ebx
+++	addl	%eax,%ebp
+++	addl	48(%rsp),%edx
+++	xorl	%ebx,%esi
+++	movl	%ebp,%edi
+++	shldl	$5,%ebp,%ebp
+++	addl	%esi,%edx
+++	xorl	%ebx,%edi
+++	shrdl	$7,%eax,%eax
+++	addl	%ebp,%edx
+++	addl	52(%rsp),%ecx
+++	xorl	%eax,%edi
+++	movl	%edx,%esi
+++	shldl	$5,%edx,%edx
+++	addl	%edi,%ecx
+++	xorl	%eax,%esi
+++	shrdl	$7,%ebp,%ebp
+++	addl	%edx,%ecx
+++	addl	56(%rsp),%ebx
+++	xorl	%ebp,%esi
+++	movl	%ecx,%edi
+++	shldl	$5,%ecx,%ecx
+++	addl	%esi,%ebx
+++	xorl	%ebp,%edi
+++	shrdl	$7,%edx,%edx
+++	addl	%ecx,%ebx
+++	addl	60(%rsp),%eax
+++	xorl	%edx,%edi
+++	movl	%ebx,%esi
+++	shldl	$5,%ebx,%ebx
+++	addl	%edi,%eax
+++	shrdl	$7,%ecx,%ecx
+++	addl	%ebx,%eax
+++	vzeroupper
+++
+++	addl	0(%r8),%eax
+++	addl	4(%r8),%esi
+++	addl	8(%r8),%ecx
+++	movl	%eax,0(%r8)
+++	addl	12(%r8),%edx
+++	movl	%esi,4(%r8)
+++	addl	16(%r8),%ebp
+++	movl	%ecx,8(%r8)
+++	movl	%edx,12(%r8)
+++	movl	%ebp,16(%r8)
+++	movq	-40(%r11),%r14
+++.cfi_restore	%r14
+++	movq	-32(%r11),%r13
+++.cfi_restore	%r13
+++	movq	-24(%r11),%r12
+++.cfi_restore	%r12
+++	movq	-16(%r11),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%r11),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%r11),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lepilogue_avx:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	sha1_block_data_order_avx,.-sha1_block_data_order_avx
+++.type	sha1_block_data_order_avx2,@function
+++.align	16
+++sha1_block_data_order_avx2:
+++_avx2_shortcut:
+++.cfi_startproc	
+++	movq	%rsp,%r11
+++.cfi_def_cfa_register	%r11
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	vzeroupper
+++	movq	%rdi,%r8
+++	movq	%rsi,%r9
+++	movq	%rdx,%r10
+++
+++	leaq	-640(%rsp),%rsp
+++	shlq	$6,%r10
+++	leaq	64(%r9),%r13
+++	andq	$-128,%rsp
+++	addq	%r9,%r10
+++	leaq	K_XX_XX+64(%rip),%r14
+++
+++	movl	0(%r8),%eax
+++	cmpq	%r10,%r13
+++	cmovaeq	%r9,%r13
+++	movl	4(%r8),%ebp
+++	movl	8(%r8),%ecx
+++	movl	12(%r8),%edx
+++	movl	16(%r8),%esi
+++	vmovdqu	64(%r14),%ymm6
+++
+++	vmovdqu	(%r9),%xmm0
+++	vmovdqu	16(%r9),%xmm1
+++	vmovdqu	32(%r9),%xmm2
+++	vmovdqu	48(%r9),%xmm3
+++	leaq	64(%r9),%r9
+++	vinserti128	$1,(%r13),%ymm0,%ymm0
+++	vinserti128	$1,16(%r13),%ymm1,%ymm1
+++	vpshufb	%ymm6,%ymm0,%ymm0
+++	vinserti128	$1,32(%r13),%ymm2,%ymm2
+++	vpshufb	%ymm6,%ymm1,%ymm1
+++	vinserti128	$1,48(%r13),%ymm3,%ymm3
+++	vpshufb	%ymm6,%ymm2,%ymm2
+++	vmovdqu	-64(%r14),%ymm11
+++	vpshufb	%ymm6,%ymm3,%ymm3
+++
+++	vpaddd	%ymm11,%ymm0,%ymm4
+++	vpaddd	%ymm11,%ymm1,%ymm5
+++	vmovdqu	%ymm4,0(%rsp)
+++	vpaddd	%ymm11,%ymm2,%ymm6
+++	vmovdqu	%ymm5,32(%rsp)
+++	vpaddd	%ymm11,%ymm3,%ymm7
+++	vmovdqu	%ymm6,64(%rsp)
+++	vmovdqu	%ymm7,96(%rsp)
+++	vpalignr	$8,%ymm0,%ymm1,%ymm4
+++	vpsrldq	$4,%ymm3,%ymm8
+++	vpxor	%ymm0,%ymm4,%ymm4
+++	vpxor	%ymm2,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	vpsrld	$31,%ymm4,%ymm8
+++	vpslldq	$12,%ymm4,%ymm10
+++	vpaddd	%ymm4,%ymm4,%ymm4
+++	vpsrld	$30,%ymm10,%ymm9
+++	vpor	%ymm8,%ymm4,%ymm4
+++	vpslld	$2,%ymm10,%ymm10
+++	vpxor	%ymm9,%ymm4,%ymm4
+++	vpxor	%ymm10,%ymm4,%ymm4
+++	vpaddd	%ymm11,%ymm4,%ymm9
+++	vmovdqu	%ymm9,128(%rsp)
+++	vpalignr	$8,%ymm1,%ymm2,%ymm5
+++	vpsrldq	$4,%ymm4,%ymm8
+++	vpxor	%ymm1,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	vpsrld	$31,%ymm5,%ymm8
+++	vmovdqu	-32(%r14),%ymm11
+++	vpslldq	$12,%ymm5,%ymm10
+++	vpaddd	%ymm5,%ymm5,%ymm5
+++	vpsrld	$30,%ymm10,%ymm9
+++	vpor	%ymm8,%ymm5,%ymm5
+++	vpslld	$2,%ymm10,%ymm10
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	vpxor	%ymm10,%ymm5,%ymm5
+++	vpaddd	%ymm11,%ymm5,%ymm9
+++	vmovdqu	%ymm9,160(%rsp)
+++	vpalignr	$8,%ymm2,%ymm3,%ymm6
+++	vpsrldq	$4,%ymm5,%ymm8
+++	vpxor	%ymm2,%ymm6,%ymm6
+++	vpxor	%ymm4,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	vpsrld	$31,%ymm6,%ymm8
+++	vpslldq	$12,%ymm6,%ymm10
+++	vpaddd	%ymm6,%ymm6,%ymm6
+++	vpsrld	$30,%ymm10,%ymm9
+++	vpor	%ymm8,%ymm6,%ymm6
+++	vpslld	$2,%ymm10,%ymm10
+++	vpxor	%ymm9,%ymm6,%ymm6
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	vpaddd	%ymm11,%ymm6,%ymm9
+++	vmovdqu	%ymm9,192(%rsp)
+++	vpalignr	$8,%ymm3,%ymm4,%ymm7
+++	vpsrldq	$4,%ymm6,%ymm8
+++	vpxor	%ymm3,%ymm7,%ymm7
+++	vpxor	%ymm5,%ymm8,%ymm8
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	vpsrld	$31,%ymm7,%ymm8
+++	vpslldq	$12,%ymm7,%ymm10
+++	vpaddd	%ymm7,%ymm7,%ymm7
+++	vpsrld	$30,%ymm10,%ymm9
+++	vpor	%ymm8,%ymm7,%ymm7
+++	vpslld	$2,%ymm10,%ymm10
+++	vpxor	%ymm9,%ymm7,%ymm7
+++	vpxor	%ymm10,%ymm7,%ymm7
+++	vpaddd	%ymm11,%ymm7,%ymm9
+++	vmovdqu	%ymm9,224(%rsp)
+++	leaq	128(%rsp),%r13
+++	jmp	.Loop_avx2
+++.align	32
+++.Loop_avx2:
+++	rorxl	$2,%ebp,%ebx
+++	andnl	%edx,%ebp,%edi
+++	andl	%ecx,%ebp
+++	xorl	%edi,%ebp
+++	jmp	.Lalign32_1
+++.align	32
+++.Lalign32_1:
+++	vpalignr	$8,%ymm6,%ymm7,%ymm8
+++	vpxor	%ymm4,%ymm0,%ymm0
+++	addl	-128(%r13),%esi
+++	andnl	%ecx,%eax,%edi
+++	vpxor	%ymm1,%ymm0,%ymm0
+++	addl	%ebp,%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	vpxor	%ymm8,%ymm0,%ymm0
+++	andl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%edi,%eax
+++	vpsrld	$30,%ymm0,%ymm8
+++	vpslld	$2,%ymm0,%ymm0
+++	addl	-124(%r13),%edx
+++	andnl	%ebx,%esi,%edi
+++	addl	%eax,%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	andl	%ebp,%esi
+++	vpor	%ymm8,%ymm0,%ymm0
+++	addl	%r12d,%edx
+++	xorl	%edi,%esi
+++	addl	-120(%r13),%ecx
+++	andnl	%ebp,%edx,%edi
+++	vpaddd	%ymm11,%ymm0,%ymm9
+++	addl	%esi,%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	andl	%eax,%edx
+++	vmovdqu	%ymm9,256(%rsp)
+++	addl	%r12d,%ecx
+++	xorl	%edi,%edx
+++	addl	-116(%r13),%ebx
+++	andnl	%eax,%ecx,%edi
+++	addl	%edx,%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	andl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%edi,%ecx
+++	addl	-96(%r13),%ebp
+++	andnl	%esi,%ebx,%edi
+++	addl	%ecx,%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	andl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%edi,%ebx
+++	vpalignr	$8,%ymm7,%ymm0,%ymm8
+++	vpxor	%ymm5,%ymm1,%ymm1
+++	addl	-92(%r13),%eax
+++	andnl	%edx,%ebp,%edi
+++	vpxor	%ymm2,%ymm1,%ymm1
+++	addl	%ebx,%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	vpxor	%ymm8,%ymm1,%ymm1
+++	andl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edi,%ebp
+++	vpsrld	$30,%ymm1,%ymm8
+++	vpslld	$2,%ymm1,%ymm1
+++	addl	-88(%r13),%esi
+++	andnl	%ecx,%eax,%edi
+++	addl	%ebp,%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	andl	%ebx,%eax
+++	vpor	%ymm8,%ymm1,%ymm1
+++	addl	%r12d,%esi
+++	xorl	%edi,%eax
+++	addl	-84(%r13),%edx
+++	andnl	%ebx,%esi,%edi
+++	vpaddd	%ymm11,%ymm1,%ymm9
+++	addl	%eax,%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	andl	%ebp,%esi
+++	vmovdqu	%ymm9,288(%rsp)
+++	addl	%r12d,%edx
+++	xorl	%edi,%esi
+++	addl	-64(%r13),%ecx
+++	andnl	%ebp,%edx,%edi
+++	addl	%esi,%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	andl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%edi,%edx
+++	addl	-60(%r13),%ebx
+++	andnl	%eax,%ecx,%edi
+++	addl	%edx,%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	andl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%edi,%ecx
+++	vpalignr	$8,%ymm0,%ymm1,%ymm8
+++	vpxor	%ymm6,%ymm2,%ymm2
+++	addl	-56(%r13),%ebp
+++	andnl	%esi,%ebx,%edi
+++	vpxor	%ymm3,%ymm2,%ymm2
+++	vmovdqu	0(%r14),%ymm11
+++	addl	%ecx,%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	vpxor	%ymm8,%ymm2,%ymm2
+++	andl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%edi,%ebx
+++	vpsrld	$30,%ymm2,%ymm8
+++	vpslld	$2,%ymm2,%ymm2
+++	addl	-52(%r13),%eax
+++	andnl	%edx,%ebp,%edi
+++	addl	%ebx,%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	andl	%ecx,%ebp
+++	vpor	%ymm8,%ymm2,%ymm2
+++	addl	%r12d,%eax
+++	xorl	%edi,%ebp
+++	addl	-32(%r13),%esi
+++	andnl	%ecx,%eax,%edi
+++	vpaddd	%ymm11,%ymm2,%ymm9
+++	addl	%ebp,%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	andl	%ebx,%eax
+++	vmovdqu	%ymm9,320(%rsp)
+++	addl	%r12d,%esi
+++	xorl	%edi,%eax
+++	addl	-28(%r13),%edx
+++	andnl	%ebx,%esi,%edi
+++	addl	%eax,%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	andl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%edi,%esi
+++	addl	-24(%r13),%ecx
+++	andnl	%ebp,%edx,%edi
+++	addl	%esi,%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	andl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%edi,%edx
+++	vpalignr	$8,%ymm1,%ymm2,%ymm8
+++	vpxor	%ymm7,%ymm3,%ymm3
+++	addl	-20(%r13),%ebx
+++	andnl	%eax,%ecx,%edi
+++	vpxor	%ymm4,%ymm3,%ymm3
+++	addl	%edx,%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	vpxor	%ymm8,%ymm3,%ymm3
+++	andl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%edi,%ecx
+++	vpsrld	$30,%ymm3,%ymm8
+++	vpslld	$2,%ymm3,%ymm3
+++	addl	0(%r13),%ebp
+++	andnl	%esi,%ebx,%edi
+++	addl	%ecx,%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	andl	%edx,%ebx
+++	vpor	%ymm8,%ymm3,%ymm3
+++	addl	%r12d,%ebp
+++	xorl	%edi,%ebx
+++	addl	4(%r13),%eax
+++	andnl	%edx,%ebp,%edi
+++	vpaddd	%ymm11,%ymm3,%ymm9
+++	addl	%ebx,%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	andl	%ecx,%ebp
+++	vmovdqu	%ymm9,352(%rsp)
+++	addl	%r12d,%eax
+++	xorl	%edi,%ebp
+++	addl	8(%r13),%esi
+++	andnl	%ecx,%eax,%edi
+++	addl	%ebp,%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	andl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%edi,%eax
+++	addl	12(%r13),%edx
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%ebx,%esi
+++	vpalignr	$8,%ymm2,%ymm3,%ymm8
+++	vpxor	%ymm0,%ymm4,%ymm4
+++	addl	32(%r13),%ecx
+++	leal	(%rcx,%rsi,1),%ecx
+++	vpxor	%ymm5,%ymm4,%ymm4
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	addl	%r12d,%ecx
+++	xorl	%ebp,%edx
+++	addl	36(%r13),%ebx
+++	vpsrld	$30,%ymm4,%ymm8
+++	vpslld	$2,%ymm4,%ymm4
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%eax,%ecx
+++	vpor	%ymm8,%ymm4,%ymm4
+++	addl	40(%r13),%ebp
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	vpaddd	%ymm11,%ymm4,%ymm9
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%esi,%ebx
+++	addl	44(%r13),%eax
+++	vmovdqu	%ymm9,384(%rsp)
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edx,%ebp
+++	addl	64(%r13),%esi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%ecx,%eax
+++	vpalignr	$8,%ymm3,%ymm4,%ymm8
+++	vpxor	%ymm1,%ymm5,%ymm5
+++	addl	68(%r13),%edx
+++	leal	(%rdx,%rax,1),%edx
+++	vpxor	%ymm6,%ymm5,%ymm5
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	addl	%r12d,%edx
+++	xorl	%ebx,%esi
+++	addl	72(%r13),%ecx
+++	vpsrld	$30,%ymm5,%ymm8
+++	vpslld	$2,%ymm5,%ymm5
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%ebp,%edx
+++	vpor	%ymm8,%ymm5,%ymm5
+++	addl	76(%r13),%ebx
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	vpaddd	%ymm11,%ymm5,%ymm9
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%eax,%ecx
+++	addl	96(%r13),%ebp
+++	vmovdqu	%ymm9,416(%rsp)
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%esi,%ebx
+++	addl	100(%r13),%eax
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edx,%ebp
+++	vpalignr	$8,%ymm4,%ymm5,%ymm8
+++	vpxor	%ymm2,%ymm6,%ymm6
+++	addl	104(%r13),%esi
+++	leal	(%rsi,%rbp,1),%esi
+++	vpxor	%ymm7,%ymm6,%ymm6
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	addl	%r12d,%esi
+++	xorl	%ecx,%eax
+++	addl	108(%r13),%edx
+++	leaq	256(%r13),%r13
+++	vpsrld	$30,%ymm6,%ymm8
+++	vpslld	$2,%ymm6,%ymm6
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%ebx,%esi
+++	vpor	%ymm8,%ymm6,%ymm6
+++	addl	-128(%r13),%ecx
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	vpaddd	%ymm11,%ymm6,%ymm9
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%ebp,%edx
+++	addl	-124(%r13),%ebx
+++	vmovdqu	%ymm9,448(%rsp)
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%eax,%ecx
+++	addl	-120(%r13),%ebp
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%esi,%ebx
+++	vpalignr	$8,%ymm5,%ymm6,%ymm8
+++	vpxor	%ymm3,%ymm7,%ymm7
+++	addl	-116(%r13),%eax
+++	leal	(%rax,%rbx,1),%eax
+++	vpxor	%ymm0,%ymm7,%ymm7
+++	vmovdqu	32(%r14),%ymm11
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	addl	%r12d,%eax
+++	xorl	%edx,%ebp
+++	addl	-96(%r13),%esi
+++	vpsrld	$30,%ymm7,%ymm8
+++	vpslld	$2,%ymm7,%ymm7
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%ecx,%eax
+++	vpor	%ymm8,%ymm7,%ymm7
+++	addl	-92(%r13),%edx
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	vpaddd	%ymm11,%ymm7,%ymm9
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%ebx,%esi
+++	addl	-88(%r13),%ecx
+++	vmovdqu	%ymm9,480(%rsp)
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%ebp,%edx
+++	addl	-84(%r13),%ebx
+++	movl	%esi,%edi
+++	xorl	%eax,%edi
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	andl	%edi,%ecx
+++	jmp	.Lalign32_2
+++.align	32
+++.Lalign32_2:
+++	vpalignr	$8,%ymm6,%ymm7,%ymm8
+++	vpxor	%ymm4,%ymm0,%ymm0
+++	addl	-64(%r13),%ebp
+++	xorl	%esi,%ecx
+++	vpxor	%ymm1,%ymm0,%ymm0
+++	movl	%edx,%edi
+++	xorl	%esi,%edi
+++	leal	(%rcx,%rbp,1),%ebp
+++	vpxor	%ymm8,%ymm0,%ymm0
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	vpsrld	$30,%ymm0,%ymm8
+++	vpslld	$2,%ymm0,%ymm0
+++	addl	%r12d,%ebp
+++	andl	%edi,%ebx
+++	addl	-60(%r13),%eax
+++	xorl	%edx,%ebx
+++	movl	%ecx,%edi
+++	xorl	%edx,%edi
+++	vpor	%ymm8,%ymm0,%ymm0
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	vpaddd	%ymm11,%ymm0,%ymm9
+++	addl	%r12d,%eax
+++	andl	%edi,%ebp
+++	addl	-56(%r13),%esi
+++	xorl	%ecx,%ebp
+++	vmovdqu	%ymm9,512(%rsp)
+++	movl	%ebx,%edi
+++	xorl	%ecx,%edi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	andl	%edi,%eax
+++	addl	-52(%r13),%edx
+++	xorl	%ebx,%eax
+++	movl	%ebp,%edi
+++	xorl	%ebx,%edi
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	andl	%edi,%esi
+++	addl	-32(%r13),%ecx
+++	xorl	%ebp,%esi
+++	movl	%eax,%edi
+++	xorl	%ebp,%edi
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	andl	%edi,%edx
+++	vpalignr	$8,%ymm7,%ymm0,%ymm8
+++	vpxor	%ymm5,%ymm1,%ymm1
+++	addl	-28(%r13),%ebx
+++	xorl	%eax,%edx
+++	vpxor	%ymm2,%ymm1,%ymm1
+++	movl	%esi,%edi
+++	xorl	%eax,%edi
+++	leal	(%rbx,%rdx,1),%ebx
+++	vpxor	%ymm8,%ymm1,%ymm1
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	vpsrld	$30,%ymm1,%ymm8
+++	vpslld	$2,%ymm1,%ymm1
+++	addl	%r12d,%ebx
+++	andl	%edi,%ecx
+++	addl	-24(%r13),%ebp
+++	xorl	%esi,%ecx
+++	movl	%edx,%edi
+++	xorl	%esi,%edi
+++	vpor	%ymm8,%ymm1,%ymm1
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	vpaddd	%ymm11,%ymm1,%ymm9
+++	addl	%r12d,%ebp
+++	andl	%edi,%ebx
+++	addl	-20(%r13),%eax
+++	xorl	%edx,%ebx
+++	vmovdqu	%ymm9,544(%rsp)
+++	movl	%ecx,%edi
+++	xorl	%edx,%edi
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	andl	%edi,%ebp
+++	addl	0(%r13),%esi
+++	xorl	%ecx,%ebp
+++	movl	%ebx,%edi
+++	xorl	%ecx,%edi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	andl	%edi,%eax
+++	addl	4(%r13),%edx
+++	xorl	%ebx,%eax
+++	movl	%ebp,%edi
+++	xorl	%ebx,%edi
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	andl	%edi,%esi
+++	vpalignr	$8,%ymm0,%ymm1,%ymm8
+++	vpxor	%ymm6,%ymm2,%ymm2
+++	addl	8(%r13),%ecx
+++	xorl	%ebp,%esi
+++	vpxor	%ymm3,%ymm2,%ymm2
+++	movl	%eax,%edi
+++	xorl	%ebp,%edi
+++	leal	(%rcx,%rsi,1),%ecx
+++	vpxor	%ymm8,%ymm2,%ymm2
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	vpsrld	$30,%ymm2,%ymm8
+++	vpslld	$2,%ymm2,%ymm2
+++	addl	%r12d,%ecx
+++	andl	%edi,%edx
+++	addl	12(%r13),%ebx
+++	xorl	%eax,%edx
+++	movl	%esi,%edi
+++	xorl	%eax,%edi
+++	vpor	%ymm8,%ymm2,%ymm2
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	vpaddd	%ymm11,%ymm2,%ymm9
+++	addl	%r12d,%ebx
+++	andl	%edi,%ecx
+++	addl	32(%r13),%ebp
+++	xorl	%esi,%ecx
+++	vmovdqu	%ymm9,576(%rsp)
+++	movl	%edx,%edi
+++	xorl	%esi,%edi
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	andl	%edi,%ebx
+++	addl	36(%r13),%eax
+++	xorl	%edx,%ebx
+++	movl	%ecx,%edi
+++	xorl	%edx,%edi
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	andl	%edi,%ebp
+++	addl	40(%r13),%esi
+++	xorl	%ecx,%ebp
+++	movl	%ebx,%edi
+++	xorl	%ecx,%edi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	andl	%edi,%eax
+++	vpalignr	$8,%ymm1,%ymm2,%ymm8
+++	vpxor	%ymm7,%ymm3,%ymm3
+++	addl	44(%r13),%edx
+++	xorl	%ebx,%eax
+++	vpxor	%ymm4,%ymm3,%ymm3
+++	movl	%ebp,%edi
+++	xorl	%ebx,%edi
+++	leal	(%rdx,%rax,1),%edx
+++	vpxor	%ymm8,%ymm3,%ymm3
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	vpsrld	$30,%ymm3,%ymm8
+++	vpslld	$2,%ymm3,%ymm3
+++	addl	%r12d,%edx
+++	andl	%edi,%esi
+++	addl	64(%r13),%ecx
+++	xorl	%ebp,%esi
+++	movl	%eax,%edi
+++	xorl	%ebp,%edi
+++	vpor	%ymm8,%ymm3,%ymm3
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	vpaddd	%ymm11,%ymm3,%ymm9
+++	addl	%r12d,%ecx
+++	andl	%edi,%edx
+++	addl	68(%r13),%ebx
+++	xorl	%eax,%edx
+++	vmovdqu	%ymm9,608(%rsp)
+++	movl	%esi,%edi
+++	xorl	%eax,%edi
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	andl	%edi,%ecx
+++	addl	72(%r13),%ebp
+++	xorl	%esi,%ecx
+++	movl	%edx,%edi
+++	xorl	%esi,%edi
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	andl	%edi,%ebx
+++	addl	76(%r13),%eax
+++	xorl	%edx,%ebx
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edx,%ebp
+++	addl	96(%r13),%esi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%ecx,%eax
+++	addl	100(%r13),%edx
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%ebx,%esi
+++	addl	104(%r13),%ecx
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%ebp,%edx
+++	addl	108(%r13),%ebx
+++	leaq	256(%r13),%r13
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%eax,%ecx
+++	addl	-128(%r13),%ebp
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%esi,%ebx
+++	addl	-124(%r13),%eax
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edx,%ebp
+++	addl	-120(%r13),%esi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%ecx,%eax
+++	addl	-116(%r13),%edx
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%ebx,%esi
+++	addl	-96(%r13),%ecx
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%ebp,%edx
+++	addl	-92(%r13),%ebx
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%eax,%ecx
+++	addl	-88(%r13),%ebp
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%esi,%ebx
+++	addl	-84(%r13),%eax
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edx,%ebp
+++	addl	-64(%r13),%esi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%ecx,%eax
+++	addl	-60(%r13),%edx
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%ebx,%esi
+++	addl	-56(%r13),%ecx
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%ebp,%edx
+++	addl	-52(%r13),%ebx
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%eax,%ecx
+++	addl	-32(%r13),%ebp
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%esi,%ebx
+++	addl	-28(%r13),%eax
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edx,%ebp
+++	addl	-24(%r13),%esi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%ecx,%eax
+++	addl	-20(%r13),%edx
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	addl	%r12d,%edx
+++	leaq	128(%r9),%r13
+++	leaq	128(%r9),%rdi
+++	cmpq	%r10,%r13
+++	cmovaeq	%r9,%r13
+++
+++
+++	addl	0(%r8),%edx
+++	addl	4(%r8),%esi
+++	addl	8(%r8),%ebp
+++	movl	%edx,0(%r8)
+++	addl	12(%r8),%ebx
+++	movl	%esi,4(%r8)
+++	movl	%edx,%eax
+++	addl	16(%r8),%ecx
+++	movl	%ebp,%r12d
+++	movl	%ebp,8(%r8)
+++	movl	%ebx,%edx
+++
+++	movl	%ebx,12(%r8)
+++	movl	%esi,%ebp
+++	movl	%ecx,16(%r8)
+++
+++	movl	%ecx,%esi
+++	movl	%r12d,%ecx
+++
+++
+++	cmpq	%r10,%r9
+++	je	.Ldone_avx2
+++	vmovdqu	64(%r14),%ymm6
+++	cmpq	%r10,%rdi
+++	ja	.Last_avx2
+++
+++	vmovdqu	-64(%rdi),%xmm0
+++	vmovdqu	-48(%rdi),%xmm1
+++	vmovdqu	-32(%rdi),%xmm2
+++	vmovdqu	-16(%rdi),%xmm3
+++	vinserti128	$1,0(%r13),%ymm0,%ymm0
+++	vinserti128	$1,16(%r13),%ymm1,%ymm1
+++	vinserti128	$1,32(%r13),%ymm2,%ymm2
+++	vinserti128	$1,48(%r13),%ymm3,%ymm3
+++	jmp	.Last_avx2
+++
+++.align	32
+++.Last_avx2:
+++	leaq	128+16(%rsp),%r13
+++	rorxl	$2,%ebp,%ebx
+++	andnl	%edx,%ebp,%edi
+++	andl	%ecx,%ebp
+++	xorl	%edi,%ebp
+++	subq	$-128,%r9
+++	addl	-128(%r13),%esi
+++	andnl	%ecx,%eax,%edi
+++	addl	%ebp,%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	andl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%edi,%eax
+++	addl	-124(%r13),%edx
+++	andnl	%ebx,%esi,%edi
+++	addl	%eax,%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	andl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%edi,%esi
+++	addl	-120(%r13),%ecx
+++	andnl	%ebp,%edx,%edi
+++	addl	%esi,%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	andl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%edi,%edx
+++	addl	-116(%r13),%ebx
+++	andnl	%eax,%ecx,%edi
+++	addl	%edx,%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	andl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%edi,%ecx
+++	addl	-96(%r13),%ebp
+++	andnl	%esi,%ebx,%edi
+++	addl	%ecx,%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	andl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%edi,%ebx
+++	addl	-92(%r13),%eax
+++	andnl	%edx,%ebp,%edi
+++	addl	%ebx,%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	andl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edi,%ebp
+++	addl	-88(%r13),%esi
+++	andnl	%ecx,%eax,%edi
+++	addl	%ebp,%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	andl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%edi,%eax
+++	addl	-84(%r13),%edx
+++	andnl	%ebx,%esi,%edi
+++	addl	%eax,%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	andl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%edi,%esi
+++	addl	-64(%r13),%ecx
+++	andnl	%ebp,%edx,%edi
+++	addl	%esi,%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	andl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%edi,%edx
+++	addl	-60(%r13),%ebx
+++	andnl	%eax,%ecx,%edi
+++	addl	%edx,%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	andl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%edi,%ecx
+++	addl	-56(%r13),%ebp
+++	andnl	%esi,%ebx,%edi
+++	addl	%ecx,%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	andl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%edi,%ebx
+++	addl	-52(%r13),%eax
+++	andnl	%edx,%ebp,%edi
+++	addl	%ebx,%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	andl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edi,%ebp
+++	addl	-32(%r13),%esi
+++	andnl	%ecx,%eax,%edi
+++	addl	%ebp,%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	andl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%edi,%eax
+++	addl	-28(%r13),%edx
+++	andnl	%ebx,%esi,%edi
+++	addl	%eax,%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	andl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%edi,%esi
+++	addl	-24(%r13),%ecx
+++	andnl	%ebp,%edx,%edi
+++	addl	%esi,%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	andl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%edi,%edx
+++	addl	-20(%r13),%ebx
+++	andnl	%eax,%ecx,%edi
+++	addl	%edx,%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	andl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%edi,%ecx
+++	addl	0(%r13),%ebp
+++	andnl	%esi,%ebx,%edi
+++	addl	%ecx,%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	andl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%edi,%ebx
+++	addl	4(%r13),%eax
+++	andnl	%edx,%ebp,%edi
+++	addl	%ebx,%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	andl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edi,%ebp
+++	addl	8(%r13),%esi
+++	andnl	%ecx,%eax,%edi
+++	addl	%ebp,%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	andl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%edi,%eax
+++	addl	12(%r13),%edx
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%ebx,%esi
+++	addl	32(%r13),%ecx
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%ebp,%edx
+++	addl	36(%r13),%ebx
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%eax,%ecx
+++	addl	40(%r13),%ebp
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%esi,%ebx
+++	addl	44(%r13),%eax
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edx,%ebp
+++	addl	64(%r13),%esi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%ecx,%eax
+++	vmovdqu	-64(%r14),%ymm11
+++	vpshufb	%ymm6,%ymm0,%ymm0
+++	addl	68(%r13),%edx
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%ebx,%esi
+++	addl	72(%r13),%ecx
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%ebp,%edx
+++	addl	76(%r13),%ebx
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%eax,%ecx
+++	addl	96(%r13),%ebp
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%esi,%ebx
+++	addl	100(%r13),%eax
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edx,%ebp
+++	vpshufb	%ymm6,%ymm1,%ymm1
+++	vpaddd	%ymm11,%ymm0,%ymm8
+++	addl	104(%r13),%esi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%ecx,%eax
+++	addl	108(%r13),%edx
+++	leaq	256(%r13),%r13
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%ebx,%esi
+++	addl	-128(%r13),%ecx
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%ebp,%edx
+++	addl	-124(%r13),%ebx
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%eax,%ecx
+++	addl	-120(%r13),%ebp
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%esi,%ebx
+++	vmovdqu	%ymm8,0(%rsp)
+++	vpshufb	%ymm6,%ymm2,%ymm2
+++	vpaddd	%ymm11,%ymm1,%ymm9
+++	addl	-116(%r13),%eax
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edx,%ebp
+++	addl	-96(%r13),%esi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%ecx,%eax
+++	addl	-92(%r13),%edx
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%ebx,%esi
+++	addl	-88(%r13),%ecx
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%ebp,%edx
+++	addl	-84(%r13),%ebx
+++	movl	%esi,%edi
+++	xorl	%eax,%edi
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	andl	%edi,%ecx
+++	vmovdqu	%ymm9,32(%rsp)
+++	vpshufb	%ymm6,%ymm3,%ymm3
+++	vpaddd	%ymm11,%ymm2,%ymm6
+++	addl	-64(%r13),%ebp
+++	xorl	%esi,%ecx
+++	movl	%edx,%edi
+++	xorl	%esi,%edi
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	andl	%edi,%ebx
+++	addl	-60(%r13),%eax
+++	xorl	%edx,%ebx
+++	movl	%ecx,%edi
+++	xorl	%edx,%edi
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	andl	%edi,%ebp
+++	addl	-56(%r13),%esi
+++	xorl	%ecx,%ebp
+++	movl	%ebx,%edi
+++	xorl	%ecx,%edi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	andl	%edi,%eax
+++	addl	-52(%r13),%edx
+++	xorl	%ebx,%eax
+++	movl	%ebp,%edi
+++	xorl	%ebx,%edi
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	andl	%edi,%esi
+++	addl	-32(%r13),%ecx
+++	xorl	%ebp,%esi
+++	movl	%eax,%edi
+++	xorl	%ebp,%edi
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	andl	%edi,%edx
+++	jmp	.Lalign32_3
+++.align	32
+++.Lalign32_3:
+++	vmovdqu	%ymm6,64(%rsp)
+++	vpaddd	%ymm11,%ymm3,%ymm7
+++	addl	-28(%r13),%ebx
+++	xorl	%eax,%edx
+++	movl	%esi,%edi
+++	xorl	%eax,%edi
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	andl	%edi,%ecx
+++	addl	-24(%r13),%ebp
+++	xorl	%esi,%ecx
+++	movl	%edx,%edi
+++	xorl	%esi,%edi
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	andl	%edi,%ebx
+++	addl	-20(%r13),%eax
+++	xorl	%edx,%ebx
+++	movl	%ecx,%edi
+++	xorl	%edx,%edi
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	andl	%edi,%ebp
+++	addl	0(%r13),%esi
+++	xorl	%ecx,%ebp
+++	movl	%ebx,%edi
+++	xorl	%ecx,%edi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	andl	%edi,%eax
+++	addl	4(%r13),%edx
+++	xorl	%ebx,%eax
+++	movl	%ebp,%edi
+++	xorl	%ebx,%edi
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	andl	%edi,%esi
+++	vmovdqu	%ymm7,96(%rsp)
+++	addl	8(%r13),%ecx
+++	xorl	%ebp,%esi
+++	movl	%eax,%edi
+++	xorl	%ebp,%edi
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	andl	%edi,%edx
+++	addl	12(%r13),%ebx
+++	xorl	%eax,%edx
+++	movl	%esi,%edi
+++	xorl	%eax,%edi
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	andl	%edi,%ecx
+++	addl	32(%r13),%ebp
+++	xorl	%esi,%ecx
+++	movl	%edx,%edi
+++	xorl	%esi,%edi
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	andl	%edi,%ebx
+++	addl	36(%r13),%eax
+++	xorl	%edx,%ebx
+++	movl	%ecx,%edi
+++	xorl	%edx,%edi
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	andl	%edi,%ebp
+++	addl	40(%r13),%esi
+++	xorl	%ecx,%ebp
+++	movl	%ebx,%edi
+++	xorl	%ecx,%edi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	andl	%edi,%eax
+++	vpalignr	$8,%ymm0,%ymm1,%ymm4
+++	addl	44(%r13),%edx
+++	xorl	%ebx,%eax
+++	movl	%ebp,%edi
+++	xorl	%ebx,%edi
+++	vpsrldq	$4,%ymm3,%ymm8
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	vpxor	%ymm0,%ymm4,%ymm4
+++	vpxor	%ymm2,%ymm8,%ymm8
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	vpxor	%ymm8,%ymm4,%ymm4
+++	andl	%edi,%esi
+++	addl	64(%r13),%ecx
+++	xorl	%ebp,%esi
+++	movl	%eax,%edi
+++	vpsrld	$31,%ymm4,%ymm8
+++	xorl	%ebp,%edi
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	vpslldq	$12,%ymm4,%ymm10
+++	vpaddd	%ymm4,%ymm4,%ymm4
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	vpsrld	$30,%ymm10,%ymm9
+++	vpor	%ymm8,%ymm4,%ymm4
+++	addl	%r12d,%ecx
+++	andl	%edi,%edx
+++	vpslld	$2,%ymm10,%ymm10
+++	vpxor	%ymm9,%ymm4,%ymm4
+++	addl	68(%r13),%ebx
+++	xorl	%eax,%edx
+++	vpxor	%ymm10,%ymm4,%ymm4
+++	movl	%esi,%edi
+++	xorl	%eax,%edi
+++	leal	(%rbx,%rdx,1),%ebx
+++	vpaddd	%ymm11,%ymm4,%ymm9
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	vmovdqu	%ymm9,128(%rsp)
+++	addl	%r12d,%ebx
+++	andl	%edi,%ecx
+++	addl	72(%r13),%ebp
+++	xorl	%esi,%ecx
+++	movl	%edx,%edi
+++	xorl	%esi,%edi
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	andl	%edi,%ebx
+++	addl	76(%r13),%eax
+++	xorl	%edx,%ebx
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edx,%ebp
+++	vpalignr	$8,%ymm1,%ymm2,%ymm5
+++	addl	96(%r13),%esi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	vpsrldq	$4,%ymm4,%ymm8
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%ecx,%eax
+++	vpxor	%ymm1,%ymm5,%ymm5
+++	vpxor	%ymm3,%ymm8,%ymm8
+++	addl	100(%r13),%edx
+++	leal	(%rdx,%rax,1),%edx
+++	vpxor	%ymm8,%ymm5,%ymm5
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	vpsrld	$31,%ymm5,%ymm8
+++	vmovdqu	-32(%r14),%ymm11
+++	xorl	%ebx,%esi
+++	addl	104(%r13),%ecx
+++	leal	(%rcx,%rsi,1),%ecx
+++	vpslldq	$12,%ymm5,%ymm10
+++	vpaddd	%ymm5,%ymm5,%ymm5
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	vpsrld	$30,%ymm10,%ymm9
+++	vpor	%ymm8,%ymm5,%ymm5
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	vpslld	$2,%ymm10,%ymm10
+++	vpxor	%ymm9,%ymm5,%ymm5
+++	xorl	%ebp,%edx
+++	addl	108(%r13),%ebx
+++	leaq	256(%r13),%r13
+++	vpxor	%ymm10,%ymm5,%ymm5
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	vpaddd	%ymm11,%ymm5,%ymm9
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%eax,%ecx
+++	vmovdqu	%ymm9,160(%rsp)
+++	addl	-128(%r13),%ebp
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%esi,%ebx
+++	vpalignr	$8,%ymm2,%ymm3,%ymm6
+++	addl	-124(%r13),%eax
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	vpsrldq	$4,%ymm5,%ymm8
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edx,%ebp
+++	vpxor	%ymm2,%ymm6,%ymm6
+++	vpxor	%ymm4,%ymm8,%ymm8
+++	addl	-120(%r13),%esi
+++	leal	(%rsi,%rbp,1),%esi
+++	vpxor	%ymm8,%ymm6,%ymm6
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	vpsrld	$31,%ymm6,%ymm8
+++	xorl	%ecx,%eax
+++	addl	-116(%r13),%edx
+++	leal	(%rdx,%rax,1),%edx
+++	vpslldq	$12,%ymm6,%ymm10
+++	vpaddd	%ymm6,%ymm6,%ymm6
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	vpsrld	$30,%ymm10,%ymm9
+++	vpor	%ymm8,%ymm6,%ymm6
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	vpslld	$2,%ymm10,%ymm10
+++	vpxor	%ymm9,%ymm6,%ymm6
+++	xorl	%ebx,%esi
+++	addl	-96(%r13),%ecx
+++	vpxor	%ymm10,%ymm6,%ymm6
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	vpaddd	%ymm11,%ymm6,%ymm9
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%ebp,%edx
+++	vmovdqu	%ymm9,192(%rsp)
+++	addl	-92(%r13),%ebx
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%eax,%ecx
+++	vpalignr	$8,%ymm3,%ymm4,%ymm7
+++	addl	-88(%r13),%ebp
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	vpsrldq	$4,%ymm6,%ymm8
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%esi,%ebx
+++	vpxor	%ymm3,%ymm7,%ymm7
+++	vpxor	%ymm5,%ymm8,%ymm8
+++	addl	-84(%r13),%eax
+++	leal	(%rax,%rbx,1),%eax
+++	vpxor	%ymm8,%ymm7,%ymm7
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	vpsrld	$31,%ymm7,%ymm8
+++	xorl	%edx,%ebp
+++	addl	-64(%r13),%esi
+++	leal	(%rsi,%rbp,1),%esi
+++	vpslldq	$12,%ymm7,%ymm10
+++	vpaddd	%ymm7,%ymm7,%ymm7
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	vpsrld	$30,%ymm10,%ymm9
+++	vpor	%ymm8,%ymm7,%ymm7
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	vpslld	$2,%ymm10,%ymm10
+++	vpxor	%ymm9,%ymm7,%ymm7
+++	xorl	%ecx,%eax
+++	addl	-60(%r13),%edx
+++	vpxor	%ymm10,%ymm7,%ymm7
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	rorxl	$2,%esi,%eax
+++	vpaddd	%ymm11,%ymm7,%ymm9
+++	xorl	%ebp,%esi
+++	addl	%r12d,%edx
+++	xorl	%ebx,%esi
+++	vmovdqu	%ymm9,224(%rsp)
+++	addl	-56(%r13),%ecx
+++	leal	(%rcx,%rsi,1),%ecx
+++	rorxl	$27,%edx,%r12d
+++	rorxl	$2,%edx,%esi
+++	xorl	%eax,%edx
+++	addl	%r12d,%ecx
+++	xorl	%ebp,%edx
+++	addl	-52(%r13),%ebx
+++	leal	(%rbx,%rdx,1),%ebx
+++	rorxl	$27,%ecx,%r12d
+++	rorxl	$2,%ecx,%edx
+++	xorl	%esi,%ecx
+++	addl	%r12d,%ebx
+++	xorl	%eax,%ecx
+++	addl	-32(%r13),%ebp
+++	leal	(%rcx,%rbp,1),%ebp
+++	rorxl	$27,%ebx,%r12d
+++	rorxl	$2,%ebx,%ecx
+++	xorl	%edx,%ebx
+++	addl	%r12d,%ebp
+++	xorl	%esi,%ebx
+++	addl	-28(%r13),%eax
+++	leal	(%rax,%rbx,1),%eax
+++	rorxl	$27,%ebp,%r12d
+++	rorxl	$2,%ebp,%ebx
+++	xorl	%ecx,%ebp
+++	addl	%r12d,%eax
+++	xorl	%edx,%ebp
+++	addl	-24(%r13),%esi
+++	leal	(%rsi,%rbp,1),%esi
+++	rorxl	$27,%eax,%r12d
+++	rorxl	$2,%eax,%ebp
+++	xorl	%ebx,%eax
+++	addl	%r12d,%esi
+++	xorl	%ecx,%eax
+++	addl	-20(%r13),%edx
+++	leal	(%rdx,%rax,1),%edx
+++	rorxl	$27,%esi,%r12d
+++	addl	%r12d,%edx
+++	leaq	128(%rsp),%r13
+++
+++
+++	addl	0(%r8),%edx
+++	addl	4(%r8),%esi
+++	addl	8(%r8),%ebp
+++	movl	%edx,0(%r8)
+++	addl	12(%r8),%ebx
+++	movl	%esi,4(%r8)
+++	movl	%edx,%eax
+++	addl	16(%r8),%ecx
+++	movl	%ebp,%r12d
+++	movl	%ebp,8(%r8)
+++	movl	%ebx,%edx
+++
+++	movl	%ebx,12(%r8)
+++	movl	%esi,%ebp
+++	movl	%ecx,16(%r8)
+++
+++	movl	%ecx,%esi
+++	movl	%r12d,%ecx
+++
+++
+++	cmpq	%r10,%r9
+++	jbe	.Loop_avx2
+++
+++.Ldone_avx2:
+++	vzeroupper
+++	movq	-40(%r11),%r14
+++.cfi_restore	%r14
+++	movq	-32(%r11),%r13
+++.cfi_restore	%r13
+++	movq	-24(%r11),%r12
+++.cfi_restore	%r12
+++	movq	-16(%r11),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%r11),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%r11),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lepilogue_avx2:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	sha1_block_data_order_avx2,.-sha1_block_data_order_avx2
+++.align	64
+++K_XX_XX:
+++.long	0x5a827999,0x5a827999,0x5a827999,0x5a827999
+++.long	0x5a827999,0x5a827999,0x5a827999,0x5a827999
+++.long	0x6ed9eba1,0x6ed9eba1,0x6ed9eba1,0x6ed9eba1
+++.long	0x6ed9eba1,0x6ed9eba1,0x6ed9eba1,0x6ed9eba1
+++.long	0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc
+++.long	0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc
+++.long	0xca62c1d6,0xca62c1d6,0xca62c1d6,0xca62c1d6
+++.long	0xca62c1d6,0xca62c1d6,0xca62c1d6,0xca62c1d6
+++.long	0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f
+++.long	0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f
+++.byte	0xf,0xe,0xd,0xc,0xb,0xa,0x9,0x8,0x7,0x6,0x5,0x4,0x3,0x2,0x1,0x0
+++.byte	83,72,65,49,32,98,108,111,99,107,32,116,114,97,110,115,102,111,114,109,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
+++.align	64
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/sha256-x86_64.S b/linux-x86_64/ypto/fipsmodule/sha256-x86_64.S
++new file mode 100644
++index 000000000..0bacd6a4a
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/sha256-x86_64.S
++@@ -0,0 +1,3973 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++
+++.extern	OPENSSL_ia32cap_P
+++.hidden OPENSSL_ia32cap_P
+++.globl	sha256_block_data_order
+++.hidden sha256_block_data_order
+++.type	sha256_block_data_order,@function
+++.align	16
+++sha256_block_data_order:
+++.cfi_startproc	
+++	leaq	OPENSSL_ia32cap_P(%rip),%r11
+++	movl	0(%r11),%r9d
+++	movl	4(%r11),%r10d
+++	movl	8(%r11),%r11d
+++	andl	$1073741824,%r9d
+++	andl	$268435968,%r10d
+++	orl	%r9d,%r10d
+++	cmpl	$1342177792,%r10d
+++	je	.Lavx_shortcut
+++	testl	$512,%r10d
+++	jnz	.Lssse3_shortcut
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++	shlq	$4,%rdx
+++	subq	$64+32,%rsp
+++	leaq	(%rsi,%rdx,4),%rdx
+++	andq	$-64,%rsp
+++	movq	%rdi,64+0(%rsp)
+++	movq	%rsi,64+8(%rsp)
+++	movq	%rdx,64+16(%rsp)
+++	movq	%rax,88(%rsp)
+++.cfi_escape	0x0f,0x06,0x77,0xd8,0x00,0x06,0x23,0x08
+++.Lprologue:
+++
+++	movl	0(%rdi),%eax
+++	movl	4(%rdi),%ebx
+++	movl	8(%rdi),%ecx
+++	movl	12(%rdi),%edx
+++	movl	16(%rdi),%r8d
+++	movl	20(%rdi),%r9d
+++	movl	24(%rdi),%r10d
+++	movl	28(%rdi),%r11d
+++	jmp	.Lloop
+++
+++.align	16
+++.Lloop:
+++	movl	%ebx,%edi
+++	leaq	K256(%rip),%rbp
+++	xorl	%ecx,%edi
+++	movl	0(%rsi),%r12d
+++	movl	%r8d,%r13d
+++	movl	%eax,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%r9d,%r15d
+++
+++	xorl	%r8d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r10d,%r15d
+++
+++	movl	%r12d,0(%rsp)
+++	xorl	%eax,%r14d
+++	andl	%r8d,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%r11d,%r12d
+++	xorl	%r10d,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%r8d,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%eax,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%eax,%r14d
+++
+++	xorl	%ebx,%r15d
+++	rorl	$6,%r13d
+++	movl	%ebx,%r11d
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%r11d
+++	addl	%r12d,%edx
+++	addl	%r12d,%r11d
+++
+++	leaq	4(%rbp),%rbp
+++	addl	%r14d,%r11d
+++	movl	4(%rsi),%r12d
+++	movl	%edx,%r13d
+++	movl	%r11d,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%r8d,%edi
+++
+++	xorl	%edx,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r9d,%edi
+++
+++	movl	%r12d,4(%rsp)
+++	xorl	%r11d,%r14d
+++	andl	%edx,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%r10d,%r12d
+++	xorl	%r9d,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%edx,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%r11d,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%r11d,%r14d
+++
+++	xorl	%eax,%edi
+++	rorl	$6,%r13d
+++	movl	%eax,%r10d
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%r10d
+++	addl	%r12d,%ecx
+++	addl	%r12d,%r10d
+++
+++	leaq	4(%rbp),%rbp
+++	addl	%r14d,%r10d
+++	movl	8(%rsi),%r12d
+++	movl	%ecx,%r13d
+++	movl	%r10d,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%edx,%r15d
+++
+++	xorl	%ecx,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r8d,%r15d
+++
+++	movl	%r12d,8(%rsp)
+++	xorl	%r10d,%r14d
+++	andl	%ecx,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%r9d,%r12d
+++	xorl	%r8d,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%ecx,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%r10d,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%r10d,%r14d
+++
+++	xorl	%r11d,%r15d
+++	rorl	$6,%r13d
+++	movl	%r11d,%r9d
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%r9d
+++	addl	%r12d,%ebx
+++	addl	%r12d,%r9d
+++
+++	leaq	4(%rbp),%rbp
+++	addl	%r14d,%r9d
+++	movl	12(%rsi),%r12d
+++	movl	%ebx,%r13d
+++	movl	%r9d,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%ecx,%edi
+++
+++	xorl	%ebx,%r13d
+++	rorl	$9,%r14d
+++	xorl	%edx,%edi
+++
+++	movl	%r12d,12(%rsp)
+++	xorl	%r9d,%r14d
+++	andl	%ebx,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%r8d,%r12d
+++	xorl	%edx,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%ebx,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%r9d,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%r9d,%r14d
+++
+++	xorl	%r10d,%edi
+++	rorl	$6,%r13d
+++	movl	%r10d,%r8d
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%r8d
+++	addl	%r12d,%eax
+++	addl	%r12d,%r8d
+++
+++	leaq	20(%rbp),%rbp
+++	addl	%r14d,%r8d
+++	movl	16(%rsi),%r12d
+++	movl	%eax,%r13d
+++	movl	%r8d,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%ebx,%r15d
+++
+++	xorl	%eax,%r13d
+++	rorl	$9,%r14d
+++	xorl	%ecx,%r15d
+++
+++	movl	%r12d,16(%rsp)
+++	xorl	%r8d,%r14d
+++	andl	%eax,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%edx,%r12d
+++	xorl	%ecx,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%eax,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%r8d,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%r8d,%r14d
+++
+++	xorl	%r9d,%r15d
+++	rorl	$6,%r13d
+++	movl	%r9d,%edx
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%edx
+++	addl	%r12d,%r11d
+++	addl	%r12d,%edx
+++
+++	leaq	4(%rbp),%rbp
+++	addl	%r14d,%edx
+++	movl	20(%rsi),%r12d
+++	movl	%r11d,%r13d
+++	movl	%edx,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%eax,%edi
+++
+++	xorl	%r11d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%ebx,%edi
+++
+++	movl	%r12d,20(%rsp)
+++	xorl	%edx,%r14d
+++	andl	%r11d,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%ecx,%r12d
+++	xorl	%ebx,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%r11d,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%edx,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%edx,%r14d
+++
+++	xorl	%r8d,%edi
+++	rorl	$6,%r13d
+++	movl	%r8d,%ecx
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%ecx
+++	addl	%r12d,%r10d
+++	addl	%r12d,%ecx
+++
+++	leaq	4(%rbp),%rbp
+++	addl	%r14d,%ecx
+++	movl	24(%rsi),%r12d
+++	movl	%r10d,%r13d
+++	movl	%ecx,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%r11d,%r15d
+++
+++	xorl	%r10d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%eax,%r15d
+++
+++	movl	%r12d,24(%rsp)
+++	xorl	%ecx,%r14d
+++	andl	%r10d,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%ebx,%r12d
+++	xorl	%eax,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%r10d,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%ecx,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%ecx,%r14d
+++
+++	xorl	%edx,%r15d
+++	rorl	$6,%r13d
+++	movl	%edx,%ebx
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%ebx
+++	addl	%r12d,%r9d
+++	addl	%r12d,%ebx
+++
+++	leaq	4(%rbp),%rbp
+++	addl	%r14d,%ebx
+++	movl	28(%rsi),%r12d
+++	movl	%r9d,%r13d
+++	movl	%ebx,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%r10d,%edi
+++
+++	xorl	%r9d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r11d,%edi
+++
+++	movl	%r12d,28(%rsp)
+++	xorl	%ebx,%r14d
+++	andl	%r9d,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%eax,%r12d
+++	xorl	%r11d,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%r9d,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%ebx,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%ebx,%r14d
+++
+++	xorl	%ecx,%edi
+++	rorl	$6,%r13d
+++	movl	%ecx,%eax
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%eax
+++	addl	%r12d,%r8d
+++	addl	%r12d,%eax
+++
+++	leaq	20(%rbp),%rbp
+++	addl	%r14d,%eax
+++	movl	32(%rsi),%r12d
+++	movl	%r8d,%r13d
+++	movl	%eax,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%r9d,%r15d
+++
+++	xorl	%r8d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r10d,%r15d
+++
+++	movl	%r12d,32(%rsp)
+++	xorl	%eax,%r14d
+++	andl	%r8d,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%r11d,%r12d
+++	xorl	%r10d,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%r8d,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%eax,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%eax,%r14d
+++
+++	xorl	%ebx,%r15d
+++	rorl	$6,%r13d
+++	movl	%ebx,%r11d
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%r11d
+++	addl	%r12d,%edx
+++	addl	%r12d,%r11d
+++
+++	leaq	4(%rbp),%rbp
+++	addl	%r14d,%r11d
+++	movl	36(%rsi),%r12d
+++	movl	%edx,%r13d
+++	movl	%r11d,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%r8d,%edi
+++
+++	xorl	%edx,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r9d,%edi
+++
+++	movl	%r12d,36(%rsp)
+++	xorl	%r11d,%r14d
+++	andl	%edx,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%r10d,%r12d
+++	xorl	%r9d,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%edx,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%r11d,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%r11d,%r14d
+++
+++	xorl	%eax,%edi
+++	rorl	$6,%r13d
+++	movl	%eax,%r10d
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%r10d
+++	addl	%r12d,%ecx
+++	addl	%r12d,%r10d
+++
+++	leaq	4(%rbp),%rbp
+++	addl	%r14d,%r10d
+++	movl	40(%rsi),%r12d
+++	movl	%ecx,%r13d
+++	movl	%r10d,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%edx,%r15d
+++
+++	xorl	%ecx,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r8d,%r15d
+++
+++	movl	%r12d,40(%rsp)
+++	xorl	%r10d,%r14d
+++	andl	%ecx,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%r9d,%r12d
+++	xorl	%r8d,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%ecx,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%r10d,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%r10d,%r14d
+++
+++	xorl	%r11d,%r15d
+++	rorl	$6,%r13d
+++	movl	%r11d,%r9d
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%r9d
+++	addl	%r12d,%ebx
+++	addl	%r12d,%r9d
+++
+++	leaq	4(%rbp),%rbp
+++	addl	%r14d,%r9d
+++	movl	44(%rsi),%r12d
+++	movl	%ebx,%r13d
+++	movl	%r9d,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%ecx,%edi
+++
+++	xorl	%ebx,%r13d
+++	rorl	$9,%r14d
+++	xorl	%edx,%edi
+++
+++	movl	%r12d,44(%rsp)
+++	xorl	%r9d,%r14d
+++	andl	%ebx,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%r8d,%r12d
+++	xorl	%edx,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%ebx,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%r9d,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%r9d,%r14d
+++
+++	xorl	%r10d,%edi
+++	rorl	$6,%r13d
+++	movl	%r10d,%r8d
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%r8d
+++	addl	%r12d,%eax
+++	addl	%r12d,%r8d
+++
+++	leaq	20(%rbp),%rbp
+++	addl	%r14d,%r8d
+++	movl	48(%rsi),%r12d
+++	movl	%eax,%r13d
+++	movl	%r8d,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%ebx,%r15d
+++
+++	xorl	%eax,%r13d
+++	rorl	$9,%r14d
+++	xorl	%ecx,%r15d
+++
+++	movl	%r12d,48(%rsp)
+++	xorl	%r8d,%r14d
+++	andl	%eax,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%edx,%r12d
+++	xorl	%ecx,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%eax,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%r8d,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%r8d,%r14d
+++
+++	xorl	%r9d,%r15d
+++	rorl	$6,%r13d
+++	movl	%r9d,%edx
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%edx
+++	addl	%r12d,%r11d
+++	addl	%r12d,%edx
+++
+++	leaq	4(%rbp),%rbp
+++	addl	%r14d,%edx
+++	movl	52(%rsi),%r12d
+++	movl	%r11d,%r13d
+++	movl	%edx,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%eax,%edi
+++
+++	xorl	%r11d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%ebx,%edi
+++
+++	movl	%r12d,52(%rsp)
+++	xorl	%edx,%r14d
+++	andl	%r11d,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%ecx,%r12d
+++	xorl	%ebx,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%r11d,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%edx,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%edx,%r14d
+++
+++	xorl	%r8d,%edi
+++	rorl	$6,%r13d
+++	movl	%r8d,%ecx
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%ecx
+++	addl	%r12d,%r10d
+++	addl	%r12d,%ecx
+++
+++	leaq	4(%rbp),%rbp
+++	addl	%r14d,%ecx
+++	movl	56(%rsi),%r12d
+++	movl	%r10d,%r13d
+++	movl	%ecx,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%r11d,%r15d
+++
+++	xorl	%r10d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%eax,%r15d
+++
+++	movl	%r12d,56(%rsp)
+++	xorl	%ecx,%r14d
+++	andl	%r10d,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%ebx,%r12d
+++	xorl	%eax,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%r10d,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%ecx,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%ecx,%r14d
+++
+++	xorl	%edx,%r15d
+++	rorl	$6,%r13d
+++	movl	%edx,%ebx
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%ebx
+++	addl	%r12d,%r9d
+++	addl	%r12d,%ebx
+++
+++	leaq	4(%rbp),%rbp
+++	addl	%r14d,%ebx
+++	movl	60(%rsi),%r12d
+++	movl	%r9d,%r13d
+++	movl	%ebx,%r14d
+++	bswapl	%r12d
+++	rorl	$14,%r13d
+++	movl	%r10d,%edi
+++
+++	xorl	%r9d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r11d,%edi
+++
+++	movl	%r12d,60(%rsp)
+++	xorl	%ebx,%r14d
+++	andl	%r9d,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%eax,%r12d
+++	xorl	%r11d,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%r9d,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%ebx,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%ebx,%r14d
+++
+++	xorl	%ecx,%edi
+++	rorl	$6,%r13d
+++	movl	%ecx,%eax
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%eax
+++	addl	%r12d,%r8d
+++	addl	%r12d,%eax
+++
+++	leaq	20(%rbp),%rbp
+++	jmp	.Lrounds_16_xx
+++.align	16
+++.Lrounds_16_xx:
+++	movl	4(%rsp),%r13d
+++	movl	56(%rsp),%r15d
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%eax
+++	movl	%r15d,%r14d
+++	rorl	$2,%r15d
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%r15d
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%r15d
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%r15d
+++	addl	36(%rsp),%r12d
+++
+++	addl	0(%rsp),%r12d
+++	movl	%r8d,%r13d
+++	addl	%r15d,%r12d
+++	movl	%eax,%r14d
+++	rorl	$14,%r13d
+++	movl	%r9d,%r15d
+++
+++	xorl	%r8d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r10d,%r15d
+++
+++	movl	%r12d,0(%rsp)
+++	xorl	%eax,%r14d
+++	andl	%r8d,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%r11d,%r12d
+++	xorl	%r10d,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%r8d,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%eax,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%eax,%r14d
+++
+++	xorl	%ebx,%r15d
+++	rorl	$6,%r13d
+++	movl	%ebx,%r11d
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%r11d
+++	addl	%r12d,%edx
+++	addl	%r12d,%r11d
+++
+++	leaq	4(%rbp),%rbp
+++	movl	8(%rsp),%r13d
+++	movl	60(%rsp),%edi
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%r11d
+++	movl	%edi,%r14d
+++	rorl	$2,%edi
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%edi
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%edi
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%edi
+++	addl	40(%rsp),%r12d
+++
+++	addl	4(%rsp),%r12d
+++	movl	%edx,%r13d
+++	addl	%edi,%r12d
+++	movl	%r11d,%r14d
+++	rorl	$14,%r13d
+++	movl	%r8d,%edi
+++
+++	xorl	%edx,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r9d,%edi
+++
+++	movl	%r12d,4(%rsp)
+++	xorl	%r11d,%r14d
+++	andl	%edx,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%r10d,%r12d
+++	xorl	%r9d,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%edx,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%r11d,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%r11d,%r14d
+++
+++	xorl	%eax,%edi
+++	rorl	$6,%r13d
+++	movl	%eax,%r10d
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%r10d
+++	addl	%r12d,%ecx
+++	addl	%r12d,%r10d
+++
+++	leaq	4(%rbp),%rbp
+++	movl	12(%rsp),%r13d
+++	movl	0(%rsp),%r15d
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%r10d
+++	movl	%r15d,%r14d
+++	rorl	$2,%r15d
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%r15d
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%r15d
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%r15d
+++	addl	44(%rsp),%r12d
+++
+++	addl	8(%rsp),%r12d
+++	movl	%ecx,%r13d
+++	addl	%r15d,%r12d
+++	movl	%r10d,%r14d
+++	rorl	$14,%r13d
+++	movl	%edx,%r15d
+++
+++	xorl	%ecx,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r8d,%r15d
+++
+++	movl	%r12d,8(%rsp)
+++	xorl	%r10d,%r14d
+++	andl	%ecx,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%r9d,%r12d
+++	xorl	%r8d,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%ecx,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%r10d,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%r10d,%r14d
+++
+++	xorl	%r11d,%r15d
+++	rorl	$6,%r13d
+++	movl	%r11d,%r9d
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%r9d
+++	addl	%r12d,%ebx
+++	addl	%r12d,%r9d
+++
+++	leaq	4(%rbp),%rbp
+++	movl	16(%rsp),%r13d
+++	movl	4(%rsp),%edi
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%r9d
+++	movl	%edi,%r14d
+++	rorl	$2,%edi
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%edi
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%edi
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%edi
+++	addl	48(%rsp),%r12d
+++
+++	addl	12(%rsp),%r12d
+++	movl	%ebx,%r13d
+++	addl	%edi,%r12d
+++	movl	%r9d,%r14d
+++	rorl	$14,%r13d
+++	movl	%ecx,%edi
+++
+++	xorl	%ebx,%r13d
+++	rorl	$9,%r14d
+++	xorl	%edx,%edi
+++
+++	movl	%r12d,12(%rsp)
+++	xorl	%r9d,%r14d
+++	andl	%ebx,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%r8d,%r12d
+++	xorl	%edx,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%ebx,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%r9d,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%r9d,%r14d
+++
+++	xorl	%r10d,%edi
+++	rorl	$6,%r13d
+++	movl	%r10d,%r8d
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%r8d
+++	addl	%r12d,%eax
+++	addl	%r12d,%r8d
+++
+++	leaq	20(%rbp),%rbp
+++	movl	20(%rsp),%r13d
+++	movl	8(%rsp),%r15d
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%r8d
+++	movl	%r15d,%r14d
+++	rorl	$2,%r15d
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%r15d
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%r15d
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%r15d
+++	addl	52(%rsp),%r12d
+++
+++	addl	16(%rsp),%r12d
+++	movl	%eax,%r13d
+++	addl	%r15d,%r12d
+++	movl	%r8d,%r14d
+++	rorl	$14,%r13d
+++	movl	%ebx,%r15d
+++
+++	xorl	%eax,%r13d
+++	rorl	$9,%r14d
+++	xorl	%ecx,%r15d
+++
+++	movl	%r12d,16(%rsp)
+++	xorl	%r8d,%r14d
+++	andl	%eax,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%edx,%r12d
+++	xorl	%ecx,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%eax,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%r8d,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%r8d,%r14d
+++
+++	xorl	%r9d,%r15d
+++	rorl	$6,%r13d
+++	movl	%r9d,%edx
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%edx
+++	addl	%r12d,%r11d
+++	addl	%r12d,%edx
+++
+++	leaq	4(%rbp),%rbp
+++	movl	24(%rsp),%r13d
+++	movl	12(%rsp),%edi
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%edx
+++	movl	%edi,%r14d
+++	rorl	$2,%edi
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%edi
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%edi
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%edi
+++	addl	56(%rsp),%r12d
+++
+++	addl	20(%rsp),%r12d
+++	movl	%r11d,%r13d
+++	addl	%edi,%r12d
+++	movl	%edx,%r14d
+++	rorl	$14,%r13d
+++	movl	%eax,%edi
+++
+++	xorl	%r11d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%ebx,%edi
+++
+++	movl	%r12d,20(%rsp)
+++	xorl	%edx,%r14d
+++	andl	%r11d,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%ecx,%r12d
+++	xorl	%ebx,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%r11d,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%edx,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%edx,%r14d
+++
+++	xorl	%r8d,%edi
+++	rorl	$6,%r13d
+++	movl	%r8d,%ecx
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%ecx
+++	addl	%r12d,%r10d
+++	addl	%r12d,%ecx
+++
+++	leaq	4(%rbp),%rbp
+++	movl	28(%rsp),%r13d
+++	movl	16(%rsp),%r15d
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%ecx
+++	movl	%r15d,%r14d
+++	rorl	$2,%r15d
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%r15d
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%r15d
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%r15d
+++	addl	60(%rsp),%r12d
+++
+++	addl	24(%rsp),%r12d
+++	movl	%r10d,%r13d
+++	addl	%r15d,%r12d
+++	movl	%ecx,%r14d
+++	rorl	$14,%r13d
+++	movl	%r11d,%r15d
+++
+++	xorl	%r10d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%eax,%r15d
+++
+++	movl	%r12d,24(%rsp)
+++	xorl	%ecx,%r14d
+++	andl	%r10d,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%ebx,%r12d
+++	xorl	%eax,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%r10d,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%ecx,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%ecx,%r14d
+++
+++	xorl	%edx,%r15d
+++	rorl	$6,%r13d
+++	movl	%edx,%ebx
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%ebx
+++	addl	%r12d,%r9d
+++	addl	%r12d,%ebx
+++
+++	leaq	4(%rbp),%rbp
+++	movl	32(%rsp),%r13d
+++	movl	20(%rsp),%edi
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%ebx
+++	movl	%edi,%r14d
+++	rorl	$2,%edi
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%edi
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%edi
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%edi
+++	addl	0(%rsp),%r12d
+++
+++	addl	28(%rsp),%r12d
+++	movl	%r9d,%r13d
+++	addl	%edi,%r12d
+++	movl	%ebx,%r14d
+++	rorl	$14,%r13d
+++	movl	%r10d,%edi
+++
+++	xorl	%r9d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r11d,%edi
+++
+++	movl	%r12d,28(%rsp)
+++	xorl	%ebx,%r14d
+++	andl	%r9d,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%eax,%r12d
+++	xorl	%r11d,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%r9d,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%ebx,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%ebx,%r14d
+++
+++	xorl	%ecx,%edi
+++	rorl	$6,%r13d
+++	movl	%ecx,%eax
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%eax
+++	addl	%r12d,%r8d
+++	addl	%r12d,%eax
+++
+++	leaq	20(%rbp),%rbp
+++	movl	36(%rsp),%r13d
+++	movl	24(%rsp),%r15d
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%eax
+++	movl	%r15d,%r14d
+++	rorl	$2,%r15d
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%r15d
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%r15d
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%r15d
+++	addl	4(%rsp),%r12d
+++
+++	addl	32(%rsp),%r12d
+++	movl	%r8d,%r13d
+++	addl	%r15d,%r12d
+++	movl	%eax,%r14d
+++	rorl	$14,%r13d
+++	movl	%r9d,%r15d
+++
+++	xorl	%r8d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r10d,%r15d
+++
+++	movl	%r12d,32(%rsp)
+++	xorl	%eax,%r14d
+++	andl	%r8d,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%r11d,%r12d
+++	xorl	%r10d,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%r8d,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%eax,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%eax,%r14d
+++
+++	xorl	%ebx,%r15d
+++	rorl	$6,%r13d
+++	movl	%ebx,%r11d
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%r11d
+++	addl	%r12d,%edx
+++	addl	%r12d,%r11d
+++
+++	leaq	4(%rbp),%rbp
+++	movl	40(%rsp),%r13d
+++	movl	28(%rsp),%edi
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%r11d
+++	movl	%edi,%r14d
+++	rorl	$2,%edi
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%edi
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%edi
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%edi
+++	addl	8(%rsp),%r12d
+++
+++	addl	36(%rsp),%r12d
+++	movl	%edx,%r13d
+++	addl	%edi,%r12d
+++	movl	%r11d,%r14d
+++	rorl	$14,%r13d
+++	movl	%r8d,%edi
+++
+++	xorl	%edx,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r9d,%edi
+++
+++	movl	%r12d,36(%rsp)
+++	xorl	%r11d,%r14d
+++	andl	%edx,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%r10d,%r12d
+++	xorl	%r9d,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%edx,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%r11d,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%r11d,%r14d
+++
+++	xorl	%eax,%edi
+++	rorl	$6,%r13d
+++	movl	%eax,%r10d
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%r10d
+++	addl	%r12d,%ecx
+++	addl	%r12d,%r10d
+++
+++	leaq	4(%rbp),%rbp
+++	movl	44(%rsp),%r13d
+++	movl	32(%rsp),%r15d
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%r10d
+++	movl	%r15d,%r14d
+++	rorl	$2,%r15d
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%r15d
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%r15d
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%r15d
+++	addl	12(%rsp),%r12d
+++
+++	addl	40(%rsp),%r12d
+++	movl	%ecx,%r13d
+++	addl	%r15d,%r12d
+++	movl	%r10d,%r14d
+++	rorl	$14,%r13d
+++	movl	%edx,%r15d
+++
+++	xorl	%ecx,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r8d,%r15d
+++
+++	movl	%r12d,40(%rsp)
+++	xorl	%r10d,%r14d
+++	andl	%ecx,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%r9d,%r12d
+++	xorl	%r8d,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%ecx,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%r10d,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%r10d,%r14d
+++
+++	xorl	%r11d,%r15d
+++	rorl	$6,%r13d
+++	movl	%r11d,%r9d
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%r9d
+++	addl	%r12d,%ebx
+++	addl	%r12d,%r9d
+++
+++	leaq	4(%rbp),%rbp
+++	movl	48(%rsp),%r13d
+++	movl	36(%rsp),%edi
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%r9d
+++	movl	%edi,%r14d
+++	rorl	$2,%edi
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%edi
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%edi
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%edi
+++	addl	16(%rsp),%r12d
+++
+++	addl	44(%rsp),%r12d
+++	movl	%ebx,%r13d
+++	addl	%edi,%r12d
+++	movl	%r9d,%r14d
+++	rorl	$14,%r13d
+++	movl	%ecx,%edi
+++
+++	xorl	%ebx,%r13d
+++	rorl	$9,%r14d
+++	xorl	%edx,%edi
+++
+++	movl	%r12d,44(%rsp)
+++	xorl	%r9d,%r14d
+++	andl	%ebx,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%r8d,%r12d
+++	xorl	%edx,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%ebx,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%r9d,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%r9d,%r14d
+++
+++	xorl	%r10d,%edi
+++	rorl	$6,%r13d
+++	movl	%r10d,%r8d
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%r8d
+++	addl	%r12d,%eax
+++	addl	%r12d,%r8d
+++
+++	leaq	20(%rbp),%rbp
+++	movl	52(%rsp),%r13d
+++	movl	40(%rsp),%r15d
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%r8d
+++	movl	%r15d,%r14d
+++	rorl	$2,%r15d
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%r15d
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%r15d
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%r15d
+++	addl	20(%rsp),%r12d
+++
+++	addl	48(%rsp),%r12d
+++	movl	%eax,%r13d
+++	addl	%r15d,%r12d
+++	movl	%r8d,%r14d
+++	rorl	$14,%r13d
+++	movl	%ebx,%r15d
+++
+++	xorl	%eax,%r13d
+++	rorl	$9,%r14d
+++	xorl	%ecx,%r15d
+++
+++	movl	%r12d,48(%rsp)
+++	xorl	%r8d,%r14d
+++	andl	%eax,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%edx,%r12d
+++	xorl	%ecx,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%eax,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%r8d,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%r8d,%r14d
+++
+++	xorl	%r9d,%r15d
+++	rorl	$6,%r13d
+++	movl	%r9d,%edx
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%edx
+++	addl	%r12d,%r11d
+++	addl	%r12d,%edx
+++
+++	leaq	4(%rbp),%rbp
+++	movl	56(%rsp),%r13d
+++	movl	44(%rsp),%edi
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%edx
+++	movl	%edi,%r14d
+++	rorl	$2,%edi
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%edi
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%edi
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%edi
+++	addl	24(%rsp),%r12d
+++
+++	addl	52(%rsp),%r12d
+++	movl	%r11d,%r13d
+++	addl	%edi,%r12d
+++	movl	%edx,%r14d
+++	rorl	$14,%r13d
+++	movl	%eax,%edi
+++
+++	xorl	%r11d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%ebx,%edi
+++
+++	movl	%r12d,52(%rsp)
+++	xorl	%edx,%r14d
+++	andl	%r11d,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%ecx,%r12d
+++	xorl	%ebx,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%r11d,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%edx,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%edx,%r14d
+++
+++	xorl	%r8d,%edi
+++	rorl	$6,%r13d
+++	movl	%r8d,%ecx
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%ecx
+++	addl	%r12d,%r10d
+++	addl	%r12d,%ecx
+++
+++	leaq	4(%rbp),%rbp
+++	movl	60(%rsp),%r13d
+++	movl	48(%rsp),%r15d
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%ecx
+++	movl	%r15d,%r14d
+++	rorl	$2,%r15d
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%r15d
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%r15d
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%r15d
+++	addl	28(%rsp),%r12d
+++
+++	addl	56(%rsp),%r12d
+++	movl	%r10d,%r13d
+++	addl	%r15d,%r12d
+++	movl	%ecx,%r14d
+++	rorl	$14,%r13d
+++	movl	%r11d,%r15d
+++
+++	xorl	%r10d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%eax,%r15d
+++
+++	movl	%r12d,56(%rsp)
+++	xorl	%ecx,%r14d
+++	andl	%r10d,%r15d
+++
+++	rorl	$5,%r13d
+++	addl	%ebx,%r12d
+++	xorl	%eax,%r15d
+++
+++	rorl	$11,%r14d
+++	xorl	%r10d,%r13d
+++	addl	%r15d,%r12d
+++
+++	movl	%ecx,%r15d
+++	addl	(%rbp),%r12d
+++	xorl	%ecx,%r14d
+++
+++	xorl	%edx,%r15d
+++	rorl	$6,%r13d
+++	movl	%edx,%ebx
+++
+++	andl	%r15d,%edi
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%edi,%ebx
+++	addl	%r12d,%r9d
+++	addl	%r12d,%ebx
+++
+++	leaq	4(%rbp),%rbp
+++	movl	0(%rsp),%r13d
+++	movl	52(%rsp),%edi
+++
+++	movl	%r13d,%r12d
+++	rorl	$11,%r13d
+++	addl	%r14d,%ebx
+++	movl	%edi,%r14d
+++	rorl	$2,%edi
+++
+++	xorl	%r12d,%r13d
+++	shrl	$3,%r12d
+++	rorl	$7,%r13d
+++	xorl	%r14d,%edi
+++	shrl	$10,%r14d
+++
+++	rorl	$17,%edi
+++	xorl	%r13d,%r12d
+++	xorl	%r14d,%edi
+++	addl	32(%rsp),%r12d
+++
+++	addl	60(%rsp),%r12d
+++	movl	%r9d,%r13d
+++	addl	%edi,%r12d
+++	movl	%ebx,%r14d
+++	rorl	$14,%r13d
+++	movl	%r10d,%edi
+++
+++	xorl	%r9d,%r13d
+++	rorl	$9,%r14d
+++	xorl	%r11d,%edi
+++
+++	movl	%r12d,60(%rsp)
+++	xorl	%ebx,%r14d
+++	andl	%r9d,%edi
+++
+++	rorl	$5,%r13d
+++	addl	%eax,%r12d
+++	xorl	%r11d,%edi
+++
+++	rorl	$11,%r14d
+++	xorl	%r9d,%r13d
+++	addl	%edi,%r12d
+++
+++	movl	%ebx,%edi
+++	addl	(%rbp),%r12d
+++	xorl	%ebx,%r14d
+++
+++	xorl	%ecx,%edi
+++	rorl	$6,%r13d
+++	movl	%ecx,%eax
+++
+++	andl	%edi,%r15d
+++	rorl	$2,%r14d
+++	addl	%r13d,%r12d
+++
+++	xorl	%r15d,%eax
+++	addl	%r12d,%r8d
+++	addl	%r12d,%eax
+++
+++	leaq	20(%rbp),%rbp
+++	cmpb	$0,3(%rbp)
+++	jnz	.Lrounds_16_xx
+++
+++	movq	64+0(%rsp),%rdi
+++	addl	%r14d,%eax
+++	leaq	64(%rsi),%rsi
+++
+++	addl	0(%rdi),%eax
+++	addl	4(%rdi),%ebx
+++	addl	8(%rdi),%ecx
+++	addl	12(%rdi),%edx
+++	addl	16(%rdi),%r8d
+++	addl	20(%rdi),%r9d
+++	addl	24(%rdi),%r10d
+++	addl	28(%rdi),%r11d
+++
+++	cmpq	64+16(%rsp),%rsi
+++
+++	movl	%eax,0(%rdi)
+++	movl	%ebx,4(%rdi)
+++	movl	%ecx,8(%rdi)
+++	movl	%edx,12(%rdi)
+++	movl	%r8d,16(%rdi)
+++	movl	%r9d,20(%rdi)
+++	movl	%r10d,24(%rdi)
+++	movl	%r11d,28(%rdi)
+++	jb	.Lloop
+++
+++	movq	88(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lepilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	sha256_block_data_order,.-sha256_block_data_order
+++.align	64
+++.type	K256,@object
+++K256:
+++.long	0x428a2f98,0x71374491,0xb5c0fbcf,0xe9b5dba5
+++.long	0x428a2f98,0x71374491,0xb5c0fbcf,0xe9b5dba5
+++.long	0x3956c25b,0x59f111f1,0x923f82a4,0xab1c5ed5
+++.long	0x3956c25b,0x59f111f1,0x923f82a4,0xab1c5ed5
+++.long	0xd807aa98,0x12835b01,0x243185be,0x550c7dc3
+++.long	0xd807aa98,0x12835b01,0x243185be,0x550c7dc3
+++.long	0x72be5d74,0x80deb1fe,0x9bdc06a7,0xc19bf174
+++.long	0x72be5d74,0x80deb1fe,0x9bdc06a7,0xc19bf174
+++.long	0xe49b69c1,0xefbe4786,0x0fc19dc6,0x240ca1cc
+++.long	0xe49b69c1,0xefbe4786,0x0fc19dc6,0x240ca1cc
+++.long	0x2de92c6f,0x4a7484aa,0x5cb0a9dc,0x76f988da
+++.long	0x2de92c6f,0x4a7484aa,0x5cb0a9dc,0x76f988da
+++.long	0x983e5152,0xa831c66d,0xb00327c8,0xbf597fc7
+++.long	0x983e5152,0xa831c66d,0xb00327c8,0xbf597fc7
+++.long	0xc6e00bf3,0xd5a79147,0x06ca6351,0x14292967
+++.long	0xc6e00bf3,0xd5a79147,0x06ca6351,0x14292967
+++.long	0x27b70a85,0x2e1b2138,0x4d2c6dfc,0x53380d13
+++.long	0x27b70a85,0x2e1b2138,0x4d2c6dfc,0x53380d13
+++.long	0x650a7354,0x766a0abb,0x81c2c92e,0x92722c85
+++.long	0x650a7354,0x766a0abb,0x81c2c92e,0x92722c85
+++.long	0xa2bfe8a1,0xa81a664b,0xc24b8b70,0xc76c51a3
+++.long	0xa2bfe8a1,0xa81a664b,0xc24b8b70,0xc76c51a3
+++.long	0xd192e819,0xd6990624,0xf40e3585,0x106aa070
+++.long	0xd192e819,0xd6990624,0xf40e3585,0x106aa070
+++.long	0x19a4c116,0x1e376c08,0x2748774c,0x34b0bcb5
+++.long	0x19a4c116,0x1e376c08,0x2748774c,0x34b0bcb5
+++.long	0x391c0cb3,0x4ed8aa4a,0x5b9cca4f,0x682e6ff3
+++.long	0x391c0cb3,0x4ed8aa4a,0x5b9cca4f,0x682e6ff3
+++.long	0x748f82ee,0x78a5636f,0x84c87814,0x8cc70208
+++.long	0x748f82ee,0x78a5636f,0x84c87814,0x8cc70208
+++.long	0x90befffa,0xa4506ceb,0xbef9a3f7,0xc67178f2
+++.long	0x90befffa,0xa4506ceb,0xbef9a3f7,0xc67178f2
+++
+++.long	0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f
+++.long	0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f
+++.long	0x03020100,0x0b0a0908,0xffffffff,0xffffffff
+++.long	0x03020100,0x0b0a0908,0xffffffff,0xffffffff
+++.long	0xffffffff,0xffffffff,0x03020100,0x0b0a0908
+++.long	0xffffffff,0xffffffff,0x03020100,0x0b0a0908
+++.byte	83,72,65,50,53,54,32,98,108,111,99,107,32,116,114,97,110,115,102,111,114,109,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
+++.type	sha256_block_data_order_ssse3,@function
+++.align	64
+++sha256_block_data_order_ssse3:
+++.cfi_startproc	
+++.Lssse3_shortcut:
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++	shlq	$4,%rdx
+++	subq	$96,%rsp
+++	leaq	(%rsi,%rdx,4),%rdx
+++	andq	$-64,%rsp
+++	movq	%rdi,64+0(%rsp)
+++	movq	%rsi,64+8(%rsp)
+++	movq	%rdx,64+16(%rsp)
+++	movq	%rax,88(%rsp)
+++.cfi_escape	0x0f,0x06,0x77,0xd8,0x00,0x06,0x23,0x08
+++.Lprologue_ssse3:
+++
+++	movl	0(%rdi),%eax
+++	movl	4(%rdi),%ebx
+++	movl	8(%rdi),%ecx
+++	movl	12(%rdi),%edx
+++	movl	16(%rdi),%r8d
+++	movl	20(%rdi),%r9d
+++	movl	24(%rdi),%r10d
+++	movl	28(%rdi),%r11d
+++
+++
+++	jmp	.Lloop_ssse3
+++.align	16
+++.Lloop_ssse3:
+++	movdqa	K256+512(%rip),%xmm7
+++	movdqu	0(%rsi),%xmm0
+++	movdqu	16(%rsi),%xmm1
+++	movdqu	32(%rsi),%xmm2
+++.byte	102,15,56,0,199
+++	movdqu	48(%rsi),%xmm3
+++	leaq	K256(%rip),%rbp
+++.byte	102,15,56,0,207
+++	movdqa	0(%rbp),%xmm4
+++	movdqa	32(%rbp),%xmm5
+++.byte	102,15,56,0,215
+++	paddd	%xmm0,%xmm4
+++	movdqa	64(%rbp),%xmm6
+++.byte	102,15,56,0,223
+++	movdqa	96(%rbp),%xmm7
+++	paddd	%xmm1,%xmm5
+++	paddd	%xmm2,%xmm6
+++	paddd	%xmm3,%xmm7
+++	movdqa	%xmm4,0(%rsp)
+++	movl	%eax,%r14d
+++	movdqa	%xmm5,16(%rsp)
+++	movl	%ebx,%edi
+++	movdqa	%xmm6,32(%rsp)
+++	xorl	%ecx,%edi
+++	movdqa	%xmm7,48(%rsp)
+++	movl	%r8d,%r13d
+++	jmp	.Lssse3_00_47
+++
+++.align	16
+++.Lssse3_00_47:
+++	subq	$-128,%rbp
+++	rorl	$14,%r13d
+++	movdqa	%xmm1,%xmm4
+++	movl	%r14d,%eax
+++	movl	%r9d,%r12d
+++	movdqa	%xmm3,%xmm7
+++	rorl	$9,%r14d
+++	xorl	%r8d,%r13d
+++	xorl	%r10d,%r12d
+++	rorl	$5,%r13d
+++	xorl	%eax,%r14d
+++.byte	102,15,58,15,224,4
+++	andl	%r8d,%r12d
+++	xorl	%r8d,%r13d
+++.byte	102,15,58,15,250,4
+++	addl	0(%rsp),%r11d
+++	movl	%eax,%r15d
+++	xorl	%r10d,%r12d
+++	rorl	$11,%r14d
+++	movdqa	%xmm4,%xmm5
+++	xorl	%ebx,%r15d
+++	addl	%r12d,%r11d
+++	movdqa	%xmm4,%xmm6
+++	rorl	$6,%r13d
+++	andl	%r15d,%edi
+++	psrld	$3,%xmm4
+++	xorl	%eax,%r14d
+++	addl	%r13d,%r11d
+++	xorl	%ebx,%edi
+++	paddd	%xmm7,%xmm0
+++	rorl	$2,%r14d
+++	addl	%r11d,%edx
+++	psrld	$7,%xmm6
+++	addl	%edi,%r11d
+++	movl	%edx,%r13d
+++	pshufd	$250,%xmm3,%xmm7
+++	addl	%r11d,%r14d
+++	rorl	$14,%r13d
+++	pslld	$14,%xmm5
+++	movl	%r14d,%r11d
+++	movl	%r8d,%r12d
+++	pxor	%xmm6,%xmm4
+++	rorl	$9,%r14d
+++	xorl	%edx,%r13d
+++	xorl	%r9d,%r12d
+++	rorl	$5,%r13d
+++	psrld	$11,%xmm6
+++	xorl	%r11d,%r14d
+++	pxor	%xmm5,%xmm4
+++	andl	%edx,%r12d
+++	xorl	%edx,%r13d
+++	pslld	$11,%xmm5
+++	addl	4(%rsp),%r10d
+++	movl	%r11d,%edi
+++	pxor	%xmm6,%xmm4
+++	xorl	%r9d,%r12d
+++	rorl	$11,%r14d
+++	movdqa	%xmm7,%xmm6
+++	xorl	%eax,%edi
+++	addl	%r12d,%r10d
+++	pxor	%xmm5,%xmm4
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r11d,%r14d
+++	psrld	$10,%xmm7
+++	addl	%r13d,%r10d
+++	xorl	%eax,%r15d
+++	paddd	%xmm4,%xmm0
+++	rorl	$2,%r14d
+++	addl	%r10d,%ecx
+++	psrlq	$17,%xmm6
+++	addl	%r15d,%r10d
+++	movl	%ecx,%r13d
+++	addl	%r10d,%r14d
+++	pxor	%xmm6,%xmm7
+++	rorl	$14,%r13d
+++	movl	%r14d,%r10d
+++	movl	%edx,%r12d
+++	rorl	$9,%r14d
+++	psrlq	$2,%xmm6
+++	xorl	%ecx,%r13d
+++	xorl	%r8d,%r12d
+++	pxor	%xmm6,%xmm7
+++	rorl	$5,%r13d
+++	xorl	%r10d,%r14d
+++	andl	%ecx,%r12d
+++	pshufd	$128,%xmm7,%xmm7
+++	xorl	%ecx,%r13d
+++	addl	8(%rsp),%r9d
+++	movl	%r10d,%r15d
+++	psrldq	$8,%xmm7
+++	xorl	%r8d,%r12d
+++	rorl	$11,%r14d
+++	xorl	%r11d,%r15d
+++	addl	%r12d,%r9d
+++	rorl	$6,%r13d
+++	paddd	%xmm7,%xmm0
+++	andl	%r15d,%edi
+++	xorl	%r10d,%r14d
+++	addl	%r13d,%r9d
+++	pshufd	$80,%xmm0,%xmm7
+++	xorl	%r11d,%edi
+++	rorl	$2,%r14d
+++	addl	%r9d,%ebx
+++	movdqa	%xmm7,%xmm6
+++	addl	%edi,%r9d
+++	movl	%ebx,%r13d
+++	psrld	$10,%xmm7
+++	addl	%r9d,%r14d
+++	rorl	$14,%r13d
+++	psrlq	$17,%xmm6
+++	movl	%r14d,%r9d
+++	movl	%ecx,%r12d
+++	pxor	%xmm6,%xmm7
+++	rorl	$9,%r14d
+++	xorl	%ebx,%r13d
+++	xorl	%edx,%r12d
+++	rorl	$5,%r13d
+++	xorl	%r9d,%r14d
+++	psrlq	$2,%xmm6
+++	andl	%ebx,%r12d
+++	xorl	%ebx,%r13d
+++	addl	12(%rsp),%r8d
+++	pxor	%xmm6,%xmm7
+++	movl	%r9d,%edi
+++	xorl	%edx,%r12d
+++	rorl	$11,%r14d
+++	pshufd	$8,%xmm7,%xmm7
+++	xorl	%r10d,%edi
+++	addl	%r12d,%r8d
+++	movdqa	0(%rbp),%xmm6
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	pslldq	$8,%xmm7
+++	xorl	%r9d,%r14d
+++	addl	%r13d,%r8d
+++	xorl	%r10d,%r15d
+++	paddd	%xmm7,%xmm0
+++	rorl	$2,%r14d
+++	addl	%r8d,%eax
+++	addl	%r15d,%r8d
+++	paddd	%xmm0,%xmm6
+++	movl	%eax,%r13d
+++	addl	%r8d,%r14d
+++	movdqa	%xmm6,0(%rsp)
+++	rorl	$14,%r13d
+++	movdqa	%xmm2,%xmm4
+++	movl	%r14d,%r8d
+++	movl	%ebx,%r12d
+++	movdqa	%xmm0,%xmm7
+++	rorl	$9,%r14d
+++	xorl	%eax,%r13d
+++	xorl	%ecx,%r12d
+++	rorl	$5,%r13d
+++	xorl	%r8d,%r14d
+++.byte	102,15,58,15,225,4
+++	andl	%eax,%r12d
+++	xorl	%eax,%r13d
+++.byte	102,15,58,15,251,4
+++	addl	16(%rsp),%edx
+++	movl	%r8d,%r15d
+++	xorl	%ecx,%r12d
+++	rorl	$11,%r14d
+++	movdqa	%xmm4,%xmm5
+++	xorl	%r9d,%r15d
+++	addl	%r12d,%edx
+++	movdqa	%xmm4,%xmm6
+++	rorl	$6,%r13d
+++	andl	%r15d,%edi
+++	psrld	$3,%xmm4
+++	xorl	%r8d,%r14d
+++	addl	%r13d,%edx
+++	xorl	%r9d,%edi
+++	paddd	%xmm7,%xmm1
+++	rorl	$2,%r14d
+++	addl	%edx,%r11d
+++	psrld	$7,%xmm6
+++	addl	%edi,%edx
+++	movl	%r11d,%r13d
+++	pshufd	$250,%xmm0,%xmm7
+++	addl	%edx,%r14d
+++	rorl	$14,%r13d
+++	pslld	$14,%xmm5
+++	movl	%r14d,%edx
+++	movl	%eax,%r12d
+++	pxor	%xmm6,%xmm4
+++	rorl	$9,%r14d
+++	xorl	%r11d,%r13d
+++	xorl	%ebx,%r12d
+++	rorl	$5,%r13d
+++	psrld	$11,%xmm6
+++	xorl	%edx,%r14d
+++	pxor	%xmm5,%xmm4
+++	andl	%r11d,%r12d
+++	xorl	%r11d,%r13d
+++	pslld	$11,%xmm5
+++	addl	20(%rsp),%ecx
+++	movl	%edx,%edi
+++	pxor	%xmm6,%xmm4
+++	xorl	%ebx,%r12d
+++	rorl	$11,%r14d
+++	movdqa	%xmm7,%xmm6
+++	xorl	%r8d,%edi
+++	addl	%r12d,%ecx
+++	pxor	%xmm5,%xmm4
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	xorl	%edx,%r14d
+++	psrld	$10,%xmm7
+++	addl	%r13d,%ecx
+++	xorl	%r8d,%r15d
+++	paddd	%xmm4,%xmm1
+++	rorl	$2,%r14d
+++	addl	%ecx,%r10d
+++	psrlq	$17,%xmm6
+++	addl	%r15d,%ecx
+++	movl	%r10d,%r13d
+++	addl	%ecx,%r14d
+++	pxor	%xmm6,%xmm7
+++	rorl	$14,%r13d
+++	movl	%r14d,%ecx
+++	movl	%r11d,%r12d
+++	rorl	$9,%r14d
+++	psrlq	$2,%xmm6
+++	xorl	%r10d,%r13d
+++	xorl	%eax,%r12d
+++	pxor	%xmm6,%xmm7
+++	rorl	$5,%r13d
+++	xorl	%ecx,%r14d
+++	andl	%r10d,%r12d
+++	pshufd	$128,%xmm7,%xmm7
+++	xorl	%r10d,%r13d
+++	addl	24(%rsp),%ebx
+++	movl	%ecx,%r15d
+++	psrldq	$8,%xmm7
+++	xorl	%eax,%r12d
+++	rorl	$11,%r14d
+++	xorl	%edx,%r15d
+++	addl	%r12d,%ebx
+++	rorl	$6,%r13d
+++	paddd	%xmm7,%xmm1
+++	andl	%r15d,%edi
+++	xorl	%ecx,%r14d
+++	addl	%r13d,%ebx
+++	pshufd	$80,%xmm1,%xmm7
+++	xorl	%edx,%edi
+++	rorl	$2,%r14d
+++	addl	%ebx,%r9d
+++	movdqa	%xmm7,%xmm6
+++	addl	%edi,%ebx
+++	movl	%r9d,%r13d
+++	psrld	$10,%xmm7
+++	addl	%ebx,%r14d
+++	rorl	$14,%r13d
+++	psrlq	$17,%xmm6
+++	movl	%r14d,%ebx
+++	movl	%r10d,%r12d
+++	pxor	%xmm6,%xmm7
+++	rorl	$9,%r14d
+++	xorl	%r9d,%r13d
+++	xorl	%r11d,%r12d
+++	rorl	$5,%r13d
+++	xorl	%ebx,%r14d
+++	psrlq	$2,%xmm6
+++	andl	%r9d,%r12d
+++	xorl	%r9d,%r13d
+++	addl	28(%rsp),%eax
+++	pxor	%xmm6,%xmm7
+++	movl	%ebx,%edi
+++	xorl	%r11d,%r12d
+++	rorl	$11,%r14d
+++	pshufd	$8,%xmm7,%xmm7
+++	xorl	%ecx,%edi
+++	addl	%r12d,%eax
+++	movdqa	32(%rbp),%xmm6
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	pslldq	$8,%xmm7
+++	xorl	%ebx,%r14d
+++	addl	%r13d,%eax
+++	xorl	%ecx,%r15d
+++	paddd	%xmm7,%xmm1
+++	rorl	$2,%r14d
+++	addl	%eax,%r8d
+++	addl	%r15d,%eax
+++	paddd	%xmm1,%xmm6
+++	movl	%r8d,%r13d
+++	addl	%eax,%r14d
+++	movdqa	%xmm6,16(%rsp)
+++	rorl	$14,%r13d
+++	movdqa	%xmm3,%xmm4
+++	movl	%r14d,%eax
+++	movl	%r9d,%r12d
+++	movdqa	%xmm1,%xmm7
+++	rorl	$9,%r14d
+++	xorl	%r8d,%r13d
+++	xorl	%r10d,%r12d
+++	rorl	$5,%r13d
+++	xorl	%eax,%r14d
+++.byte	102,15,58,15,226,4
+++	andl	%r8d,%r12d
+++	xorl	%r8d,%r13d
+++.byte	102,15,58,15,248,4
+++	addl	32(%rsp),%r11d
+++	movl	%eax,%r15d
+++	xorl	%r10d,%r12d
+++	rorl	$11,%r14d
+++	movdqa	%xmm4,%xmm5
+++	xorl	%ebx,%r15d
+++	addl	%r12d,%r11d
+++	movdqa	%xmm4,%xmm6
+++	rorl	$6,%r13d
+++	andl	%r15d,%edi
+++	psrld	$3,%xmm4
+++	xorl	%eax,%r14d
+++	addl	%r13d,%r11d
+++	xorl	%ebx,%edi
+++	paddd	%xmm7,%xmm2
+++	rorl	$2,%r14d
+++	addl	%r11d,%edx
+++	psrld	$7,%xmm6
+++	addl	%edi,%r11d
+++	movl	%edx,%r13d
+++	pshufd	$250,%xmm1,%xmm7
+++	addl	%r11d,%r14d
+++	rorl	$14,%r13d
+++	pslld	$14,%xmm5
+++	movl	%r14d,%r11d
+++	movl	%r8d,%r12d
+++	pxor	%xmm6,%xmm4
+++	rorl	$9,%r14d
+++	xorl	%edx,%r13d
+++	xorl	%r9d,%r12d
+++	rorl	$5,%r13d
+++	psrld	$11,%xmm6
+++	xorl	%r11d,%r14d
+++	pxor	%xmm5,%xmm4
+++	andl	%edx,%r12d
+++	xorl	%edx,%r13d
+++	pslld	$11,%xmm5
+++	addl	36(%rsp),%r10d
+++	movl	%r11d,%edi
+++	pxor	%xmm6,%xmm4
+++	xorl	%r9d,%r12d
+++	rorl	$11,%r14d
+++	movdqa	%xmm7,%xmm6
+++	xorl	%eax,%edi
+++	addl	%r12d,%r10d
+++	pxor	%xmm5,%xmm4
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r11d,%r14d
+++	psrld	$10,%xmm7
+++	addl	%r13d,%r10d
+++	xorl	%eax,%r15d
+++	paddd	%xmm4,%xmm2
+++	rorl	$2,%r14d
+++	addl	%r10d,%ecx
+++	psrlq	$17,%xmm6
+++	addl	%r15d,%r10d
+++	movl	%ecx,%r13d
+++	addl	%r10d,%r14d
+++	pxor	%xmm6,%xmm7
+++	rorl	$14,%r13d
+++	movl	%r14d,%r10d
+++	movl	%edx,%r12d
+++	rorl	$9,%r14d
+++	psrlq	$2,%xmm6
+++	xorl	%ecx,%r13d
+++	xorl	%r8d,%r12d
+++	pxor	%xmm6,%xmm7
+++	rorl	$5,%r13d
+++	xorl	%r10d,%r14d
+++	andl	%ecx,%r12d
+++	pshufd	$128,%xmm7,%xmm7
+++	xorl	%ecx,%r13d
+++	addl	40(%rsp),%r9d
+++	movl	%r10d,%r15d
+++	psrldq	$8,%xmm7
+++	xorl	%r8d,%r12d
+++	rorl	$11,%r14d
+++	xorl	%r11d,%r15d
+++	addl	%r12d,%r9d
+++	rorl	$6,%r13d
+++	paddd	%xmm7,%xmm2
+++	andl	%r15d,%edi
+++	xorl	%r10d,%r14d
+++	addl	%r13d,%r9d
+++	pshufd	$80,%xmm2,%xmm7
+++	xorl	%r11d,%edi
+++	rorl	$2,%r14d
+++	addl	%r9d,%ebx
+++	movdqa	%xmm7,%xmm6
+++	addl	%edi,%r9d
+++	movl	%ebx,%r13d
+++	psrld	$10,%xmm7
+++	addl	%r9d,%r14d
+++	rorl	$14,%r13d
+++	psrlq	$17,%xmm6
+++	movl	%r14d,%r9d
+++	movl	%ecx,%r12d
+++	pxor	%xmm6,%xmm7
+++	rorl	$9,%r14d
+++	xorl	%ebx,%r13d
+++	xorl	%edx,%r12d
+++	rorl	$5,%r13d
+++	xorl	%r9d,%r14d
+++	psrlq	$2,%xmm6
+++	andl	%ebx,%r12d
+++	xorl	%ebx,%r13d
+++	addl	44(%rsp),%r8d
+++	pxor	%xmm6,%xmm7
+++	movl	%r9d,%edi
+++	xorl	%edx,%r12d
+++	rorl	$11,%r14d
+++	pshufd	$8,%xmm7,%xmm7
+++	xorl	%r10d,%edi
+++	addl	%r12d,%r8d
+++	movdqa	64(%rbp),%xmm6
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	pslldq	$8,%xmm7
+++	xorl	%r9d,%r14d
+++	addl	%r13d,%r8d
+++	xorl	%r10d,%r15d
+++	paddd	%xmm7,%xmm2
+++	rorl	$2,%r14d
+++	addl	%r8d,%eax
+++	addl	%r15d,%r8d
+++	paddd	%xmm2,%xmm6
+++	movl	%eax,%r13d
+++	addl	%r8d,%r14d
+++	movdqa	%xmm6,32(%rsp)
+++	rorl	$14,%r13d
+++	movdqa	%xmm0,%xmm4
+++	movl	%r14d,%r8d
+++	movl	%ebx,%r12d
+++	movdqa	%xmm2,%xmm7
+++	rorl	$9,%r14d
+++	xorl	%eax,%r13d
+++	xorl	%ecx,%r12d
+++	rorl	$5,%r13d
+++	xorl	%r8d,%r14d
+++.byte	102,15,58,15,227,4
+++	andl	%eax,%r12d
+++	xorl	%eax,%r13d
+++.byte	102,15,58,15,249,4
+++	addl	48(%rsp),%edx
+++	movl	%r8d,%r15d
+++	xorl	%ecx,%r12d
+++	rorl	$11,%r14d
+++	movdqa	%xmm4,%xmm5
+++	xorl	%r9d,%r15d
+++	addl	%r12d,%edx
+++	movdqa	%xmm4,%xmm6
+++	rorl	$6,%r13d
+++	andl	%r15d,%edi
+++	psrld	$3,%xmm4
+++	xorl	%r8d,%r14d
+++	addl	%r13d,%edx
+++	xorl	%r9d,%edi
+++	paddd	%xmm7,%xmm3
+++	rorl	$2,%r14d
+++	addl	%edx,%r11d
+++	psrld	$7,%xmm6
+++	addl	%edi,%edx
+++	movl	%r11d,%r13d
+++	pshufd	$250,%xmm2,%xmm7
+++	addl	%edx,%r14d
+++	rorl	$14,%r13d
+++	pslld	$14,%xmm5
+++	movl	%r14d,%edx
+++	movl	%eax,%r12d
+++	pxor	%xmm6,%xmm4
+++	rorl	$9,%r14d
+++	xorl	%r11d,%r13d
+++	xorl	%ebx,%r12d
+++	rorl	$5,%r13d
+++	psrld	$11,%xmm6
+++	xorl	%edx,%r14d
+++	pxor	%xmm5,%xmm4
+++	andl	%r11d,%r12d
+++	xorl	%r11d,%r13d
+++	pslld	$11,%xmm5
+++	addl	52(%rsp),%ecx
+++	movl	%edx,%edi
+++	pxor	%xmm6,%xmm4
+++	xorl	%ebx,%r12d
+++	rorl	$11,%r14d
+++	movdqa	%xmm7,%xmm6
+++	xorl	%r8d,%edi
+++	addl	%r12d,%ecx
+++	pxor	%xmm5,%xmm4
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	xorl	%edx,%r14d
+++	psrld	$10,%xmm7
+++	addl	%r13d,%ecx
+++	xorl	%r8d,%r15d
+++	paddd	%xmm4,%xmm3
+++	rorl	$2,%r14d
+++	addl	%ecx,%r10d
+++	psrlq	$17,%xmm6
+++	addl	%r15d,%ecx
+++	movl	%r10d,%r13d
+++	addl	%ecx,%r14d
+++	pxor	%xmm6,%xmm7
+++	rorl	$14,%r13d
+++	movl	%r14d,%ecx
+++	movl	%r11d,%r12d
+++	rorl	$9,%r14d
+++	psrlq	$2,%xmm6
+++	xorl	%r10d,%r13d
+++	xorl	%eax,%r12d
+++	pxor	%xmm6,%xmm7
+++	rorl	$5,%r13d
+++	xorl	%ecx,%r14d
+++	andl	%r10d,%r12d
+++	pshufd	$128,%xmm7,%xmm7
+++	xorl	%r10d,%r13d
+++	addl	56(%rsp),%ebx
+++	movl	%ecx,%r15d
+++	psrldq	$8,%xmm7
+++	xorl	%eax,%r12d
+++	rorl	$11,%r14d
+++	xorl	%edx,%r15d
+++	addl	%r12d,%ebx
+++	rorl	$6,%r13d
+++	paddd	%xmm7,%xmm3
+++	andl	%r15d,%edi
+++	xorl	%ecx,%r14d
+++	addl	%r13d,%ebx
+++	pshufd	$80,%xmm3,%xmm7
+++	xorl	%edx,%edi
+++	rorl	$2,%r14d
+++	addl	%ebx,%r9d
+++	movdqa	%xmm7,%xmm6
+++	addl	%edi,%ebx
+++	movl	%r9d,%r13d
+++	psrld	$10,%xmm7
+++	addl	%ebx,%r14d
+++	rorl	$14,%r13d
+++	psrlq	$17,%xmm6
+++	movl	%r14d,%ebx
+++	movl	%r10d,%r12d
+++	pxor	%xmm6,%xmm7
+++	rorl	$9,%r14d
+++	xorl	%r9d,%r13d
+++	xorl	%r11d,%r12d
+++	rorl	$5,%r13d
+++	xorl	%ebx,%r14d
+++	psrlq	$2,%xmm6
+++	andl	%r9d,%r12d
+++	xorl	%r9d,%r13d
+++	addl	60(%rsp),%eax
+++	pxor	%xmm6,%xmm7
+++	movl	%ebx,%edi
+++	xorl	%r11d,%r12d
+++	rorl	$11,%r14d
+++	pshufd	$8,%xmm7,%xmm7
+++	xorl	%ecx,%edi
+++	addl	%r12d,%eax
+++	movdqa	96(%rbp),%xmm6
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	pslldq	$8,%xmm7
+++	xorl	%ebx,%r14d
+++	addl	%r13d,%eax
+++	xorl	%ecx,%r15d
+++	paddd	%xmm7,%xmm3
+++	rorl	$2,%r14d
+++	addl	%eax,%r8d
+++	addl	%r15d,%eax
+++	paddd	%xmm3,%xmm6
+++	movl	%r8d,%r13d
+++	addl	%eax,%r14d
+++	movdqa	%xmm6,48(%rsp)
+++	cmpb	$0,131(%rbp)
+++	jne	.Lssse3_00_47
+++	rorl	$14,%r13d
+++	movl	%r14d,%eax
+++	movl	%r9d,%r12d
+++	rorl	$9,%r14d
+++	xorl	%r8d,%r13d
+++	xorl	%r10d,%r12d
+++	rorl	$5,%r13d
+++	xorl	%eax,%r14d
+++	andl	%r8d,%r12d
+++	xorl	%r8d,%r13d
+++	addl	0(%rsp),%r11d
+++	movl	%eax,%r15d
+++	xorl	%r10d,%r12d
+++	rorl	$11,%r14d
+++	xorl	%ebx,%r15d
+++	addl	%r12d,%r11d
+++	rorl	$6,%r13d
+++	andl	%r15d,%edi
+++	xorl	%eax,%r14d
+++	addl	%r13d,%r11d
+++	xorl	%ebx,%edi
+++	rorl	$2,%r14d
+++	addl	%r11d,%edx
+++	addl	%edi,%r11d
+++	movl	%edx,%r13d
+++	addl	%r11d,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%r11d
+++	movl	%r8d,%r12d
+++	rorl	$9,%r14d
+++	xorl	%edx,%r13d
+++	xorl	%r9d,%r12d
+++	rorl	$5,%r13d
+++	xorl	%r11d,%r14d
+++	andl	%edx,%r12d
+++	xorl	%edx,%r13d
+++	addl	4(%rsp),%r10d
+++	movl	%r11d,%edi
+++	xorl	%r9d,%r12d
+++	rorl	$11,%r14d
+++	xorl	%eax,%edi
+++	addl	%r12d,%r10d
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r11d,%r14d
+++	addl	%r13d,%r10d
+++	xorl	%eax,%r15d
+++	rorl	$2,%r14d
+++	addl	%r10d,%ecx
+++	addl	%r15d,%r10d
+++	movl	%ecx,%r13d
+++	addl	%r10d,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%r10d
+++	movl	%edx,%r12d
+++	rorl	$9,%r14d
+++	xorl	%ecx,%r13d
+++	xorl	%r8d,%r12d
+++	rorl	$5,%r13d
+++	xorl	%r10d,%r14d
+++	andl	%ecx,%r12d
+++	xorl	%ecx,%r13d
+++	addl	8(%rsp),%r9d
+++	movl	%r10d,%r15d
+++	xorl	%r8d,%r12d
+++	rorl	$11,%r14d
+++	xorl	%r11d,%r15d
+++	addl	%r12d,%r9d
+++	rorl	$6,%r13d
+++	andl	%r15d,%edi
+++	xorl	%r10d,%r14d
+++	addl	%r13d,%r9d
+++	xorl	%r11d,%edi
+++	rorl	$2,%r14d
+++	addl	%r9d,%ebx
+++	addl	%edi,%r9d
+++	movl	%ebx,%r13d
+++	addl	%r9d,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%r9d
+++	movl	%ecx,%r12d
+++	rorl	$9,%r14d
+++	xorl	%ebx,%r13d
+++	xorl	%edx,%r12d
+++	rorl	$5,%r13d
+++	xorl	%r9d,%r14d
+++	andl	%ebx,%r12d
+++	xorl	%ebx,%r13d
+++	addl	12(%rsp),%r8d
+++	movl	%r9d,%edi
+++	xorl	%edx,%r12d
+++	rorl	$11,%r14d
+++	xorl	%r10d,%edi
+++	addl	%r12d,%r8d
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r9d,%r14d
+++	addl	%r13d,%r8d
+++	xorl	%r10d,%r15d
+++	rorl	$2,%r14d
+++	addl	%r8d,%eax
+++	addl	%r15d,%r8d
+++	movl	%eax,%r13d
+++	addl	%r8d,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%r8d
+++	movl	%ebx,%r12d
+++	rorl	$9,%r14d
+++	xorl	%eax,%r13d
+++	xorl	%ecx,%r12d
+++	rorl	$5,%r13d
+++	xorl	%r8d,%r14d
+++	andl	%eax,%r12d
+++	xorl	%eax,%r13d
+++	addl	16(%rsp),%edx
+++	movl	%r8d,%r15d
+++	xorl	%ecx,%r12d
+++	rorl	$11,%r14d
+++	xorl	%r9d,%r15d
+++	addl	%r12d,%edx
+++	rorl	$6,%r13d
+++	andl	%r15d,%edi
+++	xorl	%r8d,%r14d
+++	addl	%r13d,%edx
+++	xorl	%r9d,%edi
+++	rorl	$2,%r14d
+++	addl	%edx,%r11d
+++	addl	%edi,%edx
+++	movl	%r11d,%r13d
+++	addl	%edx,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%edx
+++	movl	%eax,%r12d
+++	rorl	$9,%r14d
+++	xorl	%r11d,%r13d
+++	xorl	%ebx,%r12d
+++	rorl	$5,%r13d
+++	xorl	%edx,%r14d
+++	andl	%r11d,%r12d
+++	xorl	%r11d,%r13d
+++	addl	20(%rsp),%ecx
+++	movl	%edx,%edi
+++	xorl	%ebx,%r12d
+++	rorl	$11,%r14d
+++	xorl	%r8d,%edi
+++	addl	%r12d,%ecx
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	xorl	%edx,%r14d
+++	addl	%r13d,%ecx
+++	xorl	%r8d,%r15d
+++	rorl	$2,%r14d
+++	addl	%ecx,%r10d
+++	addl	%r15d,%ecx
+++	movl	%r10d,%r13d
+++	addl	%ecx,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%ecx
+++	movl	%r11d,%r12d
+++	rorl	$9,%r14d
+++	xorl	%r10d,%r13d
+++	xorl	%eax,%r12d
+++	rorl	$5,%r13d
+++	xorl	%ecx,%r14d
+++	andl	%r10d,%r12d
+++	xorl	%r10d,%r13d
+++	addl	24(%rsp),%ebx
+++	movl	%ecx,%r15d
+++	xorl	%eax,%r12d
+++	rorl	$11,%r14d
+++	xorl	%edx,%r15d
+++	addl	%r12d,%ebx
+++	rorl	$6,%r13d
+++	andl	%r15d,%edi
+++	xorl	%ecx,%r14d
+++	addl	%r13d,%ebx
+++	xorl	%edx,%edi
+++	rorl	$2,%r14d
+++	addl	%ebx,%r9d
+++	addl	%edi,%ebx
+++	movl	%r9d,%r13d
+++	addl	%ebx,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%ebx
+++	movl	%r10d,%r12d
+++	rorl	$9,%r14d
+++	xorl	%r9d,%r13d
+++	xorl	%r11d,%r12d
+++	rorl	$5,%r13d
+++	xorl	%ebx,%r14d
+++	andl	%r9d,%r12d
+++	xorl	%r9d,%r13d
+++	addl	28(%rsp),%eax
+++	movl	%ebx,%edi
+++	xorl	%r11d,%r12d
+++	rorl	$11,%r14d
+++	xorl	%ecx,%edi
+++	addl	%r12d,%eax
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	xorl	%ebx,%r14d
+++	addl	%r13d,%eax
+++	xorl	%ecx,%r15d
+++	rorl	$2,%r14d
+++	addl	%eax,%r8d
+++	addl	%r15d,%eax
+++	movl	%r8d,%r13d
+++	addl	%eax,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%eax
+++	movl	%r9d,%r12d
+++	rorl	$9,%r14d
+++	xorl	%r8d,%r13d
+++	xorl	%r10d,%r12d
+++	rorl	$5,%r13d
+++	xorl	%eax,%r14d
+++	andl	%r8d,%r12d
+++	xorl	%r8d,%r13d
+++	addl	32(%rsp),%r11d
+++	movl	%eax,%r15d
+++	xorl	%r10d,%r12d
+++	rorl	$11,%r14d
+++	xorl	%ebx,%r15d
+++	addl	%r12d,%r11d
+++	rorl	$6,%r13d
+++	andl	%r15d,%edi
+++	xorl	%eax,%r14d
+++	addl	%r13d,%r11d
+++	xorl	%ebx,%edi
+++	rorl	$2,%r14d
+++	addl	%r11d,%edx
+++	addl	%edi,%r11d
+++	movl	%edx,%r13d
+++	addl	%r11d,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%r11d
+++	movl	%r8d,%r12d
+++	rorl	$9,%r14d
+++	xorl	%edx,%r13d
+++	xorl	%r9d,%r12d
+++	rorl	$5,%r13d
+++	xorl	%r11d,%r14d
+++	andl	%edx,%r12d
+++	xorl	%edx,%r13d
+++	addl	36(%rsp),%r10d
+++	movl	%r11d,%edi
+++	xorl	%r9d,%r12d
+++	rorl	$11,%r14d
+++	xorl	%eax,%edi
+++	addl	%r12d,%r10d
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r11d,%r14d
+++	addl	%r13d,%r10d
+++	xorl	%eax,%r15d
+++	rorl	$2,%r14d
+++	addl	%r10d,%ecx
+++	addl	%r15d,%r10d
+++	movl	%ecx,%r13d
+++	addl	%r10d,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%r10d
+++	movl	%edx,%r12d
+++	rorl	$9,%r14d
+++	xorl	%ecx,%r13d
+++	xorl	%r8d,%r12d
+++	rorl	$5,%r13d
+++	xorl	%r10d,%r14d
+++	andl	%ecx,%r12d
+++	xorl	%ecx,%r13d
+++	addl	40(%rsp),%r9d
+++	movl	%r10d,%r15d
+++	xorl	%r8d,%r12d
+++	rorl	$11,%r14d
+++	xorl	%r11d,%r15d
+++	addl	%r12d,%r9d
+++	rorl	$6,%r13d
+++	andl	%r15d,%edi
+++	xorl	%r10d,%r14d
+++	addl	%r13d,%r9d
+++	xorl	%r11d,%edi
+++	rorl	$2,%r14d
+++	addl	%r9d,%ebx
+++	addl	%edi,%r9d
+++	movl	%ebx,%r13d
+++	addl	%r9d,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%r9d
+++	movl	%ecx,%r12d
+++	rorl	$9,%r14d
+++	xorl	%ebx,%r13d
+++	xorl	%edx,%r12d
+++	rorl	$5,%r13d
+++	xorl	%r9d,%r14d
+++	andl	%ebx,%r12d
+++	xorl	%ebx,%r13d
+++	addl	44(%rsp),%r8d
+++	movl	%r9d,%edi
+++	xorl	%edx,%r12d
+++	rorl	$11,%r14d
+++	xorl	%r10d,%edi
+++	addl	%r12d,%r8d
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r9d,%r14d
+++	addl	%r13d,%r8d
+++	xorl	%r10d,%r15d
+++	rorl	$2,%r14d
+++	addl	%r8d,%eax
+++	addl	%r15d,%r8d
+++	movl	%eax,%r13d
+++	addl	%r8d,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%r8d
+++	movl	%ebx,%r12d
+++	rorl	$9,%r14d
+++	xorl	%eax,%r13d
+++	xorl	%ecx,%r12d
+++	rorl	$5,%r13d
+++	xorl	%r8d,%r14d
+++	andl	%eax,%r12d
+++	xorl	%eax,%r13d
+++	addl	48(%rsp),%edx
+++	movl	%r8d,%r15d
+++	xorl	%ecx,%r12d
+++	rorl	$11,%r14d
+++	xorl	%r9d,%r15d
+++	addl	%r12d,%edx
+++	rorl	$6,%r13d
+++	andl	%r15d,%edi
+++	xorl	%r8d,%r14d
+++	addl	%r13d,%edx
+++	xorl	%r9d,%edi
+++	rorl	$2,%r14d
+++	addl	%edx,%r11d
+++	addl	%edi,%edx
+++	movl	%r11d,%r13d
+++	addl	%edx,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%edx
+++	movl	%eax,%r12d
+++	rorl	$9,%r14d
+++	xorl	%r11d,%r13d
+++	xorl	%ebx,%r12d
+++	rorl	$5,%r13d
+++	xorl	%edx,%r14d
+++	andl	%r11d,%r12d
+++	xorl	%r11d,%r13d
+++	addl	52(%rsp),%ecx
+++	movl	%edx,%edi
+++	xorl	%ebx,%r12d
+++	rorl	$11,%r14d
+++	xorl	%r8d,%edi
+++	addl	%r12d,%ecx
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	xorl	%edx,%r14d
+++	addl	%r13d,%ecx
+++	xorl	%r8d,%r15d
+++	rorl	$2,%r14d
+++	addl	%ecx,%r10d
+++	addl	%r15d,%ecx
+++	movl	%r10d,%r13d
+++	addl	%ecx,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%ecx
+++	movl	%r11d,%r12d
+++	rorl	$9,%r14d
+++	xorl	%r10d,%r13d
+++	xorl	%eax,%r12d
+++	rorl	$5,%r13d
+++	xorl	%ecx,%r14d
+++	andl	%r10d,%r12d
+++	xorl	%r10d,%r13d
+++	addl	56(%rsp),%ebx
+++	movl	%ecx,%r15d
+++	xorl	%eax,%r12d
+++	rorl	$11,%r14d
+++	xorl	%edx,%r15d
+++	addl	%r12d,%ebx
+++	rorl	$6,%r13d
+++	andl	%r15d,%edi
+++	xorl	%ecx,%r14d
+++	addl	%r13d,%ebx
+++	xorl	%edx,%edi
+++	rorl	$2,%r14d
+++	addl	%ebx,%r9d
+++	addl	%edi,%ebx
+++	movl	%r9d,%r13d
+++	addl	%ebx,%r14d
+++	rorl	$14,%r13d
+++	movl	%r14d,%ebx
+++	movl	%r10d,%r12d
+++	rorl	$9,%r14d
+++	xorl	%r9d,%r13d
+++	xorl	%r11d,%r12d
+++	rorl	$5,%r13d
+++	xorl	%ebx,%r14d
+++	andl	%r9d,%r12d
+++	xorl	%r9d,%r13d
+++	addl	60(%rsp),%eax
+++	movl	%ebx,%edi
+++	xorl	%r11d,%r12d
+++	rorl	$11,%r14d
+++	xorl	%ecx,%edi
+++	addl	%r12d,%eax
+++	rorl	$6,%r13d
+++	andl	%edi,%r15d
+++	xorl	%ebx,%r14d
+++	addl	%r13d,%eax
+++	xorl	%ecx,%r15d
+++	rorl	$2,%r14d
+++	addl	%eax,%r8d
+++	addl	%r15d,%eax
+++	movl	%r8d,%r13d
+++	addl	%eax,%r14d
+++	movq	64+0(%rsp),%rdi
+++	movl	%r14d,%eax
+++
+++	addl	0(%rdi),%eax
+++	leaq	64(%rsi),%rsi
+++	addl	4(%rdi),%ebx
+++	addl	8(%rdi),%ecx
+++	addl	12(%rdi),%edx
+++	addl	16(%rdi),%r8d
+++	addl	20(%rdi),%r9d
+++	addl	24(%rdi),%r10d
+++	addl	28(%rdi),%r11d
+++
+++	cmpq	64+16(%rsp),%rsi
+++
+++	movl	%eax,0(%rdi)
+++	movl	%ebx,4(%rdi)
+++	movl	%ecx,8(%rdi)
+++	movl	%edx,12(%rdi)
+++	movl	%r8d,16(%rdi)
+++	movl	%r9d,20(%rdi)
+++	movl	%r10d,24(%rdi)
+++	movl	%r11d,28(%rdi)
+++	jb	.Lloop_ssse3
+++
+++	movq	88(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lepilogue_ssse3:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	sha256_block_data_order_ssse3,.-sha256_block_data_order_ssse3
+++.type	sha256_block_data_order_avx,@function
+++.align	64
+++sha256_block_data_order_avx:
+++.cfi_startproc	
+++.Lavx_shortcut:
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++	shlq	$4,%rdx
+++	subq	$96,%rsp
+++	leaq	(%rsi,%rdx,4),%rdx
+++	andq	$-64,%rsp
+++	movq	%rdi,64+0(%rsp)
+++	movq	%rsi,64+8(%rsp)
+++	movq	%rdx,64+16(%rsp)
+++	movq	%rax,88(%rsp)
+++.cfi_escape	0x0f,0x06,0x77,0xd8,0x00,0x06,0x23,0x08
+++.Lprologue_avx:
+++
+++	vzeroupper
+++	movl	0(%rdi),%eax
+++	movl	4(%rdi),%ebx
+++	movl	8(%rdi),%ecx
+++	movl	12(%rdi),%edx
+++	movl	16(%rdi),%r8d
+++	movl	20(%rdi),%r9d
+++	movl	24(%rdi),%r10d
+++	movl	28(%rdi),%r11d
+++	vmovdqa	K256+512+32(%rip),%xmm8
+++	vmovdqa	K256+512+64(%rip),%xmm9
+++	jmp	.Lloop_avx
+++.align	16
+++.Lloop_avx:
+++	vmovdqa	K256+512(%rip),%xmm7
+++	vmovdqu	0(%rsi),%xmm0
+++	vmovdqu	16(%rsi),%xmm1
+++	vmovdqu	32(%rsi),%xmm2
+++	vmovdqu	48(%rsi),%xmm3
+++	vpshufb	%xmm7,%xmm0,%xmm0
+++	leaq	K256(%rip),%rbp
+++	vpshufb	%xmm7,%xmm1,%xmm1
+++	vpshufb	%xmm7,%xmm2,%xmm2
+++	vpaddd	0(%rbp),%xmm0,%xmm4
+++	vpshufb	%xmm7,%xmm3,%xmm3
+++	vpaddd	32(%rbp),%xmm1,%xmm5
+++	vpaddd	64(%rbp),%xmm2,%xmm6
+++	vpaddd	96(%rbp),%xmm3,%xmm7
+++	vmovdqa	%xmm4,0(%rsp)
+++	movl	%eax,%r14d
+++	vmovdqa	%xmm5,16(%rsp)
+++	movl	%ebx,%edi
+++	vmovdqa	%xmm6,32(%rsp)
+++	xorl	%ecx,%edi
+++	vmovdqa	%xmm7,48(%rsp)
+++	movl	%r8d,%r13d
+++	jmp	.Lavx_00_47
+++
+++.align	16
+++.Lavx_00_47:
+++	subq	$-128,%rbp
+++	vpalignr	$4,%xmm0,%xmm1,%xmm4
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%eax
+++	movl	%r9d,%r12d
+++	vpalignr	$4,%xmm2,%xmm3,%xmm7
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r8d,%r13d
+++	xorl	%r10d,%r12d
+++	vpsrld	$7,%xmm4,%xmm6
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%eax,%r14d
+++	andl	%r8d,%r12d
+++	vpaddd	%xmm7,%xmm0,%xmm0
+++	xorl	%r8d,%r13d
+++	addl	0(%rsp),%r11d
+++	movl	%eax,%r15d
+++	vpsrld	$3,%xmm4,%xmm7
+++	xorl	%r10d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%ebx,%r15d
+++	vpslld	$14,%xmm4,%xmm5
+++	addl	%r12d,%r11d
+++	shrdl	$6,%r13d,%r13d
+++	andl	%r15d,%edi
+++	vpxor	%xmm6,%xmm7,%xmm4
+++	xorl	%eax,%r14d
+++	addl	%r13d,%r11d
+++	xorl	%ebx,%edi
+++	vpshufd	$250,%xmm3,%xmm7
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r11d,%edx
+++	addl	%edi,%r11d
+++	vpsrld	$11,%xmm6,%xmm6
+++	movl	%edx,%r13d
+++	addl	%r11d,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	vpxor	%xmm5,%xmm4,%xmm4
+++	movl	%r14d,%r11d
+++	movl	%r8d,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	vpslld	$11,%xmm5,%xmm5
+++	xorl	%edx,%r13d
+++	xorl	%r9d,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	vpxor	%xmm6,%xmm4,%xmm4
+++	xorl	%r11d,%r14d
+++	andl	%edx,%r12d
+++	xorl	%edx,%r13d
+++	vpsrld	$10,%xmm7,%xmm6
+++	addl	4(%rsp),%r10d
+++	movl	%r11d,%edi
+++	xorl	%r9d,%r12d
+++	vpxor	%xmm5,%xmm4,%xmm4
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%eax,%edi
+++	addl	%r12d,%r10d
+++	vpsrlq	$17,%xmm7,%xmm7
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r11d,%r14d
+++	vpaddd	%xmm4,%xmm0,%xmm0
+++	addl	%r13d,%r10d
+++	xorl	%eax,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	addl	%r10d,%ecx
+++	addl	%r15d,%r10d
+++	movl	%ecx,%r13d
+++	vpsrlq	$2,%xmm7,%xmm7
+++	addl	%r10d,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r10d
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	movl	%edx,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%ecx,%r13d
+++	vpshufb	%xmm8,%xmm6,%xmm6
+++	xorl	%r8d,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r10d,%r14d
+++	vpaddd	%xmm6,%xmm0,%xmm0
+++	andl	%ecx,%r12d
+++	xorl	%ecx,%r13d
+++	addl	8(%rsp),%r9d
+++	vpshufd	$80,%xmm0,%xmm7
+++	movl	%r10d,%r15d
+++	xorl	%r8d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	vpsrld	$10,%xmm7,%xmm6
+++	xorl	%r11d,%r15d
+++	addl	%r12d,%r9d
+++	shrdl	$6,%r13d,%r13d
+++	vpsrlq	$17,%xmm7,%xmm7
+++	andl	%r15d,%edi
+++	xorl	%r10d,%r14d
+++	addl	%r13d,%r9d
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	xorl	%r11d,%edi
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r9d,%ebx
+++	vpsrlq	$2,%xmm7,%xmm7
+++	addl	%edi,%r9d
+++	movl	%ebx,%r13d
+++	addl	%r9d,%r14d
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r9d
+++	movl	%ecx,%r12d
+++	vpshufb	%xmm9,%xmm6,%xmm6
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%ebx,%r13d
+++	xorl	%edx,%r12d
+++	vpaddd	%xmm6,%xmm0,%xmm0
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r9d,%r14d
+++	andl	%ebx,%r12d
+++	vpaddd	0(%rbp),%xmm0,%xmm6
+++	xorl	%ebx,%r13d
+++	addl	12(%rsp),%r8d
+++	movl	%r9d,%edi
+++	xorl	%edx,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r10d,%edi
+++	addl	%r12d,%r8d
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r9d,%r14d
+++	addl	%r13d,%r8d
+++	xorl	%r10d,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r8d,%eax
+++	addl	%r15d,%r8d
+++	movl	%eax,%r13d
+++	addl	%r8d,%r14d
+++	vmovdqa	%xmm6,0(%rsp)
+++	vpalignr	$4,%xmm1,%xmm2,%xmm4
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r8d
+++	movl	%ebx,%r12d
+++	vpalignr	$4,%xmm3,%xmm0,%xmm7
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%eax,%r13d
+++	xorl	%ecx,%r12d
+++	vpsrld	$7,%xmm4,%xmm6
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r8d,%r14d
+++	andl	%eax,%r12d
+++	vpaddd	%xmm7,%xmm1,%xmm1
+++	xorl	%eax,%r13d
+++	addl	16(%rsp),%edx
+++	movl	%r8d,%r15d
+++	vpsrld	$3,%xmm4,%xmm7
+++	xorl	%ecx,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r9d,%r15d
+++	vpslld	$14,%xmm4,%xmm5
+++	addl	%r12d,%edx
+++	shrdl	$6,%r13d,%r13d
+++	andl	%r15d,%edi
+++	vpxor	%xmm6,%xmm7,%xmm4
+++	xorl	%r8d,%r14d
+++	addl	%r13d,%edx
+++	xorl	%r9d,%edi
+++	vpshufd	$250,%xmm0,%xmm7
+++	shrdl	$2,%r14d,%r14d
+++	addl	%edx,%r11d
+++	addl	%edi,%edx
+++	vpsrld	$11,%xmm6,%xmm6
+++	movl	%r11d,%r13d
+++	addl	%edx,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	vpxor	%xmm5,%xmm4,%xmm4
+++	movl	%r14d,%edx
+++	movl	%eax,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	vpslld	$11,%xmm5,%xmm5
+++	xorl	%r11d,%r13d
+++	xorl	%ebx,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	vpxor	%xmm6,%xmm4,%xmm4
+++	xorl	%edx,%r14d
+++	andl	%r11d,%r12d
+++	xorl	%r11d,%r13d
+++	vpsrld	$10,%xmm7,%xmm6
+++	addl	20(%rsp),%ecx
+++	movl	%edx,%edi
+++	xorl	%ebx,%r12d
+++	vpxor	%xmm5,%xmm4,%xmm4
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r8d,%edi
+++	addl	%r12d,%ecx
+++	vpsrlq	$17,%xmm7,%xmm7
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%edx,%r14d
+++	vpaddd	%xmm4,%xmm1,%xmm1
+++	addl	%r13d,%ecx
+++	xorl	%r8d,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	addl	%ecx,%r10d
+++	addl	%r15d,%ecx
+++	movl	%r10d,%r13d
+++	vpsrlq	$2,%xmm7,%xmm7
+++	addl	%ecx,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%ecx
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	movl	%r11d,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r10d,%r13d
+++	vpshufb	%xmm8,%xmm6,%xmm6
+++	xorl	%eax,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%ecx,%r14d
+++	vpaddd	%xmm6,%xmm1,%xmm1
+++	andl	%r10d,%r12d
+++	xorl	%r10d,%r13d
+++	addl	24(%rsp),%ebx
+++	vpshufd	$80,%xmm1,%xmm7
+++	movl	%ecx,%r15d
+++	xorl	%eax,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	vpsrld	$10,%xmm7,%xmm6
+++	xorl	%edx,%r15d
+++	addl	%r12d,%ebx
+++	shrdl	$6,%r13d,%r13d
+++	vpsrlq	$17,%xmm7,%xmm7
+++	andl	%r15d,%edi
+++	xorl	%ecx,%r14d
+++	addl	%r13d,%ebx
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	xorl	%edx,%edi
+++	shrdl	$2,%r14d,%r14d
+++	addl	%ebx,%r9d
+++	vpsrlq	$2,%xmm7,%xmm7
+++	addl	%edi,%ebx
+++	movl	%r9d,%r13d
+++	addl	%ebx,%r14d
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%ebx
+++	movl	%r10d,%r12d
+++	vpshufb	%xmm9,%xmm6,%xmm6
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r9d,%r13d
+++	xorl	%r11d,%r12d
+++	vpaddd	%xmm6,%xmm1,%xmm1
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%ebx,%r14d
+++	andl	%r9d,%r12d
+++	vpaddd	32(%rbp),%xmm1,%xmm6
+++	xorl	%r9d,%r13d
+++	addl	28(%rsp),%eax
+++	movl	%ebx,%edi
+++	xorl	%r11d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%ecx,%edi
+++	addl	%r12d,%eax
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%ebx,%r14d
+++	addl	%r13d,%eax
+++	xorl	%ecx,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	addl	%eax,%r8d
+++	addl	%r15d,%eax
+++	movl	%r8d,%r13d
+++	addl	%eax,%r14d
+++	vmovdqa	%xmm6,16(%rsp)
+++	vpalignr	$4,%xmm2,%xmm3,%xmm4
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%eax
+++	movl	%r9d,%r12d
+++	vpalignr	$4,%xmm0,%xmm1,%xmm7
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r8d,%r13d
+++	xorl	%r10d,%r12d
+++	vpsrld	$7,%xmm4,%xmm6
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%eax,%r14d
+++	andl	%r8d,%r12d
+++	vpaddd	%xmm7,%xmm2,%xmm2
+++	xorl	%r8d,%r13d
+++	addl	32(%rsp),%r11d
+++	movl	%eax,%r15d
+++	vpsrld	$3,%xmm4,%xmm7
+++	xorl	%r10d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%ebx,%r15d
+++	vpslld	$14,%xmm4,%xmm5
+++	addl	%r12d,%r11d
+++	shrdl	$6,%r13d,%r13d
+++	andl	%r15d,%edi
+++	vpxor	%xmm6,%xmm7,%xmm4
+++	xorl	%eax,%r14d
+++	addl	%r13d,%r11d
+++	xorl	%ebx,%edi
+++	vpshufd	$250,%xmm1,%xmm7
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r11d,%edx
+++	addl	%edi,%r11d
+++	vpsrld	$11,%xmm6,%xmm6
+++	movl	%edx,%r13d
+++	addl	%r11d,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	vpxor	%xmm5,%xmm4,%xmm4
+++	movl	%r14d,%r11d
+++	movl	%r8d,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	vpslld	$11,%xmm5,%xmm5
+++	xorl	%edx,%r13d
+++	xorl	%r9d,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	vpxor	%xmm6,%xmm4,%xmm4
+++	xorl	%r11d,%r14d
+++	andl	%edx,%r12d
+++	xorl	%edx,%r13d
+++	vpsrld	$10,%xmm7,%xmm6
+++	addl	36(%rsp),%r10d
+++	movl	%r11d,%edi
+++	xorl	%r9d,%r12d
+++	vpxor	%xmm5,%xmm4,%xmm4
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%eax,%edi
+++	addl	%r12d,%r10d
+++	vpsrlq	$17,%xmm7,%xmm7
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r11d,%r14d
+++	vpaddd	%xmm4,%xmm2,%xmm2
+++	addl	%r13d,%r10d
+++	xorl	%eax,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	addl	%r10d,%ecx
+++	addl	%r15d,%r10d
+++	movl	%ecx,%r13d
+++	vpsrlq	$2,%xmm7,%xmm7
+++	addl	%r10d,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r10d
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	movl	%edx,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%ecx,%r13d
+++	vpshufb	%xmm8,%xmm6,%xmm6
+++	xorl	%r8d,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r10d,%r14d
+++	vpaddd	%xmm6,%xmm2,%xmm2
+++	andl	%ecx,%r12d
+++	xorl	%ecx,%r13d
+++	addl	40(%rsp),%r9d
+++	vpshufd	$80,%xmm2,%xmm7
+++	movl	%r10d,%r15d
+++	xorl	%r8d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	vpsrld	$10,%xmm7,%xmm6
+++	xorl	%r11d,%r15d
+++	addl	%r12d,%r9d
+++	shrdl	$6,%r13d,%r13d
+++	vpsrlq	$17,%xmm7,%xmm7
+++	andl	%r15d,%edi
+++	xorl	%r10d,%r14d
+++	addl	%r13d,%r9d
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	xorl	%r11d,%edi
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r9d,%ebx
+++	vpsrlq	$2,%xmm7,%xmm7
+++	addl	%edi,%r9d
+++	movl	%ebx,%r13d
+++	addl	%r9d,%r14d
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r9d
+++	movl	%ecx,%r12d
+++	vpshufb	%xmm9,%xmm6,%xmm6
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%ebx,%r13d
+++	xorl	%edx,%r12d
+++	vpaddd	%xmm6,%xmm2,%xmm2
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r9d,%r14d
+++	andl	%ebx,%r12d
+++	vpaddd	64(%rbp),%xmm2,%xmm6
+++	xorl	%ebx,%r13d
+++	addl	44(%rsp),%r8d
+++	movl	%r9d,%edi
+++	xorl	%edx,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r10d,%edi
+++	addl	%r12d,%r8d
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r9d,%r14d
+++	addl	%r13d,%r8d
+++	xorl	%r10d,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r8d,%eax
+++	addl	%r15d,%r8d
+++	movl	%eax,%r13d
+++	addl	%r8d,%r14d
+++	vmovdqa	%xmm6,32(%rsp)
+++	vpalignr	$4,%xmm3,%xmm0,%xmm4
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r8d
+++	movl	%ebx,%r12d
+++	vpalignr	$4,%xmm1,%xmm2,%xmm7
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%eax,%r13d
+++	xorl	%ecx,%r12d
+++	vpsrld	$7,%xmm4,%xmm6
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r8d,%r14d
+++	andl	%eax,%r12d
+++	vpaddd	%xmm7,%xmm3,%xmm3
+++	xorl	%eax,%r13d
+++	addl	48(%rsp),%edx
+++	movl	%r8d,%r15d
+++	vpsrld	$3,%xmm4,%xmm7
+++	xorl	%ecx,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r9d,%r15d
+++	vpslld	$14,%xmm4,%xmm5
+++	addl	%r12d,%edx
+++	shrdl	$6,%r13d,%r13d
+++	andl	%r15d,%edi
+++	vpxor	%xmm6,%xmm7,%xmm4
+++	xorl	%r8d,%r14d
+++	addl	%r13d,%edx
+++	xorl	%r9d,%edi
+++	vpshufd	$250,%xmm2,%xmm7
+++	shrdl	$2,%r14d,%r14d
+++	addl	%edx,%r11d
+++	addl	%edi,%edx
+++	vpsrld	$11,%xmm6,%xmm6
+++	movl	%r11d,%r13d
+++	addl	%edx,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	vpxor	%xmm5,%xmm4,%xmm4
+++	movl	%r14d,%edx
+++	movl	%eax,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	vpslld	$11,%xmm5,%xmm5
+++	xorl	%r11d,%r13d
+++	xorl	%ebx,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	vpxor	%xmm6,%xmm4,%xmm4
+++	xorl	%edx,%r14d
+++	andl	%r11d,%r12d
+++	xorl	%r11d,%r13d
+++	vpsrld	$10,%xmm7,%xmm6
+++	addl	52(%rsp),%ecx
+++	movl	%edx,%edi
+++	xorl	%ebx,%r12d
+++	vpxor	%xmm5,%xmm4,%xmm4
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r8d,%edi
+++	addl	%r12d,%ecx
+++	vpsrlq	$17,%xmm7,%xmm7
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%edx,%r14d
+++	vpaddd	%xmm4,%xmm3,%xmm3
+++	addl	%r13d,%ecx
+++	xorl	%r8d,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	addl	%ecx,%r10d
+++	addl	%r15d,%ecx
+++	movl	%r10d,%r13d
+++	vpsrlq	$2,%xmm7,%xmm7
+++	addl	%ecx,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%ecx
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	movl	%r11d,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r10d,%r13d
+++	vpshufb	%xmm8,%xmm6,%xmm6
+++	xorl	%eax,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%ecx,%r14d
+++	vpaddd	%xmm6,%xmm3,%xmm3
+++	andl	%r10d,%r12d
+++	xorl	%r10d,%r13d
+++	addl	56(%rsp),%ebx
+++	vpshufd	$80,%xmm3,%xmm7
+++	movl	%ecx,%r15d
+++	xorl	%eax,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	vpsrld	$10,%xmm7,%xmm6
+++	xorl	%edx,%r15d
+++	addl	%r12d,%ebx
+++	shrdl	$6,%r13d,%r13d
+++	vpsrlq	$17,%xmm7,%xmm7
+++	andl	%r15d,%edi
+++	xorl	%ecx,%r14d
+++	addl	%r13d,%ebx
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	xorl	%edx,%edi
+++	shrdl	$2,%r14d,%r14d
+++	addl	%ebx,%r9d
+++	vpsrlq	$2,%xmm7,%xmm7
+++	addl	%edi,%ebx
+++	movl	%r9d,%r13d
+++	addl	%ebx,%r14d
+++	vpxor	%xmm7,%xmm6,%xmm6
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%ebx
+++	movl	%r10d,%r12d
+++	vpshufb	%xmm9,%xmm6,%xmm6
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r9d,%r13d
+++	xorl	%r11d,%r12d
+++	vpaddd	%xmm6,%xmm3,%xmm3
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%ebx,%r14d
+++	andl	%r9d,%r12d
+++	vpaddd	96(%rbp),%xmm3,%xmm6
+++	xorl	%r9d,%r13d
+++	addl	60(%rsp),%eax
+++	movl	%ebx,%edi
+++	xorl	%r11d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%ecx,%edi
+++	addl	%r12d,%eax
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%ebx,%r14d
+++	addl	%r13d,%eax
+++	xorl	%ecx,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	addl	%eax,%r8d
+++	addl	%r15d,%eax
+++	movl	%r8d,%r13d
+++	addl	%eax,%r14d
+++	vmovdqa	%xmm6,48(%rsp)
+++	cmpb	$0,131(%rbp)
+++	jne	.Lavx_00_47
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%eax
+++	movl	%r9d,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r8d,%r13d
+++	xorl	%r10d,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%eax,%r14d
+++	andl	%r8d,%r12d
+++	xorl	%r8d,%r13d
+++	addl	0(%rsp),%r11d
+++	movl	%eax,%r15d
+++	xorl	%r10d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%ebx,%r15d
+++	addl	%r12d,%r11d
+++	shrdl	$6,%r13d,%r13d
+++	andl	%r15d,%edi
+++	xorl	%eax,%r14d
+++	addl	%r13d,%r11d
+++	xorl	%ebx,%edi
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r11d,%edx
+++	addl	%edi,%r11d
+++	movl	%edx,%r13d
+++	addl	%r11d,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r11d
+++	movl	%r8d,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%edx,%r13d
+++	xorl	%r9d,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r11d,%r14d
+++	andl	%edx,%r12d
+++	xorl	%edx,%r13d
+++	addl	4(%rsp),%r10d
+++	movl	%r11d,%edi
+++	xorl	%r9d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%eax,%edi
+++	addl	%r12d,%r10d
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r11d,%r14d
+++	addl	%r13d,%r10d
+++	xorl	%eax,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r10d,%ecx
+++	addl	%r15d,%r10d
+++	movl	%ecx,%r13d
+++	addl	%r10d,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r10d
+++	movl	%edx,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%ecx,%r13d
+++	xorl	%r8d,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r10d,%r14d
+++	andl	%ecx,%r12d
+++	xorl	%ecx,%r13d
+++	addl	8(%rsp),%r9d
+++	movl	%r10d,%r15d
+++	xorl	%r8d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r11d,%r15d
+++	addl	%r12d,%r9d
+++	shrdl	$6,%r13d,%r13d
+++	andl	%r15d,%edi
+++	xorl	%r10d,%r14d
+++	addl	%r13d,%r9d
+++	xorl	%r11d,%edi
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r9d,%ebx
+++	addl	%edi,%r9d
+++	movl	%ebx,%r13d
+++	addl	%r9d,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r9d
+++	movl	%ecx,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%ebx,%r13d
+++	xorl	%edx,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r9d,%r14d
+++	andl	%ebx,%r12d
+++	xorl	%ebx,%r13d
+++	addl	12(%rsp),%r8d
+++	movl	%r9d,%edi
+++	xorl	%edx,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r10d,%edi
+++	addl	%r12d,%r8d
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r9d,%r14d
+++	addl	%r13d,%r8d
+++	xorl	%r10d,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r8d,%eax
+++	addl	%r15d,%r8d
+++	movl	%eax,%r13d
+++	addl	%r8d,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r8d
+++	movl	%ebx,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%eax,%r13d
+++	xorl	%ecx,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r8d,%r14d
+++	andl	%eax,%r12d
+++	xorl	%eax,%r13d
+++	addl	16(%rsp),%edx
+++	movl	%r8d,%r15d
+++	xorl	%ecx,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r9d,%r15d
+++	addl	%r12d,%edx
+++	shrdl	$6,%r13d,%r13d
+++	andl	%r15d,%edi
+++	xorl	%r8d,%r14d
+++	addl	%r13d,%edx
+++	xorl	%r9d,%edi
+++	shrdl	$2,%r14d,%r14d
+++	addl	%edx,%r11d
+++	addl	%edi,%edx
+++	movl	%r11d,%r13d
+++	addl	%edx,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%edx
+++	movl	%eax,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r11d,%r13d
+++	xorl	%ebx,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%edx,%r14d
+++	andl	%r11d,%r12d
+++	xorl	%r11d,%r13d
+++	addl	20(%rsp),%ecx
+++	movl	%edx,%edi
+++	xorl	%ebx,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r8d,%edi
+++	addl	%r12d,%ecx
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%edx,%r14d
+++	addl	%r13d,%ecx
+++	xorl	%r8d,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	addl	%ecx,%r10d
+++	addl	%r15d,%ecx
+++	movl	%r10d,%r13d
+++	addl	%ecx,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%ecx
+++	movl	%r11d,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r10d,%r13d
+++	xorl	%eax,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%ecx,%r14d
+++	andl	%r10d,%r12d
+++	xorl	%r10d,%r13d
+++	addl	24(%rsp),%ebx
+++	movl	%ecx,%r15d
+++	xorl	%eax,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%edx,%r15d
+++	addl	%r12d,%ebx
+++	shrdl	$6,%r13d,%r13d
+++	andl	%r15d,%edi
+++	xorl	%ecx,%r14d
+++	addl	%r13d,%ebx
+++	xorl	%edx,%edi
+++	shrdl	$2,%r14d,%r14d
+++	addl	%ebx,%r9d
+++	addl	%edi,%ebx
+++	movl	%r9d,%r13d
+++	addl	%ebx,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%ebx
+++	movl	%r10d,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r9d,%r13d
+++	xorl	%r11d,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%ebx,%r14d
+++	andl	%r9d,%r12d
+++	xorl	%r9d,%r13d
+++	addl	28(%rsp),%eax
+++	movl	%ebx,%edi
+++	xorl	%r11d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%ecx,%edi
+++	addl	%r12d,%eax
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%ebx,%r14d
+++	addl	%r13d,%eax
+++	xorl	%ecx,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	addl	%eax,%r8d
+++	addl	%r15d,%eax
+++	movl	%r8d,%r13d
+++	addl	%eax,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%eax
+++	movl	%r9d,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r8d,%r13d
+++	xorl	%r10d,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%eax,%r14d
+++	andl	%r8d,%r12d
+++	xorl	%r8d,%r13d
+++	addl	32(%rsp),%r11d
+++	movl	%eax,%r15d
+++	xorl	%r10d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%ebx,%r15d
+++	addl	%r12d,%r11d
+++	shrdl	$6,%r13d,%r13d
+++	andl	%r15d,%edi
+++	xorl	%eax,%r14d
+++	addl	%r13d,%r11d
+++	xorl	%ebx,%edi
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r11d,%edx
+++	addl	%edi,%r11d
+++	movl	%edx,%r13d
+++	addl	%r11d,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r11d
+++	movl	%r8d,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%edx,%r13d
+++	xorl	%r9d,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r11d,%r14d
+++	andl	%edx,%r12d
+++	xorl	%edx,%r13d
+++	addl	36(%rsp),%r10d
+++	movl	%r11d,%edi
+++	xorl	%r9d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%eax,%edi
+++	addl	%r12d,%r10d
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r11d,%r14d
+++	addl	%r13d,%r10d
+++	xorl	%eax,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r10d,%ecx
+++	addl	%r15d,%r10d
+++	movl	%ecx,%r13d
+++	addl	%r10d,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r10d
+++	movl	%edx,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%ecx,%r13d
+++	xorl	%r8d,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r10d,%r14d
+++	andl	%ecx,%r12d
+++	xorl	%ecx,%r13d
+++	addl	40(%rsp),%r9d
+++	movl	%r10d,%r15d
+++	xorl	%r8d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r11d,%r15d
+++	addl	%r12d,%r9d
+++	shrdl	$6,%r13d,%r13d
+++	andl	%r15d,%edi
+++	xorl	%r10d,%r14d
+++	addl	%r13d,%r9d
+++	xorl	%r11d,%edi
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r9d,%ebx
+++	addl	%edi,%r9d
+++	movl	%ebx,%r13d
+++	addl	%r9d,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r9d
+++	movl	%ecx,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%ebx,%r13d
+++	xorl	%edx,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r9d,%r14d
+++	andl	%ebx,%r12d
+++	xorl	%ebx,%r13d
+++	addl	44(%rsp),%r8d
+++	movl	%r9d,%edi
+++	xorl	%edx,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r10d,%edi
+++	addl	%r12d,%r8d
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%r9d,%r14d
+++	addl	%r13d,%r8d
+++	xorl	%r10d,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	addl	%r8d,%eax
+++	addl	%r15d,%r8d
+++	movl	%eax,%r13d
+++	addl	%r8d,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%r8d
+++	movl	%ebx,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%eax,%r13d
+++	xorl	%ecx,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%r8d,%r14d
+++	andl	%eax,%r12d
+++	xorl	%eax,%r13d
+++	addl	48(%rsp),%edx
+++	movl	%r8d,%r15d
+++	xorl	%ecx,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r9d,%r15d
+++	addl	%r12d,%edx
+++	shrdl	$6,%r13d,%r13d
+++	andl	%r15d,%edi
+++	xorl	%r8d,%r14d
+++	addl	%r13d,%edx
+++	xorl	%r9d,%edi
+++	shrdl	$2,%r14d,%r14d
+++	addl	%edx,%r11d
+++	addl	%edi,%edx
+++	movl	%r11d,%r13d
+++	addl	%edx,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%edx
+++	movl	%eax,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r11d,%r13d
+++	xorl	%ebx,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%edx,%r14d
+++	andl	%r11d,%r12d
+++	xorl	%r11d,%r13d
+++	addl	52(%rsp),%ecx
+++	movl	%edx,%edi
+++	xorl	%ebx,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%r8d,%edi
+++	addl	%r12d,%ecx
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%edx,%r14d
+++	addl	%r13d,%ecx
+++	xorl	%r8d,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	addl	%ecx,%r10d
+++	addl	%r15d,%ecx
+++	movl	%r10d,%r13d
+++	addl	%ecx,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%ecx
+++	movl	%r11d,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r10d,%r13d
+++	xorl	%eax,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%ecx,%r14d
+++	andl	%r10d,%r12d
+++	xorl	%r10d,%r13d
+++	addl	56(%rsp),%ebx
+++	movl	%ecx,%r15d
+++	xorl	%eax,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%edx,%r15d
+++	addl	%r12d,%ebx
+++	shrdl	$6,%r13d,%r13d
+++	andl	%r15d,%edi
+++	xorl	%ecx,%r14d
+++	addl	%r13d,%ebx
+++	xorl	%edx,%edi
+++	shrdl	$2,%r14d,%r14d
+++	addl	%ebx,%r9d
+++	addl	%edi,%ebx
+++	movl	%r9d,%r13d
+++	addl	%ebx,%r14d
+++	shrdl	$14,%r13d,%r13d
+++	movl	%r14d,%ebx
+++	movl	%r10d,%r12d
+++	shrdl	$9,%r14d,%r14d
+++	xorl	%r9d,%r13d
+++	xorl	%r11d,%r12d
+++	shrdl	$5,%r13d,%r13d
+++	xorl	%ebx,%r14d
+++	andl	%r9d,%r12d
+++	xorl	%r9d,%r13d
+++	addl	60(%rsp),%eax
+++	movl	%ebx,%edi
+++	xorl	%r11d,%r12d
+++	shrdl	$11,%r14d,%r14d
+++	xorl	%ecx,%edi
+++	addl	%r12d,%eax
+++	shrdl	$6,%r13d,%r13d
+++	andl	%edi,%r15d
+++	xorl	%ebx,%r14d
+++	addl	%r13d,%eax
+++	xorl	%ecx,%r15d
+++	shrdl	$2,%r14d,%r14d
+++	addl	%eax,%r8d
+++	addl	%r15d,%eax
+++	movl	%r8d,%r13d
+++	addl	%eax,%r14d
+++	movq	64+0(%rsp),%rdi
+++	movl	%r14d,%eax
+++
+++	addl	0(%rdi),%eax
+++	leaq	64(%rsi),%rsi
+++	addl	4(%rdi),%ebx
+++	addl	8(%rdi),%ecx
+++	addl	12(%rdi),%edx
+++	addl	16(%rdi),%r8d
+++	addl	20(%rdi),%r9d
+++	addl	24(%rdi),%r10d
+++	addl	28(%rdi),%r11d
+++
+++	cmpq	64+16(%rsp),%rsi
+++
+++	movl	%eax,0(%rdi)
+++	movl	%ebx,4(%rdi)
+++	movl	%ecx,8(%rdi)
+++	movl	%edx,12(%rdi)
+++	movl	%r8d,16(%rdi)
+++	movl	%r9d,20(%rdi)
+++	movl	%r10d,24(%rdi)
+++	movl	%r11d,28(%rdi)
+++	jb	.Lloop_avx
+++
+++	movq	88(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	vzeroupper
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lepilogue_avx:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	sha256_block_data_order_avx,.-sha256_block_data_order_avx
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/sha512-x86_64.S b/linux-x86_64/ypto/fipsmodule/sha512-x86_64.S
++new file mode 100644
++index 000000000..afc47f139
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/sha512-x86_64.S
++@@ -0,0 +1,2992 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++
+++.extern	OPENSSL_ia32cap_P
+++.hidden OPENSSL_ia32cap_P
+++.globl	sha512_block_data_order
+++.hidden sha512_block_data_order
+++.type	sha512_block_data_order,@function
+++.align	16
+++sha512_block_data_order:
+++.cfi_startproc	
+++	leaq	OPENSSL_ia32cap_P(%rip),%r11
+++	movl	0(%r11),%r9d
+++	movl	4(%r11),%r10d
+++	movl	8(%r11),%r11d
+++	andl	$1073741824,%r9d
+++	andl	$268435968,%r10d
+++	orl	%r9d,%r10d
+++	cmpl	$1342177792,%r10d
+++	je	.Lavx_shortcut
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++	shlq	$4,%rdx
+++	subq	$128+32,%rsp
+++	leaq	(%rsi,%rdx,8),%rdx
+++	andq	$-64,%rsp
+++	movq	%rdi,128+0(%rsp)
+++	movq	%rsi,128+8(%rsp)
+++	movq	%rdx,128+16(%rsp)
+++	movq	%rax,152(%rsp)
+++.cfi_escape	0x0f,0x06,0x77,0x98,0x01,0x06,0x23,0x08
+++.Lprologue:
+++
+++	movq	0(%rdi),%rax
+++	movq	8(%rdi),%rbx
+++	movq	16(%rdi),%rcx
+++	movq	24(%rdi),%rdx
+++	movq	32(%rdi),%r8
+++	movq	40(%rdi),%r9
+++	movq	48(%rdi),%r10
+++	movq	56(%rdi),%r11
+++	jmp	.Lloop
+++
+++.align	16
+++.Lloop:
+++	movq	%rbx,%rdi
+++	leaq	K512(%rip),%rbp
+++	xorq	%rcx,%rdi
+++	movq	0(%rsi),%r12
+++	movq	%r8,%r13
+++	movq	%rax,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%r9,%r15
+++
+++	xorq	%r8,%r13
+++	rorq	$5,%r14
+++	xorq	%r10,%r15
+++
+++	movq	%r12,0(%rsp)
+++	xorq	%rax,%r14
+++	andq	%r8,%r15
+++
+++	rorq	$4,%r13
+++	addq	%r11,%r12
+++	xorq	%r10,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%r8,%r13
+++	addq	%r15,%r12
+++
+++	movq	%rax,%r15
+++	addq	(%rbp),%r12
+++	xorq	%rax,%r14
+++
+++	xorq	%rbx,%r15
+++	rorq	$14,%r13
+++	movq	%rbx,%r11
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%r11
+++	addq	%r12,%rdx
+++	addq	%r12,%r11
+++
+++	leaq	8(%rbp),%rbp
+++	addq	%r14,%r11
+++	movq	8(%rsi),%r12
+++	movq	%rdx,%r13
+++	movq	%r11,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%r8,%rdi
+++
+++	xorq	%rdx,%r13
+++	rorq	$5,%r14
+++	xorq	%r9,%rdi
+++
+++	movq	%r12,8(%rsp)
+++	xorq	%r11,%r14
+++	andq	%rdx,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%r10,%r12
+++	xorq	%r9,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%rdx,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%r11,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%r11,%r14
+++
+++	xorq	%rax,%rdi
+++	rorq	$14,%r13
+++	movq	%rax,%r10
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%r10
+++	addq	%r12,%rcx
+++	addq	%r12,%r10
+++
+++	leaq	24(%rbp),%rbp
+++	addq	%r14,%r10
+++	movq	16(%rsi),%r12
+++	movq	%rcx,%r13
+++	movq	%r10,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%rdx,%r15
+++
+++	xorq	%rcx,%r13
+++	rorq	$5,%r14
+++	xorq	%r8,%r15
+++
+++	movq	%r12,16(%rsp)
+++	xorq	%r10,%r14
+++	andq	%rcx,%r15
+++
+++	rorq	$4,%r13
+++	addq	%r9,%r12
+++	xorq	%r8,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%rcx,%r13
+++	addq	%r15,%r12
+++
+++	movq	%r10,%r15
+++	addq	(%rbp),%r12
+++	xorq	%r10,%r14
+++
+++	xorq	%r11,%r15
+++	rorq	$14,%r13
+++	movq	%r11,%r9
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%r9
+++	addq	%r12,%rbx
+++	addq	%r12,%r9
+++
+++	leaq	8(%rbp),%rbp
+++	addq	%r14,%r9
+++	movq	24(%rsi),%r12
+++	movq	%rbx,%r13
+++	movq	%r9,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%rcx,%rdi
+++
+++	xorq	%rbx,%r13
+++	rorq	$5,%r14
+++	xorq	%rdx,%rdi
+++
+++	movq	%r12,24(%rsp)
+++	xorq	%r9,%r14
+++	andq	%rbx,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%r8,%r12
+++	xorq	%rdx,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%rbx,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%r9,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%r9,%r14
+++
+++	xorq	%r10,%rdi
+++	rorq	$14,%r13
+++	movq	%r10,%r8
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%r8
+++	addq	%r12,%rax
+++	addq	%r12,%r8
+++
+++	leaq	24(%rbp),%rbp
+++	addq	%r14,%r8
+++	movq	32(%rsi),%r12
+++	movq	%rax,%r13
+++	movq	%r8,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%rbx,%r15
+++
+++	xorq	%rax,%r13
+++	rorq	$5,%r14
+++	xorq	%rcx,%r15
+++
+++	movq	%r12,32(%rsp)
+++	xorq	%r8,%r14
+++	andq	%rax,%r15
+++
+++	rorq	$4,%r13
+++	addq	%rdx,%r12
+++	xorq	%rcx,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%rax,%r13
+++	addq	%r15,%r12
+++
+++	movq	%r8,%r15
+++	addq	(%rbp),%r12
+++	xorq	%r8,%r14
+++
+++	xorq	%r9,%r15
+++	rorq	$14,%r13
+++	movq	%r9,%rdx
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%rdx
+++	addq	%r12,%r11
+++	addq	%r12,%rdx
+++
+++	leaq	8(%rbp),%rbp
+++	addq	%r14,%rdx
+++	movq	40(%rsi),%r12
+++	movq	%r11,%r13
+++	movq	%rdx,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%rax,%rdi
+++
+++	xorq	%r11,%r13
+++	rorq	$5,%r14
+++	xorq	%rbx,%rdi
+++
+++	movq	%r12,40(%rsp)
+++	xorq	%rdx,%r14
+++	andq	%r11,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%rcx,%r12
+++	xorq	%rbx,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%r11,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%rdx,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%rdx,%r14
+++
+++	xorq	%r8,%rdi
+++	rorq	$14,%r13
+++	movq	%r8,%rcx
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%rcx
+++	addq	%r12,%r10
+++	addq	%r12,%rcx
+++
+++	leaq	24(%rbp),%rbp
+++	addq	%r14,%rcx
+++	movq	48(%rsi),%r12
+++	movq	%r10,%r13
+++	movq	%rcx,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%r11,%r15
+++
+++	xorq	%r10,%r13
+++	rorq	$5,%r14
+++	xorq	%rax,%r15
+++
+++	movq	%r12,48(%rsp)
+++	xorq	%rcx,%r14
+++	andq	%r10,%r15
+++
+++	rorq	$4,%r13
+++	addq	%rbx,%r12
+++	xorq	%rax,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%r10,%r13
+++	addq	%r15,%r12
+++
+++	movq	%rcx,%r15
+++	addq	(%rbp),%r12
+++	xorq	%rcx,%r14
+++
+++	xorq	%rdx,%r15
+++	rorq	$14,%r13
+++	movq	%rdx,%rbx
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%rbx
+++	addq	%r12,%r9
+++	addq	%r12,%rbx
+++
+++	leaq	8(%rbp),%rbp
+++	addq	%r14,%rbx
+++	movq	56(%rsi),%r12
+++	movq	%r9,%r13
+++	movq	%rbx,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%r10,%rdi
+++
+++	xorq	%r9,%r13
+++	rorq	$5,%r14
+++	xorq	%r11,%rdi
+++
+++	movq	%r12,56(%rsp)
+++	xorq	%rbx,%r14
+++	andq	%r9,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%rax,%r12
+++	xorq	%r11,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%r9,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%rbx,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%rbx,%r14
+++
+++	xorq	%rcx,%rdi
+++	rorq	$14,%r13
+++	movq	%rcx,%rax
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%rax
+++	addq	%r12,%r8
+++	addq	%r12,%rax
+++
+++	leaq	24(%rbp),%rbp
+++	addq	%r14,%rax
+++	movq	64(%rsi),%r12
+++	movq	%r8,%r13
+++	movq	%rax,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%r9,%r15
+++
+++	xorq	%r8,%r13
+++	rorq	$5,%r14
+++	xorq	%r10,%r15
+++
+++	movq	%r12,64(%rsp)
+++	xorq	%rax,%r14
+++	andq	%r8,%r15
+++
+++	rorq	$4,%r13
+++	addq	%r11,%r12
+++	xorq	%r10,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%r8,%r13
+++	addq	%r15,%r12
+++
+++	movq	%rax,%r15
+++	addq	(%rbp),%r12
+++	xorq	%rax,%r14
+++
+++	xorq	%rbx,%r15
+++	rorq	$14,%r13
+++	movq	%rbx,%r11
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%r11
+++	addq	%r12,%rdx
+++	addq	%r12,%r11
+++
+++	leaq	8(%rbp),%rbp
+++	addq	%r14,%r11
+++	movq	72(%rsi),%r12
+++	movq	%rdx,%r13
+++	movq	%r11,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%r8,%rdi
+++
+++	xorq	%rdx,%r13
+++	rorq	$5,%r14
+++	xorq	%r9,%rdi
+++
+++	movq	%r12,72(%rsp)
+++	xorq	%r11,%r14
+++	andq	%rdx,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%r10,%r12
+++	xorq	%r9,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%rdx,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%r11,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%r11,%r14
+++
+++	xorq	%rax,%rdi
+++	rorq	$14,%r13
+++	movq	%rax,%r10
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%r10
+++	addq	%r12,%rcx
+++	addq	%r12,%r10
+++
+++	leaq	24(%rbp),%rbp
+++	addq	%r14,%r10
+++	movq	80(%rsi),%r12
+++	movq	%rcx,%r13
+++	movq	%r10,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%rdx,%r15
+++
+++	xorq	%rcx,%r13
+++	rorq	$5,%r14
+++	xorq	%r8,%r15
+++
+++	movq	%r12,80(%rsp)
+++	xorq	%r10,%r14
+++	andq	%rcx,%r15
+++
+++	rorq	$4,%r13
+++	addq	%r9,%r12
+++	xorq	%r8,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%rcx,%r13
+++	addq	%r15,%r12
+++
+++	movq	%r10,%r15
+++	addq	(%rbp),%r12
+++	xorq	%r10,%r14
+++
+++	xorq	%r11,%r15
+++	rorq	$14,%r13
+++	movq	%r11,%r9
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%r9
+++	addq	%r12,%rbx
+++	addq	%r12,%r9
+++
+++	leaq	8(%rbp),%rbp
+++	addq	%r14,%r9
+++	movq	88(%rsi),%r12
+++	movq	%rbx,%r13
+++	movq	%r9,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%rcx,%rdi
+++
+++	xorq	%rbx,%r13
+++	rorq	$5,%r14
+++	xorq	%rdx,%rdi
+++
+++	movq	%r12,88(%rsp)
+++	xorq	%r9,%r14
+++	andq	%rbx,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%r8,%r12
+++	xorq	%rdx,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%rbx,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%r9,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%r9,%r14
+++
+++	xorq	%r10,%rdi
+++	rorq	$14,%r13
+++	movq	%r10,%r8
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%r8
+++	addq	%r12,%rax
+++	addq	%r12,%r8
+++
+++	leaq	24(%rbp),%rbp
+++	addq	%r14,%r8
+++	movq	96(%rsi),%r12
+++	movq	%rax,%r13
+++	movq	%r8,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%rbx,%r15
+++
+++	xorq	%rax,%r13
+++	rorq	$5,%r14
+++	xorq	%rcx,%r15
+++
+++	movq	%r12,96(%rsp)
+++	xorq	%r8,%r14
+++	andq	%rax,%r15
+++
+++	rorq	$4,%r13
+++	addq	%rdx,%r12
+++	xorq	%rcx,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%rax,%r13
+++	addq	%r15,%r12
+++
+++	movq	%r8,%r15
+++	addq	(%rbp),%r12
+++	xorq	%r8,%r14
+++
+++	xorq	%r9,%r15
+++	rorq	$14,%r13
+++	movq	%r9,%rdx
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%rdx
+++	addq	%r12,%r11
+++	addq	%r12,%rdx
+++
+++	leaq	8(%rbp),%rbp
+++	addq	%r14,%rdx
+++	movq	104(%rsi),%r12
+++	movq	%r11,%r13
+++	movq	%rdx,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%rax,%rdi
+++
+++	xorq	%r11,%r13
+++	rorq	$5,%r14
+++	xorq	%rbx,%rdi
+++
+++	movq	%r12,104(%rsp)
+++	xorq	%rdx,%r14
+++	andq	%r11,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%rcx,%r12
+++	xorq	%rbx,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%r11,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%rdx,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%rdx,%r14
+++
+++	xorq	%r8,%rdi
+++	rorq	$14,%r13
+++	movq	%r8,%rcx
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%rcx
+++	addq	%r12,%r10
+++	addq	%r12,%rcx
+++
+++	leaq	24(%rbp),%rbp
+++	addq	%r14,%rcx
+++	movq	112(%rsi),%r12
+++	movq	%r10,%r13
+++	movq	%rcx,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%r11,%r15
+++
+++	xorq	%r10,%r13
+++	rorq	$5,%r14
+++	xorq	%rax,%r15
+++
+++	movq	%r12,112(%rsp)
+++	xorq	%rcx,%r14
+++	andq	%r10,%r15
+++
+++	rorq	$4,%r13
+++	addq	%rbx,%r12
+++	xorq	%rax,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%r10,%r13
+++	addq	%r15,%r12
+++
+++	movq	%rcx,%r15
+++	addq	(%rbp),%r12
+++	xorq	%rcx,%r14
+++
+++	xorq	%rdx,%r15
+++	rorq	$14,%r13
+++	movq	%rdx,%rbx
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%rbx
+++	addq	%r12,%r9
+++	addq	%r12,%rbx
+++
+++	leaq	8(%rbp),%rbp
+++	addq	%r14,%rbx
+++	movq	120(%rsi),%r12
+++	movq	%r9,%r13
+++	movq	%rbx,%r14
+++	bswapq	%r12
+++	rorq	$23,%r13
+++	movq	%r10,%rdi
+++
+++	xorq	%r9,%r13
+++	rorq	$5,%r14
+++	xorq	%r11,%rdi
+++
+++	movq	%r12,120(%rsp)
+++	xorq	%rbx,%r14
+++	andq	%r9,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%rax,%r12
+++	xorq	%r11,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%r9,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%rbx,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%rbx,%r14
+++
+++	xorq	%rcx,%rdi
+++	rorq	$14,%r13
+++	movq	%rcx,%rax
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%rax
+++	addq	%r12,%r8
+++	addq	%r12,%rax
+++
+++	leaq	24(%rbp),%rbp
+++	jmp	.Lrounds_16_xx
+++.align	16
+++.Lrounds_16_xx:
+++	movq	8(%rsp),%r13
+++	movq	112(%rsp),%r15
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%rax
+++	movq	%r15,%r14
+++	rorq	$42,%r15
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%r15
+++	shrq	$6,%r14
+++
+++	rorq	$19,%r15
+++	xorq	%r13,%r12
+++	xorq	%r14,%r15
+++	addq	72(%rsp),%r12
+++
+++	addq	0(%rsp),%r12
+++	movq	%r8,%r13
+++	addq	%r15,%r12
+++	movq	%rax,%r14
+++	rorq	$23,%r13
+++	movq	%r9,%r15
+++
+++	xorq	%r8,%r13
+++	rorq	$5,%r14
+++	xorq	%r10,%r15
+++
+++	movq	%r12,0(%rsp)
+++	xorq	%rax,%r14
+++	andq	%r8,%r15
+++
+++	rorq	$4,%r13
+++	addq	%r11,%r12
+++	xorq	%r10,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%r8,%r13
+++	addq	%r15,%r12
+++
+++	movq	%rax,%r15
+++	addq	(%rbp),%r12
+++	xorq	%rax,%r14
+++
+++	xorq	%rbx,%r15
+++	rorq	$14,%r13
+++	movq	%rbx,%r11
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%r11
+++	addq	%r12,%rdx
+++	addq	%r12,%r11
+++
+++	leaq	8(%rbp),%rbp
+++	movq	16(%rsp),%r13
+++	movq	120(%rsp),%rdi
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%r11
+++	movq	%rdi,%r14
+++	rorq	$42,%rdi
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%rdi
+++	shrq	$6,%r14
+++
+++	rorq	$19,%rdi
+++	xorq	%r13,%r12
+++	xorq	%r14,%rdi
+++	addq	80(%rsp),%r12
+++
+++	addq	8(%rsp),%r12
+++	movq	%rdx,%r13
+++	addq	%rdi,%r12
+++	movq	%r11,%r14
+++	rorq	$23,%r13
+++	movq	%r8,%rdi
+++
+++	xorq	%rdx,%r13
+++	rorq	$5,%r14
+++	xorq	%r9,%rdi
+++
+++	movq	%r12,8(%rsp)
+++	xorq	%r11,%r14
+++	andq	%rdx,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%r10,%r12
+++	xorq	%r9,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%rdx,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%r11,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%r11,%r14
+++
+++	xorq	%rax,%rdi
+++	rorq	$14,%r13
+++	movq	%rax,%r10
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%r10
+++	addq	%r12,%rcx
+++	addq	%r12,%r10
+++
+++	leaq	24(%rbp),%rbp
+++	movq	24(%rsp),%r13
+++	movq	0(%rsp),%r15
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%r10
+++	movq	%r15,%r14
+++	rorq	$42,%r15
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%r15
+++	shrq	$6,%r14
+++
+++	rorq	$19,%r15
+++	xorq	%r13,%r12
+++	xorq	%r14,%r15
+++	addq	88(%rsp),%r12
+++
+++	addq	16(%rsp),%r12
+++	movq	%rcx,%r13
+++	addq	%r15,%r12
+++	movq	%r10,%r14
+++	rorq	$23,%r13
+++	movq	%rdx,%r15
+++
+++	xorq	%rcx,%r13
+++	rorq	$5,%r14
+++	xorq	%r8,%r15
+++
+++	movq	%r12,16(%rsp)
+++	xorq	%r10,%r14
+++	andq	%rcx,%r15
+++
+++	rorq	$4,%r13
+++	addq	%r9,%r12
+++	xorq	%r8,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%rcx,%r13
+++	addq	%r15,%r12
+++
+++	movq	%r10,%r15
+++	addq	(%rbp),%r12
+++	xorq	%r10,%r14
+++
+++	xorq	%r11,%r15
+++	rorq	$14,%r13
+++	movq	%r11,%r9
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%r9
+++	addq	%r12,%rbx
+++	addq	%r12,%r9
+++
+++	leaq	8(%rbp),%rbp
+++	movq	32(%rsp),%r13
+++	movq	8(%rsp),%rdi
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%r9
+++	movq	%rdi,%r14
+++	rorq	$42,%rdi
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%rdi
+++	shrq	$6,%r14
+++
+++	rorq	$19,%rdi
+++	xorq	%r13,%r12
+++	xorq	%r14,%rdi
+++	addq	96(%rsp),%r12
+++
+++	addq	24(%rsp),%r12
+++	movq	%rbx,%r13
+++	addq	%rdi,%r12
+++	movq	%r9,%r14
+++	rorq	$23,%r13
+++	movq	%rcx,%rdi
+++
+++	xorq	%rbx,%r13
+++	rorq	$5,%r14
+++	xorq	%rdx,%rdi
+++
+++	movq	%r12,24(%rsp)
+++	xorq	%r9,%r14
+++	andq	%rbx,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%r8,%r12
+++	xorq	%rdx,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%rbx,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%r9,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%r9,%r14
+++
+++	xorq	%r10,%rdi
+++	rorq	$14,%r13
+++	movq	%r10,%r8
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%r8
+++	addq	%r12,%rax
+++	addq	%r12,%r8
+++
+++	leaq	24(%rbp),%rbp
+++	movq	40(%rsp),%r13
+++	movq	16(%rsp),%r15
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%r8
+++	movq	%r15,%r14
+++	rorq	$42,%r15
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%r15
+++	shrq	$6,%r14
+++
+++	rorq	$19,%r15
+++	xorq	%r13,%r12
+++	xorq	%r14,%r15
+++	addq	104(%rsp),%r12
+++
+++	addq	32(%rsp),%r12
+++	movq	%rax,%r13
+++	addq	%r15,%r12
+++	movq	%r8,%r14
+++	rorq	$23,%r13
+++	movq	%rbx,%r15
+++
+++	xorq	%rax,%r13
+++	rorq	$5,%r14
+++	xorq	%rcx,%r15
+++
+++	movq	%r12,32(%rsp)
+++	xorq	%r8,%r14
+++	andq	%rax,%r15
+++
+++	rorq	$4,%r13
+++	addq	%rdx,%r12
+++	xorq	%rcx,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%rax,%r13
+++	addq	%r15,%r12
+++
+++	movq	%r8,%r15
+++	addq	(%rbp),%r12
+++	xorq	%r8,%r14
+++
+++	xorq	%r9,%r15
+++	rorq	$14,%r13
+++	movq	%r9,%rdx
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%rdx
+++	addq	%r12,%r11
+++	addq	%r12,%rdx
+++
+++	leaq	8(%rbp),%rbp
+++	movq	48(%rsp),%r13
+++	movq	24(%rsp),%rdi
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%rdx
+++	movq	%rdi,%r14
+++	rorq	$42,%rdi
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%rdi
+++	shrq	$6,%r14
+++
+++	rorq	$19,%rdi
+++	xorq	%r13,%r12
+++	xorq	%r14,%rdi
+++	addq	112(%rsp),%r12
+++
+++	addq	40(%rsp),%r12
+++	movq	%r11,%r13
+++	addq	%rdi,%r12
+++	movq	%rdx,%r14
+++	rorq	$23,%r13
+++	movq	%rax,%rdi
+++
+++	xorq	%r11,%r13
+++	rorq	$5,%r14
+++	xorq	%rbx,%rdi
+++
+++	movq	%r12,40(%rsp)
+++	xorq	%rdx,%r14
+++	andq	%r11,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%rcx,%r12
+++	xorq	%rbx,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%r11,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%rdx,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%rdx,%r14
+++
+++	xorq	%r8,%rdi
+++	rorq	$14,%r13
+++	movq	%r8,%rcx
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%rcx
+++	addq	%r12,%r10
+++	addq	%r12,%rcx
+++
+++	leaq	24(%rbp),%rbp
+++	movq	56(%rsp),%r13
+++	movq	32(%rsp),%r15
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%rcx
+++	movq	%r15,%r14
+++	rorq	$42,%r15
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%r15
+++	shrq	$6,%r14
+++
+++	rorq	$19,%r15
+++	xorq	%r13,%r12
+++	xorq	%r14,%r15
+++	addq	120(%rsp),%r12
+++
+++	addq	48(%rsp),%r12
+++	movq	%r10,%r13
+++	addq	%r15,%r12
+++	movq	%rcx,%r14
+++	rorq	$23,%r13
+++	movq	%r11,%r15
+++
+++	xorq	%r10,%r13
+++	rorq	$5,%r14
+++	xorq	%rax,%r15
+++
+++	movq	%r12,48(%rsp)
+++	xorq	%rcx,%r14
+++	andq	%r10,%r15
+++
+++	rorq	$4,%r13
+++	addq	%rbx,%r12
+++	xorq	%rax,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%r10,%r13
+++	addq	%r15,%r12
+++
+++	movq	%rcx,%r15
+++	addq	(%rbp),%r12
+++	xorq	%rcx,%r14
+++
+++	xorq	%rdx,%r15
+++	rorq	$14,%r13
+++	movq	%rdx,%rbx
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%rbx
+++	addq	%r12,%r9
+++	addq	%r12,%rbx
+++
+++	leaq	8(%rbp),%rbp
+++	movq	64(%rsp),%r13
+++	movq	40(%rsp),%rdi
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%rbx
+++	movq	%rdi,%r14
+++	rorq	$42,%rdi
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%rdi
+++	shrq	$6,%r14
+++
+++	rorq	$19,%rdi
+++	xorq	%r13,%r12
+++	xorq	%r14,%rdi
+++	addq	0(%rsp),%r12
+++
+++	addq	56(%rsp),%r12
+++	movq	%r9,%r13
+++	addq	%rdi,%r12
+++	movq	%rbx,%r14
+++	rorq	$23,%r13
+++	movq	%r10,%rdi
+++
+++	xorq	%r9,%r13
+++	rorq	$5,%r14
+++	xorq	%r11,%rdi
+++
+++	movq	%r12,56(%rsp)
+++	xorq	%rbx,%r14
+++	andq	%r9,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%rax,%r12
+++	xorq	%r11,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%r9,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%rbx,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%rbx,%r14
+++
+++	xorq	%rcx,%rdi
+++	rorq	$14,%r13
+++	movq	%rcx,%rax
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%rax
+++	addq	%r12,%r8
+++	addq	%r12,%rax
+++
+++	leaq	24(%rbp),%rbp
+++	movq	72(%rsp),%r13
+++	movq	48(%rsp),%r15
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%rax
+++	movq	%r15,%r14
+++	rorq	$42,%r15
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%r15
+++	shrq	$6,%r14
+++
+++	rorq	$19,%r15
+++	xorq	%r13,%r12
+++	xorq	%r14,%r15
+++	addq	8(%rsp),%r12
+++
+++	addq	64(%rsp),%r12
+++	movq	%r8,%r13
+++	addq	%r15,%r12
+++	movq	%rax,%r14
+++	rorq	$23,%r13
+++	movq	%r9,%r15
+++
+++	xorq	%r8,%r13
+++	rorq	$5,%r14
+++	xorq	%r10,%r15
+++
+++	movq	%r12,64(%rsp)
+++	xorq	%rax,%r14
+++	andq	%r8,%r15
+++
+++	rorq	$4,%r13
+++	addq	%r11,%r12
+++	xorq	%r10,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%r8,%r13
+++	addq	%r15,%r12
+++
+++	movq	%rax,%r15
+++	addq	(%rbp),%r12
+++	xorq	%rax,%r14
+++
+++	xorq	%rbx,%r15
+++	rorq	$14,%r13
+++	movq	%rbx,%r11
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%r11
+++	addq	%r12,%rdx
+++	addq	%r12,%r11
+++
+++	leaq	8(%rbp),%rbp
+++	movq	80(%rsp),%r13
+++	movq	56(%rsp),%rdi
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%r11
+++	movq	%rdi,%r14
+++	rorq	$42,%rdi
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%rdi
+++	shrq	$6,%r14
+++
+++	rorq	$19,%rdi
+++	xorq	%r13,%r12
+++	xorq	%r14,%rdi
+++	addq	16(%rsp),%r12
+++
+++	addq	72(%rsp),%r12
+++	movq	%rdx,%r13
+++	addq	%rdi,%r12
+++	movq	%r11,%r14
+++	rorq	$23,%r13
+++	movq	%r8,%rdi
+++
+++	xorq	%rdx,%r13
+++	rorq	$5,%r14
+++	xorq	%r9,%rdi
+++
+++	movq	%r12,72(%rsp)
+++	xorq	%r11,%r14
+++	andq	%rdx,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%r10,%r12
+++	xorq	%r9,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%rdx,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%r11,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%r11,%r14
+++
+++	xorq	%rax,%rdi
+++	rorq	$14,%r13
+++	movq	%rax,%r10
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%r10
+++	addq	%r12,%rcx
+++	addq	%r12,%r10
+++
+++	leaq	24(%rbp),%rbp
+++	movq	88(%rsp),%r13
+++	movq	64(%rsp),%r15
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%r10
+++	movq	%r15,%r14
+++	rorq	$42,%r15
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%r15
+++	shrq	$6,%r14
+++
+++	rorq	$19,%r15
+++	xorq	%r13,%r12
+++	xorq	%r14,%r15
+++	addq	24(%rsp),%r12
+++
+++	addq	80(%rsp),%r12
+++	movq	%rcx,%r13
+++	addq	%r15,%r12
+++	movq	%r10,%r14
+++	rorq	$23,%r13
+++	movq	%rdx,%r15
+++
+++	xorq	%rcx,%r13
+++	rorq	$5,%r14
+++	xorq	%r8,%r15
+++
+++	movq	%r12,80(%rsp)
+++	xorq	%r10,%r14
+++	andq	%rcx,%r15
+++
+++	rorq	$4,%r13
+++	addq	%r9,%r12
+++	xorq	%r8,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%rcx,%r13
+++	addq	%r15,%r12
+++
+++	movq	%r10,%r15
+++	addq	(%rbp),%r12
+++	xorq	%r10,%r14
+++
+++	xorq	%r11,%r15
+++	rorq	$14,%r13
+++	movq	%r11,%r9
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%r9
+++	addq	%r12,%rbx
+++	addq	%r12,%r9
+++
+++	leaq	8(%rbp),%rbp
+++	movq	96(%rsp),%r13
+++	movq	72(%rsp),%rdi
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%r9
+++	movq	%rdi,%r14
+++	rorq	$42,%rdi
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%rdi
+++	shrq	$6,%r14
+++
+++	rorq	$19,%rdi
+++	xorq	%r13,%r12
+++	xorq	%r14,%rdi
+++	addq	32(%rsp),%r12
+++
+++	addq	88(%rsp),%r12
+++	movq	%rbx,%r13
+++	addq	%rdi,%r12
+++	movq	%r9,%r14
+++	rorq	$23,%r13
+++	movq	%rcx,%rdi
+++
+++	xorq	%rbx,%r13
+++	rorq	$5,%r14
+++	xorq	%rdx,%rdi
+++
+++	movq	%r12,88(%rsp)
+++	xorq	%r9,%r14
+++	andq	%rbx,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%r8,%r12
+++	xorq	%rdx,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%rbx,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%r9,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%r9,%r14
+++
+++	xorq	%r10,%rdi
+++	rorq	$14,%r13
+++	movq	%r10,%r8
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%r8
+++	addq	%r12,%rax
+++	addq	%r12,%r8
+++
+++	leaq	24(%rbp),%rbp
+++	movq	104(%rsp),%r13
+++	movq	80(%rsp),%r15
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%r8
+++	movq	%r15,%r14
+++	rorq	$42,%r15
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%r15
+++	shrq	$6,%r14
+++
+++	rorq	$19,%r15
+++	xorq	%r13,%r12
+++	xorq	%r14,%r15
+++	addq	40(%rsp),%r12
+++
+++	addq	96(%rsp),%r12
+++	movq	%rax,%r13
+++	addq	%r15,%r12
+++	movq	%r8,%r14
+++	rorq	$23,%r13
+++	movq	%rbx,%r15
+++
+++	xorq	%rax,%r13
+++	rorq	$5,%r14
+++	xorq	%rcx,%r15
+++
+++	movq	%r12,96(%rsp)
+++	xorq	%r8,%r14
+++	andq	%rax,%r15
+++
+++	rorq	$4,%r13
+++	addq	%rdx,%r12
+++	xorq	%rcx,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%rax,%r13
+++	addq	%r15,%r12
+++
+++	movq	%r8,%r15
+++	addq	(%rbp),%r12
+++	xorq	%r8,%r14
+++
+++	xorq	%r9,%r15
+++	rorq	$14,%r13
+++	movq	%r9,%rdx
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%rdx
+++	addq	%r12,%r11
+++	addq	%r12,%rdx
+++
+++	leaq	8(%rbp),%rbp
+++	movq	112(%rsp),%r13
+++	movq	88(%rsp),%rdi
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%rdx
+++	movq	%rdi,%r14
+++	rorq	$42,%rdi
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%rdi
+++	shrq	$6,%r14
+++
+++	rorq	$19,%rdi
+++	xorq	%r13,%r12
+++	xorq	%r14,%rdi
+++	addq	48(%rsp),%r12
+++
+++	addq	104(%rsp),%r12
+++	movq	%r11,%r13
+++	addq	%rdi,%r12
+++	movq	%rdx,%r14
+++	rorq	$23,%r13
+++	movq	%rax,%rdi
+++
+++	xorq	%r11,%r13
+++	rorq	$5,%r14
+++	xorq	%rbx,%rdi
+++
+++	movq	%r12,104(%rsp)
+++	xorq	%rdx,%r14
+++	andq	%r11,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%rcx,%r12
+++	xorq	%rbx,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%r11,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%rdx,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%rdx,%r14
+++
+++	xorq	%r8,%rdi
+++	rorq	$14,%r13
+++	movq	%r8,%rcx
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%rcx
+++	addq	%r12,%r10
+++	addq	%r12,%rcx
+++
+++	leaq	24(%rbp),%rbp
+++	movq	120(%rsp),%r13
+++	movq	96(%rsp),%r15
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%rcx
+++	movq	%r15,%r14
+++	rorq	$42,%r15
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%r15
+++	shrq	$6,%r14
+++
+++	rorq	$19,%r15
+++	xorq	%r13,%r12
+++	xorq	%r14,%r15
+++	addq	56(%rsp),%r12
+++
+++	addq	112(%rsp),%r12
+++	movq	%r10,%r13
+++	addq	%r15,%r12
+++	movq	%rcx,%r14
+++	rorq	$23,%r13
+++	movq	%r11,%r15
+++
+++	xorq	%r10,%r13
+++	rorq	$5,%r14
+++	xorq	%rax,%r15
+++
+++	movq	%r12,112(%rsp)
+++	xorq	%rcx,%r14
+++	andq	%r10,%r15
+++
+++	rorq	$4,%r13
+++	addq	%rbx,%r12
+++	xorq	%rax,%r15
+++
+++	rorq	$6,%r14
+++	xorq	%r10,%r13
+++	addq	%r15,%r12
+++
+++	movq	%rcx,%r15
+++	addq	(%rbp),%r12
+++	xorq	%rcx,%r14
+++
+++	xorq	%rdx,%r15
+++	rorq	$14,%r13
+++	movq	%rdx,%rbx
+++
+++	andq	%r15,%rdi
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%rdi,%rbx
+++	addq	%r12,%r9
+++	addq	%r12,%rbx
+++
+++	leaq	8(%rbp),%rbp
+++	movq	0(%rsp),%r13
+++	movq	104(%rsp),%rdi
+++
+++	movq	%r13,%r12
+++	rorq	$7,%r13
+++	addq	%r14,%rbx
+++	movq	%rdi,%r14
+++	rorq	$42,%rdi
+++
+++	xorq	%r12,%r13
+++	shrq	$7,%r12
+++	rorq	$1,%r13
+++	xorq	%r14,%rdi
+++	shrq	$6,%r14
+++
+++	rorq	$19,%rdi
+++	xorq	%r13,%r12
+++	xorq	%r14,%rdi
+++	addq	64(%rsp),%r12
+++
+++	addq	120(%rsp),%r12
+++	movq	%r9,%r13
+++	addq	%rdi,%r12
+++	movq	%rbx,%r14
+++	rorq	$23,%r13
+++	movq	%r10,%rdi
+++
+++	xorq	%r9,%r13
+++	rorq	$5,%r14
+++	xorq	%r11,%rdi
+++
+++	movq	%r12,120(%rsp)
+++	xorq	%rbx,%r14
+++	andq	%r9,%rdi
+++
+++	rorq	$4,%r13
+++	addq	%rax,%r12
+++	xorq	%r11,%rdi
+++
+++	rorq	$6,%r14
+++	xorq	%r9,%r13
+++	addq	%rdi,%r12
+++
+++	movq	%rbx,%rdi
+++	addq	(%rbp),%r12
+++	xorq	%rbx,%r14
+++
+++	xorq	%rcx,%rdi
+++	rorq	$14,%r13
+++	movq	%rcx,%rax
+++
+++	andq	%rdi,%r15
+++	rorq	$28,%r14
+++	addq	%r13,%r12
+++
+++	xorq	%r15,%rax
+++	addq	%r12,%r8
+++	addq	%r12,%rax
+++
+++	leaq	24(%rbp),%rbp
+++	cmpb	$0,7(%rbp)
+++	jnz	.Lrounds_16_xx
+++
+++	movq	128+0(%rsp),%rdi
+++	addq	%r14,%rax
+++	leaq	128(%rsi),%rsi
+++
+++	addq	0(%rdi),%rax
+++	addq	8(%rdi),%rbx
+++	addq	16(%rdi),%rcx
+++	addq	24(%rdi),%rdx
+++	addq	32(%rdi),%r8
+++	addq	40(%rdi),%r9
+++	addq	48(%rdi),%r10
+++	addq	56(%rdi),%r11
+++
+++	cmpq	128+16(%rsp),%rsi
+++
+++	movq	%rax,0(%rdi)
+++	movq	%rbx,8(%rdi)
+++	movq	%rcx,16(%rdi)
+++	movq	%rdx,24(%rdi)
+++	movq	%r8,32(%rdi)
+++	movq	%r9,40(%rdi)
+++	movq	%r10,48(%rdi)
+++	movq	%r11,56(%rdi)
+++	jb	.Lloop
+++
+++	movq	152(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lepilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	sha512_block_data_order,.-sha512_block_data_order
+++.align	64
+++.type	K512,@object
+++K512:
+++.quad	0x428a2f98d728ae22,0x7137449123ef65cd
+++.quad	0x428a2f98d728ae22,0x7137449123ef65cd
+++.quad	0xb5c0fbcfec4d3b2f,0xe9b5dba58189dbbc
+++.quad	0xb5c0fbcfec4d3b2f,0xe9b5dba58189dbbc
+++.quad	0x3956c25bf348b538,0x59f111f1b605d019
+++.quad	0x3956c25bf348b538,0x59f111f1b605d019
+++.quad	0x923f82a4af194f9b,0xab1c5ed5da6d8118
+++.quad	0x923f82a4af194f9b,0xab1c5ed5da6d8118
+++.quad	0xd807aa98a3030242,0x12835b0145706fbe
+++.quad	0xd807aa98a3030242,0x12835b0145706fbe
+++.quad	0x243185be4ee4b28c,0x550c7dc3d5ffb4e2
+++.quad	0x243185be4ee4b28c,0x550c7dc3d5ffb4e2
+++.quad	0x72be5d74f27b896f,0x80deb1fe3b1696b1
+++.quad	0x72be5d74f27b896f,0x80deb1fe3b1696b1
+++.quad	0x9bdc06a725c71235,0xc19bf174cf692694
+++.quad	0x9bdc06a725c71235,0xc19bf174cf692694
+++.quad	0xe49b69c19ef14ad2,0xefbe4786384f25e3
+++.quad	0xe49b69c19ef14ad2,0xefbe4786384f25e3
+++.quad	0x0fc19dc68b8cd5b5,0x240ca1cc77ac9c65
+++.quad	0x0fc19dc68b8cd5b5,0x240ca1cc77ac9c65
+++.quad	0x2de92c6f592b0275,0x4a7484aa6ea6e483
+++.quad	0x2de92c6f592b0275,0x4a7484aa6ea6e483
+++.quad	0x5cb0a9dcbd41fbd4,0x76f988da831153b5
+++.quad	0x5cb0a9dcbd41fbd4,0x76f988da831153b5
+++.quad	0x983e5152ee66dfab,0xa831c66d2db43210
+++.quad	0x983e5152ee66dfab,0xa831c66d2db43210
+++.quad	0xb00327c898fb213f,0xbf597fc7beef0ee4
+++.quad	0xb00327c898fb213f,0xbf597fc7beef0ee4
+++.quad	0xc6e00bf33da88fc2,0xd5a79147930aa725
+++.quad	0xc6e00bf33da88fc2,0xd5a79147930aa725
+++.quad	0x06ca6351e003826f,0x142929670a0e6e70
+++.quad	0x06ca6351e003826f,0x142929670a0e6e70
+++.quad	0x27b70a8546d22ffc,0x2e1b21385c26c926
+++.quad	0x27b70a8546d22ffc,0x2e1b21385c26c926
+++.quad	0x4d2c6dfc5ac42aed,0x53380d139d95b3df
+++.quad	0x4d2c6dfc5ac42aed,0x53380d139d95b3df
+++.quad	0x650a73548baf63de,0x766a0abb3c77b2a8
+++.quad	0x650a73548baf63de,0x766a0abb3c77b2a8
+++.quad	0x81c2c92e47edaee6,0x92722c851482353b
+++.quad	0x81c2c92e47edaee6,0x92722c851482353b
+++.quad	0xa2bfe8a14cf10364,0xa81a664bbc423001
+++.quad	0xa2bfe8a14cf10364,0xa81a664bbc423001
+++.quad	0xc24b8b70d0f89791,0xc76c51a30654be30
+++.quad	0xc24b8b70d0f89791,0xc76c51a30654be30
+++.quad	0xd192e819d6ef5218,0xd69906245565a910
+++.quad	0xd192e819d6ef5218,0xd69906245565a910
+++.quad	0xf40e35855771202a,0x106aa07032bbd1b8
+++.quad	0xf40e35855771202a,0x106aa07032bbd1b8
+++.quad	0x19a4c116b8d2d0c8,0x1e376c085141ab53
+++.quad	0x19a4c116b8d2d0c8,0x1e376c085141ab53
+++.quad	0x2748774cdf8eeb99,0x34b0bcb5e19b48a8
+++.quad	0x2748774cdf8eeb99,0x34b0bcb5e19b48a8
+++.quad	0x391c0cb3c5c95a63,0x4ed8aa4ae3418acb
+++.quad	0x391c0cb3c5c95a63,0x4ed8aa4ae3418acb
+++.quad	0x5b9cca4f7763e373,0x682e6ff3d6b2b8a3
+++.quad	0x5b9cca4f7763e373,0x682e6ff3d6b2b8a3
+++.quad	0x748f82ee5defb2fc,0x78a5636f43172f60
+++.quad	0x748f82ee5defb2fc,0x78a5636f43172f60
+++.quad	0x84c87814a1f0ab72,0x8cc702081a6439ec
+++.quad	0x84c87814a1f0ab72,0x8cc702081a6439ec
+++.quad	0x90befffa23631e28,0xa4506cebde82bde9
+++.quad	0x90befffa23631e28,0xa4506cebde82bde9
+++.quad	0xbef9a3f7b2c67915,0xc67178f2e372532b
+++.quad	0xbef9a3f7b2c67915,0xc67178f2e372532b
+++.quad	0xca273eceea26619c,0xd186b8c721c0c207
+++.quad	0xca273eceea26619c,0xd186b8c721c0c207
+++.quad	0xeada7dd6cde0eb1e,0xf57d4f7fee6ed178
+++.quad	0xeada7dd6cde0eb1e,0xf57d4f7fee6ed178
+++.quad	0x06f067aa72176fba,0x0a637dc5a2c898a6
+++.quad	0x06f067aa72176fba,0x0a637dc5a2c898a6
+++.quad	0x113f9804bef90dae,0x1b710b35131c471b
+++.quad	0x113f9804bef90dae,0x1b710b35131c471b
+++.quad	0x28db77f523047d84,0x32caab7b40c72493
+++.quad	0x28db77f523047d84,0x32caab7b40c72493
+++.quad	0x3c9ebe0a15c9bebc,0x431d67c49c100d4c
+++.quad	0x3c9ebe0a15c9bebc,0x431d67c49c100d4c
+++.quad	0x4cc5d4becb3e42b6,0x597f299cfc657e2a
+++.quad	0x4cc5d4becb3e42b6,0x597f299cfc657e2a
+++.quad	0x5fcb6fab3ad6faec,0x6c44198c4a475817
+++.quad	0x5fcb6fab3ad6faec,0x6c44198c4a475817
+++
+++.quad	0x0001020304050607,0x08090a0b0c0d0e0f
+++.quad	0x0001020304050607,0x08090a0b0c0d0e0f
+++.byte	83,72,65,53,49,50,32,98,108,111,99,107,32,116,114,97,110,115,102,111,114,109,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
+++.type	sha512_block_data_order_avx,@function
+++.align	64
+++sha512_block_data_order_avx:
+++.cfi_startproc	
+++.Lavx_shortcut:
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++	shlq	$4,%rdx
+++	subq	$160,%rsp
+++	leaq	(%rsi,%rdx,8),%rdx
+++	andq	$-64,%rsp
+++	movq	%rdi,128+0(%rsp)
+++	movq	%rsi,128+8(%rsp)
+++	movq	%rdx,128+16(%rsp)
+++	movq	%rax,152(%rsp)
+++.cfi_escape	0x0f,0x06,0x77,0x98,0x01,0x06,0x23,0x08
+++.Lprologue_avx:
+++
+++	vzeroupper
+++	movq	0(%rdi),%rax
+++	movq	8(%rdi),%rbx
+++	movq	16(%rdi),%rcx
+++	movq	24(%rdi),%rdx
+++	movq	32(%rdi),%r8
+++	movq	40(%rdi),%r9
+++	movq	48(%rdi),%r10
+++	movq	56(%rdi),%r11
+++	jmp	.Lloop_avx
+++.align	16
+++.Lloop_avx:
+++	vmovdqa	K512+1280(%rip),%xmm11
+++	vmovdqu	0(%rsi),%xmm0
+++	leaq	K512+128(%rip),%rbp
+++	vmovdqu	16(%rsi),%xmm1
+++	vmovdqu	32(%rsi),%xmm2
+++	vpshufb	%xmm11,%xmm0,%xmm0
+++	vmovdqu	48(%rsi),%xmm3
+++	vpshufb	%xmm11,%xmm1,%xmm1
+++	vmovdqu	64(%rsi),%xmm4
+++	vpshufb	%xmm11,%xmm2,%xmm2
+++	vmovdqu	80(%rsi),%xmm5
+++	vpshufb	%xmm11,%xmm3,%xmm3
+++	vmovdqu	96(%rsi),%xmm6
+++	vpshufb	%xmm11,%xmm4,%xmm4
+++	vmovdqu	112(%rsi),%xmm7
+++	vpshufb	%xmm11,%xmm5,%xmm5
+++	vpaddq	-128(%rbp),%xmm0,%xmm8
+++	vpshufb	%xmm11,%xmm6,%xmm6
+++	vpaddq	-96(%rbp),%xmm1,%xmm9
+++	vpshufb	%xmm11,%xmm7,%xmm7
+++	vpaddq	-64(%rbp),%xmm2,%xmm10
+++	vpaddq	-32(%rbp),%xmm3,%xmm11
+++	vmovdqa	%xmm8,0(%rsp)
+++	vpaddq	0(%rbp),%xmm4,%xmm8
+++	vmovdqa	%xmm9,16(%rsp)
+++	vpaddq	32(%rbp),%xmm5,%xmm9
+++	vmovdqa	%xmm10,32(%rsp)
+++	vpaddq	64(%rbp),%xmm6,%xmm10
+++	vmovdqa	%xmm11,48(%rsp)
+++	vpaddq	96(%rbp),%xmm7,%xmm11
+++	vmovdqa	%xmm8,64(%rsp)
+++	movq	%rax,%r14
+++	vmovdqa	%xmm9,80(%rsp)
+++	movq	%rbx,%rdi
+++	vmovdqa	%xmm10,96(%rsp)
+++	xorq	%rcx,%rdi
+++	vmovdqa	%xmm11,112(%rsp)
+++	movq	%r8,%r13
+++	jmp	.Lavx_00_47
+++
+++.align	16
+++.Lavx_00_47:
+++	addq	$256,%rbp
+++	vpalignr	$8,%xmm0,%xmm1,%xmm8
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rax
+++	vpalignr	$8,%xmm4,%xmm5,%xmm11
+++	movq	%r9,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$1,%xmm8,%xmm10
+++	xorq	%r8,%r13
+++	xorq	%r10,%r12
+++	vpaddq	%xmm11,%xmm0,%xmm0
+++	shrdq	$4,%r13,%r13
+++	xorq	%rax,%r14
+++	vpsrlq	$7,%xmm8,%xmm11
+++	andq	%r8,%r12
+++	xorq	%r8,%r13
+++	vpsllq	$56,%xmm8,%xmm9
+++	addq	0(%rsp),%r11
+++	movq	%rax,%r15
+++	vpxor	%xmm10,%xmm11,%xmm8
+++	xorq	%r10,%r12
+++	shrdq	$6,%r14,%r14
+++	vpsrlq	$7,%xmm10,%xmm10
+++	xorq	%rbx,%r15
+++	addq	%r12,%r11
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	vpsllq	$7,%xmm9,%xmm9
+++	xorq	%rax,%r14
+++	addq	%r13,%r11
+++	vpxor	%xmm10,%xmm8,%xmm8
+++	xorq	%rbx,%rdi
+++	shrdq	$28,%r14,%r14
+++	vpsrlq	$6,%xmm7,%xmm11
+++	addq	%r11,%rdx
+++	addq	%rdi,%r11
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	movq	%rdx,%r13
+++	addq	%r11,%r14
+++	vpsllq	$3,%xmm7,%xmm10
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r11
+++	vpaddq	%xmm8,%xmm0,%xmm0
+++	movq	%r8,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$19,%xmm7,%xmm9
+++	xorq	%rdx,%r13
+++	xorq	%r9,%r12
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	shrdq	$4,%r13,%r13
+++	xorq	%r11,%r14
+++	vpsllq	$42,%xmm10,%xmm10
+++	andq	%rdx,%r12
+++	xorq	%rdx,%r13
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	addq	8(%rsp),%r10
+++	movq	%r11,%rdi
+++	vpsrlq	$42,%xmm9,%xmm9
+++	xorq	%r9,%r12
+++	shrdq	$6,%r14,%r14
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	xorq	%rax,%rdi
+++	addq	%r12,%r10
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	vpaddq	%xmm11,%xmm0,%xmm0
+++	xorq	%r11,%r14
+++	addq	%r13,%r10
+++	vpaddq	-128(%rbp),%xmm0,%xmm10
+++	xorq	%rax,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%r10,%rcx
+++	addq	%r15,%r10
+++	movq	%rcx,%r13
+++	addq	%r10,%r14
+++	vmovdqa	%xmm10,0(%rsp)
+++	vpalignr	$8,%xmm1,%xmm2,%xmm8
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r10
+++	vpalignr	$8,%xmm5,%xmm6,%xmm11
+++	movq	%rdx,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$1,%xmm8,%xmm10
+++	xorq	%rcx,%r13
+++	xorq	%r8,%r12
+++	vpaddq	%xmm11,%xmm1,%xmm1
+++	shrdq	$4,%r13,%r13
+++	xorq	%r10,%r14
+++	vpsrlq	$7,%xmm8,%xmm11
+++	andq	%rcx,%r12
+++	xorq	%rcx,%r13
+++	vpsllq	$56,%xmm8,%xmm9
+++	addq	16(%rsp),%r9
+++	movq	%r10,%r15
+++	vpxor	%xmm10,%xmm11,%xmm8
+++	xorq	%r8,%r12
+++	shrdq	$6,%r14,%r14
+++	vpsrlq	$7,%xmm10,%xmm10
+++	xorq	%r11,%r15
+++	addq	%r12,%r9
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	vpsllq	$7,%xmm9,%xmm9
+++	xorq	%r10,%r14
+++	addq	%r13,%r9
+++	vpxor	%xmm10,%xmm8,%xmm8
+++	xorq	%r11,%rdi
+++	shrdq	$28,%r14,%r14
+++	vpsrlq	$6,%xmm0,%xmm11
+++	addq	%r9,%rbx
+++	addq	%rdi,%r9
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	movq	%rbx,%r13
+++	addq	%r9,%r14
+++	vpsllq	$3,%xmm0,%xmm10
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r9
+++	vpaddq	%xmm8,%xmm1,%xmm1
+++	movq	%rcx,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$19,%xmm0,%xmm9
+++	xorq	%rbx,%r13
+++	xorq	%rdx,%r12
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	shrdq	$4,%r13,%r13
+++	xorq	%r9,%r14
+++	vpsllq	$42,%xmm10,%xmm10
+++	andq	%rbx,%r12
+++	xorq	%rbx,%r13
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	addq	24(%rsp),%r8
+++	movq	%r9,%rdi
+++	vpsrlq	$42,%xmm9,%xmm9
+++	xorq	%rdx,%r12
+++	shrdq	$6,%r14,%r14
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	xorq	%r10,%rdi
+++	addq	%r12,%r8
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	vpaddq	%xmm11,%xmm1,%xmm1
+++	xorq	%r9,%r14
+++	addq	%r13,%r8
+++	vpaddq	-96(%rbp),%xmm1,%xmm10
+++	xorq	%r10,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%r8,%rax
+++	addq	%r15,%r8
+++	movq	%rax,%r13
+++	addq	%r8,%r14
+++	vmovdqa	%xmm10,16(%rsp)
+++	vpalignr	$8,%xmm2,%xmm3,%xmm8
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r8
+++	vpalignr	$8,%xmm6,%xmm7,%xmm11
+++	movq	%rbx,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$1,%xmm8,%xmm10
+++	xorq	%rax,%r13
+++	xorq	%rcx,%r12
+++	vpaddq	%xmm11,%xmm2,%xmm2
+++	shrdq	$4,%r13,%r13
+++	xorq	%r8,%r14
+++	vpsrlq	$7,%xmm8,%xmm11
+++	andq	%rax,%r12
+++	xorq	%rax,%r13
+++	vpsllq	$56,%xmm8,%xmm9
+++	addq	32(%rsp),%rdx
+++	movq	%r8,%r15
+++	vpxor	%xmm10,%xmm11,%xmm8
+++	xorq	%rcx,%r12
+++	shrdq	$6,%r14,%r14
+++	vpsrlq	$7,%xmm10,%xmm10
+++	xorq	%r9,%r15
+++	addq	%r12,%rdx
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	vpsllq	$7,%xmm9,%xmm9
+++	xorq	%r8,%r14
+++	addq	%r13,%rdx
+++	vpxor	%xmm10,%xmm8,%xmm8
+++	xorq	%r9,%rdi
+++	shrdq	$28,%r14,%r14
+++	vpsrlq	$6,%xmm1,%xmm11
+++	addq	%rdx,%r11
+++	addq	%rdi,%rdx
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	movq	%r11,%r13
+++	addq	%rdx,%r14
+++	vpsllq	$3,%xmm1,%xmm10
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rdx
+++	vpaddq	%xmm8,%xmm2,%xmm2
+++	movq	%rax,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$19,%xmm1,%xmm9
+++	xorq	%r11,%r13
+++	xorq	%rbx,%r12
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	shrdq	$4,%r13,%r13
+++	xorq	%rdx,%r14
+++	vpsllq	$42,%xmm10,%xmm10
+++	andq	%r11,%r12
+++	xorq	%r11,%r13
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	addq	40(%rsp),%rcx
+++	movq	%rdx,%rdi
+++	vpsrlq	$42,%xmm9,%xmm9
+++	xorq	%rbx,%r12
+++	shrdq	$6,%r14,%r14
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	xorq	%r8,%rdi
+++	addq	%r12,%rcx
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	vpaddq	%xmm11,%xmm2,%xmm2
+++	xorq	%rdx,%r14
+++	addq	%r13,%rcx
+++	vpaddq	-64(%rbp),%xmm2,%xmm10
+++	xorq	%r8,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%rcx,%r10
+++	addq	%r15,%rcx
+++	movq	%r10,%r13
+++	addq	%rcx,%r14
+++	vmovdqa	%xmm10,32(%rsp)
+++	vpalignr	$8,%xmm3,%xmm4,%xmm8
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rcx
+++	vpalignr	$8,%xmm7,%xmm0,%xmm11
+++	movq	%r11,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$1,%xmm8,%xmm10
+++	xorq	%r10,%r13
+++	xorq	%rax,%r12
+++	vpaddq	%xmm11,%xmm3,%xmm3
+++	shrdq	$4,%r13,%r13
+++	xorq	%rcx,%r14
+++	vpsrlq	$7,%xmm8,%xmm11
+++	andq	%r10,%r12
+++	xorq	%r10,%r13
+++	vpsllq	$56,%xmm8,%xmm9
+++	addq	48(%rsp),%rbx
+++	movq	%rcx,%r15
+++	vpxor	%xmm10,%xmm11,%xmm8
+++	xorq	%rax,%r12
+++	shrdq	$6,%r14,%r14
+++	vpsrlq	$7,%xmm10,%xmm10
+++	xorq	%rdx,%r15
+++	addq	%r12,%rbx
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	vpsllq	$7,%xmm9,%xmm9
+++	xorq	%rcx,%r14
+++	addq	%r13,%rbx
+++	vpxor	%xmm10,%xmm8,%xmm8
+++	xorq	%rdx,%rdi
+++	shrdq	$28,%r14,%r14
+++	vpsrlq	$6,%xmm2,%xmm11
+++	addq	%rbx,%r9
+++	addq	%rdi,%rbx
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	movq	%r9,%r13
+++	addq	%rbx,%r14
+++	vpsllq	$3,%xmm2,%xmm10
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rbx
+++	vpaddq	%xmm8,%xmm3,%xmm3
+++	movq	%r10,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$19,%xmm2,%xmm9
+++	xorq	%r9,%r13
+++	xorq	%r11,%r12
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	shrdq	$4,%r13,%r13
+++	xorq	%rbx,%r14
+++	vpsllq	$42,%xmm10,%xmm10
+++	andq	%r9,%r12
+++	xorq	%r9,%r13
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	addq	56(%rsp),%rax
+++	movq	%rbx,%rdi
+++	vpsrlq	$42,%xmm9,%xmm9
+++	xorq	%r11,%r12
+++	shrdq	$6,%r14,%r14
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	xorq	%rcx,%rdi
+++	addq	%r12,%rax
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	vpaddq	%xmm11,%xmm3,%xmm3
+++	xorq	%rbx,%r14
+++	addq	%r13,%rax
+++	vpaddq	-32(%rbp),%xmm3,%xmm10
+++	xorq	%rcx,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%rax,%r8
+++	addq	%r15,%rax
+++	movq	%r8,%r13
+++	addq	%rax,%r14
+++	vmovdqa	%xmm10,48(%rsp)
+++	vpalignr	$8,%xmm4,%xmm5,%xmm8
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rax
+++	vpalignr	$8,%xmm0,%xmm1,%xmm11
+++	movq	%r9,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$1,%xmm8,%xmm10
+++	xorq	%r8,%r13
+++	xorq	%r10,%r12
+++	vpaddq	%xmm11,%xmm4,%xmm4
+++	shrdq	$4,%r13,%r13
+++	xorq	%rax,%r14
+++	vpsrlq	$7,%xmm8,%xmm11
+++	andq	%r8,%r12
+++	xorq	%r8,%r13
+++	vpsllq	$56,%xmm8,%xmm9
+++	addq	64(%rsp),%r11
+++	movq	%rax,%r15
+++	vpxor	%xmm10,%xmm11,%xmm8
+++	xorq	%r10,%r12
+++	shrdq	$6,%r14,%r14
+++	vpsrlq	$7,%xmm10,%xmm10
+++	xorq	%rbx,%r15
+++	addq	%r12,%r11
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	vpsllq	$7,%xmm9,%xmm9
+++	xorq	%rax,%r14
+++	addq	%r13,%r11
+++	vpxor	%xmm10,%xmm8,%xmm8
+++	xorq	%rbx,%rdi
+++	shrdq	$28,%r14,%r14
+++	vpsrlq	$6,%xmm3,%xmm11
+++	addq	%r11,%rdx
+++	addq	%rdi,%r11
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	movq	%rdx,%r13
+++	addq	%r11,%r14
+++	vpsllq	$3,%xmm3,%xmm10
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r11
+++	vpaddq	%xmm8,%xmm4,%xmm4
+++	movq	%r8,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$19,%xmm3,%xmm9
+++	xorq	%rdx,%r13
+++	xorq	%r9,%r12
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	shrdq	$4,%r13,%r13
+++	xorq	%r11,%r14
+++	vpsllq	$42,%xmm10,%xmm10
+++	andq	%rdx,%r12
+++	xorq	%rdx,%r13
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	addq	72(%rsp),%r10
+++	movq	%r11,%rdi
+++	vpsrlq	$42,%xmm9,%xmm9
+++	xorq	%r9,%r12
+++	shrdq	$6,%r14,%r14
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	xorq	%rax,%rdi
+++	addq	%r12,%r10
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	vpaddq	%xmm11,%xmm4,%xmm4
+++	xorq	%r11,%r14
+++	addq	%r13,%r10
+++	vpaddq	0(%rbp),%xmm4,%xmm10
+++	xorq	%rax,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%r10,%rcx
+++	addq	%r15,%r10
+++	movq	%rcx,%r13
+++	addq	%r10,%r14
+++	vmovdqa	%xmm10,64(%rsp)
+++	vpalignr	$8,%xmm5,%xmm6,%xmm8
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r10
+++	vpalignr	$8,%xmm1,%xmm2,%xmm11
+++	movq	%rdx,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$1,%xmm8,%xmm10
+++	xorq	%rcx,%r13
+++	xorq	%r8,%r12
+++	vpaddq	%xmm11,%xmm5,%xmm5
+++	shrdq	$4,%r13,%r13
+++	xorq	%r10,%r14
+++	vpsrlq	$7,%xmm8,%xmm11
+++	andq	%rcx,%r12
+++	xorq	%rcx,%r13
+++	vpsllq	$56,%xmm8,%xmm9
+++	addq	80(%rsp),%r9
+++	movq	%r10,%r15
+++	vpxor	%xmm10,%xmm11,%xmm8
+++	xorq	%r8,%r12
+++	shrdq	$6,%r14,%r14
+++	vpsrlq	$7,%xmm10,%xmm10
+++	xorq	%r11,%r15
+++	addq	%r12,%r9
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	vpsllq	$7,%xmm9,%xmm9
+++	xorq	%r10,%r14
+++	addq	%r13,%r9
+++	vpxor	%xmm10,%xmm8,%xmm8
+++	xorq	%r11,%rdi
+++	shrdq	$28,%r14,%r14
+++	vpsrlq	$6,%xmm4,%xmm11
+++	addq	%r9,%rbx
+++	addq	%rdi,%r9
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	movq	%rbx,%r13
+++	addq	%r9,%r14
+++	vpsllq	$3,%xmm4,%xmm10
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r9
+++	vpaddq	%xmm8,%xmm5,%xmm5
+++	movq	%rcx,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$19,%xmm4,%xmm9
+++	xorq	%rbx,%r13
+++	xorq	%rdx,%r12
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	shrdq	$4,%r13,%r13
+++	xorq	%r9,%r14
+++	vpsllq	$42,%xmm10,%xmm10
+++	andq	%rbx,%r12
+++	xorq	%rbx,%r13
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	addq	88(%rsp),%r8
+++	movq	%r9,%rdi
+++	vpsrlq	$42,%xmm9,%xmm9
+++	xorq	%rdx,%r12
+++	shrdq	$6,%r14,%r14
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	xorq	%r10,%rdi
+++	addq	%r12,%r8
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	vpaddq	%xmm11,%xmm5,%xmm5
+++	xorq	%r9,%r14
+++	addq	%r13,%r8
+++	vpaddq	32(%rbp),%xmm5,%xmm10
+++	xorq	%r10,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%r8,%rax
+++	addq	%r15,%r8
+++	movq	%rax,%r13
+++	addq	%r8,%r14
+++	vmovdqa	%xmm10,80(%rsp)
+++	vpalignr	$8,%xmm6,%xmm7,%xmm8
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r8
+++	vpalignr	$8,%xmm2,%xmm3,%xmm11
+++	movq	%rbx,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$1,%xmm8,%xmm10
+++	xorq	%rax,%r13
+++	xorq	%rcx,%r12
+++	vpaddq	%xmm11,%xmm6,%xmm6
+++	shrdq	$4,%r13,%r13
+++	xorq	%r8,%r14
+++	vpsrlq	$7,%xmm8,%xmm11
+++	andq	%rax,%r12
+++	xorq	%rax,%r13
+++	vpsllq	$56,%xmm8,%xmm9
+++	addq	96(%rsp),%rdx
+++	movq	%r8,%r15
+++	vpxor	%xmm10,%xmm11,%xmm8
+++	xorq	%rcx,%r12
+++	shrdq	$6,%r14,%r14
+++	vpsrlq	$7,%xmm10,%xmm10
+++	xorq	%r9,%r15
+++	addq	%r12,%rdx
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	vpsllq	$7,%xmm9,%xmm9
+++	xorq	%r8,%r14
+++	addq	%r13,%rdx
+++	vpxor	%xmm10,%xmm8,%xmm8
+++	xorq	%r9,%rdi
+++	shrdq	$28,%r14,%r14
+++	vpsrlq	$6,%xmm5,%xmm11
+++	addq	%rdx,%r11
+++	addq	%rdi,%rdx
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	movq	%r11,%r13
+++	addq	%rdx,%r14
+++	vpsllq	$3,%xmm5,%xmm10
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rdx
+++	vpaddq	%xmm8,%xmm6,%xmm6
+++	movq	%rax,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$19,%xmm5,%xmm9
+++	xorq	%r11,%r13
+++	xorq	%rbx,%r12
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	shrdq	$4,%r13,%r13
+++	xorq	%rdx,%r14
+++	vpsllq	$42,%xmm10,%xmm10
+++	andq	%r11,%r12
+++	xorq	%r11,%r13
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	addq	104(%rsp),%rcx
+++	movq	%rdx,%rdi
+++	vpsrlq	$42,%xmm9,%xmm9
+++	xorq	%rbx,%r12
+++	shrdq	$6,%r14,%r14
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	xorq	%r8,%rdi
+++	addq	%r12,%rcx
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	vpaddq	%xmm11,%xmm6,%xmm6
+++	xorq	%rdx,%r14
+++	addq	%r13,%rcx
+++	vpaddq	64(%rbp),%xmm6,%xmm10
+++	xorq	%r8,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%rcx,%r10
+++	addq	%r15,%rcx
+++	movq	%r10,%r13
+++	addq	%rcx,%r14
+++	vmovdqa	%xmm10,96(%rsp)
+++	vpalignr	$8,%xmm7,%xmm0,%xmm8
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rcx
+++	vpalignr	$8,%xmm3,%xmm4,%xmm11
+++	movq	%r11,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$1,%xmm8,%xmm10
+++	xorq	%r10,%r13
+++	xorq	%rax,%r12
+++	vpaddq	%xmm11,%xmm7,%xmm7
+++	shrdq	$4,%r13,%r13
+++	xorq	%rcx,%r14
+++	vpsrlq	$7,%xmm8,%xmm11
+++	andq	%r10,%r12
+++	xorq	%r10,%r13
+++	vpsllq	$56,%xmm8,%xmm9
+++	addq	112(%rsp),%rbx
+++	movq	%rcx,%r15
+++	vpxor	%xmm10,%xmm11,%xmm8
+++	xorq	%rax,%r12
+++	shrdq	$6,%r14,%r14
+++	vpsrlq	$7,%xmm10,%xmm10
+++	xorq	%rdx,%r15
+++	addq	%r12,%rbx
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	vpsllq	$7,%xmm9,%xmm9
+++	xorq	%rcx,%r14
+++	addq	%r13,%rbx
+++	vpxor	%xmm10,%xmm8,%xmm8
+++	xorq	%rdx,%rdi
+++	shrdq	$28,%r14,%r14
+++	vpsrlq	$6,%xmm6,%xmm11
+++	addq	%rbx,%r9
+++	addq	%rdi,%rbx
+++	vpxor	%xmm9,%xmm8,%xmm8
+++	movq	%r9,%r13
+++	addq	%rbx,%r14
+++	vpsllq	$3,%xmm6,%xmm10
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rbx
+++	vpaddq	%xmm8,%xmm7,%xmm7
+++	movq	%r10,%r12
+++	shrdq	$5,%r14,%r14
+++	vpsrlq	$19,%xmm6,%xmm9
+++	xorq	%r9,%r13
+++	xorq	%r11,%r12
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	shrdq	$4,%r13,%r13
+++	xorq	%rbx,%r14
+++	vpsllq	$42,%xmm10,%xmm10
+++	andq	%r9,%r12
+++	xorq	%r9,%r13
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	addq	120(%rsp),%rax
+++	movq	%rbx,%rdi
+++	vpsrlq	$42,%xmm9,%xmm9
+++	xorq	%r11,%r12
+++	shrdq	$6,%r14,%r14
+++	vpxor	%xmm10,%xmm11,%xmm11
+++	xorq	%rcx,%rdi
+++	addq	%r12,%rax
+++	vpxor	%xmm9,%xmm11,%xmm11
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	vpaddq	%xmm11,%xmm7,%xmm7
+++	xorq	%rbx,%r14
+++	addq	%r13,%rax
+++	vpaddq	96(%rbp),%xmm7,%xmm10
+++	xorq	%rcx,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%rax,%r8
+++	addq	%r15,%rax
+++	movq	%r8,%r13
+++	addq	%rax,%r14
+++	vmovdqa	%xmm10,112(%rsp)
+++	cmpb	$0,135(%rbp)
+++	jne	.Lavx_00_47
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rax
+++	movq	%r9,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%r8,%r13
+++	xorq	%r10,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%rax,%r14
+++	andq	%r8,%r12
+++	xorq	%r8,%r13
+++	addq	0(%rsp),%r11
+++	movq	%rax,%r15
+++	xorq	%r10,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%rbx,%r15
+++	addq	%r12,%r11
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	xorq	%rax,%r14
+++	addq	%r13,%r11
+++	xorq	%rbx,%rdi
+++	shrdq	$28,%r14,%r14
+++	addq	%r11,%rdx
+++	addq	%rdi,%r11
+++	movq	%rdx,%r13
+++	addq	%r11,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r11
+++	movq	%r8,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%rdx,%r13
+++	xorq	%r9,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%r11,%r14
+++	andq	%rdx,%r12
+++	xorq	%rdx,%r13
+++	addq	8(%rsp),%r10
+++	movq	%r11,%rdi
+++	xorq	%r9,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%rax,%rdi
+++	addq	%r12,%r10
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	xorq	%r11,%r14
+++	addq	%r13,%r10
+++	xorq	%rax,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%r10,%rcx
+++	addq	%r15,%r10
+++	movq	%rcx,%r13
+++	addq	%r10,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r10
+++	movq	%rdx,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%rcx,%r13
+++	xorq	%r8,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%r10,%r14
+++	andq	%rcx,%r12
+++	xorq	%rcx,%r13
+++	addq	16(%rsp),%r9
+++	movq	%r10,%r15
+++	xorq	%r8,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%r11,%r15
+++	addq	%r12,%r9
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	xorq	%r10,%r14
+++	addq	%r13,%r9
+++	xorq	%r11,%rdi
+++	shrdq	$28,%r14,%r14
+++	addq	%r9,%rbx
+++	addq	%rdi,%r9
+++	movq	%rbx,%r13
+++	addq	%r9,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r9
+++	movq	%rcx,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%rbx,%r13
+++	xorq	%rdx,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%r9,%r14
+++	andq	%rbx,%r12
+++	xorq	%rbx,%r13
+++	addq	24(%rsp),%r8
+++	movq	%r9,%rdi
+++	xorq	%rdx,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%r10,%rdi
+++	addq	%r12,%r8
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	xorq	%r9,%r14
+++	addq	%r13,%r8
+++	xorq	%r10,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%r8,%rax
+++	addq	%r15,%r8
+++	movq	%rax,%r13
+++	addq	%r8,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r8
+++	movq	%rbx,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%rax,%r13
+++	xorq	%rcx,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%r8,%r14
+++	andq	%rax,%r12
+++	xorq	%rax,%r13
+++	addq	32(%rsp),%rdx
+++	movq	%r8,%r15
+++	xorq	%rcx,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%r9,%r15
+++	addq	%r12,%rdx
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	xorq	%r8,%r14
+++	addq	%r13,%rdx
+++	xorq	%r9,%rdi
+++	shrdq	$28,%r14,%r14
+++	addq	%rdx,%r11
+++	addq	%rdi,%rdx
+++	movq	%r11,%r13
+++	addq	%rdx,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rdx
+++	movq	%rax,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%r11,%r13
+++	xorq	%rbx,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%rdx,%r14
+++	andq	%r11,%r12
+++	xorq	%r11,%r13
+++	addq	40(%rsp),%rcx
+++	movq	%rdx,%rdi
+++	xorq	%rbx,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%r8,%rdi
+++	addq	%r12,%rcx
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	xorq	%rdx,%r14
+++	addq	%r13,%rcx
+++	xorq	%r8,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%rcx,%r10
+++	addq	%r15,%rcx
+++	movq	%r10,%r13
+++	addq	%rcx,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rcx
+++	movq	%r11,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%r10,%r13
+++	xorq	%rax,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%rcx,%r14
+++	andq	%r10,%r12
+++	xorq	%r10,%r13
+++	addq	48(%rsp),%rbx
+++	movq	%rcx,%r15
+++	xorq	%rax,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%rdx,%r15
+++	addq	%r12,%rbx
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	xorq	%rcx,%r14
+++	addq	%r13,%rbx
+++	xorq	%rdx,%rdi
+++	shrdq	$28,%r14,%r14
+++	addq	%rbx,%r9
+++	addq	%rdi,%rbx
+++	movq	%r9,%r13
+++	addq	%rbx,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rbx
+++	movq	%r10,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%r9,%r13
+++	xorq	%r11,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%rbx,%r14
+++	andq	%r9,%r12
+++	xorq	%r9,%r13
+++	addq	56(%rsp),%rax
+++	movq	%rbx,%rdi
+++	xorq	%r11,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%rcx,%rdi
+++	addq	%r12,%rax
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	xorq	%rbx,%r14
+++	addq	%r13,%rax
+++	xorq	%rcx,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%rax,%r8
+++	addq	%r15,%rax
+++	movq	%r8,%r13
+++	addq	%rax,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rax
+++	movq	%r9,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%r8,%r13
+++	xorq	%r10,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%rax,%r14
+++	andq	%r8,%r12
+++	xorq	%r8,%r13
+++	addq	64(%rsp),%r11
+++	movq	%rax,%r15
+++	xorq	%r10,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%rbx,%r15
+++	addq	%r12,%r11
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	xorq	%rax,%r14
+++	addq	%r13,%r11
+++	xorq	%rbx,%rdi
+++	shrdq	$28,%r14,%r14
+++	addq	%r11,%rdx
+++	addq	%rdi,%r11
+++	movq	%rdx,%r13
+++	addq	%r11,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r11
+++	movq	%r8,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%rdx,%r13
+++	xorq	%r9,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%r11,%r14
+++	andq	%rdx,%r12
+++	xorq	%rdx,%r13
+++	addq	72(%rsp),%r10
+++	movq	%r11,%rdi
+++	xorq	%r9,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%rax,%rdi
+++	addq	%r12,%r10
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	xorq	%r11,%r14
+++	addq	%r13,%r10
+++	xorq	%rax,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%r10,%rcx
+++	addq	%r15,%r10
+++	movq	%rcx,%r13
+++	addq	%r10,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r10
+++	movq	%rdx,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%rcx,%r13
+++	xorq	%r8,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%r10,%r14
+++	andq	%rcx,%r12
+++	xorq	%rcx,%r13
+++	addq	80(%rsp),%r9
+++	movq	%r10,%r15
+++	xorq	%r8,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%r11,%r15
+++	addq	%r12,%r9
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	xorq	%r10,%r14
+++	addq	%r13,%r9
+++	xorq	%r11,%rdi
+++	shrdq	$28,%r14,%r14
+++	addq	%r9,%rbx
+++	addq	%rdi,%r9
+++	movq	%rbx,%r13
+++	addq	%r9,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r9
+++	movq	%rcx,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%rbx,%r13
+++	xorq	%rdx,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%r9,%r14
+++	andq	%rbx,%r12
+++	xorq	%rbx,%r13
+++	addq	88(%rsp),%r8
+++	movq	%r9,%rdi
+++	xorq	%rdx,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%r10,%rdi
+++	addq	%r12,%r8
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	xorq	%r9,%r14
+++	addq	%r13,%r8
+++	xorq	%r10,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%r8,%rax
+++	addq	%r15,%r8
+++	movq	%rax,%r13
+++	addq	%r8,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%r8
+++	movq	%rbx,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%rax,%r13
+++	xorq	%rcx,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%r8,%r14
+++	andq	%rax,%r12
+++	xorq	%rax,%r13
+++	addq	96(%rsp),%rdx
+++	movq	%r8,%r15
+++	xorq	%rcx,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%r9,%r15
+++	addq	%r12,%rdx
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	xorq	%r8,%r14
+++	addq	%r13,%rdx
+++	xorq	%r9,%rdi
+++	shrdq	$28,%r14,%r14
+++	addq	%rdx,%r11
+++	addq	%rdi,%rdx
+++	movq	%r11,%r13
+++	addq	%rdx,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rdx
+++	movq	%rax,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%r11,%r13
+++	xorq	%rbx,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%rdx,%r14
+++	andq	%r11,%r12
+++	xorq	%r11,%r13
+++	addq	104(%rsp),%rcx
+++	movq	%rdx,%rdi
+++	xorq	%rbx,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%r8,%rdi
+++	addq	%r12,%rcx
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	xorq	%rdx,%r14
+++	addq	%r13,%rcx
+++	xorq	%r8,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%rcx,%r10
+++	addq	%r15,%rcx
+++	movq	%r10,%r13
+++	addq	%rcx,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rcx
+++	movq	%r11,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%r10,%r13
+++	xorq	%rax,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%rcx,%r14
+++	andq	%r10,%r12
+++	xorq	%r10,%r13
+++	addq	112(%rsp),%rbx
+++	movq	%rcx,%r15
+++	xorq	%rax,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%rdx,%r15
+++	addq	%r12,%rbx
+++	shrdq	$14,%r13,%r13
+++	andq	%r15,%rdi
+++	xorq	%rcx,%r14
+++	addq	%r13,%rbx
+++	xorq	%rdx,%rdi
+++	shrdq	$28,%r14,%r14
+++	addq	%rbx,%r9
+++	addq	%rdi,%rbx
+++	movq	%r9,%r13
+++	addq	%rbx,%r14
+++	shrdq	$23,%r13,%r13
+++	movq	%r14,%rbx
+++	movq	%r10,%r12
+++	shrdq	$5,%r14,%r14
+++	xorq	%r9,%r13
+++	xorq	%r11,%r12
+++	shrdq	$4,%r13,%r13
+++	xorq	%rbx,%r14
+++	andq	%r9,%r12
+++	xorq	%r9,%r13
+++	addq	120(%rsp),%rax
+++	movq	%rbx,%rdi
+++	xorq	%r11,%r12
+++	shrdq	$6,%r14,%r14
+++	xorq	%rcx,%rdi
+++	addq	%r12,%rax
+++	shrdq	$14,%r13,%r13
+++	andq	%rdi,%r15
+++	xorq	%rbx,%r14
+++	addq	%r13,%rax
+++	xorq	%rcx,%r15
+++	shrdq	$28,%r14,%r14
+++	addq	%rax,%r8
+++	addq	%r15,%rax
+++	movq	%r8,%r13
+++	addq	%rax,%r14
+++	movq	128+0(%rsp),%rdi
+++	movq	%r14,%rax
+++
+++	addq	0(%rdi),%rax
+++	leaq	128(%rsi),%rsi
+++	addq	8(%rdi),%rbx
+++	addq	16(%rdi),%rcx
+++	addq	24(%rdi),%rdx
+++	addq	32(%rdi),%r8
+++	addq	40(%rdi),%r9
+++	addq	48(%rdi),%r10
+++	addq	56(%rdi),%r11
+++
+++	cmpq	128+16(%rsp),%rsi
+++
+++	movq	%rax,0(%rdi)
+++	movq	%rbx,8(%rdi)
+++	movq	%rcx,16(%rdi)
+++	movq	%rdx,24(%rdi)
+++	movq	%r8,32(%rdi)
+++	movq	%r9,40(%rdi)
+++	movq	%r10,48(%rdi)
+++	movq	%r11,56(%rdi)
+++	jb	.Lloop_avx
+++
+++	movq	152(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	vzeroupper
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lepilogue_avx:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	sha512_block_data_order_avx,.-sha512_block_data_order_avx
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/vpaes-x86_64.S b/linux-x86_64/ypto/fipsmodule/vpaes-x86_64.S
++new file mode 100644
++index 000000000..27a34617a
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/vpaes-x86_64.S
++@@ -0,0 +1,1133 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++.type	_vpaes_encrypt_core,@function
+++.align	16
+++_vpaes_encrypt_core:
+++.cfi_startproc	
+++	movq	%rdx,%r9
+++	movq	$16,%r11
+++	movl	240(%rdx),%eax
+++	movdqa	%xmm9,%xmm1
+++	movdqa	.Lk_ipt(%rip),%xmm2
+++	pandn	%xmm0,%xmm1
+++	movdqu	(%r9),%xmm5
+++	psrld	$4,%xmm1
+++	pand	%xmm9,%xmm0
+++.byte	102,15,56,0,208
+++	movdqa	.Lk_ipt+16(%rip),%xmm0
+++.byte	102,15,56,0,193
+++	pxor	%xmm5,%xmm2
+++	addq	$16,%r9
+++	pxor	%xmm2,%xmm0
+++	leaq	.Lk_mc_backward(%rip),%r10
+++	jmp	.Lenc_entry
+++
+++.align	16
+++.Lenc_loop:
+++
+++	movdqa	%xmm13,%xmm4
+++	movdqa	%xmm12,%xmm0
+++.byte	102,15,56,0,226
+++.byte	102,15,56,0,195
+++	pxor	%xmm5,%xmm4
+++	movdqa	%xmm15,%xmm5
+++	pxor	%xmm4,%xmm0
+++	movdqa	-64(%r11,%r10,1),%xmm1
+++.byte	102,15,56,0,234
+++	movdqa	(%r11,%r10,1),%xmm4
+++	movdqa	%xmm14,%xmm2
+++.byte	102,15,56,0,211
+++	movdqa	%xmm0,%xmm3
+++	pxor	%xmm5,%xmm2
+++.byte	102,15,56,0,193
+++	addq	$16,%r9
+++	pxor	%xmm2,%xmm0
+++.byte	102,15,56,0,220
+++	addq	$16,%r11
+++	pxor	%xmm0,%xmm3
+++.byte	102,15,56,0,193
+++	andq	$0x30,%r11
+++	subq	$1,%rax
+++	pxor	%xmm3,%xmm0
+++
+++.Lenc_entry:
+++
+++	movdqa	%xmm9,%xmm1
+++	movdqa	%xmm11,%xmm5
+++	pandn	%xmm0,%xmm1
+++	psrld	$4,%xmm1
+++	pand	%xmm9,%xmm0
+++.byte	102,15,56,0,232
+++	movdqa	%xmm10,%xmm3
+++	pxor	%xmm1,%xmm0
+++.byte	102,15,56,0,217
+++	movdqa	%xmm10,%xmm4
+++	pxor	%xmm5,%xmm3
+++.byte	102,15,56,0,224
+++	movdqa	%xmm10,%xmm2
+++	pxor	%xmm5,%xmm4
+++.byte	102,15,56,0,211
+++	movdqa	%xmm10,%xmm3
+++	pxor	%xmm0,%xmm2
+++.byte	102,15,56,0,220
+++	movdqu	(%r9),%xmm5
+++	pxor	%xmm1,%xmm3
+++	jnz	.Lenc_loop
+++
+++
+++	movdqa	-96(%r10),%xmm4
+++	movdqa	-80(%r10),%xmm0
+++.byte	102,15,56,0,226
+++	pxor	%xmm5,%xmm4
+++.byte	102,15,56,0,195
+++	movdqa	64(%r11,%r10,1),%xmm1
+++	pxor	%xmm4,%xmm0
+++.byte	102,15,56,0,193
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_vpaes_encrypt_core,.-_vpaes_encrypt_core
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++.type	_vpaes_encrypt_core_2x,@function
+++.align	16
+++_vpaes_encrypt_core_2x:
+++.cfi_startproc	
+++	movq	%rdx,%r9
+++	movq	$16,%r11
+++	movl	240(%rdx),%eax
+++	movdqa	%xmm9,%xmm1
+++	movdqa	%xmm9,%xmm7
+++	movdqa	.Lk_ipt(%rip),%xmm2
+++	movdqa	%xmm2,%xmm8
+++	pandn	%xmm0,%xmm1
+++	pandn	%xmm6,%xmm7
+++	movdqu	(%r9),%xmm5
+++
+++	psrld	$4,%xmm1
+++	psrld	$4,%xmm7
+++	pand	%xmm9,%xmm0
+++	pand	%xmm9,%xmm6
+++.byte	102,15,56,0,208
+++.byte	102,68,15,56,0,198
+++	movdqa	.Lk_ipt+16(%rip),%xmm0
+++	movdqa	%xmm0,%xmm6
+++.byte	102,15,56,0,193
+++.byte	102,15,56,0,247
+++	pxor	%xmm5,%xmm2
+++	pxor	%xmm5,%xmm8
+++	addq	$16,%r9
+++	pxor	%xmm2,%xmm0
+++	pxor	%xmm8,%xmm6
+++	leaq	.Lk_mc_backward(%rip),%r10
+++	jmp	.Lenc2x_entry
+++
+++.align	16
+++.Lenc2x_loop:
+++
+++	movdqa	.Lk_sb1(%rip),%xmm4
+++	movdqa	.Lk_sb1+16(%rip),%xmm0
+++	movdqa	%xmm4,%xmm12
+++	movdqa	%xmm0,%xmm6
+++.byte	102,15,56,0,226
+++.byte	102,69,15,56,0,224
+++.byte	102,15,56,0,195
+++.byte	102,65,15,56,0,243
+++	pxor	%xmm5,%xmm4
+++	pxor	%xmm5,%xmm12
+++	movdqa	.Lk_sb2(%rip),%xmm5
+++	movdqa	%xmm5,%xmm13
+++	pxor	%xmm4,%xmm0
+++	pxor	%xmm12,%xmm6
+++	movdqa	-64(%r11,%r10,1),%xmm1
+++
+++.byte	102,15,56,0,234
+++.byte	102,69,15,56,0,232
+++	movdqa	(%r11,%r10,1),%xmm4
+++
+++	movdqa	.Lk_sb2+16(%rip),%xmm2
+++	movdqa	%xmm2,%xmm8
+++.byte	102,15,56,0,211
+++.byte	102,69,15,56,0,195
+++	movdqa	%xmm0,%xmm3
+++	movdqa	%xmm6,%xmm11
+++	pxor	%xmm5,%xmm2
+++	pxor	%xmm13,%xmm8
+++.byte	102,15,56,0,193
+++.byte	102,15,56,0,241
+++	addq	$16,%r9
+++	pxor	%xmm2,%xmm0
+++	pxor	%xmm8,%xmm6
+++.byte	102,15,56,0,220
+++.byte	102,68,15,56,0,220
+++	addq	$16,%r11
+++	pxor	%xmm0,%xmm3
+++	pxor	%xmm6,%xmm11
+++.byte	102,15,56,0,193
+++.byte	102,15,56,0,241
+++	andq	$0x30,%r11
+++	subq	$1,%rax
+++	pxor	%xmm3,%xmm0
+++	pxor	%xmm11,%xmm6
+++
+++.Lenc2x_entry:
+++
+++	movdqa	%xmm9,%xmm1
+++	movdqa	%xmm9,%xmm7
+++	movdqa	.Lk_inv+16(%rip),%xmm5
+++	movdqa	%xmm5,%xmm13
+++	pandn	%xmm0,%xmm1
+++	pandn	%xmm6,%xmm7
+++	psrld	$4,%xmm1
+++	psrld	$4,%xmm7
+++	pand	%xmm9,%xmm0
+++	pand	%xmm9,%xmm6
+++.byte	102,15,56,0,232
+++.byte	102,68,15,56,0,238
+++	movdqa	%xmm10,%xmm3
+++	movdqa	%xmm10,%xmm11
+++	pxor	%xmm1,%xmm0
+++	pxor	%xmm7,%xmm6
+++.byte	102,15,56,0,217
+++.byte	102,68,15,56,0,223
+++	movdqa	%xmm10,%xmm4
+++	movdqa	%xmm10,%xmm12
+++	pxor	%xmm5,%xmm3
+++	pxor	%xmm13,%xmm11
+++.byte	102,15,56,0,224
+++.byte	102,68,15,56,0,230
+++	movdqa	%xmm10,%xmm2
+++	movdqa	%xmm10,%xmm8
+++	pxor	%xmm5,%xmm4
+++	pxor	%xmm13,%xmm12
+++.byte	102,15,56,0,211
+++.byte	102,69,15,56,0,195
+++	movdqa	%xmm10,%xmm3
+++	movdqa	%xmm10,%xmm11
+++	pxor	%xmm0,%xmm2
+++	pxor	%xmm6,%xmm8
+++.byte	102,15,56,0,220
+++.byte	102,69,15,56,0,220
+++	movdqu	(%r9),%xmm5
+++
+++	pxor	%xmm1,%xmm3
+++	pxor	%xmm7,%xmm11
+++	jnz	.Lenc2x_loop
+++
+++
+++	movdqa	-96(%r10),%xmm4
+++	movdqa	-80(%r10),%xmm0
+++	movdqa	%xmm4,%xmm12
+++	movdqa	%xmm0,%xmm6
+++.byte	102,15,56,0,226
+++.byte	102,69,15,56,0,224
+++	pxor	%xmm5,%xmm4
+++	pxor	%xmm5,%xmm12
+++.byte	102,15,56,0,195
+++.byte	102,65,15,56,0,243
+++	movdqa	64(%r11,%r10,1),%xmm1
+++
+++	pxor	%xmm4,%xmm0
+++	pxor	%xmm12,%xmm6
+++.byte	102,15,56,0,193
+++.byte	102,15,56,0,241
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_vpaes_encrypt_core_2x,.-_vpaes_encrypt_core_2x
+++
+++
+++
+++
+++
+++
+++.type	_vpaes_decrypt_core,@function
+++.align	16
+++_vpaes_decrypt_core:
+++.cfi_startproc	
+++	movq	%rdx,%r9
+++	movl	240(%rdx),%eax
+++	movdqa	%xmm9,%xmm1
+++	movdqa	.Lk_dipt(%rip),%xmm2
+++	pandn	%xmm0,%xmm1
+++	movq	%rax,%r11
+++	psrld	$4,%xmm1
+++	movdqu	(%r9),%xmm5
+++	shlq	$4,%r11
+++	pand	%xmm9,%xmm0
+++.byte	102,15,56,0,208
+++	movdqa	.Lk_dipt+16(%rip),%xmm0
+++	xorq	$0x30,%r11
+++	leaq	.Lk_dsbd(%rip),%r10
+++.byte	102,15,56,0,193
+++	andq	$0x30,%r11
+++	pxor	%xmm5,%xmm2
+++	movdqa	.Lk_mc_forward+48(%rip),%xmm5
+++	pxor	%xmm2,%xmm0
+++	addq	$16,%r9
+++	addq	%r10,%r11
+++	jmp	.Ldec_entry
+++
+++.align	16
+++.Ldec_loop:
+++
+++
+++
+++	movdqa	-32(%r10),%xmm4
+++	movdqa	-16(%r10),%xmm1
+++.byte	102,15,56,0,226
+++.byte	102,15,56,0,203
+++	pxor	%xmm4,%xmm0
+++	movdqa	0(%r10),%xmm4
+++	pxor	%xmm1,%xmm0
+++	movdqa	16(%r10),%xmm1
+++
+++.byte	102,15,56,0,226
+++.byte	102,15,56,0,197
+++.byte	102,15,56,0,203
+++	pxor	%xmm4,%xmm0
+++	movdqa	32(%r10),%xmm4
+++	pxor	%xmm1,%xmm0
+++	movdqa	48(%r10),%xmm1
+++
+++.byte	102,15,56,0,226
+++.byte	102,15,56,0,197
+++.byte	102,15,56,0,203
+++	pxor	%xmm4,%xmm0
+++	movdqa	64(%r10),%xmm4
+++	pxor	%xmm1,%xmm0
+++	movdqa	80(%r10),%xmm1
+++
+++.byte	102,15,56,0,226
+++.byte	102,15,56,0,197
+++.byte	102,15,56,0,203
+++	pxor	%xmm4,%xmm0
+++	addq	$16,%r9
+++.byte	102,15,58,15,237,12
+++	pxor	%xmm1,%xmm0
+++	subq	$1,%rax
+++
+++.Ldec_entry:
+++
+++	movdqa	%xmm9,%xmm1
+++	pandn	%xmm0,%xmm1
+++	movdqa	%xmm11,%xmm2
+++	psrld	$4,%xmm1
+++	pand	%xmm9,%xmm0
+++.byte	102,15,56,0,208
+++	movdqa	%xmm10,%xmm3
+++	pxor	%xmm1,%xmm0
+++.byte	102,15,56,0,217
+++	movdqa	%xmm10,%xmm4
+++	pxor	%xmm2,%xmm3
+++.byte	102,15,56,0,224
+++	pxor	%xmm2,%xmm4
+++	movdqa	%xmm10,%xmm2
+++.byte	102,15,56,0,211
+++	movdqa	%xmm10,%xmm3
+++	pxor	%xmm0,%xmm2
+++.byte	102,15,56,0,220
+++	movdqu	(%r9),%xmm0
+++	pxor	%xmm1,%xmm3
+++	jnz	.Ldec_loop
+++
+++
+++	movdqa	96(%r10),%xmm4
+++.byte	102,15,56,0,226
+++	pxor	%xmm0,%xmm4
+++	movdqa	112(%r10),%xmm0
+++	movdqa	-352(%r11),%xmm2
+++.byte	102,15,56,0,195
+++	pxor	%xmm4,%xmm0
+++.byte	102,15,56,0,194
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_vpaes_decrypt_core,.-_vpaes_decrypt_core
+++
+++
+++
+++
+++
+++
+++.type	_vpaes_schedule_core,@function
+++.align	16
+++_vpaes_schedule_core:
+++.cfi_startproc	
+++
+++
+++
+++
+++
+++	call	_vpaes_preheat
+++	movdqa	.Lk_rcon(%rip),%xmm8
+++	movdqu	(%rdi),%xmm0
+++
+++
+++	movdqa	%xmm0,%xmm3
+++	leaq	.Lk_ipt(%rip),%r11
+++	call	_vpaes_schedule_transform
+++	movdqa	%xmm0,%xmm7
+++
+++	leaq	.Lk_sr(%rip),%r10
+++	testq	%rcx,%rcx
+++	jnz	.Lschedule_am_decrypting
+++
+++
+++	movdqu	%xmm0,(%rdx)
+++	jmp	.Lschedule_go
+++
+++.Lschedule_am_decrypting:
+++
+++	movdqa	(%r8,%r10,1),%xmm1
+++.byte	102,15,56,0,217
+++	movdqu	%xmm3,(%rdx)
+++	xorq	$0x30,%r8
+++
+++.Lschedule_go:
+++	cmpl	$192,%esi
+++	ja	.Lschedule_256
+++	je	.Lschedule_192
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++.Lschedule_128:
+++	movl	$10,%esi
+++
+++.Loop_schedule_128:
+++	call	_vpaes_schedule_round
+++	decq	%rsi
+++	jz	.Lschedule_mangle_last
+++	call	_vpaes_schedule_mangle
+++	jmp	.Loop_schedule_128
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++.align	16
+++.Lschedule_192:
+++	movdqu	8(%rdi),%xmm0
+++	call	_vpaes_schedule_transform
+++	movdqa	%xmm0,%xmm6
+++	pxor	%xmm4,%xmm4
+++	movhlps	%xmm4,%xmm6
+++	movl	$4,%esi
+++
+++.Loop_schedule_192:
+++	call	_vpaes_schedule_round
+++.byte	102,15,58,15,198,8
+++	call	_vpaes_schedule_mangle
+++	call	_vpaes_schedule_192_smear
+++	call	_vpaes_schedule_mangle
+++	call	_vpaes_schedule_round
+++	decq	%rsi
+++	jz	.Lschedule_mangle_last
+++	call	_vpaes_schedule_mangle
+++	call	_vpaes_schedule_192_smear
+++	jmp	.Loop_schedule_192
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++.align	16
+++.Lschedule_256:
+++	movdqu	16(%rdi),%xmm0
+++	call	_vpaes_schedule_transform
+++	movl	$7,%esi
+++
+++.Loop_schedule_256:
+++	call	_vpaes_schedule_mangle
+++	movdqa	%xmm0,%xmm6
+++
+++
+++	call	_vpaes_schedule_round
+++	decq	%rsi
+++	jz	.Lschedule_mangle_last
+++	call	_vpaes_schedule_mangle
+++
+++
+++	pshufd	$0xFF,%xmm0,%xmm0
+++	movdqa	%xmm7,%xmm5
+++	movdqa	%xmm6,%xmm7
+++	call	_vpaes_schedule_low_round
+++	movdqa	%xmm5,%xmm7
+++
+++	jmp	.Loop_schedule_256
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++.align	16
+++.Lschedule_mangle_last:
+++
+++	leaq	.Lk_deskew(%rip),%r11
+++	testq	%rcx,%rcx
+++	jnz	.Lschedule_mangle_last_dec
+++
+++
+++	movdqa	(%r8,%r10,1),%xmm1
+++.byte	102,15,56,0,193
+++	leaq	.Lk_opt(%rip),%r11
+++	addq	$32,%rdx
+++
+++.Lschedule_mangle_last_dec:
+++	addq	$-16,%rdx
+++	pxor	.Lk_s63(%rip),%xmm0
+++	call	_vpaes_schedule_transform
+++	movdqu	%xmm0,(%rdx)
+++
+++
+++	pxor	%xmm0,%xmm0
+++	pxor	%xmm1,%xmm1
+++	pxor	%xmm2,%xmm2
+++	pxor	%xmm3,%xmm3
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm5,%xmm5
+++	pxor	%xmm6,%xmm6
+++	pxor	%xmm7,%xmm7
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_vpaes_schedule_core,.-_vpaes_schedule_core
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++.type	_vpaes_schedule_192_smear,@function
+++.align	16
+++_vpaes_schedule_192_smear:
+++.cfi_startproc	
+++	pshufd	$0x80,%xmm6,%xmm1
+++	pshufd	$0xFE,%xmm7,%xmm0
+++	pxor	%xmm1,%xmm6
+++	pxor	%xmm1,%xmm1
+++	pxor	%xmm0,%xmm6
+++	movdqa	%xmm6,%xmm0
+++	movhlps	%xmm1,%xmm6
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_vpaes_schedule_192_smear,.-_vpaes_schedule_192_smear
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++.type	_vpaes_schedule_round,@function
+++.align	16
+++_vpaes_schedule_round:
+++.cfi_startproc	
+++
+++	pxor	%xmm1,%xmm1
+++.byte	102,65,15,58,15,200,15
+++.byte	102,69,15,58,15,192,15
+++	pxor	%xmm1,%xmm7
+++
+++
+++	pshufd	$0xFF,%xmm0,%xmm0
+++.byte	102,15,58,15,192,1
+++
+++
+++
+++
+++_vpaes_schedule_low_round:
+++
+++	movdqa	%xmm7,%xmm1
+++	pslldq	$4,%xmm7
+++	pxor	%xmm1,%xmm7
+++	movdqa	%xmm7,%xmm1
+++	pslldq	$8,%xmm7
+++	pxor	%xmm1,%xmm7
+++	pxor	.Lk_s63(%rip),%xmm7
+++
+++
+++	movdqa	%xmm9,%xmm1
+++	pandn	%xmm0,%xmm1
+++	psrld	$4,%xmm1
+++	pand	%xmm9,%xmm0
+++	movdqa	%xmm11,%xmm2
+++.byte	102,15,56,0,208
+++	pxor	%xmm1,%xmm0
+++	movdqa	%xmm10,%xmm3
+++.byte	102,15,56,0,217
+++	pxor	%xmm2,%xmm3
+++	movdqa	%xmm10,%xmm4
+++.byte	102,15,56,0,224
+++	pxor	%xmm2,%xmm4
+++	movdqa	%xmm10,%xmm2
+++.byte	102,15,56,0,211
+++	pxor	%xmm0,%xmm2
+++	movdqa	%xmm10,%xmm3
+++.byte	102,15,56,0,220
+++	pxor	%xmm1,%xmm3
+++	movdqa	%xmm13,%xmm4
+++.byte	102,15,56,0,226
+++	movdqa	%xmm12,%xmm0
+++.byte	102,15,56,0,195
+++	pxor	%xmm4,%xmm0
+++
+++
+++	pxor	%xmm7,%xmm0
+++	movdqa	%xmm0,%xmm7
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_vpaes_schedule_round,.-_vpaes_schedule_round
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++.type	_vpaes_schedule_transform,@function
+++.align	16
+++_vpaes_schedule_transform:
+++.cfi_startproc	
+++	movdqa	%xmm9,%xmm1
+++	pandn	%xmm0,%xmm1
+++	psrld	$4,%xmm1
+++	pand	%xmm9,%xmm0
+++	movdqa	(%r11),%xmm2
+++.byte	102,15,56,0,208
+++	movdqa	16(%r11),%xmm0
+++.byte	102,15,56,0,193
+++	pxor	%xmm2,%xmm0
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_vpaes_schedule_transform,.-_vpaes_schedule_transform
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++.type	_vpaes_schedule_mangle,@function
+++.align	16
+++_vpaes_schedule_mangle:
+++.cfi_startproc	
+++	movdqa	%xmm0,%xmm4
+++	movdqa	.Lk_mc_forward(%rip),%xmm5
+++	testq	%rcx,%rcx
+++	jnz	.Lschedule_mangle_dec
+++
+++
+++	addq	$16,%rdx
+++	pxor	.Lk_s63(%rip),%xmm4
+++.byte	102,15,56,0,229
+++	movdqa	%xmm4,%xmm3
+++.byte	102,15,56,0,229
+++	pxor	%xmm4,%xmm3
+++.byte	102,15,56,0,229
+++	pxor	%xmm4,%xmm3
+++
+++	jmp	.Lschedule_mangle_both
+++.align	16
+++.Lschedule_mangle_dec:
+++
+++	leaq	.Lk_dksd(%rip),%r11
+++	movdqa	%xmm9,%xmm1
+++	pandn	%xmm4,%xmm1
+++	psrld	$4,%xmm1
+++	pand	%xmm9,%xmm4
+++
+++	movdqa	0(%r11),%xmm2
+++.byte	102,15,56,0,212
+++	movdqa	16(%r11),%xmm3
+++.byte	102,15,56,0,217
+++	pxor	%xmm2,%xmm3
+++.byte	102,15,56,0,221
+++
+++	movdqa	32(%r11),%xmm2
+++.byte	102,15,56,0,212
+++	pxor	%xmm3,%xmm2
+++	movdqa	48(%r11),%xmm3
+++.byte	102,15,56,0,217
+++	pxor	%xmm2,%xmm3
+++.byte	102,15,56,0,221
+++
+++	movdqa	64(%r11),%xmm2
+++.byte	102,15,56,0,212
+++	pxor	%xmm3,%xmm2
+++	movdqa	80(%r11),%xmm3
+++.byte	102,15,56,0,217
+++	pxor	%xmm2,%xmm3
+++.byte	102,15,56,0,221
+++
+++	movdqa	96(%r11),%xmm2
+++.byte	102,15,56,0,212
+++	pxor	%xmm3,%xmm2
+++	movdqa	112(%r11),%xmm3
+++.byte	102,15,56,0,217
+++	pxor	%xmm2,%xmm3
+++
+++	addq	$-16,%rdx
+++
+++.Lschedule_mangle_both:
+++	movdqa	(%r8,%r10,1),%xmm1
+++.byte	102,15,56,0,217
+++	addq	$-16,%r8
+++	andq	$0x30,%r8
+++	movdqu	%xmm3,(%rdx)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_vpaes_schedule_mangle,.-_vpaes_schedule_mangle
+++
+++
+++
+++
+++.globl	vpaes_set_encrypt_key
+++.hidden vpaes_set_encrypt_key
+++.type	vpaes_set_encrypt_key,@function
+++.align	16
+++vpaes_set_encrypt_key:
+++.cfi_startproc	
+++#ifdef BORINGSSL_DISPATCH_TEST
+++.extern	BORINGSSL_function_hit
+++.hidden BORINGSSL_function_hit
+++	movb	$1,BORINGSSL_function_hit+5(%rip)
+++#endif
+++
+++	movl	%esi,%eax
+++	shrl	$5,%eax
+++	addl	$5,%eax
+++	movl	%eax,240(%rdx)
+++
+++	movl	$0,%ecx
+++	movl	$0x30,%r8d
+++	call	_vpaes_schedule_core
+++	xorl	%eax,%eax
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	vpaes_set_encrypt_key,.-vpaes_set_encrypt_key
+++
+++.globl	vpaes_set_decrypt_key
+++.hidden vpaes_set_decrypt_key
+++.type	vpaes_set_decrypt_key,@function
+++.align	16
+++vpaes_set_decrypt_key:
+++.cfi_startproc	
+++	movl	%esi,%eax
+++	shrl	$5,%eax
+++	addl	$5,%eax
+++	movl	%eax,240(%rdx)
+++	shll	$4,%eax
+++	leaq	16(%rdx,%rax,1),%rdx
+++
+++	movl	$1,%ecx
+++	movl	%esi,%r8d
+++	shrl	$1,%r8d
+++	andl	$32,%r8d
+++	xorl	$32,%r8d
+++	call	_vpaes_schedule_core
+++	xorl	%eax,%eax
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	vpaes_set_decrypt_key,.-vpaes_set_decrypt_key
+++
+++.globl	vpaes_encrypt
+++.hidden vpaes_encrypt
+++.type	vpaes_encrypt,@function
+++.align	16
+++vpaes_encrypt:
+++.cfi_startproc	
+++#ifdef BORINGSSL_DISPATCH_TEST
+++.extern	BORINGSSL_function_hit
+++.hidden BORINGSSL_function_hit
+++	movb	$1,BORINGSSL_function_hit+4(%rip)
+++#endif
+++	movdqu	(%rdi),%xmm0
+++	call	_vpaes_preheat
+++	call	_vpaes_encrypt_core
+++	movdqu	%xmm0,(%rsi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	vpaes_encrypt,.-vpaes_encrypt
+++
+++.globl	vpaes_decrypt
+++.hidden vpaes_decrypt
+++.type	vpaes_decrypt,@function
+++.align	16
+++vpaes_decrypt:
+++.cfi_startproc	
+++	movdqu	(%rdi),%xmm0
+++	call	_vpaes_preheat
+++	call	_vpaes_decrypt_core
+++	movdqu	%xmm0,(%rsi)
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	vpaes_decrypt,.-vpaes_decrypt
+++.globl	vpaes_cbc_encrypt
+++.hidden vpaes_cbc_encrypt
+++.type	vpaes_cbc_encrypt,@function
+++.align	16
+++vpaes_cbc_encrypt:
+++.cfi_startproc	
+++	xchgq	%rcx,%rdx
+++	subq	$16,%rcx
+++	jc	.Lcbc_abort
+++	movdqu	(%r8),%xmm6
+++	subq	%rdi,%rsi
+++	call	_vpaes_preheat
+++	cmpl	$0,%r9d
+++	je	.Lcbc_dec_loop
+++	jmp	.Lcbc_enc_loop
+++.align	16
+++.Lcbc_enc_loop:
+++	movdqu	(%rdi),%xmm0
+++	pxor	%xmm6,%xmm0
+++	call	_vpaes_encrypt_core
+++	movdqa	%xmm0,%xmm6
+++	movdqu	%xmm0,(%rsi,%rdi,1)
+++	leaq	16(%rdi),%rdi
+++	subq	$16,%rcx
+++	jnc	.Lcbc_enc_loop
+++	jmp	.Lcbc_done
+++.align	16
+++.Lcbc_dec_loop:
+++	movdqu	(%rdi),%xmm0
+++	movdqa	%xmm0,%xmm7
+++	call	_vpaes_decrypt_core
+++	pxor	%xmm6,%xmm0
+++	movdqa	%xmm7,%xmm6
+++	movdqu	%xmm0,(%rsi,%rdi,1)
+++	leaq	16(%rdi),%rdi
+++	subq	$16,%rcx
+++	jnc	.Lcbc_dec_loop
+++.Lcbc_done:
+++	movdqu	%xmm6,(%r8)
+++.Lcbc_abort:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	vpaes_cbc_encrypt,.-vpaes_cbc_encrypt
+++.globl	vpaes_ctr32_encrypt_blocks
+++.hidden vpaes_ctr32_encrypt_blocks
+++.type	vpaes_ctr32_encrypt_blocks,@function
+++.align	16
+++vpaes_ctr32_encrypt_blocks:
+++.cfi_startproc	
+++
+++	xchgq	%rcx,%rdx
+++	testq	%rcx,%rcx
+++	jz	.Lctr32_abort
+++	movdqu	(%r8),%xmm0
+++	movdqa	.Lctr_add_one(%rip),%xmm8
+++	subq	%rdi,%rsi
+++	call	_vpaes_preheat
+++	movdqa	%xmm0,%xmm6
+++	pshufb	.Lrev_ctr(%rip),%xmm6
+++
+++	testq	$1,%rcx
+++	jz	.Lctr32_prep_loop
+++
+++
+++
+++	movdqu	(%rdi),%xmm7
+++	call	_vpaes_encrypt_core
+++	pxor	%xmm7,%xmm0
+++	paddd	%xmm8,%xmm6
+++	movdqu	%xmm0,(%rsi,%rdi,1)
+++	subq	$1,%rcx
+++	leaq	16(%rdi),%rdi
+++	jz	.Lctr32_done
+++
+++.Lctr32_prep_loop:
+++
+++
+++	movdqa	%xmm6,%xmm14
+++	movdqa	%xmm6,%xmm15
+++	paddd	%xmm8,%xmm15
+++
+++.Lctr32_loop:
+++	movdqa	.Lrev_ctr(%rip),%xmm1
+++	movdqa	%xmm14,%xmm0
+++	movdqa	%xmm15,%xmm6
+++.byte	102,15,56,0,193
+++.byte	102,15,56,0,241
+++	call	_vpaes_encrypt_core_2x
+++	movdqu	(%rdi),%xmm1
+++	movdqu	16(%rdi),%xmm2
+++	movdqa	.Lctr_add_two(%rip),%xmm3
+++	pxor	%xmm1,%xmm0
+++	pxor	%xmm2,%xmm6
+++	paddd	%xmm3,%xmm14
+++	paddd	%xmm3,%xmm15
+++	movdqu	%xmm0,(%rsi,%rdi,1)
+++	movdqu	%xmm6,16(%rsi,%rdi,1)
+++	subq	$2,%rcx
+++	leaq	32(%rdi),%rdi
+++	jnz	.Lctr32_loop
+++
+++.Lctr32_done:
+++.Lctr32_abort:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	vpaes_ctr32_encrypt_blocks,.-vpaes_ctr32_encrypt_blocks
+++
+++
+++
+++
+++
+++
+++.type	_vpaes_preheat,@function
+++.align	16
+++_vpaes_preheat:
+++.cfi_startproc	
+++	leaq	.Lk_s0F(%rip),%r10
+++	movdqa	-32(%r10),%xmm10
+++	movdqa	-16(%r10),%xmm11
+++	movdqa	0(%r10),%xmm9
+++	movdqa	48(%r10),%xmm13
+++	movdqa	64(%r10),%xmm12
+++	movdqa	80(%r10),%xmm15
+++	movdqa	96(%r10),%xmm14
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	_vpaes_preheat,.-_vpaes_preheat
+++
+++
+++
+++
+++
+++.type	_vpaes_consts,@object
+++.align	64
+++_vpaes_consts:
+++.Lk_inv:
+++.quad	0x0E05060F0D080180, 0x040703090A0B0C02
+++.quad	0x01040A060F0B0780, 0x030D0E0C02050809
+++
+++.Lk_s0F:
+++.quad	0x0F0F0F0F0F0F0F0F, 0x0F0F0F0F0F0F0F0F
+++
+++.Lk_ipt:
+++.quad	0xC2B2E8985A2A7000, 0xCABAE09052227808
+++.quad	0x4C01307D317C4D00, 0xCD80B1FCB0FDCC81
+++
+++.Lk_sb1:
+++.quad	0xB19BE18FCB503E00, 0xA5DF7A6E142AF544
+++.quad	0x3618D415FAE22300, 0x3BF7CCC10D2ED9EF
+++.Lk_sb2:
+++.quad	0xE27A93C60B712400, 0x5EB7E955BC982FCD
+++.quad	0x69EB88400AE12900, 0xC2A163C8AB82234A
+++.Lk_sbo:
+++.quad	0xD0D26D176FBDC700, 0x15AABF7AC502A878
+++.quad	0xCFE474A55FBB6A00, 0x8E1E90D1412B35FA
+++
+++.Lk_mc_forward:
+++.quad	0x0407060500030201, 0x0C0F0E0D080B0A09
+++.quad	0x080B0A0904070605, 0x000302010C0F0E0D
+++.quad	0x0C0F0E0D080B0A09, 0x0407060500030201
+++.quad	0x000302010C0F0E0D, 0x080B0A0904070605
+++
+++.Lk_mc_backward:
+++.quad	0x0605040702010003, 0x0E0D0C0F0A09080B
+++.quad	0x020100030E0D0C0F, 0x0A09080B06050407
+++.quad	0x0E0D0C0F0A09080B, 0x0605040702010003
+++.quad	0x0A09080B06050407, 0x020100030E0D0C0F
+++
+++.Lk_sr:
+++.quad	0x0706050403020100, 0x0F0E0D0C0B0A0908
+++.quad	0x030E09040F0A0500, 0x0B06010C07020D08
+++.quad	0x0F060D040B020900, 0x070E050C030A0108
+++.quad	0x0B0E0104070A0D00, 0x0306090C0F020508
+++
+++.Lk_rcon:
+++.quad	0x1F8391B9AF9DEEB6, 0x702A98084D7C7D81
+++
+++.Lk_s63:
+++.quad	0x5B5B5B5B5B5B5B5B, 0x5B5B5B5B5B5B5B5B
+++
+++.Lk_opt:
+++.quad	0xFF9F4929D6B66000, 0xF7974121DEBE6808
+++.quad	0x01EDBD5150BCEC00, 0xE10D5DB1B05C0CE0
+++
+++.Lk_deskew:
+++.quad	0x07E4A34047A4E300, 0x1DFEB95A5DBEF91A
+++.quad	0x5F36B5DC83EA6900, 0x2841C2ABF49D1E77
+++
+++
+++
+++
+++
+++.Lk_dksd:
+++.quad	0xFEB91A5DA3E44700, 0x0740E3A45A1DBEF9
+++.quad	0x41C277F4B5368300, 0x5FDC69EAAB289D1E
+++.Lk_dksb:
+++.quad	0x9A4FCA1F8550D500, 0x03D653861CC94C99
+++.quad	0x115BEDA7B6FC4A00, 0xD993256F7E3482C8
+++.Lk_dkse:
+++.quad	0xD5031CCA1FC9D600, 0x53859A4C994F5086
+++.quad	0xA23196054FDC7BE8, 0xCD5EF96A20B31487
+++.Lk_dks9:
+++.quad	0xB6116FC87ED9A700, 0x4AED933482255BFC
+++.quad	0x4576516227143300, 0x8BB89FACE9DAFDCE
+++
+++
+++
+++
+++
+++.Lk_dipt:
+++.quad	0x0F505B040B545F00, 0x154A411E114E451A
+++.quad	0x86E383E660056500, 0x12771772F491F194
+++
+++.Lk_dsb9:
+++.quad	0x851C03539A86D600, 0xCAD51F504F994CC9
+++.quad	0xC03B1789ECD74900, 0x725E2C9EB2FBA565
+++.Lk_dsbd:
+++.quad	0x7D57CCDFE6B1A200, 0xF56E9B13882A4439
+++.quad	0x3CE2FAF724C6CB00, 0x2931180D15DEEFD3
+++.Lk_dsbb:
+++.quad	0xD022649296B44200, 0x602646F6B0F2D404
+++.quad	0xC19498A6CD596700, 0xF3FF0C3E3255AA6B
+++.Lk_dsbe:
+++.quad	0x46F2929626D4D000, 0x2242600464B4F6B0
+++.quad	0x0C55A6CDFFAAC100, 0x9467F36B98593E32
+++.Lk_dsbo:
+++.quad	0x1387EA537EF94000, 0xC7AA6DB9D4943E2D
+++.quad	0x12D7560F93441D00, 0xCA4B8159D8C58E9C
+++
+++
+++.Lrev_ctr:
+++.quad	0x0706050403020100, 0x0c0d0e0f0b0a0908
+++
+++
+++.Lctr_add_one:
+++.quad	0x0000000000000000, 0x0000000100000000
+++.Lctr_add_two:
+++.quad	0x0000000000000000, 0x0000000200000000
+++
+++.byte	86,101,99,116,111,114,32,80,101,114,109,117,116,97,116,105,111,110,32,65,69,83,32,102,111,114,32,120,56,54,95,54,52,47,83,83,83,69,51,44,32,77,105,107,101,32,72,97,109,98,117,114,103,32,40,83,116,97,110,102,111,114,100,32,85,110,105,118,101,114,115,105,116,121,41,0
+++.align	64
+++.size	_vpaes_consts,.-_vpaes_consts
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/x86_64-mont.S b/linux-x86_64/ypto/fipsmodule/x86_64-mont.S
++new file mode 100644
++index 000000000..bdb445421
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/x86_64-mont.S
++@@ -0,0 +1,1260 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++
+++.extern	OPENSSL_ia32cap_P
+++.hidden OPENSSL_ia32cap_P
+++
+++.globl	bn_mul_mont
+++.hidden bn_mul_mont
+++.type	bn_mul_mont,@function
+++.align	16
+++bn_mul_mont:
+++.cfi_startproc	
+++	movl	%r9d,%r9d
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++	testl	$3,%r9d
+++	jnz	.Lmul_enter
+++	cmpl	$8,%r9d
+++	jb	.Lmul_enter
+++	leaq	OPENSSL_ia32cap_P(%rip),%r11
+++	movl	8(%r11),%r11d
+++	cmpq	%rsi,%rdx
+++	jne	.Lmul4x_enter
+++	testl	$7,%r9d
+++	jz	.Lsqr8x_enter
+++	jmp	.Lmul4x_enter
+++
+++.align	16
+++.Lmul_enter:
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++
+++	negq	%r9
+++	movq	%rsp,%r11
+++	leaq	-16(%rsp,%r9,8),%r10
+++	negq	%r9
+++	andq	$-1024,%r10
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	subq	%r10,%r11
+++	andq	$-4096,%r11
+++	leaq	(%r10,%r11,1),%rsp
+++	movq	(%rsp),%r11
+++	cmpq	%r10,%rsp
+++	ja	.Lmul_page_walk
+++	jmp	.Lmul_page_walk_done
+++
+++.align	16
+++.Lmul_page_walk:
+++	leaq	-4096(%rsp),%rsp
+++	movq	(%rsp),%r11
+++	cmpq	%r10,%rsp
+++	ja	.Lmul_page_walk
+++.Lmul_page_walk_done:
+++
+++	movq	%rax,8(%rsp,%r9,8)
+++.cfi_escape	0x0f,0x0a,0x77,0x08,0x79,0x00,0x38,0x1e,0x22,0x06,0x23,0x08
+++.Lmul_body:
+++	movq	%rdx,%r12
+++	movq	(%r8),%r8
+++	movq	(%r12),%rbx
+++	movq	(%rsi),%rax
+++
+++	xorq	%r14,%r14
+++	xorq	%r15,%r15
+++
+++	movq	%r8,%rbp
+++	mulq	%rbx
+++	movq	%rax,%r10
+++	movq	(%rcx),%rax
+++
+++	imulq	%r10,%rbp
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r10
+++	movq	8(%rsi),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r13
+++
+++	leaq	1(%r15),%r15
+++	jmp	.L1st_enter
+++
+++.align	16
+++.L1st:
+++	addq	%rax,%r13
+++	movq	(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%r13
+++	movq	%r10,%r11
+++	adcq	$0,%rdx
+++	movq	%r13,-16(%rsp,%r15,8)
+++	movq	%rdx,%r13
+++
+++.L1st_enter:
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	leaq	1(%r15),%r15
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	cmpq	%r9,%r15
+++	jne	.L1st
+++
+++	addq	%rax,%r13
+++	movq	(%rsi),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-16(%rsp,%r15,8)
+++	movq	%rdx,%r13
+++	movq	%r10,%r11
+++
+++	xorq	%rdx,%rdx
+++	addq	%r11,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-8(%rsp,%r9,8)
+++	movq	%rdx,(%rsp,%r9,8)
+++
+++	leaq	1(%r14),%r14
+++	jmp	.Louter
+++.align	16
+++.Louter:
+++	movq	(%r12,%r14,8),%rbx
+++	xorq	%r15,%r15
+++	movq	%r8,%rbp
+++	movq	(%rsp),%r10
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	(%rcx),%rax
+++	adcq	$0,%rdx
+++
+++	imulq	%r10,%rbp
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r10
+++	movq	8(%rsi),%rax
+++	adcq	$0,%rdx
+++	movq	8(%rsp),%r10
+++	movq	%rdx,%r13
+++
+++	leaq	1(%r15),%r15
+++	jmp	.Linner_enter
+++
+++.align	16
+++.Linner:
+++	addq	%rax,%r13
+++	movq	(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	movq	(%rsp,%r15,8),%r10
+++	adcq	$0,%rdx
+++	movq	%r13,-16(%rsp,%r15,8)
+++	movq	%rdx,%r13
+++
+++.Linner_enter:
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%r10
+++	movq	%rdx,%r11
+++	adcq	$0,%r11
+++	leaq	1(%r15),%r15
+++
+++	mulq	%rbp
+++	cmpq	%r9,%r15
+++	jne	.Linner
+++
+++	addq	%rax,%r13
+++	movq	(%rsi),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	movq	(%rsp,%r15,8),%r10
+++	adcq	$0,%rdx
+++	movq	%r13,-16(%rsp,%r15,8)
+++	movq	%rdx,%r13
+++
+++	xorq	%rdx,%rdx
+++	addq	%r11,%r13
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-8(%rsp,%r9,8)
+++	movq	%rdx,(%rsp,%r9,8)
+++
+++	leaq	1(%r14),%r14
+++	cmpq	%r9,%r14
+++	jb	.Louter
+++
+++	xorq	%r14,%r14
+++	movq	(%rsp),%rax
+++	movq	%r9,%r15
+++
+++.align	16
+++.Lsub:	sbbq	(%rcx,%r14,8),%rax
+++	movq	%rax,(%rdi,%r14,8)
+++	movq	8(%rsp,%r14,8),%rax
+++	leaq	1(%r14),%r14
+++	decq	%r15
+++	jnz	.Lsub
+++
+++	sbbq	$0,%rax
+++	movq	$-1,%rbx
+++	xorq	%rax,%rbx
+++	xorq	%r14,%r14
+++	movq	%r9,%r15
+++
+++.Lcopy:
+++	movq	(%rdi,%r14,8),%rcx
+++	movq	(%rsp,%r14,8),%rdx
+++	andq	%rbx,%rcx
+++	andq	%rax,%rdx
+++	movq	%r9,(%rsp,%r14,8)
+++	orq	%rcx,%rdx
+++	movq	%rdx,(%rdi,%r14,8)
+++	leaq	1(%r14),%r14
+++	subq	$1,%r15
+++	jnz	.Lcopy
+++
+++	movq	8(%rsp,%r9,8),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	$1,%rax
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lmul_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_mul_mont,.-bn_mul_mont
+++.type	bn_mul4x_mont,@function
+++.align	16
+++bn_mul4x_mont:
+++.cfi_startproc	
+++	movl	%r9d,%r9d
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++.Lmul4x_enter:
+++	andl	$0x80100,%r11d
+++	cmpl	$0x80100,%r11d
+++	je	.Lmulx4x_enter
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++
+++	negq	%r9
+++	movq	%rsp,%r11
+++	leaq	-32(%rsp,%r9,8),%r10
+++	negq	%r9
+++	andq	$-1024,%r10
+++
+++	subq	%r10,%r11
+++	andq	$-4096,%r11
+++	leaq	(%r10,%r11,1),%rsp
+++	movq	(%rsp),%r11
+++	cmpq	%r10,%rsp
+++	ja	.Lmul4x_page_walk
+++	jmp	.Lmul4x_page_walk_done
+++
+++.Lmul4x_page_walk:
+++	leaq	-4096(%rsp),%rsp
+++	movq	(%rsp),%r11
+++	cmpq	%r10,%rsp
+++	ja	.Lmul4x_page_walk
+++.Lmul4x_page_walk_done:
+++
+++	movq	%rax,8(%rsp,%r9,8)
+++.cfi_escape	0x0f,0x0a,0x77,0x08,0x79,0x00,0x38,0x1e,0x22,0x06,0x23,0x08
+++.Lmul4x_body:
+++	movq	%rdi,16(%rsp,%r9,8)
+++	movq	%rdx,%r12
+++	movq	(%r8),%r8
+++	movq	(%r12),%rbx
+++	movq	(%rsi),%rax
+++
+++	xorq	%r14,%r14
+++	xorq	%r15,%r15
+++
+++	movq	%r8,%rbp
+++	mulq	%rbx
+++	movq	%rax,%r10
+++	movq	(%rcx),%rax
+++
+++	imulq	%r10,%rbp
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r10
+++	movq	8(%rsi),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	8(%rcx),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	16(%rsi),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	leaq	4(%r15),%r15
+++	adcq	$0,%rdx
+++	movq	%rdi,(%rsp)
+++	movq	%rdx,%r13
+++	jmp	.L1st4x
+++.align	16
+++.L1st4x:
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	-16(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r13
+++	movq	-8(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-24(%rsp,%r15,8)
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	-8(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	adcq	$0,%rdx
+++	movq	%rdi,-16(%rsp,%r15,8)
+++	movq	%rdx,%r13
+++
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r13
+++	movq	8(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-8(%rsp,%r15,8)
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	8(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	leaq	4(%r15),%r15
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	-16(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	adcq	$0,%rdx
+++	movq	%rdi,-32(%rsp,%r15,8)
+++	movq	%rdx,%r13
+++	cmpq	%r9,%r15
+++	jb	.L1st4x
+++
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	-16(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r13
+++	movq	-8(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-24(%rsp,%r15,8)
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	-8(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	(%rsi),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	adcq	$0,%rdx
+++	movq	%rdi,-16(%rsp,%r15,8)
+++	movq	%rdx,%r13
+++
+++	xorq	%rdi,%rdi
+++	addq	%r10,%r13
+++	adcq	$0,%rdi
+++	movq	%r13,-8(%rsp,%r15,8)
+++	movq	%rdi,(%rsp,%r15,8)
+++
+++	leaq	1(%r14),%r14
+++.align	4
+++.Louter4x:
+++	movq	(%r12,%r14,8),%rbx
+++	xorq	%r15,%r15
+++	movq	(%rsp),%r10
+++	movq	%r8,%rbp
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	(%rcx),%rax
+++	adcq	$0,%rdx
+++
+++	imulq	%r10,%rbp
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r10
+++	movq	8(%rsi),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	8(%rcx),%rax
+++	adcq	$0,%rdx
+++	addq	8(%rsp),%r11
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	16(%rsi),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	leaq	4(%r15),%r15
+++	adcq	$0,%rdx
+++	movq	%rdi,(%rsp)
+++	movq	%rdx,%r13
+++	jmp	.Linner4x
+++.align	16
+++.Linner4x:
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	-16(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	-16(%rsp,%r15,8),%r10
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r13
+++	movq	-8(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-24(%rsp,%r15,8)
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	-8(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	-8(%rsp,%r15,8),%r11
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	adcq	$0,%rdx
+++	movq	%rdi,-16(%rsp,%r15,8)
+++	movq	%rdx,%r13
+++
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	(%rsp,%r15,8),%r10
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r13
+++	movq	8(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-8(%rsp,%r15,8)
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	8(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	8(%rsp,%r15,8),%r11
+++	adcq	$0,%rdx
+++	leaq	4(%r15),%r15
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	-16(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	adcq	$0,%rdx
+++	movq	%rdi,-32(%rsp,%r15,8)
+++	movq	%rdx,%r13
+++	cmpq	%r9,%r15
+++	jb	.Linner4x
+++
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	-16(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	-16(%rsp,%r15,8),%r10
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r13
+++	movq	-8(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-24(%rsp,%r15,8)
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	-8(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	-8(%rsp,%r15,8),%r11
+++	adcq	$0,%rdx
+++	leaq	1(%r14),%r14
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	(%rsi),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	adcq	$0,%rdx
+++	movq	%rdi,-16(%rsp,%r15,8)
+++	movq	%rdx,%r13
+++
+++	xorq	%rdi,%rdi
+++	addq	%r10,%r13
+++	adcq	$0,%rdi
+++	addq	(%rsp,%r9,8),%r13
+++	adcq	$0,%rdi
+++	movq	%r13,-8(%rsp,%r15,8)
+++	movq	%rdi,(%rsp,%r15,8)
+++
+++	cmpq	%r9,%r14
+++	jb	.Louter4x
+++	movq	16(%rsp,%r9,8),%rdi
+++	leaq	-4(%r9),%r15
+++	movq	0(%rsp),%rax
+++	movq	8(%rsp),%rdx
+++	shrq	$2,%r15
+++	leaq	(%rsp),%rsi
+++	xorq	%r14,%r14
+++
+++	subq	0(%rcx),%rax
+++	movq	16(%rsi),%rbx
+++	movq	24(%rsi),%rbp
+++	sbbq	8(%rcx),%rdx
+++
+++.Lsub4x:
+++	movq	%rax,0(%rdi,%r14,8)
+++	movq	%rdx,8(%rdi,%r14,8)
+++	sbbq	16(%rcx,%r14,8),%rbx
+++	movq	32(%rsi,%r14,8),%rax
+++	movq	40(%rsi,%r14,8),%rdx
+++	sbbq	24(%rcx,%r14,8),%rbp
+++	movq	%rbx,16(%rdi,%r14,8)
+++	movq	%rbp,24(%rdi,%r14,8)
+++	sbbq	32(%rcx,%r14,8),%rax
+++	movq	48(%rsi,%r14,8),%rbx
+++	movq	56(%rsi,%r14,8),%rbp
+++	sbbq	40(%rcx,%r14,8),%rdx
+++	leaq	4(%r14),%r14
+++	decq	%r15
+++	jnz	.Lsub4x
+++
+++	movq	%rax,0(%rdi,%r14,8)
+++	movq	32(%rsi,%r14,8),%rax
+++	sbbq	16(%rcx,%r14,8),%rbx
+++	movq	%rdx,8(%rdi,%r14,8)
+++	sbbq	24(%rcx,%r14,8),%rbp
+++	movq	%rbx,16(%rdi,%r14,8)
+++
+++	sbbq	$0,%rax
+++	movq	%rbp,24(%rdi,%r14,8)
+++	pxor	%xmm0,%xmm0
+++.byte	102,72,15,110,224
+++	pcmpeqd	%xmm5,%xmm5
+++	pshufd	$0,%xmm4,%xmm4
+++	movq	%r9,%r15
+++	pxor	%xmm4,%xmm5
+++	shrq	$2,%r15
+++	xorl	%eax,%eax
+++
+++	jmp	.Lcopy4x
+++.align	16
+++.Lcopy4x:
+++	movdqa	(%rsp,%rax,1),%xmm1
+++	movdqu	(%rdi,%rax,1),%xmm2
+++	pand	%xmm4,%xmm1
+++	pand	%xmm5,%xmm2
+++	movdqa	16(%rsp,%rax,1),%xmm3
+++	movdqa	%xmm0,(%rsp,%rax,1)
+++	por	%xmm2,%xmm1
+++	movdqu	16(%rdi,%rax,1),%xmm2
+++	movdqu	%xmm1,(%rdi,%rax,1)
+++	pand	%xmm4,%xmm3
+++	pand	%xmm5,%xmm2
+++	movdqa	%xmm0,16(%rsp,%rax,1)
+++	por	%xmm2,%xmm3
+++	movdqu	%xmm3,16(%rdi,%rax,1)
+++	leaq	32(%rax),%rax
+++	decq	%r15
+++	jnz	.Lcopy4x
+++	movq	8(%rsp,%r9,8),%rsi
+++.cfi_def_cfa	%rsi, 8
+++	movq	$1,%rax
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lmul4x_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_mul4x_mont,.-bn_mul4x_mont
+++.extern	bn_sqrx8x_internal
+++.hidden bn_sqrx8x_internal
+++.extern	bn_sqr8x_internal
+++.hidden bn_sqr8x_internal
+++
+++.type	bn_sqr8x_mont,@function
+++.align	32
+++bn_sqr8x_mont:
+++.cfi_startproc	
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++.Lsqr8x_enter:
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++.Lsqr8x_prologue:
+++
+++	movl	%r9d,%r10d
+++	shll	$3,%r9d
+++	shlq	$3+2,%r10
+++	negq	%r9
+++
+++
+++
+++
+++
+++
+++	leaq	-64(%rsp,%r9,2),%r11
+++	movq	%rsp,%rbp
+++	movq	(%r8),%r8
+++	subq	%rsi,%r11
+++	andq	$4095,%r11
+++	cmpq	%r11,%r10
+++	jb	.Lsqr8x_sp_alt
+++	subq	%r11,%rbp
+++	leaq	-64(%rbp,%r9,2),%rbp
+++	jmp	.Lsqr8x_sp_done
+++
+++.align	32
+++.Lsqr8x_sp_alt:
+++	leaq	4096-64(,%r9,2),%r10
+++	leaq	-64(%rbp,%r9,2),%rbp
+++	subq	%r10,%r11
+++	movq	$0,%r10
+++	cmovcq	%r10,%r11
+++	subq	%r11,%rbp
+++.Lsqr8x_sp_done:
+++	andq	$-64,%rbp
+++	movq	%rsp,%r11
+++	subq	%rbp,%r11
+++	andq	$-4096,%r11
+++	leaq	(%r11,%rbp,1),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lsqr8x_page_walk
+++	jmp	.Lsqr8x_page_walk_done
+++
+++.align	16
+++.Lsqr8x_page_walk:
+++	leaq	-4096(%rsp),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lsqr8x_page_walk
+++.Lsqr8x_page_walk_done:
+++
+++	movq	%r9,%r10
+++	negq	%r9
+++
+++	movq	%r8,32(%rsp)
+++	movq	%rax,40(%rsp)
+++.cfi_escape	0x0f,0x05,0x77,0x28,0x06,0x23,0x08
+++.Lsqr8x_body:
+++
+++.byte	102,72,15,110,209
+++	pxor	%xmm0,%xmm0
+++.byte	102,72,15,110,207
+++.byte	102,73,15,110,218
+++	leaq	OPENSSL_ia32cap_P(%rip),%rax
+++	movl	8(%rax),%eax
+++	andl	$0x80100,%eax
+++	cmpl	$0x80100,%eax
+++	jne	.Lsqr8x_nox
+++
+++	call	bn_sqrx8x_internal
+++
+++
+++
+++
+++	leaq	(%r8,%rcx,1),%rbx
+++	movq	%rcx,%r9
+++	movq	%rcx,%rdx
+++.byte	102,72,15,126,207
+++	sarq	$3+2,%rcx
+++	jmp	.Lsqr8x_sub
+++
+++.align	32
+++.Lsqr8x_nox:
+++	call	bn_sqr8x_internal
+++
+++
+++
+++
+++	leaq	(%rdi,%r9,1),%rbx
+++	movq	%r9,%rcx
+++	movq	%r9,%rdx
+++.byte	102,72,15,126,207
+++	sarq	$3+2,%rcx
+++	jmp	.Lsqr8x_sub
+++
+++.align	32
+++.Lsqr8x_sub:
+++	movq	0(%rbx),%r12
+++	movq	8(%rbx),%r13
+++	movq	16(%rbx),%r14
+++	movq	24(%rbx),%r15
+++	leaq	32(%rbx),%rbx
+++	sbbq	0(%rbp),%r12
+++	sbbq	8(%rbp),%r13
+++	sbbq	16(%rbp),%r14
+++	sbbq	24(%rbp),%r15
+++	leaq	32(%rbp),%rbp
+++	movq	%r12,0(%rdi)
+++	movq	%r13,8(%rdi)
+++	movq	%r14,16(%rdi)
+++	movq	%r15,24(%rdi)
+++	leaq	32(%rdi),%rdi
+++	incq	%rcx
+++	jnz	.Lsqr8x_sub
+++
+++	sbbq	$0,%rax
+++	leaq	(%rbx,%r9,1),%rbx
+++	leaq	(%rdi,%r9,1),%rdi
+++
+++.byte	102,72,15,110,200
+++	pxor	%xmm0,%xmm0
+++	pshufd	$0,%xmm1,%xmm1
+++	movq	40(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	jmp	.Lsqr8x_cond_copy
+++
+++.align	32
+++.Lsqr8x_cond_copy:
+++	movdqa	0(%rbx),%xmm2
+++	movdqa	16(%rbx),%xmm3
+++	leaq	32(%rbx),%rbx
+++	movdqu	0(%rdi),%xmm4
+++	movdqu	16(%rdi),%xmm5
+++	leaq	32(%rdi),%rdi
+++	movdqa	%xmm0,-32(%rbx)
+++	movdqa	%xmm0,-16(%rbx)
+++	movdqa	%xmm0,-32(%rbx,%rdx,1)
+++	movdqa	%xmm0,-16(%rbx,%rdx,1)
+++	pcmpeqd	%xmm1,%xmm0
+++	pand	%xmm1,%xmm2
+++	pand	%xmm1,%xmm3
+++	pand	%xmm0,%xmm4
+++	pand	%xmm0,%xmm5
+++	pxor	%xmm0,%xmm0
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqu	%xmm4,-32(%rdi)
+++	movdqu	%xmm5,-16(%rdi)
+++	addq	$32,%r9
+++	jnz	.Lsqr8x_cond_copy
+++
+++	movq	$1,%rax
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lsqr8x_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_sqr8x_mont,.-bn_sqr8x_mont
+++.type	bn_mulx4x_mont,@function
+++.align	32
+++bn_mulx4x_mont:
+++.cfi_startproc	
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++.Lmulx4x_enter:
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++.Lmulx4x_prologue:
+++
+++	shll	$3,%r9d
+++	xorq	%r10,%r10
+++	subq	%r9,%r10
+++	movq	(%r8),%r8
+++	leaq	-72(%rsp,%r10,1),%rbp
+++	andq	$-128,%rbp
+++	movq	%rsp,%r11
+++	subq	%rbp,%r11
+++	andq	$-4096,%r11
+++	leaq	(%r11,%rbp,1),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lmulx4x_page_walk
+++	jmp	.Lmulx4x_page_walk_done
+++
+++.align	16
+++.Lmulx4x_page_walk:
+++	leaq	-4096(%rsp),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lmulx4x_page_walk
+++.Lmulx4x_page_walk_done:
+++
+++	leaq	(%rdx,%r9,1),%r10
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	movq	%r9,0(%rsp)
+++	shrq	$5,%r9
+++	movq	%r10,16(%rsp)
+++	subq	$1,%r9
+++	movq	%r8,24(%rsp)
+++	movq	%rdi,32(%rsp)
+++	movq	%rax,40(%rsp)
+++.cfi_escape	0x0f,0x05,0x77,0x28,0x06,0x23,0x08
+++	movq	%r9,48(%rsp)
+++	jmp	.Lmulx4x_body
+++
+++.align	32
+++.Lmulx4x_body:
+++	leaq	8(%rdx),%rdi
+++	movq	(%rdx),%rdx
+++	leaq	64+32(%rsp),%rbx
+++	movq	%rdx,%r9
+++
+++	mulxq	0(%rsi),%r8,%rax
+++	mulxq	8(%rsi),%r11,%r14
+++	addq	%rax,%r11
+++	movq	%rdi,8(%rsp)
+++	mulxq	16(%rsi),%r12,%r13
+++	adcq	%r14,%r12
+++	adcq	$0,%r13
+++
+++	movq	%r8,%rdi
+++	imulq	24(%rsp),%r8
+++	xorq	%rbp,%rbp
+++
+++	mulxq	24(%rsi),%rax,%r14
+++	movq	%r8,%rdx
+++	leaq	32(%rsi),%rsi
+++	adcxq	%rax,%r13
+++	adcxq	%rbp,%r14
+++
+++	mulxq	0(%rcx),%rax,%r10
+++	adcxq	%rax,%rdi
+++	adoxq	%r11,%r10
+++	mulxq	8(%rcx),%rax,%r11
+++	adcxq	%rax,%r10
+++	adoxq	%r12,%r11
+++.byte	0xc4,0x62,0xfb,0xf6,0xa1,0x10,0x00,0x00,0x00
+++	movq	48(%rsp),%rdi
+++	movq	%r10,-32(%rbx)
+++	adcxq	%rax,%r11
+++	adoxq	%r13,%r12
+++	mulxq	24(%rcx),%rax,%r15
+++	movq	%r9,%rdx
+++	movq	%r11,-24(%rbx)
+++	adcxq	%rax,%r12
+++	adoxq	%rbp,%r15
+++	leaq	32(%rcx),%rcx
+++	movq	%r12,-16(%rbx)
+++
+++	jmp	.Lmulx4x_1st
+++
+++.align	32
+++.Lmulx4x_1st:
+++	adcxq	%rbp,%r15
+++	mulxq	0(%rsi),%r10,%rax
+++	adcxq	%r14,%r10
+++	mulxq	8(%rsi),%r11,%r14
+++	adcxq	%rax,%r11
+++	mulxq	16(%rsi),%r12,%rax
+++	adcxq	%r14,%r12
+++	mulxq	24(%rsi),%r13,%r14
+++.byte	0x67,0x67
+++	movq	%r8,%rdx
+++	adcxq	%rax,%r13
+++	adcxq	%rbp,%r14
+++	leaq	32(%rsi),%rsi
+++	leaq	32(%rbx),%rbx
+++
+++	adoxq	%r15,%r10
+++	mulxq	0(%rcx),%rax,%r15
+++	adcxq	%rax,%r10
+++	adoxq	%r15,%r11
+++	mulxq	8(%rcx),%rax,%r15
+++	adcxq	%rax,%r11
+++	adoxq	%r15,%r12
+++	mulxq	16(%rcx),%rax,%r15
+++	movq	%r10,-40(%rbx)
+++	adcxq	%rax,%r12
+++	movq	%r11,-32(%rbx)
+++	adoxq	%r15,%r13
+++	mulxq	24(%rcx),%rax,%r15
+++	movq	%r9,%rdx
+++	movq	%r12,-24(%rbx)
+++	adcxq	%rax,%r13
+++	adoxq	%rbp,%r15
+++	leaq	32(%rcx),%rcx
+++	movq	%r13,-16(%rbx)
+++
+++	decq	%rdi
+++	jnz	.Lmulx4x_1st
+++
+++	movq	0(%rsp),%rax
+++	movq	8(%rsp),%rdi
+++	adcq	%rbp,%r15
+++	addq	%r15,%r14
+++	sbbq	%r15,%r15
+++	movq	%r14,-8(%rbx)
+++	jmp	.Lmulx4x_outer
+++
+++.align	32
+++.Lmulx4x_outer:
+++	movq	(%rdi),%rdx
+++	leaq	8(%rdi),%rdi
+++	subq	%rax,%rsi
+++	movq	%r15,(%rbx)
+++	leaq	64+32(%rsp),%rbx
+++	subq	%rax,%rcx
+++
+++	mulxq	0(%rsi),%r8,%r11
+++	xorl	%ebp,%ebp
+++	movq	%rdx,%r9
+++	mulxq	8(%rsi),%r14,%r12
+++	adoxq	-32(%rbx),%r8
+++	adcxq	%r14,%r11
+++	mulxq	16(%rsi),%r15,%r13
+++	adoxq	-24(%rbx),%r11
+++	adcxq	%r15,%r12
+++	adoxq	-16(%rbx),%r12
+++	adcxq	%rbp,%r13
+++	adoxq	%rbp,%r13
+++
+++	movq	%rdi,8(%rsp)
+++	movq	%r8,%r15
+++	imulq	24(%rsp),%r8
+++	xorl	%ebp,%ebp
+++
+++	mulxq	24(%rsi),%rax,%r14
+++	movq	%r8,%rdx
+++	adcxq	%rax,%r13
+++	adoxq	-8(%rbx),%r13
+++	adcxq	%rbp,%r14
+++	leaq	32(%rsi),%rsi
+++	adoxq	%rbp,%r14
+++
+++	mulxq	0(%rcx),%rax,%r10
+++	adcxq	%rax,%r15
+++	adoxq	%r11,%r10
+++	mulxq	8(%rcx),%rax,%r11
+++	adcxq	%rax,%r10
+++	adoxq	%r12,%r11
+++	mulxq	16(%rcx),%rax,%r12
+++	movq	%r10,-32(%rbx)
+++	adcxq	%rax,%r11
+++	adoxq	%r13,%r12
+++	mulxq	24(%rcx),%rax,%r15
+++	movq	%r9,%rdx
+++	movq	%r11,-24(%rbx)
+++	leaq	32(%rcx),%rcx
+++	adcxq	%rax,%r12
+++	adoxq	%rbp,%r15
+++	movq	48(%rsp),%rdi
+++	movq	%r12,-16(%rbx)
+++
+++	jmp	.Lmulx4x_inner
+++
+++.align	32
+++.Lmulx4x_inner:
+++	mulxq	0(%rsi),%r10,%rax
+++	adcxq	%rbp,%r15
+++	adoxq	%r14,%r10
+++	mulxq	8(%rsi),%r11,%r14
+++	adcxq	0(%rbx),%r10
+++	adoxq	%rax,%r11
+++	mulxq	16(%rsi),%r12,%rax
+++	adcxq	8(%rbx),%r11
+++	adoxq	%r14,%r12
+++	mulxq	24(%rsi),%r13,%r14
+++	movq	%r8,%rdx
+++	adcxq	16(%rbx),%r12
+++	adoxq	%rax,%r13
+++	adcxq	24(%rbx),%r13
+++	adoxq	%rbp,%r14
+++	leaq	32(%rsi),%rsi
+++	leaq	32(%rbx),%rbx
+++	adcxq	%rbp,%r14
+++
+++	adoxq	%r15,%r10
+++	mulxq	0(%rcx),%rax,%r15
+++	adcxq	%rax,%r10
+++	adoxq	%r15,%r11
+++	mulxq	8(%rcx),%rax,%r15
+++	adcxq	%rax,%r11
+++	adoxq	%r15,%r12
+++	mulxq	16(%rcx),%rax,%r15
+++	movq	%r10,-40(%rbx)
+++	adcxq	%rax,%r12
+++	adoxq	%r15,%r13
+++	mulxq	24(%rcx),%rax,%r15
+++	movq	%r9,%rdx
+++	movq	%r11,-32(%rbx)
+++	movq	%r12,-24(%rbx)
+++	adcxq	%rax,%r13
+++	adoxq	%rbp,%r15
+++	leaq	32(%rcx),%rcx
+++	movq	%r13,-16(%rbx)
+++
+++	decq	%rdi
+++	jnz	.Lmulx4x_inner
+++
+++	movq	0(%rsp),%rax
+++	movq	8(%rsp),%rdi
+++	adcq	%rbp,%r15
+++	subq	0(%rbx),%rbp
+++	adcq	%r15,%r14
+++	sbbq	%r15,%r15
+++	movq	%r14,-8(%rbx)
+++
+++	cmpq	16(%rsp),%rdi
+++	jne	.Lmulx4x_outer
+++
+++	leaq	64(%rsp),%rbx
+++	subq	%rax,%rcx
+++	negq	%r15
+++	movq	%rax,%rdx
+++	shrq	$3+2,%rax
+++	movq	32(%rsp),%rdi
+++	jmp	.Lmulx4x_sub
+++
+++.align	32
+++.Lmulx4x_sub:
+++	movq	0(%rbx),%r11
+++	movq	8(%rbx),%r12
+++	movq	16(%rbx),%r13
+++	movq	24(%rbx),%r14
+++	leaq	32(%rbx),%rbx
+++	sbbq	0(%rcx),%r11
+++	sbbq	8(%rcx),%r12
+++	sbbq	16(%rcx),%r13
+++	sbbq	24(%rcx),%r14
+++	leaq	32(%rcx),%rcx
+++	movq	%r11,0(%rdi)
+++	movq	%r12,8(%rdi)
+++	movq	%r13,16(%rdi)
+++	movq	%r14,24(%rdi)
+++	leaq	32(%rdi),%rdi
+++	decq	%rax
+++	jnz	.Lmulx4x_sub
+++
+++	sbbq	$0,%r15
+++	leaq	64(%rsp),%rbx
+++	subq	%rdx,%rdi
+++
+++.byte	102,73,15,110,207
+++	pxor	%xmm0,%xmm0
+++	pshufd	$0,%xmm1,%xmm1
+++	movq	40(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	jmp	.Lmulx4x_cond_copy
+++
+++.align	32
+++.Lmulx4x_cond_copy:
+++	movdqa	0(%rbx),%xmm2
+++	movdqa	16(%rbx),%xmm3
+++	leaq	32(%rbx),%rbx
+++	movdqu	0(%rdi),%xmm4
+++	movdqu	16(%rdi),%xmm5
+++	leaq	32(%rdi),%rdi
+++	movdqa	%xmm0,-32(%rbx)
+++	movdqa	%xmm0,-16(%rbx)
+++	pcmpeqd	%xmm1,%xmm0
+++	pand	%xmm1,%xmm2
+++	pand	%xmm1,%xmm3
+++	pand	%xmm0,%xmm4
+++	pand	%xmm0,%xmm5
+++	pxor	%xmm0,%xmm0
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqu	%xmm4,-32(%rdi)
+++	movdqu	%xmm5,-16(%rdi)
+++	subq	$32,%rdx
+++	jnz	.Lmulx4x_cond_copy
+++
+++	movq	%rdx,(%rbx)
+++
+++	movq	$1,%rax
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lmulx4x_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_mulx4x_mont,.-bn_mulx4x_mont
+++.byte	77,111,110,116,103,111,109,101,114,121,32,77,117,108,116,105,112,108,105,99,97,116,105,111,110,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
+++.align	16
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/fipsmodule/x86_64-mont5.S b/linux-x86_64/ypto/fipsmodule/x86_64-mont5.S
++new file mode 100644
++index 000000000..c86b3b0a5
++--- /dev/null
+++++ b/linux-x86_64/ypto/fipsmodule/x86_64-mont5.S
++@@ -0,0 +1,3790 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++
+++.extern	OPENSSL_ia32cap_P
+++.hidden OPENSSL_ia32cap_P
+++
+++.globl	bn_mul_mont_gather5
+++.hidden bn_mul_mont_gather5
+++.type	bn_mul_mont_gather5,@function
+++.align	64
+++bn_mul_mont_gather5:
+++.cfi_startproc	
+++	movl	%r9d,%r9d
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++	testl	$7,%r9d
+++	jnz	.Lmul_enter
+++	leaq	OPENSSL_ia32cap_P(%rip),%r11
+++	movl	8(%r11),%r11d
+++	jmp	.Lmul4x_enter
+++
+++.align	16
+++.Lmul_enter:
+++	movd	8(%rsp),%xmm5
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++
+++	negq	%r9
+++	movq	%rsp,%r11
+++	leaq	-280(%rsp,%r9,8),%r10
+++	negq	%r9
+++	andq	$-1024,%r10
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	subq	%r10,%r11
+++	andq	$-4096,%r11
+++	leaq	(%r10,%r11,1),%rsp
+++	movq	(%rsp),%r11
+++	cmpq	%r10,%rsp
+++	ja	.Lmul_page_walk
+++	jmp	.Lmul_page_walk_done
+++
+++.Lmul_page_walk:
+++	leaq	-4096(%rsp),%rsp
+++	movq	(%rsp),%r11
+++	cmpq	%r10,%rsp
+++	ja	.Lmul_page_walk
+++.Lmul_page_walk_done:
+++
+++	leaq	.Linc(%rip),%r10
+++	movq	%rax,8(%rsp,%r9,8)
+++.cfi_escape	0x0f,0x0a,0x77,0x08,0x79,0x00,0x38,0x1e,0x22,0x06,0x23,0x08
+++.Lmul_body:
+++
+++	leaq	128(%rdx),%r12
+++	movdqa	0(%r10),%xmm0
+++	movdqa	16(%r10),%xmm1
+++	leaq	24-112(%rsp,%r9,8),%r10
+++	andq	$-16,%r10
+++
+++	pshufd	$0,%xmm5,%xmm5
+++	movdqa	%xmm1,%xmm4
+++	movdqa	%xmm1,%xmm2
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++.byte	0x67
+++	movdqa	%xmm4,%xmm3
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,112(%r10)
+++	movdqa	%xmm4,%xmm0
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,128(%r10)
+++	movdqa	%xmm4,%xmm1
+++
+++	paddd	%xmm3,%xmm0
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,144(%r10)
+++	movdqa	%xmm4,%xmm2
+++
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm3,160(%r10)
+++	movdqa	%xmm4,%xmm3
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,176(%r10)
+++	movdqa	%xmm4,%xmm0
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,192(%r10)
+++	movdqa	%xmm4,%xmm1
+++
+++	paddd	%xmm3,%xmm0
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,208(%r10)
+++	movdqa	%xmm4,%xmm2
+++
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm3,224(%r10)
+++	movdqa	%xmm4,%xmm3
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,240(%r10)
+++	movdqa	%xmm4,%xmm0
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,256(%r10)
+++	movdqa	%xmm4,%xmm1
+++
+++	paddd	%xmm3,%xmm0
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,272(%r10)
+++	movdqa	%xmm4,%xmm2
+++
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm3,288(%r10)
+++	movdqa	%xmm4,%xmm3
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,304(%r10)
+++
+++	paddd	%xmm2,%xmm3
+++.byte	0x67
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,320(%r10)
+++
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,336(%r10)
+++	pand	64(%r12),%xmm0
+++
+++	pand	80(%r12),%xmm1
+++	pand	96(%r12),%xmm2
+++	movdqa	%xmm3,352(%r10)
+++	pand	112(%r12),%xmm3
+++	por	%xmm2,%xmm0
+++	por	%xmm3,%xmm1
+++	movdqa	-128(%r12),%xmm4
+++	movdqa	-112(%r12),%xmm5
+++	movdqa	-96(%r12),%xmm2
+++	pand	112(%r10),%xmm4
+++	movdqa	-80(%r12),%xmm3
+++	pand	128(%r10),%xmm5
+++	por	%xmm4,%xmm0
+++	pand	144(%r10),%xmm2
+++	por	%xmm5,%xmm1
+++	pand	160(%r10),%xmm3
+++	por	%xmm2,%xmm0
+++	por	%xmm3,%xmm1
+++	movdqa	-64(%r12),%xmm4
+++	movdqa	-48(%r12),%xmm5
+++	movdqa	-32(%r12),%xmm2
+++	pand	176(%r10),%xmm4
+++	movdqa	-16(%r12),%xmm3
+++	pand	192(%r10),%xmm5
+++	por	%xmm4,%xmm0
+++	pand	208(%r10),%xmm2
+++	por	%xmm5,%xmm1
+++	pand	224(%r10),%xmm3
+++	por	%xmm2,%xmm0
+++	por	%xmm3,%xmm1
+++	movdqa	0(%r12),%xmm4
+++	movdqa	16(%r12),%xmm5
+++	movdqa	32(%r12),%xmm2
+++	pand	240(%r10),%xmm4
+++	movdqa	48(%r12),%xmm3
+++	pand	256(%r10),%xmm5
+++	por	%xmm4,%xmm0
+++	pand	272(%r10),%xmm2
+++	por	%xmm5,%xmm1
+++	pand	288(%r10),%xmm3
+++	por	%xmm2,%xmm0
+++	por	%xmm3,%xmm1
+++	por	%xmm1,%xmm0
+++	pshufd	$0x4e,%xmm0,%xmm1
+++	por	%xmm1,%xmm0
+++	leaq	256(%r12),%r12
+++.byte	102,72,15,126,195
+++
+++	movq	(%r8),%r8
+++	movq	(%rsi),%rax
+++
+++	xorq	%r14,%r14
+++	xorq	%r15,%r15
+++
+++	movq	%r8,%rbp
+++	mulq	%rbx
+++	movq	%rax,%r10
+++	movq	(%rcx),%rax
+++
+++	imulq	%r10,%rbp
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r10
+++	movq	8(%rsi),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r13
+++
+++	leaq	1(%r15),%r15
+++	jmp	.L1st_enter
+++
+++.align	16
+++.L1st:
+++	addq	%rax,%r13
+++	movq	(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%r13
+++	movq	%r10,%r11
+++	adcq	$0,%rdx
+++	movq	%r13,-16(%rsp,%r15,8)
+++	movq	%rdx,%r13
+++
+++.L1st_enter:
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	leaq	1(%r15),%r15
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	cmpq	%r9,%r15
+++	jne	.L1st
+++
+++
+++	addq	%rax,%r13
+++	adcq	$0,%rdx
+++	addq	%r11,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-16(%rsp,%r9,8)
+++	movq	%rdx,%r13
+++	movq	%r10,%r11
+++
+++	xorq	%rdx,%rdx
+++	addq	%r11,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-8(%rsp,%r9,8)
+++	movq	%rdx,(%rsp,%r9,8)
+++
+++	leaq	1(%r14),%r14
+++	jmp	.Louter
+++.align	16
+++.Louter:
+++	leaq	24+128(%rsp,%r9,8),%rdx
+++	andq	$-16,%rdx
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm5,%xmm5
+++	movdqa	-128(%r12),%xmm0
+++	movdqa	-112(%r12),%xmm1
+++	movdqa	-96(%r12),%xmm2
+++	movdqa	-80(%r12),%xmm3
+++	pand	-128(%rdx),%xmm0
+++	pand	-112(%rdx),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	-96(%rdx),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	-80(%rdx),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqa	-64(%r12),%xmm0
+++	movdqa	-48(%r12),%xmm1
+++	movdqa	-32(%r12),%xmm2
+++	movdqa	-16(%r12),%xmm3
+++	pand	-64(%rdx),%xmm0
+++	pand	-48(%rdx),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	-32(%rdx),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	-16(%rdx),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqa	0(%r12),%xmm0
+++	movdqa	16(%r12),%xmm1
+++	movdqa	32(%r12),%xmm2
+++	movdqa	48(%r12),%xmm3
+++	pand	0(%rdx),%xmm0
+++	pand	16(%rdx),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	32(%rdx),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	48(%rdx),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqa	64(%r12),%xmm0
+++	movdqa	80(%r12),%xmm1
+++	movdqa	96(%r12),%xmm2
+++	movdqa	112(%r12),%xmm3
+++	pand	64(%rdx),%xmm0
+++	pand	80(%rdx),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	96(%rdx),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	112(%rdx),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	por	%xmm5,%xmm4
+++	pshufd	$0x4e,%xmm4,%xmm0
+++	por	%xmm4,%xmm0
+++	leaq	256(%r12),%r12
+++
+++	movq	(%rsi),%rax
+++.byte	102,72,15,126,195
+++
+++	xorq	%r15,%r15
+++	movq	%r8,%rbp
+++	movq	(%rsp),%r10
+++
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	(%rcx),%rax
+++	adcq	$0,%rdx
+++
+++	imulq	%r10,%rbp
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r10
+++	movq	8(%rsi),%rax
+++	adcq	$0,%rdx
+++	movq	8(%rsp),%r10
+++	movq	%rdx,%r13
+++
+++	leaq	1(%r15),%r15
+++	jmp	.Linner_enter
+++
+++.align	16
+++.Linner:
+++	addq	%rax,%r13
+++	movq	(%rsi,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	movq	(%rsp,%r15,8),%r10
+++	adcq	$0,%rdx
+++	movq	%r13,-16(%rsp,%r15,8)
+++	movq	%rdx,%r13
+++
+++.Linner_enter:
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	(%rcx,%r15,8),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%r10
+++	movq	%rdx,%r11
+++	adcq	$0,%r11
+++	leaq	1(%r15),%r15
+++
+++	mulq	%rbp
+++	cmpq	%r9,%r15
+++	jne	.Linner
+++
+++	addq	%rax,%r13
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	movq	(%rsp,%r9,8),%r10
+++	adcq	$0,%rdx
+++	movq	%r13,-16(%rsp,%r9,8)
+++	movq	%rdx,%r13
+++
+++	xorq	%rdx,%rdx
+++	addq	%r11,%r13
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-8(%rsp,%r9,8)
+++	movq	%rdx,(%rsp,%r9,8)
+++
+++	leaq	1(%r14),%r14
+++	cmpq	%r9,%r14
+++	jb	.Louter
+++
+++	xorq	%r14,%r14
+++	movq	(%rsp),%rax
+++	leaq	(%rsp),%rsi
+++	movq	%r9,%r15
+++	jmp	.Lsub
+++.align	16
+++.Lsub:	sbbq	(%rcx,%r14,8),%rax
+++	movq	%rax,(%rdi,%r14,8)
+++	movq	8(%rsi,%r14,8),%rax
+++	leaq	1(%r14),%r14
+++	decq	%r15
+++	jnz	.Lsub
+++
+++	sbbq	$0,%rax
+++	movq	$-1,%rbx
+++	xorq	%rax,%rbx
+++	xorq	%r14,%r14
+++	movq	%r9,%r15
+++
+++.Lcopy:
+++	movq	(%rdi,%r14,8),%rcx
+++	movq	(%rsp,%r14,8),%rdx
+++	andq	%rbx,%rcx
+++	andq	%rax,%rdx
+++	movq	%r14,(%rsp,%r14,8)
+++	orq	%rcx,%rdx
+++	movq	%rdx,(%rdi,%r14,8)
+++	leaq	1(%r14),%r14
+++	subq	$1,%r15
+++	jnz	.Lcopy
+++
+++	movq	8(%rsp,%r9,8),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	$1,%rax
+++
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lmul_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_mul_mont_gather5,.-bn_mul_mont_gather5
+++.type	bn_mul4x_mont_gather5,@function
+++.align	32
+++bn_mul4x_mont_gather5:
+++.cfi_startproc	
+++.byte	0x67
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++.Lmul4x_enter:
+++	andl	$0x80108,%r11d
+++	cmpl	$0x80108,%r11d
+++	je	.Lmulx4x_enter
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++.Lmul4x_prologue:
+++
+++.byte	0x67
+++	shll	$3,%r9d
+++	leaq	(%r9,%r9,2),%r10
+++	negq	%r9
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	leaq	-320(%rsp,%r9,2),%r11
+++	movq	%rsp,%rbp
+++	subq	%rdi,%r11
+++	andq	$4095,%r11
+++	cmpq	%r11,%r10
+++	jb	.Lmul4xsp_alt
+++	subq	%r11,%rbp
+++	leaq	-320(%rbp,%r9,2),%rbp
+++	jmp	.Lmul4xsp_done
+++
+++.align	32
+++.Lmul4xsp_alt:
+++	leaq	4096-320(,%r9,2),%r10
+++	leaq	-320(%rbp,%r9,2),%rbp
+++	subq	%r10,%r11
+++	movq	$0,%r10
+++	cmovcq	%r10,%r11
+++	subq	%r11,%rbp
+++.Lmul4xsp_done:
+++	andq	$-64,%rbp
+++	movq	%rsp,%r11
+++	subq	%rbp,%r11
+++	andq	$-4096,%r11
+++	leaq	(%r11,%rbp,1),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lmul4x_page_walk
+++	jmp	.Lmul4x_page_walk_done
+++
+++.Lmul4x_page_walk:
+++	leaq	-4096(%rsp),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lmul4x_page_walk
+++.Lmul4x_page_walk_done:
+++
+++	negq	%r9
+++
+++	movq	%rax,40(%rsp)
+++.cfi_escape	0x0f,0x05,0x77,0x28,0x06,0x23,0x08
+++.Lmul4x_body:
+++
+++	call	mul4x_internal
+++
+++	movq	40(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	$1,%rax
+++
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lmul4x_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_mul4x_mont_gather5,.-bn_mul4x_mont_gather5
+++
+++.type	mul4x_internal,@function
+++.align	32
+++mul4x_internal:
+++.cfi_startproc	
+++	shlq	$5,%r9
+++	movd	8(%rax),%xmm5
+++	leaq	.Linc(%rip),%rax
+++	leaq	128(%rdx,%r9,1),%r13
+++	shrq	$5,%r9
+++	movdqa	0(%rax),%xmm0
+++	movdqa	16(%rax),%xmm1
+++	leaq	88-112(%rsp,%r9,1),%r10
+++	leaq	128(%rdx),%r12
+++
+++	pshufd	$0,%xmm5,%xmm5
+++	movdqa	%xmm1,%xmm4
+++.byte	0x67,0x67
+++	movdqa	%xmm1,%xmm2
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++.byte	0x67
+++	movdqa	%xmm4,%xmm3
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,112(%r10)
+++	movdqa	%xmm4,%xmm0
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,128(%r10)
+++	movdqa	%xmm4,%xmm1
+++
+++	paddd	%xmm3,%xmm0
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,144(%r10)
+++	movdqa	%xmm4,%xmm2
+++
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm3,160(%r10)
+++	movdqa	%xmm4,%xmm3
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,176(%r10)
+++	movdqa	%xmm4,%xmm0
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,192(%r10)
+++	movdqa	%xmm4,%xmm1
+++
+++	paddd	%xmm3,%xmm0
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,208(%r10)
+++	movdqa	%xmm4,%xmm2
+++
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm3,224(%r10)
+++	movdqa	%xmm4,%xmm3
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,240(%r10)
+++	movdqa	%xmm4,%xmm0
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,256(%r10)
+++	movdqa	%xmm4,%xmm1
+++
+++	paddd	%xmm3,%xmm0
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,272(%r10)
+++	movdqa	%xmm4,%xmm2
+++
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm3,288(%r10)
+++	movdqa	%xmm4,%xmm3
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,304(%r10)
+++
+++	paddd	%xmm2,%xmm3
+++.byte	0x67
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,320(%r10)
+++
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,336(%r10)
+++	pand	64(%r12),%xmm0
+++
+++	pand	80(%r12),%xmm1
+++	pand	96(%r12),%xmm2
+++	movdqa	%xmm3,352(%r10)
+++	pand	112(%r12),%xmm3
+++	por	%xmm2,%xmm0
+++	por	%xmm3,%xmm1
+++	movdqa	-128(%r12),%xmm4
+++	movdqa	-112(%r12),%xmm5
+++	movdqa	-96(%r12),%xmm2
+++	pand	112(%r10),%xmm4
+++	movdqa	-80(%r12),%xmm3
+++	pand	128(%r10),%xmm5
+++	por	%xmm4,%xmm0
+++	pand	144(%r10),%xmm2
+++	por	%xmm5,%xmm1
+++	pand	160(%r10),%xmm3
+++	por	%xmm2,%xmm0
+++	por	%xmm3,%xmm1
+++	movdqa	-64(%r12),%xmm4
+++	movdqa	-48(%r12),%xmm5
+++	movdqa	-32(%r12),%xmm2
+++	pand	176(%r10),%xmm4
+++	movdqa	-16(%r12),%xmm3
+++	pand	192(%r10),%xmm5
+++	por	%xmm4,%xmm0
+++	pand	208(%r10),%xmm2
+++	por	%xmm5,%xmm1
+++	pand	224(%r10),%xmm3
+++	por	%xmm2,%xmm0
+++	por	%xmm3,%xmm1
+++	movdqa	0(%r12),%xmm4
+++	movdqa	16(%r12),%xmm5
+++	movdqa	32(%r12),%xmm2
+++	pand	240(%r10),%xmm4
+++	movdqa	48(%r12),%xmm3
+++	pand	256(%r10),%xmm5
+++	por	%xmm4,%xmm0
+++	pand	272(%r10),%xmm2
+++	por	%xmm5,%xmm1
+++	pand	288(%r10),%xmm3
+++	por	%xmm2,%xmm0
+++	por	%xmm3,%xmm1
+++	por	%xmm1,%xmm0
+++	pshufd	$0x4e,%xmm0,%xmm1
+++	por	%xmm1,%xmm0
+++	leaq	256(%r12),%r12
+++.byte	102,72,15,126,195
+++
+++	movq	%r13,16+8(%rsp)
+++	movq	%rdi,56+8(%rsp)
+++
+++	movq	(%r8),%r8
+++	movq	(%rsi),%rax
+++	leaq	(%rsi,%r9,1),%rsi
+++	negq	%r9
+++
+++	movq	%r8,%rbp
+++	mulq	%rbx
+++	movq	%rax,%r10
+++	movq	(%rcx),%rax
+++
+++	imulq	%r10,%rbp
+++	leaq	64+8(%rsp),%r14
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r10
+++	movq	8(%rsi,%r9,1),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	8(%rcx),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	16(%rsi,%r9,1),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	leaq	32(%r9),%r15
+++	leaq	32(%rcx),%rcx
+++	adcq	$0,%rdx
+++	movq	%rdi,(%r14)
+++	movq	%rdx,%r13
+++	jmp	.L1st4x
+++
+++.align	32
+++.L1st4x:
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	-16(%rcx),%rax
+++	leaq	32(%r14),%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r13
+++	movq	-8(%rsi,%r15,1),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-24(%r14)
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	-8(%rcx),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	(%rsi,%r15,1),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	adcq	$0,%rdx
+++	movq	%rdi,-16(%r14)
+++	movq	%rdx,%r13
+++
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	0(%rcx),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r13
+++	movq	8(%rsi,%r15,1),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-8(%r14)
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	8(%rcx),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	16(%rsi,%r15,1),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	leaq	32(%rcx),%rcx
+++	adcq	$0,%rdx
+++	movq	%rdi,(%r14)
+++	movq	%rdx,%r13
+++
+++	addq	$32,%r15
+++	jnz	.L1st4x
+++
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	-16(%rcx),%rax
+++	leaq	32(%r14),%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r13
+++	movq	-8(%rsi),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%r13,-24(%r14)
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	-8(%rcx),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	(%rsi,%r9,1),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	adcq	$0,%rdx
+++	movq	%rdi,-16(%r14)
+++	movq	%rdx,%r13
+++
+++	leaq	(%rcx,%r9,1),%rcx
+++
+++	xorq	%rdi,%rdi
+++	addq	%r10,%r13
+++	adcq	$0,%rdi
+++	movq	%r13,-8(%r14)
+++
+++	jmp	.Louter4x
+++
+++.align	32
+++.Louter4x:
+++	leaq	16+128(%r14),%rdx
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm5,%xmm5
+++	movdqa	-128(%r12),%xmm0
+++	movdqa	-112(%r12),%xmm1
+++	movdqa	-96(%r12),%xmm2
+++	movdqa	-80(%r12),%xmm3
+++	pand	-128(%rdx),%xmm0
+++	pand	-112(%rdx),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	-96(%rdx),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	-80(%rdx),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqa	-64(%r12),%xmm0
+++	movdqa	-48(%r12),%xmm1
+++	movdqa	-32(%r12),%xmm2
+++	movdqa	-16(%r12),%xmm3
+++	pand	-64(%rdx),%xmm0
+++	pand	-48(%rdx),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	-32(%rdx),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	-16(%rdx),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqa	0(%r12),%xmm0
+++	movdqa	16(%r12),%xmm1
+++	movdqa	32(%r12),%xmm2
+++	movdqa	48(%r12),%xmm3
+++	pand	0(%rdx),%xmm0
+++	pand	16(%rdx),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	32(%rdx),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	48(%rdx),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqa	64(%r12),%xmm0
+++	movdqa	80(%r12),%xmm1
+++	movdqa	96(%r12),%xmm2
+++	movdqa	112(%r12),%xmm3
+++	pand	64(%rdx),%xmm0
+++	pand	80(%rdx),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	96(%rdx),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	112(%rdx),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	por	%xmm5,%xmm4
+++	pshufd	$0x4e,%xmm4,%xmm0
+++	por	%xmm4,%xmm0
+++	leaq	256(%r12),%r12
+++.byte	102,72,15,126,195
+++
+++	movq	(%r14,%r9,1),%r10
+++	movq	%r8,%rbp
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	(%rcx),%rax
+++	adcq	$0,%rdx
+++
+++	imulq	%r10,%rbp
+++	movq	%rdx,%r11
+++	movq	%rdi,(%r14)
+++
+++	leaq	(%r14,%r9,1),%r14
+++
+++	mulq	%rbp
+++	addq	%rax,%r10
+++	movq	8(%rsi,%r9,1),%rax
+++	adcq	$0,%rdx
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	8(%rcx),%rax
+++	adcq	$0,%rdx
+++	addq	8(%r14),%r11
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	16(%rsi,%r9,1),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	leaq	32(%r9),%r15
+++	leaq	32(%rcx),%rcx
+++	adcq	$0,%rdx
+++	movq	%rdx,%r13
+++	jmp	.Linner4x
+++
+++.align	32
+++.Linner4x:
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	-16(%rcx),%rax
+++	adcq	$0,%rdx
+++	addq	16(%r14),%r10
+++	leaq	32(%r14),%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r13
+++	movq	-8(%rsi,%r15,1),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%rdi,-32(%r14)
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	-8(%rcx),%rax
+++	adcq	$0,%rdx
+++	addq	-8(%r14),%r11
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	(%rsi,%r15,1),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	adcq	$0,%rdx
+++	movq	%r13,-24(%r14)
+++	movq	%rdx,%r13
+++
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	0(%rcx),%rax
+++	adcq	$0,%rdx
+++	addq	(%r14),%r10
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r13
+++	movq	8(%rsi,%r15,1),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%rdi,-16(%r14)
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	8(%rcx),%rax
+++	adcq	$0,%rdx
+++	addq	8(%r14),%r11
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	16(%rsi,%r15,1),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	leaq	32(%rcx),%rcx
+++	adcq	$0,%rdx
+++	movq	%r13,-8(%r14)
+++	movq	%rdx,%r13
+++
+++	addq	$32,%r15
+++	jnz	.Linner4x
+++
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	-16(%rcx),%rax
+++	adcq	$0,%rdx
+++	addq	16(%r14),%r10
+++	leaq	32(%r14),%r14
+++	adcq	$0,%rdx
+++	movq	%rdx,%r11
+++
+++	mulq	%rbp
+++	addq	%rax,%r13
+++	movq	-8(%rsi),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r13
+++	adcq	$0,%rdx
+++	movq	%rdi,-32(%r14)
+++	movq	%rdx,%rdi
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	%rbp,%rax
+++	movq	-8(%rcx),%rbp
+++	adcq	$0,%rdx
+++	addq	-8(%r14),%r11
+++	adcq	$0,%rdx
+++	movq	%rdx,%r10
+++
+++	mulq	%rbp
+++	addq	%rax,%rdi
+++	movq	(%rsi,%r9,1),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%rdi
+++	adcq	$0,%rdx
+++	movq	%r13,-24(%r14)
+++	movq	%rdx,%r13
+++
+++	movq	%rdi,-16(%r14)
+++	leaq	(%rcx,%r9,1),%rcx
+++
+++	xorq	%rdi,%rdi
+++	addq	%r10,%r13
+++	adcq	$0,%rdi
+++	addq	(%r14),%r13
+++	adcq	$0,%rdi
+++	movq	%r13,-8(%r14)
+++
+++	cmpq	16+8(%rsp),%r12
+++	jb	.Louter4x
+++	xorq	%rax,%rax
+++	subq	%r13,%rbp
+++	adcq	%r15,%r15
+++	orq	%r15,%rdi
+++	subq	%rdi,%rax
+++	leaq	(%r14,%r9,1),%rbx
+++	movq	(%rcx),%r12
+++	leaq	(%rcx),%rbp
+++	movq	%r9,%rcx
+++	sarq	$3+2,%rcx
+++	movq	56+8(%rsp),%rdi
+++	decq	%r12
+++	xorq	%r10,%r10
+++	movq	8(%rbp),%r13
+++	movq	16(%rbp),%r14
+++	movq	24(%rbp),%r15
+++	jmp	.Lsqr4x_sub_entry
+++.cfi_endproc	
+++.size	mul4x_internal,.-mul4x_internal
+++.globl	bn_power5
+++.hidden bn_power5
+++.type	bn_power5,@function
+++.align	32
+++bn_power5:
+++.cfi_startproc	
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++	leaq	OPENSSL_ia32cap_P(%rip),%r11
+++	movl	8(%r11),%r11d
+++	andl	$0x80108,%r11d
+++	cmpl	$0x80108,%r11d
+++	je	.Lpowerx5_enter
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++.Lpower5_prologue:
+++
+++	shll	$3,%r9d
+++	leal	(%r9,%r9,2),%r10d
+++	negq	%r9
+++	movq	(%r8),%r8
+++
+++
+++
+++
+++
+++
+++
+++
+++	leaq	-320(%rsp,%r9,2),%r11
+++	movq	%rsp,%rbp
+++	subq	%rdi,%r11
+++	andq	$4095,%r11
+++	cmpq	%r11,%r10
+++	jb	.Lpwr_sp_alt
+++	subq	%r11,%rbp
+++	leaq	-320(%rbp,%r9,2),%rbp
+++	jmp	.Lpwr_sp_done
+++
+++.align	32
+++.Lpwr_sp_alt:
+++	leaq	4096-320(,%r9,2),%r10
+++	leaq	-320(%rbp,%r9,2),%rbp
+++	subq	%r10,%r11
+++	movq	$0,%r10
+++	cmovcq	%r10,%r11
+++	subq	%r11,%rbp
+++.Lpwr_sp_done:
+++	andq	$-64,%rbp
+++	movq	%rsp,%r11
+++	subq	%rbp,%r11
+++	andq	$-4096,%r11
+++	leaq	(%r11,%rbp,1),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lpwr_page_walk
+++	jmp	.Lpwr_page_walk_done
+++
+++.Lpwr_page_walk:
+++	leaq	-4096(%rsp),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lpwr_page_walk
+++.Lpwr_page_walk_done:
+++
+++	movq	%r9,%r10
+++	negq	%r9
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	movq	%r8,32(%rsp)
+++	movq	%rax,40(%rsp)
+++.cfi_escape	0x0f,0x05,0x77,0x28,0x06,0x23,0x08
+++.Lpower5_body:
+++.byte	102,72,15,110,207
+++.byte	102,72,15,110,209
+++.byte	102,73,15,110,218
+++.byte	102,72,15,110,226
+++
+++	call	__bn_sqr8x_internal
+++	call	__bn_post4x_internal
+++	call	__bn_sqr8x_internal
+++	call	__bn_post4x_internal
+++	call	__bn_sqr8x_internal
+++	call	__bn_post4x_internal
+++	call	__bn_sqr8x_internal
+++	call	__bn_post4x_internal
+++	call	__bn_sqr8x_internal
+++	call	__bn_post4x_internal
+++
+++.byte	102,72,15,126,209
+++.byte	102,72,15,126,226
+++	movq	%rsi,%rdi
+++	movq	40(%rsp),%rax
+++	leaq	32(%rsp),%r8
+++
+++	call	mul4x_internal
+++
+++	movq	40(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	$1,%rax
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lpower5_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_power5,.-bn_power5
+++
+++.globl	bn_sqr8x_internal
+++.hidden bn_sqr8x_internal
+++.hidden	bn_sqr8x_internal
+++.type	bn_sqr8x_internal,@function
+++.align	32
+++bn_sqr8x_internal:
+++__bn_sqr8x_internal:
+++.cfi_startproc	
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	leaq	32(%r10),%rbp
+++	leaq	(%rsi,%r9,1),%rsi
+++
+++	movq	%r9,%rcx
+++
+++
+++	movq	-32(%rsi,%rbp,1),%r14
+++	leaq	48+8(%rsp,%r9,2),%rdi
+++	movq	-24(%rsi,%rbp,1),%rax
+++	leaq	-32(%rdi,%rbp,1),%rdi
+++	movq	-16(%rsi,%rbp,1),%rbx
+++	movq	%rax,%r15
+++
+++	mulq	%r14
+++	movq	%rax,%r10
+++	movq	%rbx,%rax
+++	movq	%rdx,%r11
+++	movq	%r10,-24(%rdi,%rbp,1)
+++
+++	mulq	%r14
+++	addq	%rax,%r11
+++	movq	%rbx,%rax
+++	adcq	$0,%rdx
+++	movq	%r11,-16(%rdi,%rbp,1)
+++	movq	%rdx,%r10
+++
+++
+++	movq	-8(%rsi,%rbp,1),%rbx
+++	mulq	%r15
+++	movq	%rax,%r12
+++	movq	%rbx,%rax
+++	movq	%rdx,%r13
+++
+++	leaq	(%rbp),%rcx
+++	mulq	%r14
+++	addq	%rax,%r10
+++	movq	%rbx,%rax
+++	movq	%rdx,%r11
+++	adcq	$0,%r11
+++	addq	%r12,%r10
+++	adcq	$0,%r11
+++	movq	%r10,-8(%rdi,%rcx,1)
+++	jmp	.Lsqr4x_1st
+++
+++.align	32
+++.Lsqr4x_1st:
+++	movq	(%rsi,%rcx,1),%rbx
+++	mulq	%r15
+++	addq	%rax,%r13
+++	movq	%rbx,%rax
+++	movq	%rdx,%r12
+++	adcq	$0,%r12
+++
+++	mulq	%r14
+++	addq	%rax,%r11
+++	movq	%rbx,%rax
+++	movq	8(%rsi,%rcx,1),%rbx
+++	movq	%rdx,%r10
+++	adcq	$0,%r10
+++	addq	%r13,%r11
+++	adcq	$0,%r10
+++
+++
+++	mulq	%r15
+++	addq	%rax,%r12
+++	movq	%rbx,%rax
+++	movq	%r11,(%rdi,%rcx,1)
+++	movq	%rdx,%r13
+++	adcq	$0,%r13
+++
+++	mulq	%r14
+++	addq	%rax,%r10
+++	movq	%rbx,%rax
+++	movq	16(%rsi,%rcx,1),%rbx
+++	movq	%rdx,%r11
+++	adcq	$0,%r11
+++	addq	%r12,%r10
+++	adcq	$0,%r11
+++
+++	mulq	%r15
+++	addq	%rax,%r13
+++	movq	%rbx,%rax
+++	movq	%r10,8(%rdi,%rcx,1)
+++	movq	%rdx,%r12
+++	adcq	$0,%r12
+++
+++	mulq	%r14
+++	addq	%rax,%r11
+++	movq	%rbx,%rax
+++	movq	24(%rsi,%rcx,1),%rbx
+++	movq	%rdx,%r10
+++	adcq	$0,%r10
+++	addq	%r13,%r11
+++	adcq	$0,%r10
+++
+++
+++	mulq	%r15
+++	addq	%rax,%r12
+++	movq	%rbx,%rax
+++	movq	%r11,16(%rdi,%rcx,1)
+++	movq	%rdx,%r13
+++	adcq	$0,%r13
+++	leaq	32(%rcx),%rcx
+++
+++	mulq	%r14
+++	addq	%rax,%r10
+++	movq	%rbx,%rax
+++	movq	%rdx,%r11
+++	adcq	$0,%r11
+++	addq	%r12,%r10
+++	adcq	$0,%r11
+++	movq	%r10,-8(%rdi,%rcx,1)
+++
+++	cmpq	$0,%rcx
+++	jne	.Lsqr4x_1st
+++
+++	mulq	%r15
+++	addq	%rax,%r13
+++	leaq	16(%rbp),%rbp
+++	adcq	$0,%rdx
+++	addq	%r11,%r13
+++	adcq	$0,%rdx
+++
+++	movq	%r13,(%rdi)
+++	movq	%rdx,%r12
+++	movq	%rdx,8(%rdi)
+++	jmp	.Lsqr4x_outer
+++
+++.align	32
+++.Lsqr4x_outer:
+++	movq	-32(%rsi,%rbp,1),%r14
+++	leaq	48+8(%rsp,%r9,2),%rdi
+++	movq	-24(%rsi,%rbp,1),%rax
+++	leaq	-32(%rdi,%rbp,1),%rdi
+++	movq	-16(%rsi,%rbp,1),%rbx
+++	movq	%rax,%r15
+++
+++	mulq	%r14
+++	movq	-24(%rdi,%rbp,1),%r10
+++	addq	%rax,%r10
+++	movq	%rbx,%rax
+++	adcq	$0,%rdx
+++	movq	%r10,-24(%rdi,%rbp,1)
+++	movq	%rdx,%r11
+++
+++	mulq	%r14
+++	addq	%rax,%r11
+++	movq	%rbx,%rax
+++	adcq	$0,%rdx
+++	addq	-16(%rdi,%rbp,1),%r11
+++	movq	%rdx,%r10
+++	adcq	$0,%r10
+++	movq	%r11,-16(%rdi,%rbp,1)
+++
+++	xorq	%r12,%r12
+++
+++	movq	-8(%rsi,%rbp,1),%rbx
+++	mulq	%r15
+++	addq	%rax,%r12
+++	movq	%rbx,%rax
+++	adcq	$0,%rdx
+++	addq	-8(%rdi,%rbp,1),%r12
+++	movq	%rdx,%r13
+++	adcq	$0,%r13
+++
+++	mulq	%r14
+++	addq	%rax,%r10
+++	movq	%rbx,%rax
+++	adcq	$0,%rdx
+++	addq	%r12,%r10
+++	movq	%rdx,%r11
+++	adcq	$0,%r11
+++	movq	%r10,-8(%rdi,%rbp,1)
+++
+++	leaq	(%rbp),%rcx
+++	jmp	.Lsqr4x_inner
+++
+++.align	32
+++.Lsqr4x_inner:
+++	movq	(%rsi,%rcx,1),%rbx
+++	mulq	%r15
+++	addq	%rax,%r13
+++	movq	%rbx,%rax
+++	movq	%rdx,%r12
+++	adcq	$0,%r12
+++	addq	(%rdi,%rcx,1),%r13
+++	adcq	$0,%r12
+++
+++.byte	0x67
+++	mulq	%r14
+++	addq	%rax,%r11
+++	movq	%rbx,%rax
+++	movq	8(%rsi,%rcx,1),%rbx
+++	movq	%rdx,%r10
+++	adcq	$0,%r10
+++	addq	%r13,%r11
+++	adcq	$0,%r10
+++
+++	mulq	%r15
+++	addq	%rax,%r12
+++	movq	%r11,(%rdi,%rcx,1)
+++	movq	%rbx,%rax
+++	movq	%rdx,%r13
+++	adcq	$0,%r13
+++	addq	8(%rdi,%rcx,1),%r12
+++	leaq	16(%rcx),%rcx
+++	adcq	$0,%r13
+++
+++	mulq	%r14
+++	addq	%rax,%r10
+++	movq	%rbx,%rax
+++	adcq	$0,%rdx
+++	addq	%r12,%r10
+++	movq	%rdx,%r11
+++	adcq	$0,%r11
+++	movq	%r10,-8(%rdi,%rcx,1)
+++
+++	cmpq	$0,%rcx
+++	jne	.Lsqr4x_inner
+++
+++.byte	0x67
+++	mulq	%r15
+++	addq	%rax,%r13
+++	adcq	$0,%rdx
+++	addq	%r11,%r13
+++	adcq	$0,%rdx
+++
+++	movq	%r13,(%rdi)
+++	movq	%rdx,%r12
+++	movq	%rdx,8(%rdi)
+++
+++	addq	$16,%rbp
+++	jnz	.Lsqr4x_outer
+++
+++
+++	movq	-32(%rsi),%r14
+++	leaq	48+8(%rsp,%r9,2),%rdi
+++	movq	-24(%rsi),%rax
+++	leaq	-32(%rdi,%rbp,1),%rdi
+++	movq	-16(%rsi),%rbx
+++	movq	%rax,%r15
+++
+++	mulq	%r14
+++	addq	%rax,%r10
+++	movq	%rbx,%rax
+++	movq	%rdx,%r11
+++	adcq	$0,%r11
+++
+++	mulq	%r14
+++	addq	%rax,%r11
+++	movq	%rbx,%rax
+++	movq	%r10,-24(%rdi)
+++	movq	%rdx,%r10
+++	adcq	$0,%r10
+++	addq	%r13,%r11
+++	movq	-8(%rsi),%rbx
+++	adcq	$0,%r10
+++
+++	mulq	%r15
+++	addq	%rax,%r12
+++	movq	%rbx,%rax
+++	movq	%r11,-16(%rdi)
+++	movq	%rdx,%r13
+++	adcq	$0,%r13
+++
+++	mulq	%r14
+++	addq	%rax,%r10
+++	movq	%rbx,%rax
+++	movq	%rdx,%r11
+++	adcq	$0,%r11
+++	addq	%r12,%r10
+++	adcq	$0,%r11
+++	movq	%r10,-8(%rdi)
+++
+++	mulq	%r15
+++	addq	%rax,%r13
+++	movq	-16(%rsi),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%r13
+++	adcq	$0,%rdx
+++
+++	movq	%r13,(%rdi)
+++	movq	%rdx,%r12
+++	movq	%rdx,8(%rdi)
+++
+++	mulq	%rbx
+++	addq	$16,%rbp
+++	xorq	%r14,%r14
+++	subq	%r9,%rbp
+++	xorq	%r15,%r15
+++
+++	addq	%r12,%rax
+++	adcq	$0,%rdx
+++	movq	%rax,8(%rdi)
+++	movq	%rdx,16(%rdi)
+++	movq	%r15,24(%rdi)
+++
+++	movq	-16(%rsi,%rbp,1),%rax
+++	leaq	48+8(%rsp),%rdi
+++	xorq	%r10,%r10
+++	movq	8(%rdi),%r11
+++
+++	leaq	(%r14,%r10,2),%r12
+++	shrq	$63,%r10
+++	leaq	(%rcx,%r11,2),%r13
+++	shrq	$63,%r11
+++	orq	%r10,%r13
+++	movq	16(%rdi),%r10
+++	movq	%r11,%r14
+++	mulq	%rax
+++	negq	%r15
+++	movq	24(%rdi),%r11
+++	adcq	%rax,%r12
+++	movq	-8(%rsi,%rbp,1),%rax
+++	movq	%r12,(%rdi)
+++	adcq	%rdx,%r13
+++
+++	leaq	(%r14,%r10,2),%rbx
+++	movq	%r13,8(%rdi)
+++	sbbq	%r15,%r15
+++	shrq	$63,%r10
+++	leaq	(%rcx,%r11,2),%r8
+++	shrq	$63,%r11
+++	orq	%r10,%r8
+++	movq	32(%rdi),%r10
+++	movq	%r11,%r14
+++	mulq	%rax
+++	negq	%r15
+++	movq	40(%rdi),%r11
+++	adcq	%rax,%rbx
+++	movq	0(%rsi,%rbp,1),%rax
+++	movq	%rbx,16(%rdi)
+++	adcq	%rdx,%r8
+++	leaq	16(%rbp),%rbp
+++	movq	%r8,24(%rdi)
+++	sbbq	%r15,%r15
+++	leaq	64(%rdi),%rdi
+++	jmp	.Lsqr4x_shift_n_add
+++
+++.align	32
+++.Lsqr4x_shift_n_add:
+++	leaq	(%r14,%r10,2),%r12
+++	shrq	$63,%r10
+++	leaq	(%rcx,%r11,2),%r13
+++	shrq	$63,%r11
+++	orq	%r10,%r13
+++	movq	-16(%rdi),%r10
+++	movq	%r11,%r14
+++	mulq	%rax
+++	negq	%r15
+++	movq	-8(%rdi),%r11
+++	adcq	%rax,%r12
+++	movq	-8(%rsi,%rbp,1),%rax
+++	movq	%r12,-32(%rdi)
+++	adcq	%rdx,%r13
+++
+++	leaq	(%r14,%r10,2),%rbx
+++	movq	%r13,-24(%rdi)
+++	sbbq	%r15,%r15
+++	shrq	$63,%r10
+++	leaq	(%rcx,%r11,2),%r8
+++	shrq	$63,%r11
+++	orq	%r10,%r8
+++	movq	0(%rdi),%r10
+++	movq	%r11,%r14
+++	mulq	%rax
+++	negq	%r15
+++	movq	8(%rdi),%r11
+++	adcq	%rax,%rbx
+++	movq	0(%rsi,%rbp,1),%rax
+++	movq	%rbx,-16(%rdi)
+++	adcq	%rdx,%r8
+++
+++	leaq	(%r14,%r10,2),%r12
+++	movq	%r8,-8(%rdi)
+++	sbbq	%r15,%r15
+++	shrq	$63,%r10
+++	leaq	(%rcx,%r11,2),%r13
+++	shrq	$63,%r11
+++	orq	%r10,%r13
+++	movq	16(%rdi),%r10
+++	movq	%r11,%r14
+++	mulq	%rax
+++	negq	%r15
+++	movq	24(%rdi),%r11
+++	adcq	%rax,%r12
+++	movq	8(%rsi,%rbp,1),%rax
+++	movq	%r12,0(%rdi)
+++	adcq	%rdx,%r13
+++
+++	leaq	(%r14,%r10,2),%rbx
+++	movq	%r13,8(%rdi)
+++	sbbq	%r15,%r15
+++	shrq	$63,%r10
+++	leaq	(%rcx,%r11,2),%r8
+++	shrq	$63,%r11
+++	orq	%r10,%r8
+++	movq	32(%rdi),%r10
+++	movq	%r11,%r14
+++	mulq	%rax
+++	negq	%r15
+++	movq	40(%rdi),%r11
+++	adcq	%rax,%rbx
+++	movq	16(%rsi,%rbp,1),%rax
+++	movq	%rbx,16(%rdi)
+++	adcq	%rdx,%r8
+++	movq	%r8,24(%rdi)
+++	sbbq	%r15,%r15
+++	leaq	64(%rdi),%rdi
+++	addq	$32,%rbp
+++	jnz	.Lsqr4x_shift_n_add
+++
+++	leaq	(%r14,%r10,2),%r12
+++.byte	0x67
+++	shrq	$63,%r10
+++	leaq	(%rcx,%r11,2),%r13
+++	shrq	$63,%r11
+++	orq	%r10,%r13
+++	movq	-16(%rdi),%r10
+++	movq	%r11,%r14
+++	mulq	%rax
+++	negq	%r15
+++	movq	-8(%rdi),%r11
+++	adcq	%rax,%r12
+++	movq	-8(%rsi),%rax
+++	movq	%r12,-32(%rdi)
+++	adcq	%rdx,%r13
+++
+++	leaq	(%r14,%r10,2),%rbx
+++	movq	%r13,-24(%rdi)
+++	sbbq	%r15,%r15
+++	shrq	$63,%r10
+++	leaq	(%rcx,%r11,2),%r8
+++	shrq	$63,%r11
+++	orq	%r10,%r8
+++	mulq	%rax
+++	negq	%r15
+++	adcq	%rax,%rbx
+++	adcq	%rdx,%r8
+++	movq	%rbx,-16(%rdi)
+++	movq	%r8,-8(%rdi)
+++.byte	102,72,15,126,213
+++__bn_sqr8x_reduction:
+++	xorq	%rax,%rax
+++	leaq	(%r9,%rbp,1),%rcx
+++	leaq	48+8(%rsp,%r9,2),%rdx
+++	movq	%rcx,0+8(%rsp)
+++	leaq	48+8(%rsp,%r9,1),%rdi
+++	movq	%rdx,8+8(%rsp)
+++	negq	%r9
+++	jmp	.L8x_reduction_loop
+++
+++.align	32
+++.L8x_reduction_loop:
+++	leaq	(%rdi,%r9,1),%rdi
+++.byte	0x66
+++	movq	0(%rdi),%rbx
+++	movq	8(%rdi),%r9
+++	movq	16(%rdi),%r10
+++	movq	24(%rdi),%r11
+++	movq	32(%rdi),%r12
+++	movq	40(%rdi),%r13
+++	movq	48(%rdi),%r14
+++	movq	56(%rdi),%r15
+++	movq	%rax,(%rdx)
+++	leaq	64(%rdi),%rdi
+++
+++.byte	0x67
+++	movq	%rbx,%r8
+++	imulq	32+8(%rsp),%rbx
+++	movq	0(%rbp),%rax
+++	movl	$8,%ecx
+++	jmp	.L8x_reduce
+++
+++.align	32
+++.L8x_reduce:
+++	mulq	%rbx
+++	movq	8(%rbp),%rax
+++	negq	%r8
+++	movq	%rdx,%r8
+++	adcq	$0,%r8
+++
+++	mulq	%rbx
+++	addq	%rax,%r9
+++	movq	16(%rbp),%rax
+++	adcq	$0,%rdx
+++	addq	%r9,%r8
+++	movq	%rbx,48-8+8(%rsp,%rcx,8)
+++	movq	%rdx,%r9
+++	adcq	$0,%r9
+++
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	24(%rbp),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r9
+++	movq	32+8(%rsp),%rsi
+++	movq	%rdx,%r10
+++	adcq	$0,%r10
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	32(%rbp),%rax
+++	adcq	$0,%rdx
+++	imulq	%r8,%rsi
+++	addq	%r11,%r10
+++	movq	%rdx,%r11
+++	adcq	$0,%r11
+++
+++	mulq	%rbx
+++	addq	%rax,%r12
+++	movq	40(%rbp),%rax
+++	adcq	$0,%rdx
+++	addq	%r12,%r11
+++	movq	%rdx,%r12
+++	adcq	$0,%r12
+++
+++	mulq	%rbx
+++	addq	%rax,%r13
+++	movq	48(%rbp),%rax
+++	adcq	$0,%rdx
+++	addq	%r13,%r12
+++	movq	%rdx,%r13
+++	adcq	$0,%r13
+++
+++	mulq	%rbx
+++	addq	%rax,%r14
+++	movq	56(%rbp),%rax
+++	adcq	$0,%rdx
+++	addq	%r14,%r13
+++	movq	%rdx,%r14
+++	adcq	$0,%r14
+++
+++	mulq	%rbx
+++	movq	%rsi,%rbx
+++	addq	%rax,%r15
+++	movq	0(%rbp),%rax
+++	adcq	$0,%rdx
+++	addq	%r15,%r14
+++	movq	%rdx,%r15
+++	adcq	$0,%r15
+++
+++	decl	%ecx
+++	jnz	.L8x_reduce
+++
+++	leaq	64(%rbp),%rbp
+++	xorq	%rax,%rax
+++	movq	8+8(%rsp),%rdx
+++	cmpq	0+8(%rsp),%rbp
+++	jae	.L8x_no_tail
+++
+++.byte	0x66
+++	addq	0(%rdi),%r8
+++	adcq	8(%rdi),%r9
+++	adcq	16(%rdi),%r10
+++	adcq	24(%rdi),%r11
+++	adcq	32(%rdi),%r12
+++	adcq	40(%rdi),%r13
+++	adcq	48(%rdi),%r14
+++	adcq	56(%rdi),%r15
+++	sbbq	%rsi,%rsi
+++
+++	movq	48+56+8(%rsp),%rbx
+++	movl	$8,%ecx
+++	movq	0(%rbp),%rax
+++	jmp	.L8x_tail
+++
+++.align	32
+++.L8x_tail:
+++	mulq	%rbx
+++	addq	%rax,%r8
+++	movq	8(%rbp),%rax
+++	movq	%r8,(%rdi)
+++	movq	%rdx,%r8
+++	adcq	$0,%r8
+++
+++	mulq	%rbx
+++	addq	%rax,%r9
+++	movq	16(%rbp),%rax
+++	adcq	$0,%rdx
+++	addq	%r9,%r8
+++	leaq	8(%rdi),%rdi
+++	movq	%rdx,%r9
+++	adcq	$0,%r9
+++
+++	mulq	%rbx
+++	addq	%rax,%r10
+++	movq	24(%rbp),%rax
+++	adcq	$0,%rdx
+++	addq	%r10,%r9
+++	movq	%rdx,%r10
+++	adcq	$0,%r10
+++
+++	mulq	%rbx
+++	addq	%rax,%r11
+++	movq	32(%rbp),%rax
+++	adcq	$0,%rdx
+++	addq	%r11,%r10
+++	movq	%rdx,%r11
+++	adcq	$0,%r11
+++
+++	mulq	%rbx
+++	addq	%rax,%r12
+++	movq	40(%rbp),%rax
+++	adcq	$0,%rdx
+++	addq	%r12,%r11
+++	movq	%rdx,%r12
+++	adcq	$0,%r12
+++
+++	mulq	%rbx
+++	addq	%rax,%r13
+++	movq	48(%rbp),%rax
+++	adcq	$0,%rdx
+++	addq	%r13,%r12
+++	movq	%rdx,%r13
+++	adcq	$0,%r13
+++
+++	mulq	%rbx
+++	addq	%rax,%r14
+++	movq	56(%rbp),%rax
+++	adcq	$0,%rdx
+++	addq	%r14,%r13
+++	movq	%rdx,%r14
+++	adcq	$0,%r14
+++
+++	mulq	%rbx
+++	movq	48-16+8(%rsp,%rcx,8),%rbx
+++	addq	%rax,%r15
+++	adcq	$0,%rdx
+++	addq	%r15,%r14
+++	movq	0(%rbp),%rax
+++	movq	%rdx,%r15
+++	adcq	$0,%r15
+++
+++	decl	%ecx
+++	jnz	.L8x_tail
+++
+++	leaq	64(%rbp),%rbp
+++	movq	8+8(%rsp),%rdx
+++	cmpq	0+8(%rsp),%rbp
+++	jae	.L8x_tail_done
+++
+++	movq	48+56+8(%rsp),%rbx
+++	negq	%rsi
+++	movq	0(%rbp),%rax
+++	adcq	0(%rdi),%r8
+++	adcq	8(%rdi),%r9
+++	adcq	16(%rdi),%r10
+++	adcq	24(%rdi),%r11
+++	adcq	32(%rdi),%r12
+++	adcq	40(%rdi),%r13
+++	adcq	48(%rdi),%r14
+++	adcq	56(%rdi),%r15
+++	sbbq	%rsi,%rsi
+++
+++	movl	$8,%ecx
+++	jmp	.L8x_tail
+++
+++.align	32
+++.L8x_tail_done:
+++	xorq	%rax,%rax
+++	addq	(%rdx),%r8
+++	adcq	$0,%r9
+++	adcq	$0,%r10
+++	adcq	$0,%r11
+++	adcq	$0,%r12
+++	adcq	$0,%r13
+++	adcq	$0,%r14
+++	adcq	$0,%r15
+++	adcq	$0,%rax
+++
+++	negq	%rsi
+++.L8x_no_tail:
+++	adcq	0(%rdi),%r8
+++	adcq	8(%rdi),%r9
+++	adcq	16(%rdi),%r10
+++	adcq	24(%rdi),%r11
+++	adcq	32(%rdi),%r12
+++	adcq	40(%rdi),%r13
+++	adcq	48(%rdi),%r14
+++	adcq	56(%rdi),%r15
+++	adcq	$0,%rax
+++	movq	-8(%rbp),%rcx
+++	xorq	%rsi,%rsi
+++
+++.byte	102,72,15,126,213
+++
+++	movq	%r8,0(%rdi)
+++	movq	%r9,8(%rdi)
+++.byte	102,73,15,126,217
+++	movq	%r10,16(%rdi)
+++	movq	%r11,24(%rdi)
+++	movq	%r12,32(%rdi)
+++	movq	%r13,40(%rdi)
+++	movq	%r14,48(%rdi)
+++	movq	%r15,56(%rdi)
+++	leaq	64(%rdi),%rdi
+++
+++	cmpq	%rdx,%rdi
+++	jb	.L8x_reduction_loop
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_sqr8x_internal,.-bn_sqr8x_internal
+++.type	__bn_post4x_internal,@function
+++.align	32
+++__bn_post4x_internal:
+++.cfi_startproc	
+++	movq	0(%rbp),%r12
+++	leaq	(%rdi,%r9,1),%rbx
+++	movq	%r9,%rcx
+++.byte	102,72,15,126,207
+++	negq	%rax
+++.byte	102,72,15,126,206
+++	sarq	$3+2,%rcx
+++	decq	%r12
+++	xorq	%r10,%r10
+++	movq	8(%rbp),%r13
+++	movq	16(%rbp),%r14
+++	movq	24(%rbp),%r15
+++	jmp	.Lsqr4x_sub_entry
+++
+++.align	16
+++.Lsqr4x_sub:
+++	movq	0(%rbp),%r12
+++	movq	8(%rbp),%r13
+++	movq	16(%rbp),%r14
+++	movq	24(%rbp),%r15
+++.Lsqr4x_sub_entry:
+++	leaq	32(%rbp),%rbp
+++	notq	%r12
+++	notq	%r13
+++	notq	%r14
+++	notq	%r15
+++	andq	%rax,%r12
+++	andq	%rax,%r13
+++	andq	%rax,%r14
+++	andq	%rax,%r15
+++
+++	negq	%r10
+++	adcq	0(%rbx),%r12
+++	adcq	8(%rbx),%r13
+++	adcq	16(%rbx),%r14
+++	adcq	24(%rbx),%r15
+++	movq	%r12,0(%rdi)
+++	leaq	32(%rbx),%rbx
+++	movq	%r13,8(%rdi)
+++	sbbq	%r10,%r10
+++	movq	%r14,16(%rdi)
+++	movq	%r15,24(%rdi)
+++	leaq	32(%rdi),%rdi
+++
+++	incq	%rcx
+++	jnz	.Lsqr4x_sub
+++
+++	movq	%r9,%r10
+++	negq	%r9
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__bn_post4x_internal,.-__bn_post4x_internal
+++.globl	bn_from_montgomery
+++.hidden bn_from_montgomery
+++.type	bn_from_montgomery,@function
+++.align	32
+++bn_from_montgomery:
+++.cfi_startproc	
+++	testl	$7,%r9d
+++	jz	bn_from_mont8x
+++	xorl	%eax,%eax
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_from_montgomery,.-bn_from_montgomery
+++
+++.type	bn_from_mont8x,@function
+++.align	32
+++bn_from_mont8x:
+++.cfi_startproc	
+++.byte	0x67
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++.Lfrom_prologue:
+++
+++	shll	$3,%r9d
+++	leaq	(%r9,%r9,2),%r10
+++	negq	%r9
+++	movq	(%r8),%r8
+++
+++
+++
+++
+++
+++
+++
+++
+++	leaq	-320(%rsp,%r9,2),%r11
+++	movq	%rsp,%rbp
+++	subq	%rdi,%r11
+++	andq	$4095,%r11
+++	cmpq	%r11,%r10
+++	jb	.Lfrom_sp_alt
+++	subq	%r11,%rbp
+++	leaq	-320(%rbp,%r9,2),%rbp
+++	jmp	.Lfrom_sp_done
+++
+++.align	32
+++.Lfrom_sp_alt:
+++	leaq	4096-320(,%r9,2),%r10
+++	leaq	-320(%rbp,%r9,2),%rbp
+++	subq	%r10,%r11
+++	movq	$0,%r10
+++	cmovcq	%r10,%r11
+++	subq	%r11,%rbp
+++.Lfrom_sp_done:
+++	andq	$-64,%rbp
+++	movq	%rsp,%r11
+++	subq	%rbp,%r11
+++	andq	$-4096,%r11
+++	leaq	(%r11,%rbp,1),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lfrom_page_walk
+++	jmp	.Lfrom_page_walk_done
+++
+++.Lfrom_page_walk:
+++	leaq	-4096(%rsp),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lfrom_page_walk
+++.Lfrom_page_walk_done:
+++
+++	movq	%r9,%r10
+++	negq	%r9
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	movq	%r8,32(%rsp)
+++	movq	%rax,40(%rsp)
+++.cfi_escape	0x0f,0x05,0x77,0x28,0x06,0x23,0x08
+++.Lfrom_body:
+++	movq	%r9,%r11
+++	leaq	48(%rsp),%rax
+++	pxor	%xmm0,%xmm0
+++	jmp	.Lmul_by_1
+++
+++.align	32
+++.Lmul_by_1:
+++	movdqu	(%rsi),%xmm1
+++	movdqu	16(%rsi),%xmm2
+++	movdqu	32(%rsi),%xmm3
+++	movdqa	%xmm0,(%rax,%r9,1)
+++	movdqu	48(%rsi),%xmm4
+++	movdqa	%xmm0,16(%rax,%r9,1)
+++.byte	0x48,0x8d,0xb6,0x40,0x00,0x00,0x00
+++	movdqa	%xmm1,(%rax)
+++	movdqa	%xmm0,32(%rax,%r9,1)
+++	movdqa	%xmm2,16(%rax)
+++	movdqa	%xmm0,48(%rax,%r9,1)
+++	movdqa	%xmm3,32(%rax)
+++	movdqa	%xmm4,48(%rax)
+++	leaq	64(%rax),%rax
+++	subq	$64,%r11
+++	jnz	.Lmul_by_1
+++
+++.byte	102,72,15,110,207
+++.byte	102,72,15,110,209
+++.byte	0x67
+++	movq	%rcx,%rbp
+++.byte	102,73,15,110,218
+++	leaq	OPENSSL_ia32cap_P(%rip),%r11
+++	movl	8(%r11),%r11d
+++	andl	$0x80108,%r11d
+++	cmpl	$0x80108,%r11d
+++	jne	.Lfrom_mont_nox
+++
+++	leaq	(%rax,%r9,1),%rdi
+++	call	__bn_sqrx8x_reduction
+++	call	__bn_postx4x_internal
+++
+++	pxor	%xmm0,%xmm0
+++	leaq	48(%rsp),%rax
+++	jmp	.Lfrom_mont_zero
+++
+++.align	32
+++.Lfrom_mont_nox:
+++	call	__bn_sqr8x_reduction
+++	call	__bn_post4x_internal
+++
+++	pxor	%xmm0,%xmm0
+++	leaq	48(%rsp),%rax
+++	jmp	.Lfrom_mont_zero
+++
+++.align	32
+++.Lfrom_mont_zero:
+++	movq	40(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movdqa	%xmm0,0(%rax)
+++	movdqa	%xmm0,16(%rax)
+++	movdqa	%xmm0,32(%rax)
+++	movdqa	%xmm0,48(%rax)
+++	leaq	64(%rax),%rax
+++	subq	$32,%r9
+++	jnz	.Lfrom_mont_zero
+++
+++	movq	$1,%rax
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lfrom_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_from_mont8x,.-bn_from_mont8x
+++.type	bn_mulx4x_mont_gather5,@function
+++.align	32
+++bn_mulx4x_mont_gather5:
+++.cfi_startproc	
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++.Lmulx4x_enter:
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++.Lmulx4x_prologue:
+++
+++	shll	$3,%r9d
+++	leaq	(%r9,%r9,2),%r10
+++	negq	%r9
+++	movq	(%r8),%r8
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	leaq	-320(%rsp,%r9,2),%r11
+++	movq	%rsp,%rbp
+++	subq	%rdi,%r11
+++	andq	$4095,%r11
+++	cmpq	%r11,%r10
+++	jb	.Lmulx4xsp_alt
+++	subq	%r11,%rbp
+++	leaq	-320(%rbp,%r9,2),%rbp
+++	jmp	.Lmulx4xsp_done
+++
+++.Lmulx4xsp_alt:
+++	leaq	4096-320(,%r9,2),%r10
+++	leaq	-320(%rbp,%r9,2),%rbp
+++	subq	%r10,%r11
+++	movq	$0,%r10
+++	cmovcq	%r10,%r11
+++	subq	%r11,%rbp
+++.Lmulx4xsp_done:
+++	andq	$-64,%rbp
+++	movq	%rsp,%r11
+++	subq	%rbp,%r11
+++	andq	$-4096,%r11
+++	leaq	(%r11,%rbp,1),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lmulx4x_page_walk
+++	jmp	.Lmulx4x_page_walk_done
+++
+++.Lmulx4x_page_walk:
+++	leaq	-4096(%rsp),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lmulx4x_page_walk
+++.Lmulx4x_page_walk_done:
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	movq	%r8,32(%rsp)
+++	movq	%rax,40(%rsp)
+++.cfi_escape	0x0f,0x05,0x77,0x28,0x06,0x23,0x08
+++.Lmulx4x_body:
+++	call	mulx4x_internal
+++
+++	movq	40(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	$1,%rax
+++
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lmulx4x_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_mulx4x_mont_gather5,.-bn_mulx4x_mont_gather5
+++
+++.type	mulx4x_internal,@function
+++.align	32
+++mulx4x_internal:
+++.cfi_startproc	
+++	movq	%r9,8(%rsp)
+++	movq	%r9,%r10
+++	negq	%r9
+++	shlq	$5,%r9
+++	negq	%r10
+++	leaq	128(%rdx,%r9,1),%r13
+++	shrq	$5+5,%r9
+++	movd	8(%rax),%xmm5
+++	subq	$1,%r9
+++	leaq	.Linc(%rip),%rax
+++	movq	%r13,16+8(%rsp)
+++	movq	%r9,24+8(%rsp)
+++	movq	%rdi,56+8(%rsp)
+++	movdqa	0(%rax),%xmm0
+++	movdqa	16(%rax),%xmm1
+++	leaq	88-112(%rsp,%r10,1),%r10
+++	leaq	128(%rdx),%rdi
+++
+++	pshufd	$0,%xmm5,%xmm5
+++	movdqa	%xmm1,%xmm4
+++.byte	0x67
+++	movdqa	%xmm1,%xmm2
+++.byte	0x67
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm4,%xmm3
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,112(%r10)
+++	movdqa	%xmm4,%xmm0
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,128(%r10)
+++	movdqa	%xmm4,%xmm1
+++
+++	paddd	%xmm3,%xmm0
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,144(%r10)
+++	movdqa	%xmm4,%xmm2
+++
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm3,160(%r10)
+++	movdqa	%xmm4,%xmm3
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,176(%r10)
+++	movdqa	%xmm4,%xmm0
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,192(%r10)
+++	movdqa	%xmm4,%xmm1
+++
+++	paddd	%xmm3,%xmm0
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,208(%r10)
+++	movdqa	%xmm4,%xmm2
+++
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm3,224(%r10)
+++	movdqa	%xmm4,%xmm3
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,240(%r10)
+++	movdqa	%xmm4,%xmm0
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,256(%r10)
+++	movdqa	%xmm4,%xmm1
+++
+++	paddd	%xmm3,%xmm0
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,272(%r10)
+++	movdqa	%xmm4,%xmm2
+++
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm3,288(%r10)
+++	movdqa	%xmm4,%xmm3
+++.byte	0x67
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,304(%r10)
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,320(%r10)
+++
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,336(%r10)
+++
+++	pand	64(%rdi),%xmm0
+++	pand	80(%rdi),%xmm1
+++	pand	96(%rdi),%xmm2
+++	movdqa	%xmm3,352(%r10)
+++	pand	112(%rdi),%xmm3
+++	por	%xmm2,%xmm0
+++	por	%xmm3,%xmm1
+++	movdqa	-128(%rdi),%xmm4
+++	movdqa	-112(%rdi),%xmm5
+++	movdqa	-96(%rdi),%xmm2
+++	pand	112(%r10),%xmm4
+++	movdqa	-80(%rdi),%xmm3
+++	pand	128(%r10),%xmm5
+++	por	%xmm4,%xmm0
+++	pand	144(%r10),%xmm2
+++	por	%xmm5,%xmm1
+++	pand	160(%r10),%xmm3
+++	por	%xmm2,%xmm0
+++	por	%xmm3,%xmm1
+++	movdqa	-64(%rdi),%xmm4
+++	movdqa	-48(%rdi),%xmm5
+++	movdqa	-32(%rdi),%xmm2
+++	pand	176(%r10),%xmm4
+++	movdqa	-16(%rdi),%xmm3
+++	pand	192(%r10),%xmm5
+++	por	%xmm4,%xmm0
+++	pand	208(%r10),%xmm2
+++	por	%xmm5,%xmm1
+++	pand	224(%r10),%xmm3
+++	por	%xmm2,%xmm0
+++	por	%xmm3,%xmm1
+++	movdqa	0(%rdi),%xmm4
+++	movdqa	16(%rdi),%xmm5
+++	movdqa	32(%rdi),%xmm2
+++	pand	240(%r10),%xmm4
+++	movdqa	48(%rdi),%xmm3
+++	pand	256(%r10),%xmm5
+++	por	%xmm4,%xmm0
+++	pand	272(%r10),%xmm2
+++	por	%xmm5,%xmm1
+++	pand	288(%r10),%xmm3
+++	por	%xmm2,%xmm0
+++	por	%xmm3,%xmm1
+++	pxor	%xmm1,%xmm0
+++	pshufd	$0x4e,%xmm0,%xmm1
+++	por	%xmm1,%xmm0
+++	leaq	256(%rdi),%rdi
+++.byte	102,72,15,126,194
+++	leaq	64+32+8(%rsp),%rbx
+++
+++	movq	%rdx,%r9
+++	mulxq	0(%rsi),%r8,%rax
+++	mulxq	8(%rsi),%r11,%r12
+++	addq	%rax,%r11
+++	mulxq	16(%rsi),%rax,%r13
+++	adcq	%rax,%r12
+++	adcq	$0,%r13
+++	mulxq	24(%rsi),%rax,%r14
+++
+++	movq	%r8,%r15
+++	imulq	32+8(%rsp),%r8
+++	xorq	%rbp,%rbp
+++	movq	%r8,%rdx
+++
+++	movq	%rdi,8+8(%rsp)
+++
+++	leaq	32(%rsi),%rsi
+++	adcxq	%rax,%r13
+++	adcxq	%rbp,%r14
+++
+++	mulxq	0(%rcx),%rax,%r10
+++	adcxq	%rax,%r15
+++	adoxq	%r11,%r10
+++	mulxq	8(%rcx),%rax,%r11
+++	adcxq	%rax,%r10
+++	adoxq	%r12,%r11
+++	mulxq	16(%rcx),%rax,%r12
+++	movq	24+8(%rsp),%rdi
+++	movq	%r10,-32(%rbx)
+++	adcxq	%rax,%r11
+++	adoxq	%r13,%r12
+++	mulxq	24(%rcx),%rax,%r15
+++	movq	%r9,%rdx
+++	movq	%r11,-24(%rbx)
+++	adcxq	%rax,%r12
+++	adoxq	%rbp,%r15
+++	leaq	32(%rcx),%rcx
+++	movq	%r12,-16(%rbx)
+++	jmp	.Lmulx4x_1st
+++
+++.align	32
+++.Lmulx4x_1st:
+++	adcxq	%rbp,%r15
+++	mulxq	0(%rsi),%r10,%rax
+++	adcxq	%r14,%r10
+++	mulxq	8(%rsi),%r11,%r14
+++	adcxq	%rax,%r11
+++	mulxq	16(%rsi),%r12,%rax
+++	adcxq	%r14,%r12
+++	mulxq	24(%rsi),%r13,%r14
+++.byte	0x67,0x67
+++	movq	%r8,%rdx
+++	adcxq	%rax,%r13
+++	adcxq	%rbp,%r14
+++	leaq	32(%rsi),%rsi
+++	leaq	32(%rbx),%rbx
+++
+++	adoxq	%r15,%r10
+++	mulxq	0(%rcx),%rax,%r15
+++	adcxq	%rax,%r10
+++	adoxq	%r15,%r11
+++	mulxq	8(%rcx),%rax,%r15
+++	adcxq	%rax,%r11
+++	adoxq	%r15,%r12
+++	mulxq	16(%rcx),%rax,%r15
+++	movq	%r10,-40(%rbx)
+++	adcxq	%rax,%r12
+++	movq	%r11,-32(%rbx)
+++	adoxq	%r15,%r13
+++	mulxq	24(%rcx),%rax,%r15
+++	movq	%r9,%rdx
+++	movq	%r12,-24(%rbx)
+++	adcxq	%rax,%r13
+++	adoxq	%rbp,%r15
+++	leaq	32(%rcx),%rcx
+++	movq	%r13,-16(%rbx)
+++
+++	decq	%rdi
+++	jnz	.Lmulx4x_1st
+++
+++	movq	8(%rsp),%rax
+++	adcq	%rbp,%r15
+++	leaq	(%rsi,%rax,1),%rsi
+++	addq	%r15,%r14
+++	movq	8+8(%rsp),%rdi
+++	adcq	%rbp,%rbp
+++	movq	%r14,-8(%rbx)
+++	jmp	.Lmulx4x_outer
+++
+++.align	32
+++.Lmulx4x_outer:
+++	leaq	16-256(%rbx),%r10
+++	pxor	%xmm4,%xmm4
+++.byte	0x67,0x67
+++	pxor	%xmm5,%xmm5
+++	movdqa	-128(%rdi),%xmm0
+++	movdqa	-112(%rdi),%xmm1
+++	movdqa	-96(%rdi),%xmm2
+++	pand	256(%r10),%xmm0
+++	movdqa	-80(%rdi),%xmm3
+++	pand	272(%r10),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	288(%r10),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	304(%r10),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqa	-64(%rdi),%xmm0
+++	movdqa	-48(%rdi),%xmm1
+++	movdqa	-32(%rdi),%xmm2
+++	pand	320(%r10),%xmm0
+++	movdqa	-16(%rdi),%xmm3
+++	pand	336(%r10),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	352(%r10),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	368(%r10),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqa	0(%rdi),%xmm0
+++	movdqa	16(%rdi),%xmm1
+++	movdqa	32(%rdi),%xmm2
+++	pand	384(%r10),%xmm0
+++	movdqa	48(%rdi),%xmm3
+++	pand	400(%r10),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	416(%r10),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	432(%r10),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqa	64(%rdi),%xmm0
+++	movdqa	80(%rdi),%xmm1
+++	movdqa	96(%rdi),%xmm2
+++	pand	448(%r10),%xmm0
+++	movdqa	112(%rdi),%xmm3
+++	pand	464(%r10),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	480(%r10),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	496(%r10),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	por	%xmm5,%xmm4
+++	pshufd	$0x4e,%xmm4,%xmm0
+++	por	%xmm4,%xmm0
+++	leaq	256(%rdi),%rdi
+++.byte	102,72,15,126,194
+++
+++	movq	%rbp,(%rbx)
+++	leaq	32(%rbx,%rax,1),%rbx
+++	mulxq	0(%rsi),%r8,%r11
+++	xorq	%rbp,%rbp
+++	movq	%rdx,%r9
+++	mulxq	8(%rsi),%r14,%r12
+++	adoxq	-32(%rbx),%r8
+++	adcxq	%r14,%r11
+++	mulxq	16(%rsi),%r15,%r13
+++	adoxq	-24(%rbx),%r11
+++	adcxq	%r15,%r12
+++	mulxq	24(%rsi),%rdx,%r14
+++	adoxq	-16(%rbx),%r12
+++	adcxq	%rdx,%r13
+++	leaq	(%rcx,%rax,1),%rcx
+++	leaq	32(%rsi),%rsi
+++	adoxq	-8(%rbx),%r13
+++	adcxq	%rbp,%r14
+++	adoxq	%rbp,%r14
+++
+++	movq	%r8,%r15
+++	imulq	32+8(%rsp),%r8
+++
+++	movq	%r8,%rdx
+++	xorq	%rbp,%rbp
+++	movq	%rdi,8+8(%rsp)
+++
+++	mulxq	0(%rcx),%rax,%r10
+++	adcxq	%rax,%r15
+++	adoxq	%r11,%r10
+++	mulxq	8(%rcx),%rax,%r11
+++	adcxq	%rax,%r10
+++	adoxq	%r12,%r11
+++	mulxq	16(%rcx),%rax,%r12
+++	adcxq	%rax,%r11
+++	adoxq	%r13,%r12
+++	mulxq	24(%rcx),%rax,%r15
+++	movq	%r9,%rdx
+++	movq	24+8(%rsp),%rdi
+++	movq	%r10,-32(%rbx)
+++	adcxq	%rax,%r12
+++	movq	%r11,-24(%rbx)
+++	adoxq	%rbp,%r15
+++	movq	%r12,-16(%rbx)
+++	leaq	32(%rcx),%rcx
+++	jmp	.Lmulx4x_inner
+++
+++.align	32
+++.Lmulx4x_inner:
+++	mulxq	0(%rsi),%r10,%rax
+++	adcxq	%rbp,%r15
+++	adoxq	%r14,%r10
+++	mulxq	8(%rsi),%r11,%r14
+++	adcxq	0(%rbx),%r10
+++	adoxq	%rax,%r11
+++	mulxq	16(%rsi),%r12,%rax
+++	adcxq	8(%rbx),%r11
+++	adoxq	%r14,%r12
+++	mulxq	24(%rsi),%r13,%r14
+++	movq	%r8,%rdx
+++	adcxq	16(%rbx),%r12
+++	adoxq	%rax,%r13
+++	adcxq	24(%rbx),%r13
+++	adoxq	%rbp,%r14
+++	leaq	32(%rsi),%rsi
+++	leaq	32(%rbx),%rbx
+++	adcxq	%rbp,%r14
+++
+++	adoxq	%r15,%r10
+++	mulxq	0(%rcx),%rax,%r15
+++	adcxq	%rax,%r10
+++	adoxq	%r15,%r11
+++	mulxq	8(%rcx),%rax,%r15
+++	adcxq	%rax,%r11
+++	adoxq	%r15,%r12
+++	mulxq	16(%rcx),%rax,%r15
+++	movq	%r10,-40(%rbx)
+++	adcxq	%rax,%r12
+++	adoxq	%r15,%r13
+++	movq	%r11,-32(%rbx)
+++	mulxq	24(%rcx),%rax,%r15
+++	movq	%r9,%rdx
+++	leaq	32(%rcx),%rcx
+++	movq	%r12,-24(%rbx)
+++	adcxq	%rax,%r13
+++	adoxq	%rbp,%r15
+++	movq	%r13,-16(%rbx)
+++
+++	decq	%rdi
+++	jnz	.Lmulx4x_inner
+++
+++	movq	0+8(%rsp),%rax
+++	adcq	%rbp,%r15
+++	subq	0(%rbx),%rdi
+++	movq	8+8(%rsp),%rdi
+++	movq	16+8(%rsp),%r10
+++	adcq	%r15,%r14
+++	leaq	(%rsi,%rax,1),%rsi
+++	adcq	%rbp,%rbp
+++	movq	%r14,-8(%rbx)
+++
+++	cmpq	%r10,%rdi
+++	jb	.Lmulx4x_outer
+++
+++	movq	-8(%rcx),%r10
+++	movq	%rbp,%r8
+++	movq	(%rcx,%rax,1),%r12
+++	leaq	(%rcx,%rax,1),%rbp
+++	movq	%rax,%rcx
+++	leaq	(%rbx,%rax,1),%rdi
+++	xorl	%eax,%eax
+++	xorq	%r15,%r15
+++	subq	%r14,%r10
+++	adcq	%r15,%r15
+++	orq	%r15,%r8
+++	sarq	$3+2,%rcx
+++	subq	%r8,%rax
+++	movq	56+8(%rsp),%rdx
+++	decq	%r12
+++	movq	8(%rbp),%r13
+++	xorq	%r8,%r8
+++	movq	16(%rbp),%r14
+++	movq	24(%rbp),%r15
+++	jmp	.Lsqrx4x_sub_entry
+++.cfi_endproc	
+++.size	mulx4x_internal,.-mulx4x_internal
+++.type	bn_powerx5,@function
+++.align	32
+++bn_powerx5:
+++.cfi_startproc	
+++	movq	%rsp,%rax
+++.cfi_def_cfa_register	%rax
+++.Lpowerx5_enter:
+++	pushq	%rbx
+++.cfi_offset	%rbx,-16
+++	pushq	%rbp
+++.cfi_offset	%rbp,-24
+++	pushq	%r12
+++.cfi_offset	%r12,-32
+++	pushq	%r13
+++.cfi_offset	%r13,-40
+++	pushq	%r14
+++.cfi_offset	%r14,-48
+++	pushq	%r15
+++.cfi_offset	%r15,-56
+++.Lpowerx5_prologue:
+++
+++	shll	$3,%r9d
+++	leaq	(%r9,%r9,2),%r10
+++	negq	%r9
+++	movq	(%r8),%r8
+++
+++
+++
+++
+++
+++
+++
+++
+++	leaq	-320(%rsp,%r9,2),%r11
+++	movq	%rsp,%rbp
+++	subq	%rdi,%r11
+++	andq	$4095,%r11
+++	cmpq	%r11,%r10
+++	jb	.Lpwrx_sp_alt
+++	subq	%r11,%rbp
+++	leaq	-320(%rbp,%r9,2),%rbp
+++	jmp	.Lpwrx_sp_done
+++
+++.align	32
+++.Lpwrx_sp_alt:
+++	leaq	4096-320(,%r9,2),%r10
+++	leaq	-320(%rbp,%r9,2),%rbp
+++	subq	%r10,%r11
+++	movq	$0,%r10
+++	cmovcq	%r10,%r11
+++	subq	%r11,%rbp
+++.Lpwrx_sp_done:
+++	andq	$-64,%rbp
+++	movq	%rsp,%r11
+++	subq	%rbp,%r11
+++	andq	$-4096,%r11
+++	leaq	(%r11,%rbp,1),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lpwrx_page_walk
+++	jmp	.Lpwrx_page_walk_done
+++
+++.Lpwrx_page_walk:
+++	leaq	-4096(%rsp),%rsp
+++	movq	(%rsp),%r10
+++	cmpq	%rbp,%rsp
+++	ja	.Lpwrx_page_walk
+++.Lpwrx_page_walk_done:
+++
+++	movq	%r9,%r10
+++	negq	%r9
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	pxor	%xmm0,%xmm0
+++.byte	102,72,15,110,207
+++.byte	102,72,15,110,209
+++.byte	102,73,15,110,218
+++.byte	102,72,15,110,226
+++	movq	%r8,32(%rsp)
+++	movq	%rax,40(%rsp)
+++.cfi_escape	0x0f,0x05,0x77,0x28,0x06,0x23,0x08
+++.Lpowerx5_body:
+++
+++	call	__bn_sqrx8x_internal
+++	call	__bn_postx4x_internal
+++	call	__bn_sqrx8x_internal
+++	call	__bn_postx4x_internal
+++	call	__bn_sqrx8x_internal
+++	call	__bn_postx4x_internal
+++	call	__bn_sqrx8x_internal
+++	call	__bn_postx4x_internal
+++	call	__bn_sqrx8x_internal
+++	call	__bn_postx4x_internal
+++
+++	movq	%r10,%r9
+++	movq	%rsi,%rdi
+++.byte	102,72,15,126,209
+++.byte	102,72,15,126,226
+++	movq	40(%rsp),%rax
+++
+++	call	mulx4x_internal
+++
+++	movq	40(%rsp),%rsi
+++.cfi_def_cfa	%rsi,8
+++	movq	$1,%rax
+++
+++	movq	-48(%rsi),%r15
+++.cfi_restore	%r15
+++	movq	-40(%rsi),%r14
+++.cfi_restore	%r14
+++	movq	-32(%rsi),%r13
+++.cfi_restore	%r13
+++	movq	-24(%rsi),%r12
+++.cfi_restore	%r12
+++	movq	-16(%rsi),%rbp
+++.cfi_restore	%rbp
+++	movq	-8(%rsi),%rbx
+++.cfi_restore	%rbx
+++	leaq	(%rsi),%rsp
+++.cfi_def_cfa_register	%rsp
+++.Lpowerx5_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_powerx5,.-bn_powerx5
+++
+++.globl	bn_sqrx8x_internal
+++.hidden bn_sqrx8x_internal
+++.hidden	bn_sqrx8x_internal
+++.type	bn_sqrx8x_internal,@function
+++.align	32
+++bn_sqrx8x_internal:
+++__bn_sqrx8x_internal:
+++.cfi_startproc	
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	leaq	48+8(%rsp),%rdi
+++	leaq	(%rsi,%r9,1),%rbp
+++	movq	%r9,0+8(%rsp)
+++	movq	%rbp,8+8(%rsp)
+++	jmp	.Lsqr8x_zero_start
+++
+++.align	32
+++.byte	0x66,0x66,0x66,0x2e,0x0f,0x1f,0x84,0x00,0x00,0x00,0x00,0x00
+++.Lsqrx8x_zero:
+++.byte	0x3e
+++	movdqa	%xmm0,0(%rdi)
+++	movdqa	%xmm0,16(%rdi)
+++	movdqa	%xmm0,32(%rdi)
+++	movdqa	%xmm0,48(%rdi)
+++.Lsqr8x_zero_start:
+++	movdqa	%xmm0,64(%rdi)
+++	movdqa	%xmm0,80(%rdi)
+++	movdqa	%xmm0,96(%rdi)
+++	movdqa	%xmm0,112(%rdi)
+++	leaq	128(%rdi),%rdi
+++	subq	$64,%r9
+++	jnz	.Lsqrx8x_zero
+++
+++	movq	0(%rsi),%rdx
+++
+++	xorq	%r10,%r10
+++	xorq	%r11,%r11
+++	xorq	%r12,%r12
+++	xorq	%r13,%r13
+++	xorq	%r14,%r14
+++	xorq	%r15,%r15
+++	leaq	48+8(%rsp),%rdi
+++	xorq	%rbp,%rbp
+++	jmp	.Lsqrx8x_outer_loop
+++
+++.align	32
+++.Lsqrx8x_outer_loop:
+++	mulxq	8(%rsi),%r8,%rax
+++	adcxq	%r9,%r8
+++	adoxq	%rax,%r10
+++	mulxq	16(%rsi),%r9,%rax
+++	adcxq	%r10,%r9
+++	adoxq	%rax,%r11
+++.byte	0xc4,0xe2,0xab,0xf6,0x86,0x18,0x00,0x00,0x00
+++	adcxq	%r11,%r10
+++	adoxq	%rax,%r12
+++.byte	0xc4,0xe2,0xa3,0xf6,0x86,0x20,0x00,0x00,0x00
+++	adcxq	%r12,%r11
+++	adoxq	%rax,%r13
+++	mulxq	40(%rsi),%r12,%rax
+++	adcxq	%r13,%r12
+++	adoxq	%rax,%r14
+++	mulxq	48(%rsi),%r13,%rax
+++	adcxq	%r14,%r13
+++	adoxq	%r15,%rax
+++	mulxq	56(%rsi),%r14,%r15
+++	movq	8(%rsi),%rdx
+++	adcxq	%rax,%r14
+++	adoxq	%rbp,%r15
+++	adcq	64(%rdi),%r15
+++	movq	%r8,8(%rdi)
+++	movq	%r9,16(%rdi)
+++	sbbq	%rcx,%rcx
+++	xorq	%rbp,%rbp
+++
+++
+++	mulxq	16(%rsi),%r8,%rbx
+++	mulxq	24(%rsi),%r9,%rax
+++	adcxq	%r10,%r8
+++	adoxq	%rbx,%r9
+++	mulxq	32(%rsi),%r10,%rbx
+++	adcxq	%r11,%r9
+++	adoxq	%rax,%r10
+++.byte	0xc4,0xe2,0xa3,0xf6,0x86,0x28,0x00,0x00,0x00
+++	adcxq	%r12,%r10
+++	adoxq	%rbx,%r11
+++.byte	0xc4,0xe2,0x9b,0xf6,0x9e,0x30,0x00,0x00,0x00
+++	adcxq	%r13,%r11
+++	adoxq	%r14,%r12
+++.byte	0xc4,0x62,0x93,0xf6,0xb6,0x38,0x00,0x00,0x00
+++	movq	16(%rsi),%rdx
+++	adcxq	%rax,%r12
+++	adoxq	%rbx,%r13
+++	adcxq	%r15,%r13
+++	adoxq	%rbp,%r14
+++	adcxq	%rbp,%r14
+++
+++	movq	%r8,24(%rdi)
+++	movq	%r9,32(%rdi)
+++
+++	mulxq	24(%rsi),%r8,%rbx
+++	mulxq	32(%rsi),%r9,%rax
+++	adcxq	%r10,%r8
+++	adoxq	%rbx,%r9
+++	mulxq	40(%rsi),%r10,%rbx
+++	adcxq	%r11,%r9
+++	adoxq	%rax,%r10
+++.byte	0xc4,0xe2,0xa3,0xf6,0x86,0x30,0x00,0x00,0x00
+++	adcxq	%r12,%r10
+++	adoxq	%r13,%r11
+++.byte	0xc4,0x62,0x9b,0xf6,0xae,0x38,0x00,0x00,0x00
+++.byte	0x3e
+++	movq	24(%rsi),%rdx
+++	adcxq	%rbx,%r11
+++	adoxq	%rax,%r12
+++	adcxq	%r14,%r12
+++	movq	%r8,40(%rdi)
+++	movq	%r9,48(%rdi)
+++	mulxq	32(%rsi),%r8,%rax
+++	adoxq	%rbp,%r13
+++	adcxq	%rbp,%r13
+++
+++	mulxq	40(%rsi),%r9,%rbx
+++	adcxq	%r10,%r8
+++	adoxq	%rax,%r9
+++	mulxq	48(%rsi),%r10,%rax
+++	adcxq	%r11,%r9
+++	adoxq	%r12,%r10
+++	mulxq	56(%rsi),%r11,%r12
+++	movq	32(%rsi),%rdx
+++	movq	40(%rsi),%r14
+++	adcxq	%rbx,%r10
+++	adoxq	%rax,%r11
+++	movq	48(%rsi),%r15
+++	adcxq	%r13,%r11
+++	adoxq	%rbp,%r12
+++	adcxq	%rbp,%r12
+++
+++	movq	%r8,56(%rdi)
+++	movq	%r9,64(%rdi)
+++
+++	mulxq	%r14,%r9,%rax
+++	movq	56(%rsi),%r8
+++	adcxq	%r10,%r9
+++	mulxq	%r15,%r10,%rbx
+++	adoxq	%rax,%r10
+++	adcxq	%r11,%r10
+++	mulxq	%r8,%r11,%rax
+++	movq	%r14,%rdx
+++	adoxq	%rbx,%r11
+++	adcxq	%r12,%r11
+++
+++	adcxq	%rbp,%rax
+++
+++	mulxq	%r15,%r14,%rbx
+++	mulxq	%r8,%r12,%r13
+++	movq	%r15,%rdx
+++	leaq	64(%rsi),%rsi
+++	adcxq	%r14,%r11
+++	adoxq	%rbx,%r12
+++	adcxq	%rax,%r12
+++	adoxq	%rbp,%r13
+++
+++.byte	0x67,0x67
+++	mulxq	%r8,%r8,%r14
+++	adcxq	%r8,%r13
+++	adcxq	%rbp,%r14
+++
+++	cmpq	8+8(%rsp),%rsi
+++	je	.Lsqrx8x_outer_break
+++
+++	negq	%rcx
+++	movq	$-8,%rcx
+++	movq	%rbp,%r15
+++	movq	64(%rdi),%r8
+++	adcxq	72(%rdi),%r9
+++	adcxq	80(%rdi),%r10
+++	adcxq	88(%rdi),%r11
+++	adcq	96(%rdi),%r12
+++	adcq	104(%rdi),%r13
+++	adcq	112(%rdi),%r14
+++	adcq	120(%rdi),%r15
+++	leaq	(%rsi),%rbp
+++	leaq	128(%rdi),%rdi
+++	sbbq	%rax,%rax
+++
+++	movq	-64(%rsi),%rdx
+++	movq	%rax,16+8(%rsp)
+++	movq	%rdi,24+8(%rsp)
+++
+++
+++	xorl	%eax,%eax
+++	jmp	.Lsqrx8x_loop
+++
+++.align	32
+++.Lsqrx8x_loop:
+++	movq	%r8,%rbx
+++	mulxq	0(%rbp),%rax,%r8
+++	adcxq	%rax,%rbx
+++	adoxq	%r9,%r8
+++
+++	mulxq	8(%rbp),%rax,%r9
+++	adcxq	%rax,%r8
+++	adoxq	%r10,%r9
+++
+++	mulxq	16(%rbp),%rax,%r10
+++	adcxq	%rax,%r9
+++	adoxq	%r11,%r10
+++
+++	mulxq	24(%rbp),%rax,%r11
+++	adcxq	%rax,%r10
+++	adoxq	%r12,%r11
+++
+++.byte	0xc4,0x62,0xfb,0xf6,0xa5,0x20,0x00,0x00,0x00
+++	adcxq	%rax,%r11
+++	adoxq	%r13,%r12
+++
+++	mulxq	40(%rbp),%rax,%r13
+++	adcxq	%rax,%r12
+++	adoxq	%r14,%r13
+++
+++	mulxq	48(%rbp),%rax,%r14
+++	movq	%rbx,(%rdi,%rcx,8)
+++	movl	$0,%ebx
+++	adcxq	%rax,%r13
+++	adoxq	%r15,%r14
+++
+++.byte	0xc4,0x62,0xfb,0xf6,0xbd,0x38,0x00,0x00,0x00
+++	movq	8(%rsi,%rcx,8),%rdx
+++	adcxq	%rax,%r14
+++	adoxq	%rbx,%r15
+++	adcxq	%rbx,%r15
+++
+++.byte	0x67
+++	incq	%rcx
+++	jnz	.Lsqrx8x_loop
+++
+++	leaq	64(%rbp),%rbp
+++	movq	$-8,%rcx
+++	cmpq	8+8(%rsp),%rbp
+++	je	.Lsqrx8x_break
+++
+++	subq	16+8(%rsp),%rbx
+++.byte	0x66
+++	movq	-64(%rsi),%rdx
+++	adcxq	0(%rdi),%r8
+++	adcxq	8(%rdi),%r9
+++	adcq	16(%rdi),%r10
+++	adcq	24(%rdi),%r11
+++	adcq	32(%rdi),%r12
+++	adcq	40(%rdi),%r13
+++	adcq	48(%rdi),%r14
+++	adcq	56(%rdi),%r15
+++	leaq	64(%rdi),%rdi
+++.byte	0x67
+++	sbbq	%rax,%rax
+++	xorl	%ebx,%ebx
+++	movq	%rax,16+8(%rsp)
+++	jmp	.Lsqrx8x_loop
+++
+++.align	32
+++.Lsqrx8x_break:
+++	xorq	%rbp,%rbp
+++	subq	16+8(%rsp),%rbx
+++	adcxq	%rbp,%r8
+++	movq	24+8(%rsp),%rcx
+++	adcxq	%rbp,%r9
+++	movq	0(%rsi),%rdx
+++	adcq	$0,%r10
+++	movq	%r8,0(%rdi)
+++	adcq	$0,%r11
+++	adcq	$0,%r12
+++	adcq	$0,%r13
+++	adcq	$0,%r14
+++	adcq	$0,%r15
+++	cmpq	%rcx,%rdi
+++	je	.Lsqrx8x_outer_loop
+++
+++	movq	%r9,8(%rdi)
+++	movq	8(%rcx),%r9
+++	movq	%r10,16(%rdi)
+++	movq	16(%rcx),%r10
+++	movq	%r11,24(%rdi)
+++	movq	24(%rcx),%r11
+++	movq	%r12,32(%rdi)
+++	movq	32(%rcx),%r12
+++	movq	%r13,40(%rdi)
+++	movq	40(%rcx),%r13
+++	movq	%r14,48(%rdi)
+++	movq	48(%rcx),%r14
+++	movq	%r15,56(%rdi)
+++	movq	56(%rcx),%r15
+++	movq	%rcx,%rdi
+++	jmp	.Lsqrx8x_outer_loop
+++
+++.align	32
+++.Lsqrx8x_outer_break:
+++	movq	%r9,72(%rdi)
+++.byte	102,72,15,126,217
+++	movq	%r10,80(%rdi)
+++	movq	%r11,88(%rdi)
+++	movq	%r12,96(%rdi)
+++	movq	%r13,104(%rdi)
+++	movq	%r14,112(%rdi)
+++	leaq	48+8(%rsp),%rdi
+++	movq	(%rsi,%rcx,1),%rdx
+++
+++	movq	8(%rdi),%r11
+++	xorq	%r10,%r10
+++	movq	0+8(%rsp),%r9
+++	adoxq	%r11,%r11
+++	movq	16(%rdi),%r12
+++	movq	24(%rdi),%r13
+++
+++
+++.align	32
+++.Lsqrx4x_shift_n_add:
+++	mulxq	%rdx,%rax,%rbx
+++	adoxq	%r12,%r12
+++	adcxq	%r10,%rax
+++.byte	0x48,0x8b,0x94,0x0e,0x08,0x00,0x00,0x00
+++.byte	0x4c,0x8b,0x97,0x20,0x00,0x00,0x00
+++	adoxq	%r13,%r13
+++	adcxq	%r11,%rbx
+++	movq	40(%rdi),%r11
+++	movq	%rax,0(%rdi)
+++	movq	%rbx,8(%rdi)
+++
+++	mulxq	%rdx,%rax,%rbx
+++	adoxq	%r10,%r10
+++	adcxq	%r12,%rax
+++	movq	16(%rsi,%rcx,1),%rdx
+++	movq	48(%rdi),%r12
+++	adoxq	%r11,%r11
+++	adcxq	%r13,%rbx
+++	movq	56(%rdi),%r13
+++	movq	%rax,16(%rdi)
+++	movq	%rbx,24(%rdi)
+++
+++	mulxq	%rdx,%rax,%rbx
+++	adoxq	%r12,%r12
+++	adcxq	%r10,%rax
+++	movq	24(%rsi,%rcx,1),%rdx
+++	leaq	32(%rcx),%rcx
+++	movq	64(%rdi),%r10
+++	adoxq	%r13,%r13
+++	adcxq	%r11,%rbx
+++	movq	72(%rdi),%r11
+++	movq	%rax,32(%rdi)
+++	movq	%rbx,40(%rdi)
+++
+++	mulxq	%rdx,%rax,%rbx
+++	adoxq	%r10,%r10
+++	adcxq	%r12,%rax
+++	jrcxz	.Lsqrx4x_shift_n_add_break
+++.byte	0x48,0x8b,0x94,0x0e,0x00,0x00,0x00,0x00
+++	adoxq	%r11,%r11
+++	adcxq	%r13,%rbx
+++	movq	80(%rdi),%r12
+++	movq	88(%rdi),%r13
+++	movq	%rax,48(%rdi)
+++	movq	%rbx,56(%rdi)
+++	leaq	64(%rdi),%rdi
+++	nop
+++	jmp	.Lsqrx4x_shift_n_add
+++
+++.align	32
+++.Lsqrx4x_shift_n_add_break:
+++	adcxq	%r13,%rbx
+++	movq	%rax,48(%rdi)
+++	movq	%rbx,56(%rdi)
+++	leaq	64(%rdi),%rdi
+++.byte	102,72,15,126,213
+++__bn_sqrx8x_reduction:
+++	xorl	%eax,%eax
+++	movq	32+8(%rsp),%rbx
+++	movq	48+8(%rsp),%rdx
+++	leaq	-64(%rbp,%r9,1),%rcx
+++
+++	movq	%rcx,0+8(%rsp)
+++	movq	%rdi,8+8(%rsp)
+++
+++	leaq	48+8(%rsp),%rdi
+++	jmp	.Lsqrx8x_reduction_loop
+++
+++.align	32
+++.Lsqrx8x_reduction_loop:
+++	movq	8(%rdi),%r9
+++	movq	16(%rdi),%r10
+++	movq	24(%rdi),%r11
+++	movq	32(%rdi),%r12
+++	movq	%rdx,%r8
+++	imulq	%rbx,%rdx
+++	movq	40(%rdi),%r13
+++	movq	48(%rdi),%r14
+++	movq	56(%rdi),%r15
+++	movq	%rax,24+8(%rsp)
+++
+++	leaq	64(%rdi),%rdi
+++	xorq	%rsi,%rsi
+++	movq	$-8,%rcx
+++	jmp	.Lsqrx8x_reduce
+++
+++.align	32
+++.Lsqrx8x_reduce:
+++	movq	%r8,%rbx
+++	mulxq	0(%rbp),%rax,%r8
+++	adcxq	%rbx,%rax
+++	adoxq	%r9,%r8
+++
+++	mulxq	8(%rbp),%rbx,%r9
+++	adcxq	%rbx,%r8
+++	adoxq	%r10,%r9
+++
+++	mulxq	16(%rbp),%rbx,%r10
+++	adcxq	%rbx,%r9
+++	adoxq	%r11,%r10
+++
+++	mulxq	24(%rbp),%rbx,%r11
+++	adcxq	%rbx,%r10
+++	adoxq	%r12,%r11
+++
+++.byte	0xc4,0x62,0xe3,0xf6,0xa5,0x20,0x00,0x00,0x00
+++	movq	%rdx,%rax
+++	movq	%r8,%rdx
+++	adcxq	%rbx,%r11
+++	adoxq	%r13,%r12
+++
+++	mulxq	32+8(%rsp),%rbx,%rdx
+++	movq	%rax,%rdx
+++	movq	%rax,64+48+8(%rsp,%rcx,8)
+++
+++	mulxq	40(%rbp),%rax,%r13
+++	adcxq	%rax,%r12
+++	adoxq	%r14,%r13
+++
+++	mulxq	48(%rbp),%rax,%r14
+++	adcxq	%rax,%r13
+++	adoxq	%r15,%r14
+++
+++	mulxq	56(%rbp),%rax,%r15
+++	movq	%rbx,%rdx
+++	adcxq	%rax,%r14
+++	adoxq	%rsi,%r15
+++	adcxq	%rsi,%r15
+++
+++.byte	0x67,0x67,0x67
+++	incq	%rcx
+++	jnz	.Lsqrx8x_reduce
+++
+++	movq	%rsi,%rax
+++	cmpq	0+8(%rsp),%rbp
+++	jae	.Lsqrx8x_no_tail
+++
+++	movq	48+8(%rsp),%rdx
+++	addq	0(%rdi),%r8
+++	leaq	64(%rbp),%rbp
+++	movq	$-8,%rcx
+++	adcxq	8(%rdi),%r9
+++	adcxq	16(%rdi),%r10
+++	adcq	24(%rdi),%r11
+++	adcq	32(%rdi),%r12
+++	adcq	40(%rdi),%r13
+++	adcq	48(%rdi),%r14
+++	adcq	56(%rdi),%r15
+++	leaq	64(%rdi),%rdi
+++	sbbq	%rax,%rax
+++
+++	xorq	%rsi,%rsi
+++	movq	%rax,16+8(%rsp)
+++	jmp	.Lsqrx8x_tail
+++
+++.align	32
+++.Lsqrx8x_tail:
+++	movq	%r8,%rbx
+++	mulxq	0(%rbp),%rax,%r8
+++	adcxq	%rax,%rbx
+++	adoxq	%r9,%r8
+++
+++	mulxq	8(%rbp),%rax,%r9
+++	adcxq	%rax,%r8
+++	adoxq	%r10,%r9
+++
+++	mulxq	16(%rbp),%rax,%r10
+++	adcxq	%rax,%r9
+++	adoxq	%r11,%r10
+++
+++	mulxq	24(%rbp),%rax,%r11
+++	adcxq	%rax,%r10
+++	adoxq	%r12,%r11
+++
+++.byte	0xc4,0x62,0xfb,0xf6,0xa5,0x20,0x00,0x00,0x00
+++	adcxq	%rax,%r11
+++	adoxq	%r13,%r12
+++
+++	mulxq	40(%rbp),%rax,%r13
+++	adcxq	%rax,%r12
+++	adoxq	%r14,%r13
+++
+++	mulxq	48(%rbp),%rax,%r14
+++	adcxq	%rax,%r13
+++	adoxq	%r15,%r14
+++
+++	mulxq	56(%rbp),%rax,%r15
+++	movq	72+48+8(%rsp,%rcx,8),%rdx
+++	adcxq	%rax,%r14
+++	adoxq	%rsi,%r15
+++	movq	%rbx,(%rdi,%rcx,8)
+++	movq	%r8,%rbx
+++	adcxq	%rsi,%r15
+++
+++	incq	%rcx
+++	jnz	.Lsqrx8x_tail
+++
+++	cmpq	0+8(%rsp),%rbp
+++	jae	.Lsqrx8x_tail_done
+++
+++	subq	16+8(%rsp),%rsi
+++	movq	48+8(%rsp),%rdx
+++	leaq	64(%rbp),%rbp
+++	adcq	0(%rdi),%r8
+++	adcq	8(%rdi),%r9
+++	adcq	16(%rdi),%r10
+++	adcq	24(%rdi),%r11
+++	adcq	32(%rdi),%r12
+++	adcq	40(%rdi),%r13
+++	adcq	48(%rdi),%r14
+++	adcq	56(%rdi),%r15
+++	leaq	64(%rdi),%rdi
+++	sbbq	%rax,%rax
+++	subq	$8,%rcx
+++
+++	xorq	%rsi,%rsi
+++	movq	%rax,16+8(%rsp)
+++	jmp	.Lsqrx8x_tail
+++
+++.align	32
+++.Lsqrx8x_tail_done:
+++	xorq	%rax,%rax
+++	addq	24+8(%rsp),%r8
+++	adcq	$0,%r9
+++	adcq	$0,%r10
+++	adcq	$0,%r11
+++	adcq	$0,%r12
+++	adcq	$0,%r13
+++	adcq	$0,%r14
+++	adcq	$0,%r15
+++	adcq	$0,%rax
+++
+++	subq	16+8(%rsp),%rsi
+++.Lsqrx8x_no_tail:
+++	adcq	0(%rdi),%r8
+++.byte	102,72,15,126,217
+++	adcq	8(%rdi),%r9
+++	movq	56(%rbp),%rsi
+++.byte	102,72,15,126,213
+++	adcq	16(%rdi),%r10
+++	adcq	24(%rdi),%r11
+++	adcq	32(%rdi),%r12
+++	adcq	40(%rdi),%r13
+++	adcq	48(%rdi),%r14
+++	adcq	56(%rdi),%r15
+++	adcq	$0,%rax
+++
+++	movq	32+8(%rsp),%rbx
+++	movq	64(%rdi,%rcx,1),%rdx
+++
+++	movq	%r8,0(%rdi)
+++	leaq	64(%rdi),%r8
+++	movq	%r9,8(%rdi)
+++	movq	%r10,16(%rdi)
+++	movq	%r11,24(%rdi)
+++	movq	%r12,32(%rdi)
+++	movq	%r13,40(%rdi)
+++	movq	%r14,48(%rdi)
+++	movq	%r15,56(%rdi)
+++
+++	leaq	64(%rdi,%rcx,1),%rdi
+++	cmpq	8+8(%rsp),%r8
+++	jb	.Lsqrx8x_reduction_loop
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_sqrx8x_internal,.-bn_sqrx8x_internal
+++.align	32
+++.type	__bn_postx4x_internal,@function
+++__bn_postx4x_internal:
+++.cfi_startproc	
+++	movq	0(%rbp),%r12
+++	movq	%rcx,%r10
+++	movq	%rcx,%r9
+++	negq	%rax
+++	sarq	$3+2,%rcx
+++
+++.byte	102,72,15,126,202
+++.byte	102,72,15,126,206
+++	decq	%r12
+++	movq	8(%rbp),%r13
+++	xorq	%r8,%r8
+++	movq	16(%rbp),%r14
+++	movq	24(%rbp),%r15
+++	jmp	.Lsqrx4x_sub_entry
+++
+++.align	16
+++.Lsqrx4x_sub:
+++	movq	0(%rbp),%r12
+++	movq	8(%rbp),%r13
+++	movq	16(%rbp),%r14
+++	movq	24(%rbp),%r15
+++.Lsqrx4x_sub_entry:
+++	andnq	%rax,%r12,%r12
+++	leaq	32(%rbp),%rbp
+++	andnq	%rax,%r13,%r13
+++	andnq	%rax,%r14,%r14
+++	andnq	%rax,%r15,%r15
+++
+++	negq	%r8
+++	adcq	0(%rdi),%r12
+++	adcq	8(%rdi),%r13
+++	adcq	16(%rdi),%r14
+++	adcq	24(%rdi),%r15
+++	movq	%r12,0(%rdx)
+++	leaq	32(%rdi),%rdi
+++	movq	%r13,8(%rdx)
+++	sbbq	%r8,%r8
+++	movq	%r14,16(%rdx)
+++	movq	%r15,24(%rdx)
+++	leaq	32(%rdx),%rdx
+++
+++	incq	%rcx
+++	jnz	.Lsqrx4x_sub
+++
+++	negq	%r9
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	__bn_postx4x_internal,.-__bn_postx4x_internal
+++.globl	bn_scatter5
+++.hidden bn_scatter5
+++.type	bn_scatter5,@function
+++.align	16
+++bn_scatter5:
+++.cfi_startproc	
+++	cmpl	$0,%esi
+++	jz	.Lscatter_epilogue
+++	leaq	(%rdx,%rcx,8),%rdx
+++.Lscatter:
+++	movq	(%rdi),%rax
+++	leaq	8(%rdi),%rdi
+++	movq	%rax,(%rdx)
+++	leaq	256(%rdx),%rdx
+++	subl	$1,%esi
+++	jnz	.Lscatter
+++.Lscatter_epilogue:
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.size	bn_scatter5,.-bn_scatter5
+++
+++.globl	bn_gather5
+++.hidden bn_gather5
+++.type	bn_gather5,@function
+++.align	32
+++bn_gather5:
+++.cfi_startproc	
+++.LSEH_begin_bn_gather5:
+++
+++.byte	0x4c,0x8d,0x14,0x24
+++.cfi_def_cfa_register	%r10
+++.byte	0x48,0x81,0xec,0x08,0x01,0x00,0x00
+++	leaq	.Linc(%rip),%rax
+++	andq	$-16,%rsp
+++
+++	movd	%ecx,%xmm5
+++	movdqa	0(%rax),%xmm0
+++	movdqa	16(%rax),%xmm1
+++	leaq	128(%rdx),%r11
+++	leaq	128(%rsp),%rax
+++
+++	pshufd	$0,%xmm5,%xmm5
+++	movdqa	%xmm1,%xmm4
+++	movdqa	%xmm1,%xmm2
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm4,%xmm3
+++
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,-128(%rax)
+++	movdqa	%xmm4,%xmm0
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,-112(%rax)
+++	movdqa	%xmm4,%xmm1
+++
+++	paddd	%xmm3,%xmm0
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,-96(%rax)
+++	movdqa	%xmm4,%xmm2
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm3,-80(%rax)
+++	movdqa	%xmm4,%xmm3
+++
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,-64(%rax)
+++	movdqa	%xmm4,%xmm0
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,-48(%rax)
+++	movdqa	%xmm4,%xmm1
+++
+++	paddd	%xmm3,%xmm0
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,-32(%rax)
+++	movdqa	%xmm4,%xmm2
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm3,-16(%rax)
+++	movdqa	%xmm4,%xmm3
+++
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,0(%rax)
+++	movdqa	%xmm4,%xmm0
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,16(%rax)
+++	movdqa	%xmm4,%xmm1
+++
+++	paddd	%xmm3,%xmm0
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,32(%rax)
+++	movdqa	%xmm4,%xmm2
+++	paddd	%xmm0,%xmm1
+++	pcmpeqd	%xmm5,%xmm0
+++	movdqa	%xmm3,48(%rax)
+++	movdqa	%xmm4,%xmm3
+++
+++	paddd	%xmm1,%xmm2
+++	pcmpeqd	%xmm5,%xmm1
+++	movdqa	%xmm0,64(%rax)
+++	movdqa	%xmm4,%xmm0
+++
+++	paddd	%xmm2,%xmm3
+++	pcmpeqd	%xmm5,%xmm2
+++	movdqa	%xmm1,80(%rax)
+++	movdqa	%xmm4,%xmm1
+++
+++	paddd	%xmm3,%xmm0
+++	pcmpeqd	%xmm5,%xmm3
+++	movdqa	%xmm2,96(%rax)
+++	movdqa	%xmm4,%xmm2
+++	movdqa	%xmm3,112(%rax)
+++	jmp	.Lgather
+++
+++.align	32
+++.Lgather:
+++	pxor	%xmm4,%xmm4
+++	pxor	%xmm5,%xmm5
+++	movdqa	-128(%r11),%xmm0
+++	movdqa	-112(%r11),%xmm1
+++	movdqa	-96(%r11),%xmm2
+++	pand	-128(%rax),%xmm0
+++	movdqa	-80(%r11),%xmm3
+++	pand	-112(%rax),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	-96(%rax),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	-80(%rax),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqa	-64(%r11),%xmm0
+++	movdqa	-48(%r11),%xmm1
+++	movdqa	-32(%r11),%xmm2
+++	pand	-64(%rax),%xmm0
+++	movdqa	-16(%r11),%xmm3
+++	pand	-48(%rax),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	-32(%rax),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	-16(%rax),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqa	0(%r11),%xmm0
+++	movdqa	16(%r11),%xmm1
+++	movdqa	32(%r11),%xmm2
+++	pand	0(%rax),%xmm0
+++	movdqa	48(%r11),%xmm3
+++	pand	16(%rax),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	32(%rax),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	48(%rax),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	movdqa	64(%r11),%xmm0
+++	movdqa	80(%r11),%xmm1
+++	movdqa	96(%r11),%xmm2
+++	pand	64(%rax),%xmm0
+++	movdqa	112(%r11),%xmm3
+++	pand	80(%rax),%xmm1
+++	por	%xmm0,%xmm4
+++	pand	96(%rax),%xmm2
+++	por	%xmm1,%xmm5
+++	pand	112(%rax),%xmm3
+++	por	%xmm2,%xmm4
+++	por	%xmm3,%xmm5
+++	por	%xmm5,%xmm4
+++	leaq	256(%r11),%r11
+++	pshufd	$0x4e,%xmm4,%xmm0
+++	por	%xmm4,%xmm0
+++	movq	%xmm0,(%rdi)
+++	leaq	8(%rdi),%rdi
+++	subl	$1,%esi
+++	jnz	.Lgather
+++
+++	leaq	(%r10),%rsp
+++.cfi_def_cfa_register	%rsp
+++	.byte	0xf3,0xc3
+++.LSEH_end_bn_gather5:
+++.cfi_endproc	
+++.size	bn_gather5,.-bn_gather5
+++.align	64
+++.Linc:
+++.long	0,0, 1,1
+++.long	2,2, 2,2
+++.byte	77,111,110,116,103,111,109,101,114,121,32,77,117,108,116,105,112,108,105,99,97,116,105,111,110,32,119,105,116,104,32,115,99,97,116,116,101,114,47,103,97,116,104,101,114,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++diff --git a/linux-x86_64/ypto/test/trampoline-x86_64.S b/linux-x86_64/ypto/test/trampoline-x86_64.S
++new file mode 100644
++index 000000000..9f7c0d817
++--- /dev/null
+++++ b/linux-x86_64/ypto/test/trampoline-x86_64.S
++@@ -0,0 +1,518 @@
+++# This file is generated from a similarly-named Perl script in the BoringSSL
+++# source tree. Do not edit by hand.
+++
+++#if defined(__has_feature)
+++#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+++#define OPENSSL_NO_ASM
+++#endif
+++#endif
+++
+++#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+++#if defined(BORINGSSL_PREFIX)
+++#include <boringssl_prefix_symbols_asm.h>
+++#endif
+++.text	
+++
+++
+++
+++
+++
+++
+++
+++
+++.type	abi_test_trampoline, @function
+++.globl	abi_test_trampoline
+++.hidden abi_test_trampoline
+++.align	16
+++abi_test_trampoline:
+++.Labi_test_trampoline_seh_begin:
+++.cfi_startproc	
+++
+++
+++
+++
+++
+++
+++
+++
+++
+++	subq	$120,%rsp
+++.cfi_adjust_cfa_offset	120
+++.Labi_test_trampoline_seh_prolog_alloc:
+++	movq	%r8,48(%rsp)
+++	movq	%rbx,64(%rsp)
+++.cfi_offset	rbx, -64
+++.Labi_test_trampoline_seh_prolog_rbx:
+++	movq	%rbp,72(%rsp)
+++.cfi_offset	rbp, -56
+++.Labi_test_trampoline_seh_prolog_rbp:
+++	movq	%r12,80(%rsp)
+++.cfi_offset	r12, -48
+++.Labi_test_trampoline_seh_prolog_r12:
+++	movq	%r13,88(%rsp)
+++.cfi_offset	r13, -40
+++.Labi_test_trampoline_seh_prolog_r13:
+++	movq	%r14,96(%rsp)
+++.cfi_offset	r14, -32
+++.Labi_test_trampoline_seh_prolog_r14:
+++	movq	%r15,104(%rsp)
+++.cfi_offset	r15, -24
+++.Labi_test_trampoline_seh_prolog_r15:
+++.Labi_test_trampoline_seh_prolog_end:
+++	movq	0(%rsi),%rbx
+++	movq	8(%rsi),%rbp
+++	movq	16(%rsi),%r12
+++	movq	24(%rsi),%r13
+++	movq	32(%rsi),%r14
+++	movq	40(%rsi),%r15
+++
+++	movq	%rdi,32(%rsp)
+++	movq	%rsi,40(%rsp)
+++
+++
+++
+++
+++	movq	%rdx,%r10
+++	movq	%rcx,%r11
+++	decq	%r11
+++	js	.Largs_done
+++	movq	(%r10),%rdi
+++	addq	$8,%r10
+++	decq	%r11
+++	js	.Largs_done
+++	movq	(%r10),%rsi
+++	addq	$8,%r10
+++	decq	%r11
+++	js	.Largs_done
+++	movq	(%r10),%rdx
+++	addq	$8,%r10
+++	decq	%r11
+++	js	.Largs_done
+++	movq	(%r10),%rcx
+++	addq	$8,%r10
+++	decq	%r11
+++	js	.Largs_done
+++	movq	(%r10),%r8
+++	addq	$8,%r10
+++	decq	%r11
+++	js	.Largs_done
+++	movq	(%r10),%r9
+++	addq	$8,%r10
+++	leaq	0(%rsp),%rax
+++.Largs_loop:
+++	decq	%r11
+++	js	.Largs_done
+++
+++
+++
+++
+++
+++
+++	movq	%r11,56(%rsp)
+++	movq	(%r10),%r11
+++	movq	%r11,(%rax)
+++	movq	56(%rsp),%r11
+++
+++	addq	$8,%r10
+++	addq	$8,%rax
+++	jmp	.Largs_loop
+++
+++.Largs_done:
+++	movq	32(%rsp),%rax
+++	movq	48(%rsp),%r10
+++	testq	%r10,%r10
+++	jz	.Lno_unwind
+++
+++
+++	pushfq
+++	orq	$0x100,0(%rsp)
+++	popfq
+++
+++
+++
+++	nop
+++.globl	abi_test_unwind_start
+++.hidden abi_test_unwind_start
+++abi_test_unwind_start:
+++
+++	call	*%rax
+++.globl	abi_test_unwind_return
+++.hidden abi_test_unwind_return
+++abi_test_unwind_return:
+++
+++
+++
+++
+++	pushfq
+++	andq	$-0x101,0(%rsp)
+++	popfq
+++.globl	abi_test_unwind_stop
+++.hidden abi_test_unwind_stop
+++abi_test_unwind_stop:
+++
+++	jmp	.Lcall_done
+++
+++.Lno_unwind:
+++	call	*%rax
+++
+++.Lcall_done:
+++
+++	movq	40(%rsp),%rsi
+++	movq	%rbx,0(%rsi)
+++	movq	%rbp,8(%rsi)
+++	movq	%r12,16(%rsi)
+++	movq	%r13,24(%rsi)
+++	movq	%r14,32(%rsi)
+++	movq	%r15,40(%rsi)
+++	movq	64(%rsp),%rbx
+++.cfi_restore	rbx
+++	movq	72(%rsp),%rbp
+++.cfi_restore	rbp
+++	movq	80(%rsp),%r12
+++.cfi_restore	r12
+++	movq	88(%rsp),%r13
+++.cfi_restore	r13
+++	movq	96(%rsp),%r14
+++.cfi_restore	r14
+++	movq	104(%rsp),%r15
+++.cfi_restore	r15
+++	addq	$120,%rsp
+++.cfi_adjust_cfa_offset	-120
+++
+++
+++	.byte	0xf3,0xc3
+++.cfi_endproc	
+++.Labi_test_trampoline_seh_end:
+++.size	abi_test_trampoline,.-abi_test_trampoline
+++.type	abi_test_clobber_rax, @function
+++.globl	abi_test_clobber_rax
+++.hidden abi_test_clobber_rax
+++.align	16
+++abi_test_clobber_rax:
+++	xorq	%rax,%rax
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_rax,.-abi_test_clobber_rax
+++.type	abi_test_clobber_rbx, @function
+++.globl	abi_test_clobber_rbx
+++.hidden abi_test_clobber_rbx
+++.align	16
+++abi_test_clobber_rbx:
+++	xorq	%rbx,%rbx
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_rbx,.-abi_test_clobber_rbx
+++.type	abi_test_clobber_rcx, @function
+++.globl	abi_test_clobber_rcx
+++.hidden abi_test_clobber_rcx
+++.align	16
+++abi_test_clobber_rcx:
+++	xorq	%rcx,%rcx
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_rcx,.-abi_test_clobber_rcx
+++.type	abi_test_clobber_rdx, @function
+++.globl	abi_test_clobber_rdx
+++.hidden abi_test_clobber_rdx
+++.align	16
+++abi_test_clobber_rdx:
+++	xorq	%rdx,%rdx
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_rdx,.-abi_test_clobber_rdx
+++.type	abi_test_clobber_rdi, @function
+++.globl	abi_test_clobber_rdi
+++.hidden abi_test_clobber_rdi
+++.align	16
+++abi_test_clobber_rdi:
+++	xorq	%rdi,%rdi
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_rdi,.-abi_test_clobber_rdi
+++.type	abi_test_clobber_rsi, @function
+++.globl	abi_test_clobber_rsi
+++.hidden abi_test_clobber_rsi
+++.align	16
+++abi_test_clobber_rsi:
+++	xorq	%rsi,%rsi
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_rsi,.-abi_test_clobber_rsi
+++.type	abi_test_clobber_rbp, @function
+++.globl	abi_test_clobber_rbp
+++.hidden abi_test_clobber_rbp
+++.align	16
+++abi_test_clobber_rbp:
+++	xorq	%rbp,%rbp
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_rbp,.-abi_test_clobber_rbp
+++.type	abi_test_clobber_r8, @function
+++.globl	abi_test_clobber_r8
+++.hidden abi_test_clobber_r8
+++.align	16
+++abi_test_clobber_r8:
+++	xorq	%r8,%r8
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_r8,.-abi_test_clobber_r8
+++.type	abi_test_clobber_r9, @function
+++.globl	abi_test_clobber_r9
+++.hidden abi_test_clobber_r9
+++.align	16
+++abi_test_clobber_r9:
+++	xorq	%r9,%r9
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_r9,.-abi_test_clobber_r9
+++.type	abi_test_clobber_r10, @function
+++.globl	abi_test_clobber_r10
+++.hidden abi_test_clobber_r10
+++.align	16
+++abi_test_clobber_r10:
+++	xorq	%r10,%r10
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_r10,.-abi_test_clobber_r10
+++.type	abi_test_clobber_r11, @function
+++.globl	abi_test_clobber_r11
+++.hidden abi_test_clobber_r11
+++.align	16
+++abi_test_clobber_r11:
+++	xorq	%r11,%r11
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_r11,.-abi_test_clobber_r11
+++.type	abi_test_clobber_r12, @function
+++.globl	abi_test_clobber_r12
+++.hidden abi_test_clobber_r12
+++.align	16
+++abi_test_clobber_r12:
+++	xorq	%r12,%r12
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_r12,.-abi_test_clobber_r12
+++.type	abi_test_clobber_r13, @function
+++.globl	abi_test_clobber_r13
+++.hidden abi_test_clobber_r13
+++.align	16
+++abi_test_clobber_r13:
+++	xorq	%r13,%r13
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_r13,.-abi_test_clobber_r13
+++.type	abi_test_clobber_r14, @function
+++.globl	abi_test_clobber_r14
+++.hidden abi_test_clobber_r14
+++.align	16
+++abi_test_clobber_r14:
+++	xorq	%r14,%r14
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_r14,.-abi_test_clobber_r14
+++.type	abi_test_clobber_r15, @function
+++.globl	abi_test_clobber_r15
+++.hidden abi_test_clobber_r15
+++.align	16
+++abi_test_clobber_r15:
+++	xorq	%r15,%r15
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_r15,.-abi_test_clobber_r15
+++.type	abi_test_clobber_xmm0, @function
+++.globl	abi_test_clobber_xmm0
+++.hidden abi_test_clobber_xmm0
+++.align	16
+++abi_test_clobber_xmm0:
+++	pxor	%xmm0,%xmm0
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm0,.-abi_test_clobber_xmm0
+++.type	abi_test_clobber_xmm1, @function
+++.globl	abi_test_clobber_xmm1
+++.hidden abi_test_clobber_xmm1
+++.align	16
+++abi_test_clobber_xmm1:
+++	pxor	%xmm1,%xmm1
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm1,.-abi_test_clobber_xmm1
+++.type	abi_test_clobber_xmm2, @function
+++.globl	abi_test_clobber_xmm2
+++.hidden abi_test_clobber_xmm2
+++.align	16
+++abi_test_clobber_xmm2:
+++	pxor	%xmm2,%xmm2
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm2,.-abi_test_clobber_xmm2
+++.type	abi_test_clobber_xmm3, @function
+++.globl	abi_test_clobber_xmm3
+++.hidden abi_test_clobber_xmm3
+++.align	16
+++abi_test_clobber_xmm3:
+++	pxor	%xmm3,%xmm3
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm3,.-abi_test_clobber_xmm3
+++.type	abi_test_clobber_xmm4, @function
+++.globl	abi_test_clobber_xmm4
+++.hidden abi_test_clobber_xmm4
+++.align	16
+++abi_test_clobber_xmm4:
+++	pxor	%xmm4,%xmm4
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm4,.-abi_test_clobber_xmm4
+++.type	abi_test_clobber_xmm5, @function
+++.globl	abi_test_clobber_xmm5
+++.hidden abi_test_clobber_xmm5
+++.align	16
+++abi_test_clobber_xmm5:
+++	pxor	%xmm5,%xmm5
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm5,.-abi_test_clobber_xmm5
+++.type	abi_test_clobber_xmm6, @function
+++.globl	abi_test_clobber_xmm6
+++.hidden abi_test_clobber_xmm6
+++.align	16
+++abi_test_clobber_xmm6:
+++	pxor	%xmm6,%xmm6
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm6,.-abi_test_clobber_xmm6
+++.type	abi_test_clobber_xmm7, @function
+++.globl	abi_test_clobber_xmm7
+++.hidden abi_test_clobber_xmm7
+++.align	16
+++abi_test_clobber_xmm7:
+++	pxor	%xmm7,%xmm7
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm7,.-abi_test_clobber_xmm7
+++.type	abi_test_clobber_xmm8, @function
+++.globl	abi_test_clobber_xmm8
+++.hidden abi_test_clobber_xmm8
+++.align	16
+++abi_test_clobber_xmm8:
+++	pxor	%xmm8,%xmm8
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm8,.-abi_test_clobber_xmm8
+++.type	abi_test_clobber_xmm9, @function
+++.globl	abi_test_clobber_xmm9
+++.hidden abi_test_clobber_xmm9
+++.align	16
+++abi_test_clobber_xmm9:
+++	pxor	%xmm9,%xmm9
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm9,.-abi_test_clobber_xmm9
+++.type	abi_test_clobber_xmm10, @function
+++.globl	abi_test_clobber_xmm10
+++.hidden abi_test_clobber_xmm10
+++.align	16
+++abi_test_clobber_xmm10:
+++	pxor	%xmm10,%xmm10
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm10,.-abi_test_clobber_xmm10
+++.type	abi_test_clobber_xmm11, @function
+++.globl	abi_test_clobber_xmm11
+++.hidden abi_test_clobber_xmm11
+++.align	16
+++abi_test_clobber_xmm11:
+++	pxor	%xmm11,%xmm11
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm11,.-abi_test_clobber_xmm11
+++.type	abi_test_clobber_xmm12, @function
+++.globl	abi_test_clobber_xmm12
+++.hidden abi_test_clobber_xmm12
+++.align	16
+++abi_test_clobber_xmm12:
+++	pxor	%xmm12,%xmm12
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm12,.-abi_test_clobber_xmm12
+++.type	abi_test_clobber_xmm13, @function
+++.globl	abi_test_clobber_xmm13
+++.hidden abi_test_clobber_xmm13
+++.align	16
+++abi_test_clobber_xmm13:
+++	pxor	%xmm13,%xmm13
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm13,.-abi_test_clobber_xmm13
+++.type	abi_test_clobber_xmm14, @function
+++.globl	abi_test_clobber_xmm14
+++.hidden abi_test_clobber_xmm14
+++.align	16
+++abi_test_clobber_xmm14:
+++	pxor	%xmm14,%xmm14
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm14,.-abi_test_clobber_xmm14
+++.type	abi_test_clobber_xmm15, @function
+++.globl	abi_test_clobber_xmm15
+++.hidden abi_test_clobber_xmm15
+++.align	16
+++abi_test_clobber_xmm15:
+++	pxor	%xmm15,%xmm15
+++	.byte	0xf3,0xc3
+++.size	abi_test_clobber_xmm15,.-abi_test_clobber_xmm15
+++
+++
+++
+++.type	abi_test_bad_unwind_wrong_register, @function
+++.globl	abi_test_bad_unwind_wrong_register
+++.hidden abi_test_bad_unwind_wrong_register
+++.align	16
+++abi_test_bad_unwind_wrong_register:
+++.cfi_startproc	
+++.Labi_test_bad_unwind_wrong_register_seh_begin:
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r13,-16
+++.Labi_test_bad_unwind_wrong_register_seh_push_r13:
+++
+++
+++
+++	nop
+++	popq	%r12
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r12
+++	.byte	0xf3,0xc3
+++.Labi_test_bad_unwind_wrong_register_seh_end:
+++.cfi_endproc	
+++.size	abi_test_bad_unwind_wrong_register,.-abi_test_bad_unwind_wrong_register
+++
+++
+++
+++
+++.type	abi_test_bad_unwind_temporary, @function
+++.globl	abi_test_bad_unwind_temporary
+++.hidden abi_test_bad_unwind_temporary
+++.align	16
+++abi_test_bad_unwind_temporary:
+++.cfi_startproc	
+++.Labi_test_bad_unwind_temporary_seh_begin:
+++	pushq	%r12
+++.cfi_adjust_cfa_offset	8
+++.cfi_offset	%r12,-16
+++.Labi_test_bad_unwind_temporary_seh_push_r12:
+++
+++	movq	%r12,%rax
+++	incq	%rax
+++	movq	%rax,(%rsp)
+++
+++
+++
+++	movq	%r12,(%rsp)
+++
+++
+++	popq	%r12
+++.cfi_adjust_cfa_offset	-8
+++.cfi_restore	%r12
+++	.byte	0xf3,0xc3
+++.Labi_test_bad_unwind_temporary_seh_end:
+++.cfi_endproc	
+++.size	abi_test_bad_unwind_temporary,.-abi_test_bad_unwind_temporary
+++
+++
+++
+++
+++.type	abi_test_set_direction_flag, @function
+++.globl	abi_test_get_and_clear_direction_flag
+++.hidden abi_test_get_and_clear_direction_flag
+++abi_test_get_and_clear_direction_flag:
+++	pushfq
+++	popq	%rax
+++	andq	$0x400,%rax
+++	shrq	$10,%rax
+++	cld
+++	.byte	0xf3,0xc3
+++.size	abi_test_get_and_clear_direction_flag,.-abi_test_get_and_clear_direction_flag
+++
+++
+++
+++.type	abi_test_set_direction_flag, @function
+++.globl	abi_test_set_direction_flag
+++.hidden abi_test_set_direction_flag
+++abi_test_set_direction_flag:
+++	std
+++	.byte	0xf3,0xc3
+++.size	abi_test_set_direction_flag,.-abi_test_set_direction_flag
+++#endif
+++.section	.note.GNU-stack,"",@progbits
++-- 
++2.23.0
++
+diff --git a/third_party/cpuinfo/cpuinfo.BUILD b/third_party/cpuinfo/cpuinfo.BUILD
+index eb2937d20ef..cf178f84510 100644
+--- a/third_party/cpuinfo/cpuinfo.BUILD
++++ b/third_party/cpuinfo/cpuinfo.BUILD
+@@ -109,6 +109,7 @@ cc_library(
+         ":linux_mips64": COMMON_SRCS + LINUX_SRCS,
+         ":linux_riscv64": COMMON_SRCS + LINUX_SRCS,
+         ":linux_s390x": COMMON_SRCS + LINUX_SRCS,
++        ":linux_ppc64le": COMMON_SRCS + LINUX_SRCS,
+         ":macos_x86_64": COMMON_SRCS + X86_SRCS + MACH_SRCS + MACH_X86_SRCS,
+         ":macos_arm64": COMMON_SRCS + MACH_SRCS + MACH_ARM_SRCS,
+         ":windows_x86_64": COMMON_SRCS + X86_SRCS + WINDOWS_X86_SRCS,
+diff --git a/third_party/grpc/generate_cc_env_fix.patch b/third_party/grpc/generate_cc_env_fix.patch
+index 51832fe9628..84093655824 100644
+--- a/third_party/grpc/generate_cc_env_fix.patch
++++ b/third_party/grpc/generate_cc_env_fix.patch
+@@ -1,10 +1,66 @@
++From d2cef9a0b4c07030426aebcff4d596056daa80c1 Mon Sep 17 00:00:00 2001
++From: Nishidha Panpaliya <npanpa23@in.ibm.com>
++Date: Mon, 7 Mar 2022 15:13:10 +0000
++Subject: [PATCH] Patch thirdparty upb
++
++---
++ bazel/generate_cc.bzl     |  1 +
++ third_party/upb/upb/upb.c | 16 +++-------------
++ 2 files changed, 4 insertions(+), 13 deletions(-)
++
++diff --git a/bazel/generate_cc.bzl b/bazel/generate_cc.bzl
++index 484959ebb7..81d52fd28f 100644
+ --- a/bazel/generate_cc.bzl
+ +++ b/bazel/generate_cc.bzl
+-@@ -141,6 +141,7 @@ def generate_cc_impl(ctx):
++@@ -140,6 +140,7 @@ def generate_cc_impl(ctx):
+          outputs = out_files,
+          executable = ctx.executable._protoc,
+          arguments = arguments,
+ +        use_default_shell_env = True,
+      )
+-
++ 
+      return struct(files = depset(out_files))
++diff --git a/third_party/upb/upb/upb.c b/third_party/upb/upb/upb.c
++index 266ea7d7f9..14a97bc584 100644
++--- a/third_party/upb/upb/upb.c
+++++ b/third_party/upb/upb/upb.c
++@@ -11,16 +11,6 @@
++ 
++ #include "upb/port_def.inc"
++ 
++-/* Guarantee null-termination and provide ellipsis truncation.
++- * It may be tempting to "optimize" this by initializing these final
++- * four bytes up-front and then being careful never to overwrite them,
++- * this is safer and simpler. */
++-static void nullz(upb_status *status) {
++-  const char *ellipsis = "...";
++-  size_t len = strlen(ellipsis);
++-  UPB_ASSERT(sizeof(status->msg) > len);
++-  memcpy(status->msg + sizeof(status->msg) - len, ellipsis, len);
++-}
++ 
++ /* upb_status *****************************************************************/
++ 
++@@ -37,8 +27,8 @@ const char *upb_status_errmsg(const upb_status *status) { return status->msg; }
++ void upb_status_seterrmsg(upb_status *status, const char *msg) {
++   if (!status) return;
++   status->ok = false;
++-  strncpy(status->msg, msg, sizeof(status->msg));
++-  nullz(status);
+++  strncpy(status->msg, msg, UPB_STATUS_MAX_MESSAGE - 1);
+++  status->msg[UPB_STATUS_MAX_MESSAGE - 1] = '\0';
++ }
++ 
++ void upb_status_seterrf(upb_status *status, const char *fmt, ...) {
++@@ -52,7 +42,7 @@ void upb_status_vseterrf(upb_status *status, const char *fmt, va_list args) {
++   if (!status) return;
++   status->ok = false;
++   _upb_vsnprintf(status->msg, sizeof(status->msg), fmt, args);
++-  nullz(status);
+++  status->msg[UPB_STATUS_MAX_MESSAGE - 1] = '\0';
++ }
++ 
++ /* upb_alloc ******************************************************************/
++-- 
++2.34.1
++
+diff --git a/third_party/grpc/upb_gcc10.patch b/third_party/grpc/upb_gcc10.patch
+new file mode 100644
+index 00000000000..7d747c48083
+--- /dev/null
++++ b/third_party/grpc/upb_gcc10.patch
+@@ -0,0 +1,54 @@
++From 57028552ed072263cd6656e5feed67ff89a55e43 Mon Sep 17 00:00:00 2001
++From: Nishidha Panpaliya <npanpa23@in.ibm.com>
++Date: Mon, 7 Mar 2022 15:08:33 +0000
++Subject: [PATCH] Fix for build failure with GCC10
++
++---
++ upb/upb.c | 17 +++--------------
++ 1 file changed, 3 insertions(+), 14 deletions(-)
++
++diff --git a/upb/upb.c b/upb/upb.c
++index 266ea7d..1410b2d 100644
++--- a/upb/upb.c
+++++ b/upb/upb.c
++@@ -11,17 +11,6 @@
++ 
++ #include "upb/port_def.inc"
++ 
++-/* Guarantee null-termination and provide ellipsis truncation.
++- * It may be tempting to "optimize" this by initializing these final
++- * four bytes up-front and then being careful never to overwrite them,
++- * this is safer and simpler. */
++-static void nullz(upb_status *status) {
++-  const char *ellipsis = "...";
++-  size_t len = strlen(ellipsis);
++-  UPB_ASSERT(sizeof(status->msg) > len);
++-  memcpy(status->msg + sizeof(status->msg) - len, ellipsis, len);
++-}
++-
++ /* upb_status *****************************************************************/
++ 
++ void upb_status_clear(upb_status *status) {
++@@ -37,8 +26,8 @@ const char *upb_status_errmsg(const upb_status *status) { return status->msg; }
++ void upb_status_seterrmsg(upb_status *status, const char *msg) {
++   if (!status) return;
++   status->ok = false;
++-  strncpy(status->msg, msg, sizeof(status->msg));
++-  nullz(status);
+++  strncpy(status->msg, msg, UPB_STATUS_MAX_MESSAGE - 1);
+++  status->msg[UPB_STATUS_MAX_MESSAGE - 1] = '\0';
++ }
++ 
++ void upb_status_seterrf(upb_status *status, const char *fmt, ...) {
++@@ -52,7 +41,7 @@ void upb_status_vseterrf(upb_status *status, const char *fmt, va_list args) {
++   if (!status) return;
++   status->ok = false;
++   _upb_vsnprintf(status->msg, sizeof(status->msg), fmt, args);
++-  nullz(status);
+++  status->msg[UPB_STATUS_MAX_MESSAGE - 1] = '\0';
++ }
++ 
++ /* upb_alloc ******************************************************************/
++-- 
++2.34.1
++
+diff --git a/third_party/llvm/fix_ppc64le.patch b/third_party/llvm/fix_ppc64le.patch
+new file mode 100644
+index 00000000000..3ed53b48164
+--- /dev/null
++++ b/third_party/llvm/fix_ppc64le.patch
+@@ -0,0 +1,24 @@
++From dffd421dd8a2410598f0053678f4c0bb9714daf9 Mon Sep 17 00:00:00 2001
++From: Nishidha Panpaliya <npanpa23@in.ibm.com>
++Date: Mon, 21 Mar 2022 08:14:31 -0400
++Subject: [PATCH] Fix build on ppc64le
++
++---
++ utils/bazel/llvm-project-overlay/llvm/config.bzl | 1 +
++ 1 file changed, 1 insertion(+)
++
++diff --git a/utils/bazel/llvm-project-overlay/llvm/config.bzl b/utils/bazel/llvm-project-overlay/llvm/config.bzl
++index 772714f38941..9ed63e8d44a3 100644
++--- a/utils/bazel/llvm-project-overlay/llvm/config.bzl
+++++ b/utils/bazel/llvm-project-overlay/llvm/config.bzl
++@@ -86,6 +86,7 @@ llvm_config_defines = os_defines + select({
++     "//llvm:macos_arm64": native_arch_defines("AArch64", "arm64-apple-darwin"),
++     "@bazel_tools//src/conditions:darwin": native_arch_defines("X86", "x86_64-unknown-darwin"),
++     "@bazel_tools//src/conditions:linux_aarch64": native_arch_defines("AArch64", "aarch64-unknown-linux-gnu"),
+++    "@bazel_tools//src/conditions:linux_ppc64le": native_arch_defines("PowerPC", "powerpc64le-unknown-linux-gnu"),
++     "//conditions:default": native_arch_defines("X86", "x86_64-unknown-linux-gnu"),
++ }) + [
++     # These shouldn't be needed by the C++11 standard, but are for some
++-- 
++2.23.0
++
+diff --git a/third_party/systemlibs/sqlite.BUILD b/third_party/systemlibs/sqlite.BUILD
+index 88a84a96137..31fc5ed5e88 100644
+--- a/third_party/systemlibs/sqlite.BUILD
++++ b/third_party/systemlibs/sqlite.BUILD
+@@ -1,12 +1,37 @@
+ licenses(["unencumbered"])  # Public Domain
+ 
++HEADERS = [
++   "sqlite3.h",
++   "sqlite3ext.h",
++]
++
++LIBS = [
++   "libsqlite3.so",
++   "libsqlite3.so.0",
++   "libsqlite3.so.0.8.6",
++]
++
+ # Production build of SQLite library that's baked into TensorFlow.
+ cc_library(
+     name = "org_sqlite",
+-    linkopts = ["-lsqlite3"],
++    hdrs = HEADERS,
++    srcs = LIBS,
++    includes = ["."],
+     visibility = ["//visibility:public"],
+ )
+ 
++genrule(
++    name = "sqlite-files",
++    outs = HEADERS + LIBS,
++    cmd = """
++      cp -fL "$(INCLUDEDIR)/sqlite3.h" "$(@D)" &&
++      cp -fL "$(INCLUDEDIR)/sqlite3ext.h" "$(@D)" &&
++      cp -fL "$(LIBDIR)/libsqlite3.so.0.8.6" "$(@D)" &&
++      ln -sf "$(LIBDIR)/libsqlite3.so.0.8.6" "$(@D)/libsqlite3.so.0" &&
++      ln -sf "$(LIBDIR)/libsqlite3.so.0.8.6" "$(@D)/libsqlite3.so"
++    """,
++)
++
+ # This is a Copybara sync helper for Google.
+ py_library(
+     name = "python",
+-- 
+2.34.1
+
-- 
2.34.1

